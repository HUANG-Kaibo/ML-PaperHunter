{
  "title": "How do Language Models Bind Entities in Context?",
  "title_zh": "语言模型如何在上下文中绑定实体？",
  "abstract": "Language models (LMs) can recall facts mentioned in context, as shown by their performance on reading comprehension tasks. When the context describes facts about more than one entity, the LM has to correctly bind attributes to their corresponding entity. We show, via causal experiments, that LMs' internal activations represent binding information by exhibiting appropriate binding ID vectors at the entity and attribute positions. We further show that binding ID vectors form a subspace and often transfer across tasks. Our results demonstrate that LMs learn interpretable strategies for representing symbolic knowledge in context, and that studying context activations is a fruitful direction for understanding LM cognition.",
  "abstract_zh": "语言模型（LM）能够回忆上下文中提到的事实，这在阅读理解任务中的表现得到了证明。当上下文描述多个实体的事实时，LM必须正确地将属性绑定到相应的实体。我们通过因果实验表明，LM的内部激活表示绑定信息，通过在实体和属性位置展示适当的绑定ID向量。我们进一步表明，绑定ID向量形成一个子空间，并且通常在任务之间转移。我们的结果表明，LM学习了在上下文中表示符号知识的可解释策略，研究上下文激活是理解LM认知的一个富有成效的方向。"
}
{
  "title": "Detecting Pretraining Data from Large Language Models",
  "title_zh": "标题：从大型语言模型中检测预训练数据",
  "abstract": "Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method MIN-K PROB based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. MIN-K PROB can be applied without any knowledge about the pretrainig corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that MIN-K PROB achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply MIN-K PROB to two real-world scenarios, copyrighted book detection and contaminated downstream example detection, and find that it to be a consistently effective solution.",
  "abstract_zh": "摘要：尽管大型语言模型（LLMs）被广泛部署，但用于训练它们的数据很少被披露。考虑到这些数据的惊人规模，可能高达数万亿个标记，几乎可以肯定其中包含潜在问题文本，如受版权保护的材料、个人可识别信息以及用于广泛报道的参考基准的测试数据。然而，我们目前无法知道这些类型的数据包含哪些或以何种比例存在。本文研究了预训练数据检测问题：给定一段文本和对LLM的黑箱访问，而不知道预训练数据，我们能否确定模型是否在提供的文本上进行了训练？为促进这一研究，我们引入了一个动态基准WIKIMIA，该基准使用在模型训练前后创建的数据来支持真实数据检测。我们还引入了一种基于简单假设的新检测方法MIN-K PROB：未见示例可能包含一些在LLM下概率较低的异常词，而已见示例则不太可能有如此低概率的词。MIN-K PROB可以在没有任何关于预训练语料库的知识或额外训练的情况下应用，区别于以往需要在与预训练数据相似的数据上训练参考模型的检测方法。此外，我们的实验表明，MIN-K PROB在WIKIMIA上相较于这些先前方法提高了7.4%。我们将MIN-K PROB应用于两个现实场景，即版权书籍检测和受污染下游示例检测，发现它始终是一种有效的解决方案。"
}
{
  "title": "AgentBench: Evaluating LLMs as Agents",
  "title_zh": "代理基准：评估大型语言模型作为代理的能力",
  "abstract": "Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. \nAs a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments.\nWe present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting.\nOur extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors.\nWe identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents.\nTraining on code and high quality multi-turn alignment data could improve agent performance.\nDatasets, environments, and an integrated evaluation package for AgentBench are released.",
  "abstract_zh": "大型语言模型（LLMs）正变得越来越智能和自主，目标是超越传统自然语言处理任务的现实世界实用任务。因此，迫切需要在互动环境中评估LLMs作为代理在挑战性任务上的表现。我们提出了AgentBench，这是一个多维度的演变基准，目前由8个不同的环境组成，用于评估LLM作为代理在多轮开放式生成设置中的推理和决策能力。我们对27个基于API和开源（OSS）的LLMs进行了广泛测试，结果表明，尽管顶级商业LLMs在复杂环境中表现出强大的代理能力，但它们与OSS竞争者之间的性能差距显著。我们识别了环境和LLMs失败的典型原因，显示出较差的长期推理、决策和遵循指令能力是开发可用LLM代理的主要障碍。对代码和高质量多轮对齐数据的训练可以提高代理性能。我们发布了AgentBench的数据集、环境和集成评估包。"
}
{
  "title": "Image Translation as Diffusion Visual Programmers",
  "title_zh": "图像翻译作为扩散视觉程序员",
  "abstract": "We introduce the novel Diffusion Visual Programmer (DVP), a neuro-symbolic image translation framework. Our proposed DVP seamlessly embeds a condition-flexible diffusion model within the GPT architecture, orchestrating a coherent sequence of visual programs ($i.e.$, computer vision models) for various pro-symbolic steps, which span RoI identification, style transfer, and position manipulation, facilitating transparent and controllable image translation processes. Extensive experiments demonstrate DVP’s remarkable performance, surpassing concurrent arts. This success can be attributed to several key features of DVP: First, DVP achieves condition-flexible translation via instance normalization, enabling the model to eliminate sensitivity caused by the manual guidance and optimally focus on textual descriptions for high-quality content generation. Second, the frame work enhances in-context reasoning by deciphering intricate high-dimensional concepts in feature spaces into more accessible low-dimensional symbols ($e.g.$, [Prompt], [RoI object]), allowing for localized, context-free editing while maintaining overall coherence. Last but not least, DVP improves systemic controllability and explainability by offering explicit symbolic representations at each programming stage, empowering users to intuitively interpret and modify results. Our research marks a substantial step towards harmonizing artificial image translation processes with cognitive intelligence, promising broader applications.",
  "abstract_zh": "我们介绍了一种新颖的扩散视觉程序员（DVP），这是一种神经符号图像翻译框架。我们提出的DVP在GPT架构中无缝嵌入了条件灵活的扩散模型，协调生成一系列连贯的视觉程序（即计算机视觉模型），用于各种符号前的步骤，包括区域识别、风格迁移和位置操作，从而促进透明和可控的图像翻译过程。大量实验表明DVP的卓越性能，超越了当前的艺术。这一成功归因于DVP的几个关键特性：首先，DVP通过实例归一化实现条件灵活的翻译，使模型能够消除手动指导带来的敏感性，并最佳化地专注于文本描述以生成高质量内容。其次，该框架通过将特征空间中的复杂高维概念解码为更易于访问的低维符号（例如，[提示]，[RoI对象]），增强了上下文推理，允许在保持整体一致性的同时进行局部、无上下文的编辑。最后但同样重要的是，DVP通过在每个编程阶段提供明确的符号表示，改善了系统的可控性和可解释性，使用户能够直观地解释和修改结果。我们的研究标志着在将人工图像翻译过程与认知智能协调方面迈出了重要一步，承诺更广泛的应用。"
}
{
  "title": "Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion",
  "title_zh": "一致性轨迹模型：学习扩散的概率流常微分方程轨迹",
  "abstract": "Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 1.92). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, unlike CM, CTM's access to the score function can streamline the adoption of established controllable/conditional generation methods from the diffusion community. This access also enables the computation of likelihood. The code is available at https://github.com/sony/ctm.",
  "abstract_zh": "一致性模型（CM）（Song et al., 2023）以牺牲样本质量为代价加速基于分数的扩散模型采样，但缺乏自然的方式来权衡质量与速度。为了解决这一限制，我们提出了一致性轨迹模型（CTM），这是一个包含CM和基于分数模型作为特例的广义模型。CTM训练一个单一的神经网络，可以在一次前向传播中输出分数（即对数密度的梯度），并允许在扩散过程中沿着概率流常微分方程（ODE）在任何初始和最终时间之间进行无限制的遍历。CTM高效结合对抗训练和去噪分数匹配损失以增强性能，并在CIFAR-10（FID 1.73）和64X64分辨率的ImageNet上实现了单步扩散模型采样的新最先进FID。CTM还启用了新的采样方案家族，包括沿ODE解轨迹的长跳跃，既有确定性也有随机性。随着计算预算的增加，它始终提高样本质量，避免了CM中观察到的退化。此外，与CM不同，CTM对分数函数的访问可以简化从扩散社区采用已建立的可控/条件生成方法。这一访问还使得计算似然成为可能。代码可在https://github.com/sony/ctm获取。"
}
{
  "title": "VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models",
  "title_zh": "标题：VDC：基于多模态大语言模型的视觉-语言不一致性多功能数据清洗器",
  "abstract": "The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.\nHence, it is critical to detect dirty samples to improve the quality and realiability of dataset. \nExisting detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other fields.\nIn this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. \nTo capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.\nIt consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.\nExtensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples.\nThe code is available at [https://github.com/zihao-ai/vdc](https://github.com/zihao-ai/vdc).",
  "abstract_zh": "摘要：数据在构建人工智能系统中的作用最近被新兴的数据中心人工智能概念所强调。不幸的是，在现实世界中，数据集可能包含脏样本，例如来自后门攻击的中毒样本、众包中的噪声标签，甚至它们的混合物。这些脏样本的存在使得深度神经网络脆弱且不可靠。因此，检测脏样本对于提高数据集的质量和可靠性至关重要。现有的检测器仅专注于检测中毒样本或噪声标签，通常在处理来自其他领域的脏样本时容易出现弱泛化。在本文中，我们发现各种脏样本的共同点是图像与相关标签之间的视觉-语言不一致性。为了捕捉模态之间的语义不一致性，我们提出了多功能数据清洗器（VDC），利用多模态大语言模型（MLLM）在跨模态对齐和推理中的卓越能力。它由三个连续模块组成：视觉问题生成模块，用于生成关于图像的深刻问题；视觉问答模块，通过使用MLLM回答问题来获取视觉内容的语义；最后是视觉答案评估模块，用于评估不一致性。大量实验表明其在各种类别和类型的脏样本上的优越性能和泛化能力。代码可在[https://github.com/zihao-ai/vdc](https://github.com/zihao-ai/vdc)获取。"
}
{
  "title": "Provable Reward-Agnostic Preference-Based Reinforcement Learning",
  "title_zh": "可证明的与奖励无关的基于偏好的强化学习",
  "abstract": "Preference-based Reinforcement Learning (PbRL) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While PbRL has demonstrated practical success in fine-tuning language models, existing theoretical work focuses on regret minimization and fails to capture most of the practical frameworks. In this study, we fill in such a gap between theoretical PbRL and practical algorithms by proposing a theoretical reward-agnostic PbRL framework where exploratory trajectories that enable accurate learning of hidden reward functions are acquired before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing theoretical literature. Specifically, our framework can incorporate linear and low-rank MDPs with efficient sample complexity. Additionally, we investigate reward-agnostic RL with action-based comparison feedback and introduce an efficient querying algorithm tailored to this scenario.",
  "abstract_zh": "基于偏好的强化学习（PbRL）是一种范式，其中强化学习代理通过对轨迹的成对偏好反馈来学习优化任务，而不是依赖于显式的奖励信号。尽管PbRL在微调语言模型方面取得了实际成功，但现有的理论工作主要集中在遗憾最小化上，未能涵盖大多数实际框架。本研究通过提出一个理论上的与奖励无关的PbRL框架，填补了理论PbRL与实际算法之间的空白，该框架在收集任何人类反馈之前获取能够准确学习隐藏奖励函数的探索性轨迹。理论分析表明，与现有理论文献相比，我们的算法在偏好模型下学习最优策略所需的人类反馈更少，且具有线性参数化和未知转移的特点。具体而言，我们的框架可以结合线性和低秩的马尔可夫决策过程（MDP），并具有高效的样本复杂度。此外，我们还研究了具有基于动作比较反馈的与奖励无关的强化学习，并引入了一种针对该场景的高效查询算法。"
}
{
  "title": "Conversational Drug Editing Using Retrieval and Domain Feedback",
  "title_zh": "对话式药物编辑：基于检索和领域反馈的框架",
  "abstract": "Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery. However, existing works mainly focus on investigating the capabilities of conversational LLMs on chemical reactions and retrosynthesis. While drug editing, a critical task in the drug discovery pipeline, remains largely unexplored. To bridge this gap, we propose ChatDrug, a framework to facilitate the systematic investigation of drug editing using LLMs. ChatDrug jointly leverages a prompt module, a retrieval and domain feedback module, and a conversation module to streamline effective drug editing. We empirically show that ChatDrug reaches the best performance on all 39 drug editing tasks, encompassing small molecules, peptides, and proteins. We further demonstrate, through 10 case studies, that ChatDrug can successfully identify the key substructures for manipulation, generating diverse and valid suggestions for drug editing. Promisingly, we also show that ChatDrug can offer insightful explanations from a domain-specific perspective, enhancing interpretability and enabling informed decision-making.",
  "abstract_zh": "最近在对话式大型语言模型（LLMs）方面的进展，如ChatGPT，在药物发现等多个领域展现了显著的潜力。然而，现有研究主要集中在化学反应和逆合成的能力上，而药物编辑这一药物发现流程中的关键任务仍然未被充分探索。为了解决这一问题，我们提出了ChatDrug，一个利用LLMs系统性研究药物编辑的框架。ChatDrug结合了提示模块、检索和领域反馈模块以及对话模块，以简化有效的药物编辑。我们通过实验证明，ChatDrug在所有39个药物编辑任务中表现最佳，涵盖小分子、肽和蛋白质。通过10个案例研究，我们进一步展示了ChatDrug能够成功识别关键的子结构进行操作，为药物编辑生成多样且有效的建议。令人鼓舞的是，我们还表明ChatDrug能够从领域特定的角度提供深刻的解释，增强可解释性并支持明智的决策。"
}
{
  "title": "Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining",
  "title_zh": "变压器作为决策者：通过监督预训练实现可证明的上下文强化学习",
  "abstract": "Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how transformers can be trained to perform ICRL have not been theoretically well-understood. In particular, it is unclear which reinforcement-learning algorithms transformers can perform in context, and how distribution mismatch in offline training data affects the learned algorithms. \n\nThis paper provides a theoretical framework that analyzes supervised pretraining for ICRL. This includes two recently proposed training methods --- algorithm distillation and decision-pretrained transformers. First, assuming model realizability, we prove the supervised-pretrained transformer will imitate the conditional expectation of the expert algorithm given the observed trajectory. The generalization error will scale with model capacity and a distribution divergence factor between the expert and offline algorithms. Second, we show transformers with ReLU attention can efficiently approximate near-optimal online reinforcement learning algorithms like LinUCB and Thompson sampling for stochastic linear bandits, and UCB-VI for tabular Markov decision processes. This provides the first quantitative analysis of the ICRL capabilities of transformers pretrained from offline trajectories.",
  "abstract_zh": "大型变压器模型在离线强化学习数据集上进行预训练，展示了显著的上下文强化学习（ICRL）能力，当被提示以来自未见环境的交互轨迹时，它们能够做出良好的决策。然而，变压器何时以及如何被训练以执行ICRL尚未得到理论上的充分理解。特别是，目前尚不清楚变压器可以在上下文中执行哪些强化学习算法，以及离线训练数据中的分布不匹配如何影响学习到的算法。本文提供了一个理论框架，分析了ICRL的监督预训练。这包括两种最近提出的训练方法——算法蒸馏和决策预训练变压器。首先，假设模型可实现性，我们证明监督预训练的变压器将模仿给定观察到的轨迹的专家算法的条件期望。泛化误差将与模型容量和专家与离线算法之间的分布差异因子成比例。其次，我们展示了具有ReLU注意力的变压器可以有效地逼近近似最优的在线强化学习算法，如LinUCB和汤普森采样用于随机线性赌博机，以及UCB-VI用于表格马尔可夫决策过程。这为从离线轨迹预训练的变压器的ICRL能力提供了首次定量分析。"
}
{
  "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning",
  "title_zh": "MAmmoTH：通过混合指令调优构建数学通用模型",
  "abstract": "We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT-4’s CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.",
  "abstract_zh": "我们介绍了MAmmoTH，一系列专门针对一般数学问题解决的开源大型语言模型（LLMs）。MAmmoTH模型是在我们精心策划的指令调优数据集MathInstruct上训练的。MathInstruct由13个数学数据集及其中间推理组成，其中六个是我们新近策划的推理。它呈现了链式思维（CoT）和程序思维（PoT）推理的独特混合，并确保了数学各个领域的广泛覆盖。CoT和PoT的混合不仅释放了工具使用的潜力，还允许针对不同数学问题采用不同的思维过程。因此，MAmmoTH系列在九个数学推理数据集上显著超越现有的开源模型，平均准确率提高了16%到32%。值得注意的是，我们的MAmmoTH-7B模型在MATH（一个竞赛级数据集）上达到了33%的准确率，超过了最佳开源7B模型（WizardMath）23%，而MAmmoTH-34B模型在MATH上达到了44%的准确率，甚至超过了GPT-4的CoT结果。我们的工作强调了在开发优越的数学通用模型中，覆盖多样化问题和使用混合推理的重要性。"
}
{
  "title": "Adapting Large Language Models via Reading Comprehension",
  "title_zh": "标题：通过阅读理解适应大型语言模型",
  "abstract": "We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension--practice after reading improves the ability to answer questions based on the learned knowledge--we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specific reading comprehension texts can improve the model's performance even on general benchmarks, showing the potential to develop a general model across even more domains. Our model, code, and data are available at https://github.com/microsoft/LMOps.",
  "abstract_zh": "摘要：我们探讨了在特定领域语料上继续预训练如何影响大型语言模型，发现对原始语料的训练赋予模型领域知识，但严重损害了其回答问题的提示能力。受到人类通过阅读理解学习的启发——阅读后的练习提高了基于所学知识回答问题的能力——我们提出了一种将原始语料转化为阅读理解文本的简单方法。每个原始文本都通过与其内容相关的一系列任务进行丰富。我们的方法高度可扩展，适用于任何预训练语料，在生物医学、金融和法律三个不同领域的各种任务中始终提高性能。值得注意的是，我们的7B语言模型在性能上与规模更大的领域特定模型（如BloombergGPT-50B）具有竞争力。此外，我们还证明领域特定的阅读理解文本甚至可以提高模型在通用基准上的表现，显示出在更多领域开发通用模型的潜力。我们的模型、代码和数据可在https://github.com/microsoft/LMOps获得。"
}
{
  "title": "Is Self-Repair a Silver Bullet for Code Generation?",
  "title_zh": "自我修复是代码生成的灵丹妙药吗？",
  "abstract": "Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair---in which the model debugs and repairs its own code---has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.",
  "abstract_zh": "大型语言模型在代码生成方面表现出色，但在执行复杂任务时仍然存在困难。自我修复——模型调试和修复自身代码的过程——最近成为提升性能的一种流行方式。然而，尽管其日益受到关注，现有的自我修复研究范围有限；在许多情况下，其有效性仍然不甚清楚。本文分析了Code Llama、GPT-3.5和GPT-4在HumanEval和APPS问题上进行自我修复的能力。我们发现，当考虑到执行修复的成本时，性能提升往往是适度的，在数据的不同子集之间变化很大，有时甚至根本没有提升。我们假设这是因为自我修复受到模型对自身代码提供反馈能力的瓶颈；通过使用更强大的模型来人为提升反馈质量，我们观察到性能提升显著更大。同样，我们对GPT-4提供人类参与者反馈的小规模研究表明，即使对于最强大的模型，自我修复仍远远落后于人类级别的调试所能达到的效果。"
}
{
  "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
  "title_zh": "梦LLM：协同多模态理解与创作",
  "abstract": "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.",
  "abstract_zh": "本文介绍了DreamLLM，这是一种学习框架，首次实现了多功能的多模态大型语言模型（MLLM），并充分利用了多模态理解与创作之间常被忽视的协同作用。DreamLLM基于两个基本原则运作。第一个原则关注通过在原始多模态空间中直接采样来生成语言和图像后验的建模。这种方法避免了像CLIP这样的外部特征提取器固有的局限性和信息损失，从而获得更全面的多模态理解。第二，DreamLLM促进了原始交错文档的生成，建模文本和图像内容以及非结构化布局。这使得DreamLLM能够有效学习所有条件、边际和联合多模态分布。因此，DreamLLM成为首个能够生成自由形式交错内容的MLLM。全面的实验突显了DreamLLM作为零-shot多模态通才的优越性能，得益于增强的学习协同。项目页面：https://dreamllm.github.io。"
}
{
  "title": "Retrieval meets Long Context Large Language Models",
  "title_zh": "检索与长上下文大型语言模型的结合",
  "abstract": "Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.",
  "abstract_zh": "扩展大型语言模型（LLMs）的上下文窗口最近变得越来越流行，而将检索与LLMs结合的解决方案已经存在多年。自然的问题是：i）检索增强与长上下文窗口，哪种方法对下游任务更好？ ii）这两种方法能否结合以获得两者的最佳效果？在这项工作中，我们通过研究两种最先进的预训练LLMs，即一个专有的43B GPT和Llama2-70B，来回答这些问题。或许令人惊讶的是，我们发现使用简单检索增强的4K上下文窗口LLM在生成时可以在长上下文任务上实现与通过位置插值微调的16K上下文窗口LLM相当的性能，同时计算量要小得多。更重要的是，我们证明了检索可以显著提高LLMs的性能，无论其扩展的上下文窗口大小如何。我们的最佳模型，具有32K上下文窗口的检索增强Llama2-70B，在九个长上下文任务（包括问答、基于查询的摘要和上下文少样本学习任务）的平均得分上超越了GPT-3.5-turbo-16k和Davinci003。同时，它在生成速度上也明显快于其非检索的Llama2-70B-32k基线。我们的研究为从业者在选择检索增强与LLM的长上下文扩展时提供了普遍的见解。"
}
{
  "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks",
  "title_zh": "教会语言模型通过合成任务减少幻觉",
  "abstract": "Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing to make LLMs hallucinate less is challenging, as hallucination is hard to efficiently, cheaply, and reliably evaluate at each optimization step. In this work, we show that reducing hallucination on a _synthetic task_ can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix tuning on the synthetic task, then uses the system message on realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, we reduce hallucination for two 13B-parameter LLMs using supervision signal from only a synthetic retrieval task. We also find that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively _increase_ hallucination. Overall, SynTra demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.",
  "abstract_zh": "大型语言模型（LLMs）在抽象摘要任务中（如基于文档的问题回答、会议摘要和临床报告生成）经常出现幻觉，即使上下文中包含所有必要信息。然而，优化LLMs以减少幻觉是具有挑战性的，因为在每个优化步骤中，幻觉的评估既困难又不经济且不可靠。在本研究中，我们展示了在_合成任务_上减少幻觉也可以减少在现实世界下游任务中的幻觉。我们的方法SynTra首先设计了一个合成任务，在该任务中容易引发和测量幻觉。接下来，它通过在合成任务上进行前缀调优来优化LLM的系统消息，然后在现实的、难以优化的任务上使用该系统消息。在三个现实的抽象摘要任务中，我们使用仅来自合成检索任务的监督信号减少了两个13B参数LLM的幻觉。我们还发现，优化系统消息而不是模型权重可能是关键；在合成任务上微调整个模型可能会反直觉地_增加_幻觉。总体而言，SynTra证明了使用合成数据的额外灵活性可以在实践中帮助减轻不良行为。"
}
{
  "title": "Statistical Rejection Sampling Improves Preference Optimization",
  "title_zh": "统计拒绝采样改善偏好优化",
  "abstract": "Improving the alignment of language models with human preferences remains an active research challenge. Previous approaches have primarily utilized online Reinforcement Learning from Human Feedback (RLHF). Recently, offline methods such as Sequence Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have emerged as attractive alternatives, offering improvements in stability and scalability while maintaining competitive performance. SLiC refines its loss function using sequence pairs sampled from a supervised fine-tuned (SFT) policy, while DPO directly optimizes language models based on preference data, foregoing the need for a separate reward model. However, the maximum likelihood estimator (MLE) of the target optimal policy requires labeled preference pairs sampled from that policy. The absence of a reward model in DPO constrains its ability to sample preference pairs from the optimal policy. Meanwhile, SLiC can only sample preference pairs from the SFT policy. To address these limitations, we introduce a novel approach called Statistical Rejection Sampling Optimization (RSO) designed to source preference data from the target optimal policy using rejection sampling, enabling a more accurate estimation of the optimal policy. We also propose a unified framework that enhances the loss functions used in both SLiC and DPO from a preference modeling standpoint. Through extensive experiments across diverse tasks, we demonstrate that RSO consistently outperforms both SLiC and DPO as evaluated by both Large Language Models (LLMs) and human raters.",
  "abstract_zh": "改善语言模型与人类偏好的对齐仍然是一个活跃的研究挑战。之前的方法主要利用了来自人类反馈的在线强化学习（RLHF）。最近，离线方法如序列似然校准（SLiC）和直接偏好优化（DPO）作为有吸引力的替代方案出现，提供了在保持竞争性能的同时提高稳定性和可扩展性。SLiC通过从监督微调（SFT）策略中采样的序列对来优化其损失函数，而DPO则基于偏好数据直接优化语言模型，省去了单独的奖励模型的需求。然而，目标最优策略的最大似然估计（MLE）需要从该策略中采样的标记偏好对。DPO中缺乏奖励模型限制了其从最优策略中采样偏好对的能力。同时，SLiC只能从SFT策略中采样偏好对。为了解决这些限制，我们提出了一种新方法，称为统计拒绝采样优化（RSO），旨在利用拒绝采样从目标最优策略中获取偏好数据，从而实现对最优策略的更准确估计。我们还提出了一个统一框架，从偏好建模的角度增强SLiC和DPO中使用的损失函数。通过在多种任务上进行广泛实验，我们证明RSO在大型语言模型（LLMs）和人类评审者的评估中始终优于SLiC和DPO。"
}
{
  "title": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs",
  "title_zh": "告诉你的模型关注哪里：后期注意力引导方法用于大型语言模型",
  "abstract": "In human-written articles, we often leverage the subtleties of text style, such as bold and italics, to guide the attention of readers. These textual emphases are vital for the readers to grasp the conveyed information.  When interacting with large language models (LLMs), we have a similar need -- steering the model to pay closer attention to user-specified information, e.g., an instruction. Existing methods, however, are constrained to process plain text and do not support such a mechanism. This motivates us to introduce PASTA -- Post-hoc Attention STeering Approach, a method that allows LLMs to read text with user-specified emphasis marks. To this end, PASTA identifies a small subset of attention heads and applies precise attention reweighting on them, directing the model attention to user-specified parts. Like prompting, PASTA is applied at inference time and does not require changing any model parameters. Experiments demonstrate that PASTA can substantially enhance an LLM's ability to follow user instructions or integrate new knowledge from user inputs, leading to a significant performance improvement on a variety of tasks, e.g., an average accuracy improvement of 22\\% for LLAMA-7B. Our code is publicly available at https://github.com/QingruZhang/PASTA .",
  "abstract_zh": "在人工撰写的文章中，我们常常利用文本风格的细微差别，如粗体和斜体，来引导读者的注意力。这些文本强调对读者理解所传达的信息至关重要。在与大型语言模型（LLMs）交互时，我们也有类似的需求——引导模型更加关注用户指定的信息，例如指令。然而，现有方法仅限于处理纯文本，并不支持这种机制。这促使我们引入PASTA——后期注意力引导方法，这是一种允许LLMs读取带有用户指定强调标记的文本的方法。为此，PASTA识别出一小部分注意力头，并对其进行精确的注意力重加权，指引模型注意用户指定的部分。与提示类似，PASTA在推理时应用，不需要更改任何模型参数。实验表明，PASTA可以显著增强LLM遵循用户指令或整合用户输入新知识的能力，从而在多种任务上实现显著的性能提升，例如LLAMA-7B的平均准确率提高了22%。我们的代码已公开发布在https://github.com/QingruZhang/PASTA。"
}
{
  "title": "SALMON: Self-Alignment with Principle-Following Reward Models",
  "title_zh": "标题：SALMON：基于原则遵循奖励模型的自我对齐",
  "abstract": "Supervised Fine-Tuning (SFT) on human demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful alignment paradigm for Large Language Model (LLM) AI-assistant agents. However, a significant limitation of this approach is its substantial dependency on high-quality human annotations, making its broader application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and task-specific response preferences. To address this issue, we present a novel alignment paradigm in this paper, termed SALMON (Self-ALignMent with principle-fOllowiNg reward models). This paradigm offers the ability to align base language models with minimal human supervision, using only a select set of human-defined principles, yet achieves superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this reward model can generate reward scores based on arbitrary human-defined principles. Therefore, during the RL training phase, by merely adjusting these principles, we gain full control over the preferences of the reward model, subsequently influencing the behavior of the RL-trained policy model, and eliminating the traditional reliance on exhaustive online human preference collection. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.",
  "abstract_zh": "摘要：基于人类示范的监督微调（SFT）结合人类反馈的强化学习（RLHF）构成了大型语言模型（LLM）AI助手代理的强大对齐范式。然而，这种方法的一个显著限制是对高质量人类注释的高度依赖，使其在复杂任务中的广泛应用面临挑战，因为获取一致的响应示范和任务特定的响应偏好存在困难。为了解决这个问题，我们在本文中提出了一种新颖的对齐范式，称为SALMON（基于原则遵循的自我对齐）。该范式能够在最小人类监督的情况下对基础语言模型进行对齐，仅使用一组人类定义的原则，但却能实现更优的性能。我们方法的核心是一个遵循原则的奖励模型。该奖励模型在合成偏好数据上进行训练，可以基于任意人类定义的原则生成奖励分数。因此，在RL训练阶段，仅通过调整这些原则，我们就能完全控制奖励模型的偏好，进而影响RL训练的策略模型的行为，消除了对传统全面在线人类偏好收集的依赖。将我们的方法应用于LLaMA-2-70b基础语言模型，我们开发了一个名为Dromedary-2的AI助手。仅使用6个上下文学习示例和31个人类定义的原则，Dromedary-2在多个基准数据集上显著超越了包括LLaMA-2-Chat-70b在内的多个最先进AI系统的性能。我们已开源代码和模型权重，以鼓励进一步研究如何在提高监督效率、改善可控性和可扩展监督的情况下对LLM基础的AI代理进行对齐。"
}
{
  "title": "Are Bert Family Good Instruction Followers?  A Study on Their Potential And Limitations",
  "title_zh": "标题：BERT家族是否是优秀的指令跟随者？关于它们的潜力与局限性的研究",
  "abstract": "Language modeling at scale has proven very effective and brought unprecedented success to natural language models. Many typical representatives, especially decoder-only models, e.g., BLOOM and LLaMA, and encoder-decoder models, e.g., Flan-T5 and AlexaTM, have exhibited incredible instruction-following capabilities while keeping strong task completion ability. These large language models can achieve superior performance in various tasks and even yield emergent capabilities, e.g., reasoning and universal generalization. Though the above two paradigms are mainstream and well explored, the potential of the BERT family, which are encoder-only based models and have ever been one of the most representative pre-trained models, also deserves attention, at least should be discussed. In this work, we adopt XML-R to explore the effectiveness of the BERT family for instruction following and zero-shot learning. We first design a simple yet effective strategy to utilize the encoder-only models for generation tasks and then conduct multi-task instruction tuning.  Experimental results demonstrate that our fine-tuned model, Instruct-XMLR, outperforms Bloomz on all evaluation tasks and achieves comparable performance with mT0 on most tasks. Surprisingly, Instruct-XMLR also possesses strong task and language generalization abilities, indicating that Instruct-XMLR can also serve as a good instruction follower and zero-shot learner. Besides, Instruct-XMLR can accelerate decoding due to its non-autoregressive generation manner, achieving around 3 times speedup compared with current autoregressive large language models. Although we also witnessed several limitations through our experiments, such as the performance decline in long-generation tasks and the shortcoming of length prediction, Instruct-XMLR can still become a good member of the family of current large language models.",
  "abstract_zh": "摘要：大规模语言建模已被证明非常有效，并为自然语言模型带来了前所未有的成功。许多典型代表，特别是仅解码器模型，如BLOOM和LLaMA，以及编码-解码模型，如Flan-T5和AlexaTM，在保持强大的任务完成能力的同时，展现了令人难以置信的指令跟随能力。这些大型语言模型在各种任务中能够实现卓越的性能，甚至产生突现能力，如推理和普遍泛化。尽管上述两种范式是主流且已被充分探索，但BERT家族的潜力，作为基于仅编码器的模型，曾是最具代表性的预训练模型之一，也值得关注，至少应该进行讨论。在这项工作中，我们采用XML-R来探索BERT家族在指令跟随和零-shot学习中的有效性。我们首先设计了一种简单而有效的策略，以利用仅编码器模型进行生成任务，然后进行多任务指令调优。实验结果表明，我们微调的模型Instruct-XMLR在所有评估任务上均优于Bloomz，并在大多数任务上与mT0的性能相当。令人惊讶的是，Instruct-XMLR还具备强大的任务和语言泛化能力，表明Instruct-XMLR也可以作为一个优秀的指令跟随者和零-shot学习者。此外，由于其非自回归生成方式，Instruct-XMLR可以加速解码，与当前的自回归大型语言模型相比，速度提高约3倍。尽管我们在实验中也观察到了一些局限性，例如在长生成任务中的性能下降和长度预测的不足，但Instruct-XMLR仍然可以成为当前大型语言模型家族中的一员。"
}
{
  "title": "Privacy-Preserving In-Context Learning for Large Language Models",
  "title_zh": "隐私保护的大型语言模型上下文学习",
  "abstract": "In-context learning (ICL) is an important capability of Large Language Models (LLMs), enabling these models to dynamically adapt based on specific, in-context exemplars, thereby improving accuracy and relevance.\nHowever, LLM's responses may leak the sensitive private information contained in in-context exemplars. \nTo address this challenge, we propose Differentially Private In-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. \nThe key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets. \nBased on the general paradigm of DP-ICL, we instantiate several techniques showing how to privatize ICL for text classification and language generation. \nWe experiment on four text classification benchmarks and two language generation tasks, and our empirical findings suggest that our DP-ICL achieves a strong utility-privacy tradeoff.",
  "abstract_zh": "上下文学习（ICL）是大型语言模型（LLMs）的一个重要能力，使这些模型能够根据特定的上下文示例动态适应，从而提高准确性和相关性。然而，LLM的响应可能会泄露包含在上下文示例中的敏感私人信息。为了解决这一挑战，我们提出了差分隐私上下文学习（DP-ICL），这是一个用于私有化ICL任务的通用范式。DP-ICL范式的关键思想是通过基于不相交示例集的LLM响应集的噪声共识生成差分隐私响应。基于DP-ICL的通用范式，我们实例化了几种技术，展示如何为文本分类和语言生成私有化ICL。我们在四个文本分类基准和两个语言生成任务上进行了实验，实证结果表明我们的DP-ICL实现了良好的效用-隐私权衡。"
}
{
  "title": "The Unlocking Spell on Base LLMs:  Rethinking Alignment via In-Context Learning",
  "title_zh": "基础大型语言模型的解锁咒语：通过上下文学习重新思考对齐",
  "abstract": "Alignment tuning has become the de facto standard practice for enabling base large language models (LLMs) to serve as open-domain AI assistants. The alignment tuning process typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al., 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be \"superficial.\" This raises questions about how exactly the alignment tuning transforms a base LLM. \n\nWe analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterparts (e.g., Llama-2 and Llama-2-chat). Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions (i.e., they share the top-ranked tokens). Most distribution shifts occur with stylistic tokens (e.g., discourse markers, safety disclaimers). This direct evidence strongly supports the hypothesis that alignment tuning primarily learns to adopt the language style of AI assistants, and that the knowledge required for answering user queries predominantly comes from the base LLMs themselves. \n\nBased on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL (Untuned LLMs with Restyled In-context Alignment). URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named just-eval-instruct. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT (Mistral-7b-Instruct) or SFT+RLHF (Llama-2-70b-chat). We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
  "abstract_zh": "对齐调优已成为使基础大型语言模型（LLMs）作为开放领域AI助手的事实标准实践。对齐调优过程通常涉及通过监督微调（SFT）进行指令学习，以及通过人类反馈的强化学习（RLHF）进行偏好调优。最近的研究LIMA（Zhou et al., 2023）表明，仅使用1000个示例进行SFT也能实现显著的对齐性能，这暗示对齐调优的效果可能是“表面的”。这引发了关于对齐调优如何具体转变基础LLM的问题。我们通过检查基础LLM与其对齐版本（例如Llama-2和Llama-2-chat）之间的标记分布变化来分析对齐调优的效果。我们的发现揭示，基础LLM及其对齐调优版本在大多数标记位置的解码表现几乎相同（即它们共享排名最高的标记）。大多数分布变化发生在风格标记上（例如，话语标记、安全免责声明）。这一直接证据强烈支持了对齐调优主要学习采用AI助手语言风格的假设，而回答用户查询所需的知识主要来自基础LLM本身。基于这些发现，我们通过提出研究问题重新思考LLM的对齐：我们能多有效地对齐基础LLM而不使用SFT或RLHF？为此，我们引入了一种简单的无调优对齐方法URIAL（未调优的LLMs与重新风格化的上下文对齐）。URIAL通过与基础LLM的上下文学习（ICL）实现有效对齐，仅需三个恒定的风格示例和一个系统提示。我们在一个多样化的示例集上进行了细致且可解释的评估，命名为just-eval-instruct。结果表明，使用URIAL的基础LLM可以匹配甚至超越与SFT（Mistral-7b-Instruct）或SFT+RLHF（Llama-2-70b-chat）对齐的LLM的表现。我们展示了通过战略提示和ICL可以显著缩小无调优和基于调优的对齐方法之间的差距。我们对对齐调优表面性质的发现以及URIAL的结果表明，对对齐的深入分析和理论理解对未来的LLM研究至关重要。"
}
{
  "title": "Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation",
  "title_zh": "标题：文本到图像定制导航：从LyCORIS微调到模型评估",
  "abstract": "Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",
  "abstract_zh": "摘要：文本到图像生成模型因其能够从文本提示生成高保真图像而受到极大关注。其中，Stable Diffusion作为这一快速发展的领域中的领先开源模型，独树一帜。然而，微调这些模型的复杂性带来了从新方法整合到系统评估的多重挑战。为了解决这些问题，本文介绍了LyCORIS（超越传统方法的Lora，Stable Diffusion的其他秩适应实现），这是一个提供多种Stable Diffusion微调方法的开源库。此外，我们还提出了一个全面的框架，用于系统评估不同的微调技术。该框架采用多样化的指标组合，深入探讨微调的多个方面，包括超参数调整以及在不同概念类别中使用不同提示类型的评估。通过这种全面的方法，我们的工作提供了对微调参数细微影响的重要见解，弥合了前沿研究与实际应用之间的差距。"
}
{
  "title": "CLEX: Continuous  Length Extrapolation for Large Language Models",
  "title_zh": "CLEX：大型语言模型的连续长度外推",
  "abstract": "Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical long-context applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending the dynamics to desired context lengths beyond the training sequence length, CLEX facilitates the length extrapolation with impressive performance in practical tasks. We demonstrate that CLEX can be seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such as LLaMA and GPT-NeoX, with negligible impact on training and inference latency. Experimental results reveal that CLEX can effectively extend the context window to over 4× or almost 8× training length, with no deterioration in performance. Furthermore, when evaluated on the practical LongBench benchmark, our model trained on a 4k length exhibits competitive performance against state-of-the-art open-source models trained on context lengths up to 32k. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.",
  "abstract_zh": "基于Transformer的大型语言模型（LLMs）在许多自然语言处理任务中取得了开创性进展，但其卓越能力受到Transformer预设上下文窗口的限制。尽管位置嵌入（PE）缩放方法在将上下文窗口扩展到特定长度方面有效，但在外推能力上存在显著限制，或在上下文窗口内牺牲部分性能。长度外推方法虽然理论上能够将上下文窗口扩展到超过训练序列长度，但在实际长上下文应用中往往表现不佳。为了解决这些挑战，我们提出了针对LLMs的连续长度外推（CLEX）。我们将PE缩放方法推广为通过长度缩放因子的常微分方程来建模连续动态，从而克服当前为特定长度设计的PE缩放方法的限制。此外，通过将动态扩展到超出训练序列长度的期望上下文长度，CLEX在实际任务中实现了令人印象深刻的长度外推性能。我们展示了CLEX可以无缝集成到配备旋转位置嵌入的LLMs中，如LLaMA和GPT-NeoX，对训练和推理延迟的影响微乎其微。实验结果表明，CLEX可以有效地将上下文窗口扩展到超过4倍或几乎8倍的训练长度，而性能没有下降。此外，在实际的LongBench基准测试中，我们在4k长度上训练的模型表现出与训练上下文长度高达32k的最先进开源模型的竞争力。我们的代码可在https://github.com/DAMO-NLP-SG/CLEX获取。"
}
{
  "title": "STARC: A General Framework For Quantifying Differences Between Reward Functions",
  "title_zh": "STARC：一个量化奖励函数差异的通用框架",
  "abstract": "In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a *reward function*. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use *reward learning algorithms*, which attempt to *learn* a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to anticipate in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for *quantifying* the difference between reward functions. In this paper we provide a solution to this problem, in the form of a class of pseudometrics on the space of all reward functions that we call STARC (STAndardised Reward Comparison) metrics. We show that STARC metrics induce both an upper and a lower bound on worst-case regret, which implies that our metrics are tight, and that any metric with the same properties must be bilipschitz equivalent to ours. Moreover, we also identify a number of issues with reward metrics proposed by earlier works. Finally, we evaluate our metrics empirically, to demonstrate their practical efficacy. STARC metrics can be used to make both theoretical and empirical analysis of reward learning algorithms both easier and more principled.",
  "abstract_zh": "为了使用强化学习解决任务，首先需要将该任务的目标形式化为*奖励函数*。然而，对于许多现实世界的任务，手动指定一个永远不会激励不良行为的奖励函数是非常困难的。因此，使用*奖励学习算法*来从数据中*学习*奖励函数变得越来越流行。然而，奖励学习的理论基础尚未得到充分发展。特别是，通常不知道给定的奖励学习算法在高概率下何时会学习到一个安全可优化的奖励函数。这意味着奖励学习算法通常必须通过实证评估，这非常昂贵，并且它们的失败模式很难提前预测。推导更好理论保证的一个障碍是缺乏良好的*量化*奖励函数之间差异的方法。在本文中，我们提供了一个解决方案，以一类我们称之为STARC（标准化奖励比较）度量的伪度量形式，定义在所有奖励函数的空间上。我们表明，STARC度量对最坏情况的遗憾诱导了上下界，这意味着我们的度量是紧的，并且任何具有相同属性的度量必须与我们的度量是双利普希茨等价的。此外，我们还识别了早期工作提出的奖励度量的一些问题。最后，我们通过实证评估我们的度量，以证明其实际有效性。STARC度量可以使奖励学习算法的理论和实证分析变得更简单和更有原则。"
}
{
  "title": "YaRN: Efficient Context Window Extension of Large Language Models",
  "title_zh": "YaRN：大型语言模型的高效上下文窗口扩展",
  "abstract": "Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length.",
  "abstract_zh": "旋转位置嵌入（RoPE）已被证明能够有效编码基于变换器的语言模型中的位置信息。然而，这些模型无法超越其训练时的序列长度进行泛化。我们提出了YaRN（又一个RoPE扩展方法），这是一种计算效率高的方法，可以扩展此类模型的上下文窗口，所需的标记数量比以前的方法少10倍，训练步骤少2.5倍。使用YaRN，我们展示了LLaMA模型能够有效利用并推断出比其原始预训练允许的上下文长度更长的上下文，同时在上下文窗口扩展方面超越了以前的最先进技术。此外，我们证明了YaRN具备超越微调数据集有限上下文的能力。使用YaRN微调的模型已在线提供并可重现，支持高达128k的上下文长度。"
}
{
  "title": "Linearity of Relation Decoding in Transformer Language Models",
  "title_zh": "标题：变换器语言模型中关系解码的线性特性",
  "abstract": "Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.",
  "abstract_zh": "摘要：变换器语言模型（LM）中编码的许多知识可以用关系来表达：词与其同义词之间的关系、实体与其属性之间的关系等。我们表明，对于某些关系的子集，这种计算可以通过对主题表示进行单一线性变换来很好地近似。线性关系表示可以通过从单个提示构建LM的一阶近似来获得，并且它们存在于各种事实、常识和语言关系中。然而，我们也识别出许多情况下，LM预测准确捕捉了关系知识，但这些知识并未在线性表示中编码。因此，我们的结果揭示了一种简单、可解释但异质部署的知识表示策略在变换器LM中。"
}
{
  "title": "SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs",
  "title_zh": "标题：SuRe：利用答案候选者总结检索结果以实现大语言模型的开放域问答",
  "abstract": "Large language models (LLMs) have made significant advancements in various natural language processing tasks, including question answering (QA) tasks. While incorporating new information with the retrieval of relevant passages is a promising way to improve QA with LLMs, the existing methods often require additional fine-tuning which becomes infeasible with recent LLMs. Augmenting retrieved passages via prompting has the potential to address this limitation, but this direction has been limitedly explored. To this end, we design a simple yet effective framework to enhance open-domain QA (ODQA) with LLMs, based on the summarized retrieval (SuRe). SuRe helps LLMs predict more accurate answers for a given question, which are well-supported by the summarized retrieval that could be viewed as an explicit rationale extracted from the retrieved passages. Specifically, SuRe first constructs summaries of the retrieved passages for each of the multiple answer candidates. Then, SuRe confirms the most plausible answer from the candidate set by evaluating the validity and ranking of the generated summaries. Experimental results on diverse ODQA benchmarks demonstrate the superiority of SuRe, with improvements of up to 4.6\\% in exact match (EM) and 4.0\\% in F1 score over standard prompting approaches. SuRe also can be integrated with a broad range of retrieval methods and LLMs. Finally, the generated summaries from SuRe show additional advantages to measure the importance of retrieved passages and serve as more preferred rationales by models and humans.",
  "abstract_zh": "摘要：大型语言模型（LLMs）在各种自然语言处理任务中取得了显著进展，包括问答（QA）任务。虽然通过检索相关段落来整合新信息是改善LLMs问答的一个有前景的方法，但现有方法通常需要额外的微调，这在最近的LLMs中变得不可行。通过提示增强检索到的段落有潜力解决这一限制，但这一方向的探索有限。为此，我们设计了一个简单而有效的框架，以基于总结检索（SuRe）来增强开放域问答（ODQA）。SuRe帮助LLMs为给定问题预测更准确的答案，这些答案得到了可以视为从检索段落中提取的明确理由的总结检索的良好支持。具体而言，SuRe首先为每个多个答案候选者构建检索段落的摘要。然后，SuRe通过评估生成摘要的有效性和排名来确认候选集中最可信的答案。在多样化的ODQA基准上的实验结果表明，SuRe的优越性，相较于标准提示方法，准确匹配（EM）提高了最高4.6\\%，F1得分提高了4.0\\%。SuRe还可以与广泛的检索方法和LLMs集成。最后，SuRe生成的摘要在衡量检索段落的重要性方面显示出额外的优势，并作为模型和人类更偏好的理由。"
}
{
  "title": "Are Models Biased on Text without Gender-related Language?",
  "title_zh": "标题：模型在没有性别相关语言的文本上是否存在偏见？",
  "abstract": "In the large language models era, it is imperative to measure and understand how gender biases present in the training data influence model behavior.\nPrevious works construct benchmarks around known stereotypes (e.g., occupations) and demonstrate high levels of gender bias in large language models, raising serious concerns about models exhibiting undesirable behaviors.\nWe expand on existing literature by asking the question: \\textit{Do large language models still favor one gender over the other in non-stereotypical settings?}\nTo tackle this question, we restrict language model evaluation to a \\textit{neutral} subset, in which sentences are free of pronounced word-gender associations. \nAfter characterizing these associations in terms of pretraining data statistics,\nwe use them to (1) create a new benchmark with low gender-word associations, and (2) repurpose popular benchmarks in the gendered pronoun setting | WinoBias and \\Winogender |, removing pronounced gender-correlated words.\nSurprisingly, when testing $20+$ models (e.g., Llama-2, Pythia, and OPT) in the proposed benchmarks, we still detect critically high gender bias across all tested models. \nFor instance, after adjusting for strong word-gender associations, we find that all models still exhibit clear gender preferences in about $60\\%$-$95\\%$ of the sentences, representing a small change (up to $5\\%$) from the original \\textit{stereotypical} setting.\nBy demonstrating that measured bias is not necessarily due to the presence of highly gender-associated words, our work highlights important questions about bias evaluation as well as potentially underlying model biases.",
  "abstract_zh": "摘要：在大型语言模型时代，衡量和理解训练数据中存在的性别偏见如何影响模型行为至关重要。以往的研究围绕已知的刻板印象（例如职业）构建基准，并展示了大型语言模型中存在的高度性别偏见，这引发了对模型表现出不良行为的严重担忧。我们通过提出问题扩展现有文献：\\textit{在非刻板印象的环境中，大型语言模型是否仍然偏向某一性别？}为了解决这个问题，我们将语言模型评估限制在一个\\textit{中性}子集，其中句子没有明显的词性别关联。在对这些关联进行预训练数据统计特征描述后，我们利用这些关联（1）创建一个低性别词关联的新基准，以及（2）重新利用在性别代词设置下的流行基准 | WinoBias 和 \\Winogender |，去除明显的性别相关词。令人惊讶的是，在提出的基准中测试$20+$个模型（例如，Llama-2、Pythia和OPT）时，我们仍然在所有测试模型中检测到严重的性别偏见。例如，在调整强词性别关联后，我们发现所有模型在约$60\\%$-$95\\%$的句子中仍然表现出明显的性别偏好，与原始\\textit{刻板印象}设置相比，变化很小（最多$5\\%$）。通过证明测量的偏见不一定是由于高度性别关联词的存在，我们的工作突出了关于偏见评估以及潜在的模型偏见的重要问题。"
}
{
  "title": "Towards LLM4QPE: Unsupervised Pretraining of Quantum Property Estimation and A Benchmark",
  "title_zh": "朝向LLM4QPE：量子属性估计的无监督预训练及基准测试",
  "abstract": "Estimating the properties of quantum systems such as quantum phase has been critical in addressing the essential quantum many-body problems in physics and chemistry. Deep learning models have been recently introduced to property estimation, surpassing  conventional statistical approaches. However, these methods are tailored to the specific task and quantum data at hand. It remains an open and attractive question for devising a more universal task-agnostic pretraining model for quantum property estimation. In this paper, we propose LLM4QPE, a large language model style quantum task-agnostic pretraining and finetuning paradigm that 1) performs unsupervised pretraining on diverse quantum systems with different physical conditions; 2) uses the pretrained model for supervised finetuning and delivers high performance with limited training data, on downstream tasks. It mitigates the cost for quantum data collection and speeds up convergence. Extensive experiments show the promising efficacy of LLM4QPE in various tasks including classifying quantum phases of matter on Rydberg atom model and predicting two-body correlation function on anisotropic Heisenberg model.",
  "abstract_zh": "估计量子系统的属性，如量子相位，对于解决物理和化学中的基本量子多体问题至关重要。最近，深度学习模型被引入到属性估计中，超越了传统统计方法。然而，这些方法是针对特定任务和量子数据量身定制的。设计一个更通用的与任务无关的量子属性估计预训练模型仍然是一个开放且有吸引力的问题。本文提出了LLM4QPE，一种大型语言模型风格的量子任务无关预训练和微调范式，1）在具有不同物理条件的多样量子系统上进行无监督预训练；2）利用预训练模型进行监督微调，并在下游任务中以有限的训练数据提供高性能。它减轻了量子数据收集的成本，并加快了收敛速度。大量实验表明，LLM4QPE在包括对Rydberg原子模型的量子相位分类和对各向异性海森堡模型的二体关联函数预测等各种任务中展现了良好的效果。"
}
{
  "title": "Can Large Language Models Infer Causation from Correlation?",
  "title_zh": "标题：大型语言模型能从相关性推断因果关系吗？",
  "abstract": "Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task CORR2CAUSE, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate 17 existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize – they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. CORR2CAUSE is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs’ pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.",
  "abstract_zh": "摘要：因果推断是人类智能的一个标志。尽管因果自然语言处理（CausalNLP）领域近年来引起了广泛关注，但现有的自然语言处理因果推断数据集主要依赖于从经验知识（例如常识知识）中发现因果关系。在本研究中，我们提出了第一个基准数据集，以测试大型语言模型（LLMs）的纯因果推断能力。具体而言，我们制定了一项新任务CORR2CAUSE，该任务接受一组相关性陈述并确定变量之间的因果关系。我们整理了一个超过20万样本的大规模数据集，并在其上评估了17个现有的LLMs。通过实验，我们发现LLMs在因果推断能力方面的一个关键缺陷，并显示这些模型在该任务上的表现几乎接近随机。通过微调，我们在一定程度上缓解了这一缺陷，但我们发现这些模型仍然无法泛化——它们只能在分布内设置中进行因果推断，当查询中使用的变量名称和文本表达与训练集中的相似时，但在通过扰动这些查询生成的分布外设置中失败。CORR2CAUSE是LLMs的一项挑战性任务，将有助于指导未来研究以改善LLMs的纯推理能力和泛化能力。我们的数据可在 https://huggingface.co/datasets/causalnlp/corr2cause 获取。我们的代码可在 https://github.com/causalNLP/corr2cause 找到。"
}
{
  "title": "CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules",
  "title_zh": "代码链：通过自我修订链与代表性子模块实现模块化代码生成",
  "abstract": "Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extracting and clustering the generated sub-modules and selecting the cluster representatives as the more generic and re-usable implementations, and 2) augmenting the original chain-of-thought prompt with these selected module-implementations and instructing the LLM to re-generate new modularized solutions. We find that by naturally encouraging the LLM to reuse the previously developed and verified sub-modules, CodeChain can significantly boost both modularity as well as correctness of the generated solutions, achieving relative pass@1 improvements of 35\\% on APPS and 76\\% on CodeContests. It is shown to be effective on both OpenAI LLMs as well as open-sourced LLMs like WizardCoder. We also conduct comprehensive ablation studies with different methods of prompting, number of clusters, model sizes, program qualities, etc., to provide useful insights that underpin CodeChain's success.",
  "abstract_zh": "大型语言模型（LLMs）在解决人类评估（HumanEval）或MBPP基准中的简单编程任务方面已经相当熟练。然而，解决更复杂和竞争性的编程任务仍然对这些模型构成挑战，这可能是因为它们倾向于生成整体代码块，而不是将其分解为逻辑子任务和子模块。另一方面，经验丰富的程序员本能地编写模块化代码以解决复杂任务，通常会重用先前开发的模块。为了解决这一差距，我们提出了CodeChain，这是一个新颖的推理框架，通过自我修订链引导模块化代码生成，每次修订都由先前迭代中生成的一些代表性子模块指导。具体而言，CodeChain首先指示LLM通过思维链提示生成模块化代码。然后，它通过迭代两个步骤应用自我修订链：1）提取和聚类生成的子模块，并选择聚类代表作为更通用和可重用的实现，2）用这些选定的模块实现增强原始思维链提示，并指示LLM重新生成新的模块化解决方案。我们发现，通过自然鼓励LLM重用先前开发和验证的子模块，CodeChain可以显著提高生成解决方案的模块性和正确性，在APPS上实现35\\%的相对通过率提升，在CodeContests上实现76\\%的提升。它在OpenAI LLM和开源LLM（如WizardCoder）上均显示出有效性。我们还进行了全面的消融研究，探讨不同的提示方法、聚类数量、模型规模、程序质量等，以提供支持CodeChain成功的有用见解。"
}
{
  "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
  "title_zh": "SliceGPT：通过删除行和列压缩大型语言模型",
  "abstract": "Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA-2 70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA-2 70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models.",
  "abstract_zh": "大型语言模型已成为自然语言处理的基石，但其使用在计算和内存资源方面带来了巨大的成本。稀疏化提供了解决方案以缓解这些资源限制，最近的研究表明，训练后的模型可以进行稀疏化。现有的稀疏化技术面临挑战，因为它们需要额外的数据结构，并且在当前硬件上提供的加速有限。本文提出了SliceGPT，一种新的后训练稀疏化方案，它用更小的（稠密）矩阵替换每个权重矩阵，从而减少网络的嵌入维度。通过大量实验，我们表明SliceGPT可以在保持LLAMA-2 70B、OPT 66B和Phi-2模型分别99%、99%和90%的零-shot任务性能的同时，去除多达25%的模型参数（包括嵌入）。我们的切片模型在更少的GPU上运行，并且在没有任何额外代码优化的情况下运行更快：在24GB消费级GPU上，我们将LLAMA-2 70B的推理总计算量减少到稠密模型的64%；在40GB A100 GPU上，我们将其减少到66%。我们提供了一个新的见解，即变换器网络中的计算不变性，这使得SliceGPT成为可能，我们希望它能激发和促成未来减少预训练模型内存和计算需求的途径。"
}
{
  "title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
  "title_zh": "微调语言模型生成稳定的无机材料文本",
  "abstract": "We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90\\% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned  LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49\\% vs 28\\%) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models' ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic data.",
  "abstract_zh": "我们提出对大型语言模型进行微调，以生成稳定的材料。尽管不寻常，但在文本编码的原子数据上微调大型语言模型易于实现且可靠，约90%的采样结构遵循原子位置和电荷的物理约束。通过使用来自学习的机器学习势能和金标准密度泛函理论计算的能量高于壳层的计算，我们展示了我们最强的模型（微调的LLaMA-2 70B）能够以约两倍的速度（49%对比28%）生成预测为亚稳态的材料，超越了竞争的扩散模型CDVAE。由于文本提示的固有灵活性，我们的模型可以同时用于无条件生成稳定材料、填充部分结构和文本条件生成。最后，我们展示了语言模型捕捉晶体结构关键对称性的能力随着模型规模的增加而提高，这表明预训练大型语言模型的偏差意外地适合原子数据。"
}
{
  "title": "Multilingual Jailbreak Challenges in Large Language Models",
  "title_zh": "多语言越狱挑战在大型语言模型中的研究",
  "abstract": "While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit about three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for GPT-4. To handle such a challenge in the multilingual context, we propose a novel \\textsc{Self-Defense} framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation.  Data is available at \\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.",
  "abstract_zh": "虽然大型语言模型（LLMs）在广泛任务中表现出显著能力，但它们也带来了潜在的安全隐患，例如“越狱”问题，其中恶意指令可以操纵LLMs表现出不良行为。尽管已经开发了几种预防措施来减轻与LLMs相关的潜在风险，但这些措施主要集中在英语上。本研究揭示了LLMs中存在的多语言越狱挑战，并考虑了两种潜在的风险场景：无意和故意。无意场景涉及用户使用非英语提示查询LLMs，并无意中绕过安全机制，而故意场景则涉及恶意用户将恶意指令与多语言提示结合，以故意攻击LLMs。实验结果显示，在无意场景中，随着可用语言的减少，不安全内容的比例增加。具体而言，低资源语言遭遇有害内容的可能性约为高资源语言的三倍，ChatGPT和GPT-4均如此。在故意场景中，多语言提示可能加剧恶意指令的负面影响，产生惊人高的不安全输出率：ChatGPT为80.92\\%，GPT-4为40.71\\%。为应对多语言环境中的这一挑战，我们提出了一种新颖的\\textsc{Self-Defense}框架，自动生成多语言训练数据以进行安全微调。实验结果表明，使用该数据微调的ChatGPT可以显著减少不安全内容的生成。数据可在\\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}获取。"
}
{
  "title": "Let's Verify Step by Step",
  "title_zh": "逐步验证",
  "abstract": "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.",
  "abstract_zh": "近年来，大型语言模型在执行复杂的多步骤推理方面有了显著提升。然而，即使是最先进的模型仍然会定期产生逻辑错误。为了训练更可靠的模型，我们可以转向结果监督，它为最终结果提供反馈，或过程监督，它为每个中间推理步骤提供反馈。考虑到训练可靠模型的重要性，以及人类反馈的高成本，仔细比较这两种方法显得尤为重要。近期的研究已经开始了这种比较，但仍然存在许多未解的问题。我们进行自己的调查，发现过程监督在训练模型解决具有挑战性的MATH数据集问题方面显著优于结果监督。我们的过程监督模型解决了MATH测试集代表性子集中的78%问题。此外，我们还表明，主动学习显著提高了过程监督的有效性。为了支持相关研究，我们还发布了PRM800K，这是用于训练我们最佳奖励模型的80万个步骤级人类反馈标签的完整数据集。"
}
{
  "title": "RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment",
  "title_zh": "RLCD：通过对比蒸馏进行强化学习以实现语言模型对齐",
  "abstract": "We propose Reinforcement Learning from Contrastive Distillation (RLCD), a method for aligning language models to follow principles expressed in natural language (e.g., to be more harmless) without using human feedback. RLCD creates preference pairs from two contrasting model outputs, one using a positive prompt designed to encourage following the given principles, and one using a negative prompt designed to encourage violating them. Using two different prompts causes model outputs to be more differentiated on average, resulting in cleaner preference labels in the absence of human annotations. We then use the preference pairs to train a preference model, which is in turn used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks—harmlessness, helpfulness, and story outline generation—and when using both 7B and 30B model scales for simulating preference data",
  "abstract_zh": "我们提出了通过对比蒸馏进行强化学习（RLCD），这是一种使语言模型遵循自然语言中表达的原则（例如，更加无害）的方法，而无需使用人类反馈。RLCD从两个对比的模型输出中创建偏好对，一个使用旨在鼓励遵循给定原则的正向提示，另一个使用旨在鼓励违反这些原则的负向提示。使用两种不同的提示使模型输出在平均上更加区分，从而在没有人类注释的情况下产生更清晰的偏好标签。然后，我们使用这些偏好对来训练一个偏好模型，该模型反过来通过强化学习改善一个基础的未对齐语言模型。实证结果表明，RLCD在三个不同的对齐任务——无害性、帮助性和故事大纲生成——以及在使用7B和30B模型规模模拟偏好数据时，均优于RLAIF（Bai et al., 2022b）和上下文蒸馏（Huang et al., 2022）基线。"
}
{
  "title": "LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors",
  "title_zh": "标题：大型语言模型与视觉语言模型的结合：通过细粒度描述符提升开放词汇物体检测",
  "abstract": "Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grained text descriptions of object parts (e.g., pedals and bells). This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context prompts and hierarchical textual descriptors that enable precise region-text alignment as well as open-vocabulary detection training in general. Specifically, the conditional context prompt transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training. In addition, we introduce large language models as an interactive and implicit knowledge repository which enables iterative mining and refining visually oriented textual descriptors for precise region-text alignment. Extensive experiments over multiple large-scale benchmarks show that DVDet outperforms the state-of-the-art consistently by large margins.",
  "abstract_zh": "摘要：受到视觉语言模型（VLMs）在图像分类任务中卓越的零-shot 能力的启发，开放词汇物体检测通过将广泛的 VLM 知识提炼到检测器训练中，吸引了越来越多的关注。然而，大多数现有的开放词汇检测器仅通过将区域嵌入与类别标签（例如，自行车）对齐来学习，忽视了 VLM 在将视觉嵌入与物体部件的细粒度文本描述（例如，踏板和铃铛）对齐方面的能力。本文提出了 DVDet，一种描述符增强的开放词汇检测器，引入条件上下文提示和分层文本描述符，使得精确的区域-文本对齐以及开放词汇检测训练成为可能。具体而言，条件上下文提示将区域嵌入转换为类似图像的表示，可以直接整合到一般的开放词汇检测训练中。此外，我们引入大型语言模型作为交互式和隐式知识库，使得能够迭代挖掘和精炼视觉导向的文本描述符，以实现精确的区域-文本对齐。在多个大规模基准上的广泛实验表明，DVDet 一直以较大幅度超越最先进的技术水平。"
}
{
  "title": "GROOT: Learning to Follow Instructions by Watching Gameplay Videos",
  "title_zh": "标题：GROOT：通过观看游戏视频学习遵循指令",
  "abstract": "We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning framework is derived to allow learning such instruction-following controllers from gameplay videos while producing a video instruction encoder that induces a structured goal space. We implement our agent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers. We evaluate GROOT against open-world counterparts and human players on a proposed Minecraft SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap as well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis of the induced goal space further demonstrates some interesting emergent properties, including the goal composition and complex gameplay behavior synthesis.",
  "abstract_zh": "摘要：我们研究了在开放世界环境中构建能够遵循开放式指令的控制器的问题。我们提出将参考视频作为指令，这提供了丰富的目标规范，同时消除了对昂贵的文本-游戏注释的需求。我们推导出一个新的学习框架，以便从游戏视频中学习这种遵循指令的控制器，同时生成一个视频指令编码器，诱导出一个结构化的目标空间。我们在基于因果变换器的简单而有效的编码器-解码器架构中实现了我们的代理GROOT。我们在提出的Minecraft SkillForge基准上将GROOT与开放世界的对手和人类玩家进行了评估。Elo评分清楚地表明，GROOT正在缩小人机差距，并且在最佳通用代理基线之上表现出70%的胜率。对诱导目标空间的定性分析进一步展示了一些有趣的涌现特性，包括目标组合和复杂游戏行为合成。"
}
{
  "title": "Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs",
  "title_zh": "层次上下文合并：为预训练大语言模型提供更好的长上下文理解",
  "abstract": "Large language models (LLMs) have established new standards in various natural language processing tasks. \nHowever, a primary constraint they face is the context limit, i.e., the maximum number of tokens they can process.\nTo relax the constraint, \nprevious works have explored architectural changes and modifications in positional encoding, but they often require expensive training or do not address the computational demands of self-attention.\nIn this paper, we present Hierarchical cOntext MERging (HOMER), a new training-free scheme designed to overcome the limitations. HOMER harnesses a divide-and-conquer methodology, segmenting extensive inputs into manageable units. The segments are then processed collectively, employing a hierarchical strategy that fuses adjacent chunks at progressive transformer layers. A token reduction technique precedes each fusion, ensuring memory usage efficiency.\nWe also propose an optimized computational order reducing the memory requirement to logarithmically scale with respect to input length, making it especially favorable for environments with tight memory restrictions. \nOur experimental results demonstrate the superior performance and memory efficiency of the proposed method, opening doors for broader applications of LLMs in scenarios with extended context requirements.\nCode is available at [this https URL](https://github.com/alinlab/HOMER).",
  "abstract_zh": "大型语言模型（LLMs）在各种自然语言处理任务中设立了新的标准。然而，它们面临的主要限制是上下文限制，即它们可以处理的最大标记数量。为了放宽这一限制，以往的研究探索了架构变化和位置编码的修改，但往往需要昂贵的训练或未能解决自注意力的计算需求。本文提出了一种新的无训练方案——层次上下文合并（HOMER），旨在克服这些限制。HOMER利用分而治之的方法，将大量输入分割为可管理的单元。然后，采用层次策略在逐步的变换器层中集体处理这些片段，融合相邻的块。在每次融合之前采用标记减少技术，以确保内存使用效率。我们还提出了一种优化的计算顺序，使内存需求与输入长度呈对数缩放，特别适合内存限制较紧的环境。我们的实验结果表明，所提方法在性能和内存效率上优于现有方法，为LLMs在需要扩展上下文的场景中的更广泛应用打开了大门。代码可在[此链接](https://github.com/alinlab/HOMER)获取。"
}
{
  "title": "MoLE: Mixture of LoRA Experts",
  "title_zh": "MoLE：LoRA专家混合",
  "abstract": "LoRA has gained widespread acceptance in the fine-tuning of large pre-trained models to cater to a diverse array of downstream tasks, showcasing notable effectiveness and efficiency, thereby solidifying its position as one of the most prevalent fine-tuning techniques. Due to the modular nature of LoRA's plug-and-play plugins, researchers have delved into the amalgamation of multiple LoRAs to empower models to excel across various downstream tasks. Nonetheless, extant approaches for LoRA fusion grapple with inherent challenges. Direct arithmetic merging may result in the loss of the original pre-trained model's generative capabilities or the distinct identity of LoRAs, thereby yielding suboptimal outcomes. On the other hand, Reference tuning-based fusion exhibits limitations concerning the requisite flexibility for the effective combination of multiple LoRAs. In response to these challenges, this paper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses hierarchical control and unfettered branch selection. The MoLE approach not only achieves superior LoRA fusion performance in comparison to direct arithmetic merging but also retains the crucial flexibility for combining LoRAs effectively. Extensive experimental evaluations conducted in both the Natural Language Processing (NLP) and Vision \\& Language (V\\&L) domains substantiate the efficacy of MoLE.",
  "abstract_zh": "LoRA在大规模预训练模型的微调中得到了广泛应用，以满足多样化的下游任务，展现出显著的有效性和效率，从而巩固了其作为最普遍的微调技术之一的地位。由于LoRA插件的模块化特性，研究人员探索了多种LoRA的融合，以增强模型在各种下游任务中的表现。然而，现有的LoRA融合方法面临固有挑战。直接的算术合并可能导致原始预训练模型的生成能力或LoRA的独特身份丧失，从而产生次优结果。另一方面，基于参考调优的融合在有效组合多个LoRA所需的灵活性方面存在局限。针对这些挑战，本文提出了LoRA专家混合（MoLE）方法，该方法利用分层控制和自由分支选择。与直接的算术合并相比，MoLE方法不仅实现了更优的LoRA融合性能，还保留了有效组合LoRA所需的重要灵活性。在自然语言处理（NLP）和视觉与语言（V&L）领域进行的广泛实验评估证实了MoLE的有效性。"
}
{
  "title": "Unbiased Watermark for Large Language Models",
  "title_zh": "无偏水印在大型语言模型中的应用",
  "abstract": "The recent advancements in large language models (LLMs) have sparked a growing apprehension regarding the potential misuse. One approach to mitigating this risk is to incorporate watermarking techniques into LLMs, allowing for the tracking and attribution of model outputs. This study examines a crucial aspect of watermarking: how significantly watermarks impact the quality of model-generated outputs. Previous studies have suggested a trade-off between watermark strength and output quality. However, our research demonstrates that it is possible to integrate watermarks without affecting the output probability distribution with appropriate implementation. We refer to this type of watermark as an unbiased watermark. This has significant implications for the use of LLMs, as it becomes impossible for users to discern whether a service provider has incorporated watermarks or not. Furthermore, the presence of watermarks does not compromise the performance of the model in downstream tasks, ensuring that the overall utility of the language model is preserved. Our findings contribute to the ongoing discussion around responsible AI development, suggesting that unbiased watermarks can serve as an effective means of tracking and attributing model outputs without sacrificing output quality.",
  "abstract_zh": "近期大型语言模型（LLMs）的进展引发了对潜在滥用的日益担忧。减轻这一风险的一种方法是将水印技术纳入LLMs，从而实现对模型输出的追踪和归属。本研究考察了水印的一个关键方面：水印对模型生成输出质量的影响程度。以往研究表明水印强度与输出质量之间存在权衡。然而，我们的研究表明，通过适当的实施，可以在不影响输出概率分布的情况下集成水印。我们将这种水印称为无偏水印。这对LLMs的使用具有重要意义，因为用户无法辨别服务提供商是否已集成水印。此外，水印的存在不会影响模型在下游任务中的性能，确保语言模型的整体效用得以保留。我们的研究结果为负责任的人工智能发展提供了贡献，表明无偏水印可以作为一种有效手段，追踪和归属模型输出而不牺牲输出质量。"
}
{
  "title": "In-context Autoencoder for Context Compression in a Large Language Model",
  "title_zh": "上下文自编码器在大型语言模型中的上下文压缩",
  "abstract": "We propose the In-context Autoencoder (ICAE), leveraging the power of a large language models (LLM) to compress a long context into short compact memory slots that can be directly conditioned on by the LLM for various purposes. ICAE is first pretrained using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context; Then, it is fine-tuned on instruction data for producing desirable responses to various prompts. Experiments demonstrate that our lightweight ICAE, introducing about 1% additional parameters, effectively achieves $4\\times$ context compression based on Llama, offering advantages in both improved latency and GPU memory cost during inference, and showing an interesting insight in memorization as well as potential for scalability. These promising results imply a novel perspective on the connection between working memory in cognitive science and representation learning in LLMs, revealing ICAE's significant implications in addressing the long context problem and suggesting further research in LLM context management. Our data, code and models are available at https://github.com/getao/icae.",
  "abstract_zh": "我们提出了上下文自编码器（ICAE），利用大型语言模型（LLM）的强大能力，将长上下文压缩为短小的内存槽，LLM可以直接基于这些内存槽进行各种目的的条件处理。ICAE首先在大量文本数据上使用自编码和语言建模目标进行预训练，使其能够生成准确且全面表示原始上下文的内存槽；然后，它在指令数据上进行微调，以产生对各种提示的理想响应。实验表明，我们的轻量级ICAE仅引入约1%的额外参数，基于Llama有效实现了$4\\times$的上下文压缩，在推理过程中在延迟和GPU内存成本方面都提供了优势，并在记忆化方面展现了有趣的见解以及扩展潜力。这些有希望的结果暗示了认知科学中的工作记忆与LLM中的表示学习之间联系的新视角，揭示了ICAE在解决长上下文问题方面的重要意义，并建议进一步研究LLM上下文管理。我们的数据、代码和模型可在https://github.com/getao/icae获取。"
}
{
  "title": "Large Language Models as Generalizable Policies for Embodied Tasks",
  "title_zh": "大型语言模型作为具身任务的可推广策略",
  "abstract": "We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangement.",
  "abstract_zh": "我们展示了大型语言模型（LLMs）可以适应为具身视觉任务的可推广策略。我们的方法称为大型语言模型强化学习策略（LLaRP），它将一个预训练的冻结LLM调整为接受文本指令和视觉自我中心观察作为输入，并直接在环境中输出动作。通过强化学习，我们训练LLaRP仅通过环境交互进行观察和行动。我们表明LLaRP对任务指令的复杂改述具有鲁棒性，并且能够推广到需要新颖最优行为的新任务。特别是在1,000个未见任务上，它的成功率达到42%，是其他常见学习基线或LLMs零样本应用成功率的1.7倍。最后，为了帮助社区研究语言条件下的大规模多任务具身AI问题，我们发布了一个新基准，语言重排，包含150,000个训练任务和1,000个测试任务，用于语言条件下的重排。"
}
{
  "title": "Towards Understanding Sycophancy in Language Models",
  "title_zh": "标题：理解语言模型中的阿谀奉承",
  "abstract": "Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgments are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of RLHF models, likely driven in part by human preference judgments favoring sycophantic responses.",
  "abstract_zh": "摘要：人类反馈的强化学习（RLHF）是一种流行的高质量人工智能助手训练技术。然而，RLHF也可能鼓励模型的响应更符合用户信念而非真实响应，这种行为被称为阿谀奉承。我们调查了RLHF训练模型中阿谀奉承的普遍性以及人类偏好判断是否对此负责。我们首先证明五个最先进的人工智能助手在四个不同的自由文本生成任务中始终表现出阿谀奉承的行为。为了理解人类偏好是否驱动了这种广泛观察到的RLHF模型行为，我们分析了现有的人类偏好数据。我们发现，当响应与用户的观点一致时，更可能被偏好。此外，无论是人类还是偏好模型（PMs）在相当一部分时间内更倾向于令人信服的阿谀奉承响应而非正确响应。针对PMs优化模型输出有时也会牺牲真实性以支持阿谀奉承。总体而言，我们的结果表明，阿谀奉承是RLHF模型的一种普遍行为，可能部分是由于人类偏好判断偏向阿谀奉承的响应。"
}
{
  "title": "Evaluating Large Language Models at Evaluating Instruction Following",
  "title_zh": "评估大型语言模型在遵循指令方面的能力",
  "abstract": "As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever-increasing list of models. This paper investigates the efficacy of these “LLM evaluators”, particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the\ninstructions. We introduce a challenging meta-evaluation benchmark, LLMBAR, designed to test the ability of an LLM evaluator to discern instruction-following outputs. The authors curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that could mislead an LLM evaluator. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBAR and even the highest-scoring LLM evaluators have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBAR, we hope to offer more insight into the behavior of LLM evaluators and foster research in developing better instruction-following models.",
  "abstract_zh": "随着大型语言模型（LLMs）研究的加速，基于LLM的评估已成为一种可扩展且具有成本效益的人类评估替代方案，用于比较日益增长的模型列表。本文探讨了这些“LLM评估者”的有效性，特别是在评估遵循指令的能力方面，这是一个衡量生成文本与指令的贴合程度的指标。我们引入了一个具有挑战性的元评估基准LLMBAR，旨在测试LLM评估者辨别遵循指令输出的能力。作者整理了419对输出，其中一对遵循指令而另一对则偏离，但可能具有误导性特征，可能会误导LLM评估者。与现有的元评估相反，我们发现不同的评估者（即LLM和提示的组合）在LLMBAR上的表现各异，甚至得分最高的LLM评估者也有相当大的改进空间。我们还提出了一套新颖的提示策略，进一步缩小了LLM与人类评估者之间的差距。通过LLMBAR，我们希望提供更多关于LLM评估者行为的见解，并促进开发更好的遵循指令模型的研究。"
}
{
  "title": "Dissecting learning and forgetting in language model finetuning",
  "title_zh": "标题：剖析语言模型微调中的学习与遗忘",
  "abstract": "Finetuning language models on domain-specific corpus is a common approach to enhance their domain knowledge and capability. While improving performance on domain tasks, it often brings a side-effect of forgetting of the model's general abilities. In this study, we analyze the effects of finetuning on language models by dissecting its impacts on the modeling of topic, style, and factual knowledge in text. Our method uses instruction-following LLMs such as ChatGPT to auto-generate controlled-variable text examples which we use to probe the model. Our findings reveal that finetuning results in significant shifts in the language model's topic and style priors, while actual knowledge learning only contributes to a small fraction of the total probability change. Analysis shows that the adaptation of topic and style priors behave akin to learning simple features: they are learned rapidly and require little model capacity. They are also learned independently and primarily at the beginning of a text sequence. In contrast, factual knowledge is learned stably but slowly and requires significant model capacity to learn. The research offers insights and understanding into the finer dynamics of learning and forgetting in language models, and can potentially inform future research on improving domain adaptation and addressing the challenges of forgetting in continual learning of language models.",
  "abstract_zh": "摘要：在特定领域语料上微调语言模型是一种常见的方法，以增强其领域知识和能力。尽管在领域任务上提高了性能，但这往往会带来模型一般能力遗忘的副作用。在本研究中，我们通过剖析微调对文本中主题、风格和事实知识建模的影响，分析微调对语言模型的影响。我们的方法使用指令跟随的LLM，如ChatGPT，自动生成控制变量的文本示例，以探测模型。我们的发现揭示，微调导致语言模型的主题和风格先验发生显著变化，而实际知识学习仅对总概率变化贡献了一小部分。分析表明，主题和风格先验的适应行为类似于学习简单特征：它们学习迅速且需要较少的模型容量。它们也独立学习，主要发生在文本序列的开始部分。相比之下，事实知识的学习稳定但缓慢，并且需要显著的模型容量。该研究提供了对语言模型学习与遗忘细微动态的洞察与理解，并可能为未来研究提供信息，以改善领域适应性并应对语言模型持续学习中的遗忘挑战。"
}
{
  "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback",
  "title_zh": "标题：Motif：来自人工智能反馈的内在动机",
  "abstract": "Exploring rich environments and evaluating one's actions without prior knowledge is immensely challenging. In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model (LLM) with an agent. Motif is based on the idea of grounding LLMs for decision-making without requiring them to interact with the environment: it elicits preferences from an LLM over pairs of captions to construct an intrinsic reward, which is then used to train agents with reinforcement learning. We evaluate Motif's performance and behavior on the challenging, open-ended and procedurally-generated NetHack game. Surprisingly, by only learning to maximize its intrinsic reward, Motif achieves a higher game score than an algorithm directly trained to maximize the score itself. When combining Motif's intrinsic reward with the environment reward, our method significantly outperforms existing approaches and makes progress on tasks where no advancements have ever been made without demonstrations. Finally, we show that Motif mostly generates intuitive human-aligned behaviors which can be steered easily through prompt modifications, while scaling well with the LLM size and the amount of information given in the prompt.",
  "abstract_zh": "摘要：探索丰富的环境并在没有先验知识的情况下评估自己的行为是极具挑战性的。本文提出了Motif，一种将大型语言模型（LLM）的先验知识与智能体接口的通用方法。Motif基于将LLM用于决策的理念，而无需与环境互动：它通过从LLM中引发对一对字幕的偏好来构建内在奖励，然后利用该奖励训练智能体进行强化学习。我们在具有挑战性、开放式和程序生成的NetHack游戏中评估了Motif的性能和行为。令人惊讶的是，仅通过学习最大化其内在奖励，Motif的游戏得分超过了直接训练以最大化得分的算法。当将Motif的内在奖励与环境奖励结合时，我们的方法显著优于现有方法，并在没有演示的情况下在从未取得进展的任务上取得了进展。最后，我们展示了Motif主要生成直观的人类对齐行为，这些行为可以通过提示修改轻松引导，同时在LLM规模和提示中提供的信息量上具有良好的扩展性。"
}
{
  "title": "Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community",
  "title_zh": "社会奖励：通过来自在线创意社区的百万用户反馈评估和增强生成性人工智能",
  "abstract": "Social reward as a form of community recognition provides a strong source of\nmotivation for users of online platforms to actively engage and contribute with\ncontent to accumulate peers approval. In the realm of text-conditioned image\nsynthesis, the recent surge in progress has ushered in a collaborative era where\nusers and AI systems coalesce to refine visual creations. This co-creative pro-\ncess in the landscape of online social networks empowers users to craft original\nvisual artworks seeking for community validation. Nevertheless, assessing these\nmodels in the context of collective community preference introduces distinct chal-\nlenges. Existing evaluation methods predominantly center on limited size user\nstudies guided by image quality and alignment with prompts. This work pio-\nneers a paradigm shift, unveiling Social Reward - an innovative reward modeling\nframework that leverages implicit feedback from social network users engaged\nin creative editing of generated images. We embark on an extensive journey of\ndataset curation and refinement, drawing from Picsart: an online visual creation\nand editing platform, yielding a first million-user-scale dataset of implicit human\npreferences for user-generated visual art named Picsart Image-Social. Our anal-\nysis exposes the shortcomings of current metrics in modeling community creative\npreference of text-to-image models’ outputs, compelling us to introduce a novel\npredictive model explicitly tailored to address these limitations. Rigorous quan-\ntitative experiments and user study show that our Social Reward model aligns\nbetter with social popularity than existing metrics. Furthermore, we utilize So-\ncial Reward to fine-tune text-to-image models, yielding images that are more fa-\nvored by not only Social Reward, but also other established metrics. These find-\nings highlight the relevance and effectiveness of Social Reward in assessing com-\nmunity appreciation for AI-generated artworks, establishing a closer alignment\nwith users’ creative goals: creating popular visual art. Codes can be accessed at\nhttps://github.com/Picsart-AI-Research/Social-Reward",
  "abstract_zh": "社会奖励作为一种社区认可的形式，为在线平台用户积极参与和贡献内容以积累同伴认可提供了强大的动机来源。在文本条件图像合成领域，近期的进展激发了一个协作时代，用户与人工智能系统共同努力，完善视觉创作。这个在线社交网络中的共同创作过程使用户能够创作寻求社区验证的原创视觉艺术作品。然而，在集体社区偏好的背景下评估这些模型引入了独特的挑战。现有的评估方法主要集中在有限规模的用户研究上，侧重于图像质量和与提示的一致性。本研究开创了一种范式转变，揭示了社会奖励——一个创新的奖励建模框架，利用参与生成图像创意编辑的社交网络用户的隐性反馈。我们开展了一项广泛的数据集策划和完善工作，借助Picsart：一个在线视觉创作和编辑平台，产生了一个首个百万用户规模的用户生成视觉艺术隐性人类偏好的数据集，命名为Picsart Image-Social。我们的分析揭示了当前指标在建模社区对文本到图像模型输出的创意偏好方面的不足，促使我们引入一种新颖的预测模型，专门针对这些局限性进行调整。严格的定量实验和用户研究表明，我们的社会奖励模型与社交流行度的契合度优于现有指标。此外，我们利用社会奖励对文本到图像模型进行微调，生成的图像不仅更受社会奖励的青睐，也更符合其他已建立的指标。这些发现突显了社会奖励在评估社区对人工智能生成艺术作品的欣赏方面的相关性和有效性，建立了与用户创作目标——创造受欢迎的视觉艺术——更紧密的对齐。代码可在https://github.com/Picsart-AI-Research/Social-Reward获取。"
}
{
  "title": "Compositional Preference Models for Aligning LMs",
  "title_zh": "组合偏好模型用于对齐语言模型",
  "abstract": "As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset.\nWe propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. Through these simple steps, CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment.\nOur experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using CPMs tend to be preferred over samples obtained using conventional PMs.\nOverall, our approach demonstrates the benefits of endowing PMs with priors about which features determine human preferences while relying on LM capabilities to extract those features in a scalable and robust way.",
  "abstract_zh": "随着语言模型（LM）能力的提升，将其与人类偏好对齐变得愈发重要。然而，当前用于此目的的偏好模型（PM）训练主流范式存在根本性局限性，如缺乏透明度和可扩展性，以及对偏好数据集的过拟合倾向。我们提出了组合偏好模型（CPM），这是一种新颖的PM框架，它将一个全局偏好评估分解为多个可解释特征，从提示的LM中获取这些特征的标量评分，并使用逻辑回归分类器聚合这些评分。通过这些简单步骤，CPM允许控制用于训练偏好模型的偏好数据属性，并基于被认为是人类偏好判断基础的特征构建模型。我们的实验表明，CPM不仅改善了泛化能力，并且比标准PM对过度优化更具鲁棒性，而且使用CPM获得的最佳样本往往比使用传统PM获得的样本更受欢迎。总体而言，我们的方法展示了赋予PM关于哪些特征决定人类偏好的先验知识的好处，同时依赖LM能力以可扩展和鲁棒的方式提取这些特征。"
}
{
  "title": "A Benchmark for Learning to Translate a New Language from One Grammar Book",
  "title_zh": "从一本语法书学习翻译新语言的基准测试",
  "abstract": "Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang—a language with less than 200 speakers and therefore virtually no presence on the web—using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 language learning than L1 language acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human performance, achieving 44.7 chrF on Kalamang to English translation and 45.8 chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a human who learned Kalamang from the same reference materials. We hope that MTOB will help measure LLM capabilities along a new dimension, and that the methods developed to solve it could help expand access to language technology for underserved communities by leveraging qualitatively different kinds of data than traditional machine translation.",
  "abstract_zh": "大型语言模型（LLMs）在上下文学习或轻量级微调方面表现出色。人们自然会想知道这些模型在真正新任务中的适应能力，但如何找到在互联网规模训练集中未见过的任务呢？我们转向一个明确受到网络数据稀缺影响的领域：低资源语言。本文介绍了MTOB（来自一本书的机器翻译），这是一个学习在英语和Kalamang（这种语言的使用者不到200人，因此几乎没有网络存在）之间翻译的基准，使用了数百页的田野语言学参考材料。这种任务框架的新颖之处在于，它要求模型从一本人类可读的语法解释书中学习语言，而不是从大量挖掘的领域数据语料库中学习，更类似于L2语言学习而非L1语言习得。我们展示了使用当前LLMs的基线表现出色，但仍未达到人类表现，Kalamang到英语翻译的chrF得分为44.7，英语到Kalamang翻译的chrF得分为45.8，而人类在相同参考材料中学习Kalamang的得分为51.6和57.0。我们希望MTOB能够帮助衡量LLM能力的新维度，并且为解决此问题而开发的方法能够通过利用与传统机器翻译 qualitatively 不同的数据类型，帮助扩大对服务不足社区的语言技术访问。"
}
{
  "title": "GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs",
  "title_zh": "图谱关怀：利用个性化知识图谱增强医疗预测",
  "abstract": "Clinical predictive models often rely on patients’ electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge\ngraphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose GraphCare, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed\n(BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, GraphCare surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6% and 6.6% for mortality and readmission, and F1-score by 7.9% and 10.8% for LOS and drug recommendation, respectively. Notably, GraphCare demonstrates a substantial edge in scenarios with limited data availability. Our findings highlight the potential of using external KGs in healthcare prediction tasks and demonstrate the promise of GraphCare in generating personalized KGs for promoting personalized medicine.",
  "abstract_zh": "临床预测模型通常依赖于患者的电子健康记录（EHR），但将医学知识整合以增强预测和决策面临挑战。这是因为个性化预测需要个性化知识图谱（KG），而从患者EHR数据生成这些图谱是困难的。为了解决这个问题，我们提出了GraphCare，这是一个开放世界框架，利用外部KG来改善基于EHR的预测。我们的方法从大型语言模型（LLM）和外部生物医学KG中提取知识，以构建特定于患者的KG，然后用于训练我们提出的双重注意力增强（BAT）图神经网络（GNN）进行医疗预测。在两个公共数据集MIMIC-III和MIMIC-IV上，GraphCare在四个重要的医疗预测任务中超越了基线：死亡率、再入院、住院时长（LOS）和药物推荐。在MIMIC-III上，它将死亡率和再入院的AUROC分别提高了17.6%和6.6%，并将LOS和药物推荐的F1-score分别提高了7.9%和10.8%。值得注意的是，GraphCare在数据可用性有限的情况下表现出显著优势。我们的研究结果突显了在医疗预测任务中使用外部KG的潜力，并展示了GraphCare在生成个性化KG以促进个性化医疗方面的前景。"
}
{
  "title": "Text2Reward: Reward Shaping with Language Models for Reinforcement Learning",
  "title_zh": "文本奖励：基于语言模型的强化学习奖励塑形",
  "abstract": "Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation and shaping of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates shaped dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes or unshaped dense rewards with a constant function across timesteps, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io/",
  "abstract_zh": "设计奖励函数是强化学习中的一个长期挑战；它需要专业知识或领域数据，导致开发成本高昂。为了解决这个问题，我们提出了Text2Reward，一个基于大型语言模型（LLMs）的无数据框架，自动生成和塑形密集奖励函数。给定用自然语言描述的目标，Text2Reward生成作为可执行程序的塑形密集奖励函数，基于环境的紧凑表示。与逆向强化学习和最近使用LLMs编写稀疏奖励代码或在时间步长上使用常数函数的未塑形密集奖励的工作不同，Text2Reward生成可解释的、自由形式的密集奖励代码，涵盖广泛的任务，利用现有的软件包，并允许通过人类反馈进行迭代改进。我们在两个机器人操作基准（ManiSkill2，MetaWorld）和MuJoCo的两个运动环境上评估Text2Reward。在17个操作任务中的13个任务中，使用生成的奖励代码训练的策略实现了与专家编写的奖励代码相似或更好的任务成功率和收敛速度。对于运动任务，我们的方法学习了六种新的运动行为，成功率超过94%。此外，我们还展示了使用我们的方法在模拟器中训练的策略可以在现实世界中部署。最后，Text2Reward通过人类反馈进一步改进了策略的奖励函数。视频结果可在https://text-to-reward.github.io/查看。"
}
{
  "title": "Language Model Inversion",
  "title_zh": "语言模型反演",
  "abstract": "Given a prompt, language models produce a distribution over all possible next tokens; when the prompt is unknown, can we use this distributional information to recover the prompt? We consider the problem of anguage model inversion and show that next-token probabilities contain a surprising amount of information about the preceding text. Often we can recover the text in cases where it is hidden from the user, motivating a method for recovering unknown prompts given only the model's current distribution output. We consider a variety of model access scenarios, and show how even without predictions for every token in the vocabulary we can recover the probability vector through search and reconstruction of the input. On LLAMA-7B, our inversion method reconstructs prompts with a BLEU of $59$ and token-level F1 of $77$ and recovers $23\\%$ of prompts exactly",
  "abstract_zh": "给定一个提示，语言模型生成所有可能下一个标记的分布；当提示未知时，我们能否利用这种分布信息来恢复提示？我们考虑语言模型反演的问题，并展示下一个标记的概率包含了关于前文的惊人信息。在许多情况下，我们可以恢复用户无法看到的文本，这激励了一种仅根据模型当前分布输出恢复未知提示的方法。我们考虑了多种模型访问场景，并展示即使没有对词汇表中每个标记的预测，我们也能通过搜索和重建输入来恢复概率向量。在LLAMA-7B上，我们的反演方法以$59$的BLEU和$77$的标记级F1重建提示，并准确恢复了$23\\%$的提示。"
}
{
  "title": "Tailoring Self-Rationalizers with Multi-Reward Distillation",
  "title_zh": "定制多奖励蒸馏的自我理性化模型",
  "abstract": "Large language models (LMs) are capable of generating free-text rationales to aid question answering. However, prior work 1) suggests that useful self-rationalization is emergent only at significant scales (e.g., 175B parameter GPT-3); and 2) focuses largely on downstream performance, ignoring the semantics of the rationales themselves, e.g., are they faithful, true, and helpful for humans? In this work, we enable small-scale LMs (∼200x smaller than GPT-3) to generate rationales that not only improve downstream task performance, but are also more plausible, consistent, and diverse, assessed both by automatic and human evaluation. Our method, MaRio (Multi-rewArd RatIOnalization), is a multi-reward conditioned self-rationalization algorithm that optimizes multiple distinct properties like plausibility, diversity and consistency. Results on three difficult question-answering datasets StrategyQA, QuaRel and OpenBookQA show that not only does MaRio improve task accuracy, but it also improves the self-rationalization quality of small LMs across the aforementioned axes better than a supervised fine-tuning (SFT) baseline. Extensive human evaluations confirm that MaRio rationales are preferred vs. SFT rationales, as well as qualitative improvements in plausibility and consistency.",
  "abstract_zh": "大型语言模型（LM）能够生成自由文本的理性化内容以辅助问答。然而，先前的研究表明，1）有用的自我理性化仅在显著规模（例如，175B参数的GPT-3）时出现；2）主要关注下游性能，忽略了理性化内容本身的语义，例如，它们是否真实、可信且对人类有帮助？在本研究中，我们使小规模LM（约比GPT-3小200倍）能够生成不仅提高下游任务性能，而且在自动和人工评估中被认为更可信、一致和多样化的理性化内容。我们的方法MaRio（多奖励理性化）是一种多奖励条件的自我理性化算法，优化可信性、多样性和一致性等多个不同属性。在三个困难的问答数据集StrategyQA、QuaRel和OpenBookQA上的结果表明，MaRio不仅提高了任务准确性，而且在上述维度上也比监督微调（SFT）基线更好地提高了小型LM的自我理性化质量。广泛的人类评估确认MaRio的理性化内容比SFT的理性化内容更受欢迎，并在可信性和一致性方面有定性改善。"
}
{
  "title": "Controlling Vision-Language Models for Multi-Task Image Restoration",
  "title_zh": "控制视觉-语言模型进行多任务图像修复",
  "abstract": "Vision-language models such as CLIP have shown great impact on diverse downstream tasks for zero-shot or label-free predictions. However, when it comes to low-level vision such as image restoration their performance deteriorates dramatically due to corrupted inputs. In this paper, we present a degradation-aware vision-language model (DA-CLIP) to better transfer pretrained vision-language models to low-level vision tasks as a multi-task framework for image restoration. More specifically, DA-CLIP trains an additional controller that adapts the fixed CLIP image encoder to predict high-quality feature embeddings. By integrating the embedding into an image restoration network via cross-attention, we are able to pilot the model to learn a high-fidelity image reconstruction. The controller itself will also output a degradation feature that matches the real corruptions of the input, yielding a natural classifier for different degradation types. In addition, we construct a mixed degradation dataset with synthetic captions for DA-CLIP training. Our approach advances state-of-the-art performance on both degradation-specific and unified image restoration tasks, showing a promising direction of prompting image restoration with large-scale pretrained vision-language models. Our code is available at https://github.com/Algolzw/daclip-uir.",
  "abstract_zh": "视觉-语言模型如CLIP在零样本或无标签预测的多种下游任务中表现出巨大影响。然而，在低级视觉任务如图像修复中，由于输入损坏，其性能显著下降。本文提出了一种降级感知的视觉-语言模型（DA-CLIP），旨在将预训练的视觉-语言模型更好地转移到低级视觉任务中，作为图像修复的多任务框架。具体而言，DA-CLIP训练了一个额外的控制器，使固定的CLIP图像编码器能够预测高质量的特征嵌入。通过交叉注意力将嵌入整合到图像修复网络中，我们能够引导模型学习高保真的图像重建。控制器本身还会输出与输入的真实损坏相匹配的降级特征，从而为不同的降级类型提供自然分类器。此外，我们构建了一个带有合成标题的混合降级数据集用于DA-CLIP训练。我们的方法在降级特定和统一图像修复任务上都取得了最先进的性能，展示了利用大规模预训练视觉-语言模型促进图像修复的有希望方向。我们的代码可在https://github.com/Algolzw/daclip-uir获取。"
}
{
  "title": "Frozen Transformers in Language Models Are Effective Visual Encoder Layers",
  "title_zh": "冻结变压器在语言模型中是有效的视觉编码层",
  "abstract": "This paper reveals that large language models (LLMs), despite being trained solely on text data, are \\emph{surprisingly} strong encoders for \\emph{purely} visual tasks in the absence of language. Even more intriguingly, this can be achieved by a simple yet previously overlooked strategy -- employing a \\emph{frozen} transformer block from \\emph{pre-trained} LLMs as a constituent encoder layer to directly process visual tokens. Our work pushes the boundaries of leveraging LLMs for computer vision tasks, significantly departing from conventional practices that typically necessitate a multi-modal vision-language setup with associated language prompts, inputs, or outputs. We demonstrate that our approach consistently enhances performance across \\emph{a diverse range of tasks}, encompassing pure 2D or 3D visual recognition tasks (e.g., image and point cloud classification), temporal modeling tasks (e.g., action recognition), non-semantic tasks (e.g., motion forecasting), and multi-modal tasks (e.g., 2D/3D visual question answering and image-text retrieval). Such improvements are a general phenomenon, applicable to various types of LLMs (e.g., LLaMA and OPT) and different LLM transformer blocks. We additionally propose the \\emph{information filtering} hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding -- the pre-trained LLM transformer blocks discern informative visual tokens and further amplify their effect. This hypothesis is empirically supported by the observation that the feature activation, after training with LLM transformer blocks, exhibits a stronger focus on relevant regions. We hope that our work inspires new perspectives on utilizing LLMs and deepening our understanding of their underlying mechanisms.",
  "abstract_zh": "本文揭示了大型语言模型（LLMs）尽管仅在文本数据上训练，但在没有语言的情况下，对于纯视觉任务却是\\emph{令人惊讶}的强大编码器。更有趣的是，这可以通过一种简单但之前被忽视的策略实现——使用来自\\emph{预训练} LLM的\\emph{冻结}变压器块作为组成编码层，直接处理视觉标记。我们的工作推动了利用LLMs进行计算机视觉任务的边界，显著偏离了通常需要多模态视觉-语言设置及相关语言提示、输入或输出的传统做法。我们证明了我们的方法在\\emph{多种任务}中持续提升性能，包括纯2D或3D视觉识别任务（如图像和点云分类）、时间建模任务（如动作识别）、非语义任务（如运动预测）和多模态任务（如2D/3D视觉问答和图像-文本检索）。这种改进是一种普遍现象，适用于各种类型的LLMs（如LLaMA和OPT）及不同的LLM变压器块。我们还提出了\\emph{信息过滤}假设，以解释预训练LLMs在视觉编码中的有效性——预训练的LLM变压器块能够辨别信息丰富的视觉标记并进一步放大其效果。该假设得到了实证支持，观察到经过LLM变压器块训练后的特征激活对相关区域的关注度更强。我们希望我们的工作能激发对利用LLMs的新视角，并加深对其基本机制的理解。"
}
{
  "title": "Measuring Vision-Language STEM Skills of Neural Models",
  "title_zh": "测量神经模型的视觉-语言STEM技能",
  "abstract": "We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes $448$ skills and $1,073,146$ questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills ($2.5$% in the third grade) in our dataset. In fact, these models are still well below (averaging $54.7$%) the performance of elementary students, not to mention near expert-level performance. To understand and increase the performance on our dataset, we teach the models on a training split of our dataset.\nEven though we observe improved performance, the model performance remains relatively low compared to average elementary students. To solve STEM problems, we will need novel algorithmic innovations from the community.",
  "abstract_zh": "我们提出了一项新的挑战，以测试神经模型的STEM技能。现实世界中的问题通常需要结合STEM（科学、技术、工程和数学）知识来寻找解决方案。与现有数据集不同，我们的数据集要求理解STEM的多模态视觉-语言信息。我们的数据集是该挑战中最大和最全面的数据集之一，包含448项技能和1,073,146个涵盖所有STEM学科的问题。与通常侧重于考察专家级能力的现有数据集相比，我们的数据集包括基于K-12课程设计的基础技能和问题。我们还将最新的基础模型，如CLIP和GPT-3.5-Turbo，添加到我们的基准测试中。结果表明，最近的模型进展仅帮助掌握我们数据集中非常有限的低年级技能（在三年级中为2.5%）。实际上，这些模型的表现仍远低于（平均54.7%）小学生的表现，更不用说接近专家级表现。为了理解和提高我们数据集上的表现，我们在数据集的训练分割上对模型进行了训练。尽管我们观察到性能有所提高，但与平均小学生相比，模型的表现仍然相对较低。要解决STEM问题，我们需要社区的创新算法。"
}
{
  "title": "Large Language Models Are Not Robust Multiple Choice Selectors",
  "title_zh": "大型语言模型并不是稳健的多项选择选择器",
  "abstract": "Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent “selection bias”, namely, they prefer to select specific option IDs as answers (like “Option A”). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs’ token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model’s prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.",
  "abstract_zh": "多项选择题（MCQs）作为评估大型语言模型（LLMs）的常见且重要的任务格式。本文表明，现代LLMs由于其固有的“选择偏见”，对MCQs中的选项位置变化非常脆弱，即它们倾向于选择特定的选项ID作为答案（如“选项A”）。通过对20个LLMs在三个基准上的广泛实证分析，我们指出这种行为偏见主要源于LLMs的标记偏见，即模型在从选项ID预测答案时，事先对特定选项ID标记（例如A/B/C/D）分配了更多的概率质量。为减轻选择偏见，我们提出了一种无标签的推理时去偏见方法，称为PriDe，它将模型对选项ID的先验偏见与整体预测分布分开。PriDe首先通过在少量测试样本上排列选项内容来估计先验，然后将估计的先验应用于去偏见其余样本。我们证明它以高计算效率实现了可解释和可转移的去偏见。我们希望这项工作能引起更广泛的研究关注，关注现代LLMs的偏见和稳健性。"
}
{
  "title": "ARGS: Alignment as Reward-Guided Search",
  "title_zh": "标题：ARGS：作为奖励引导搜索的对齐",
  "abstract": "Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive RL training. By adjusting the model’s probabilistic predictions using a reward signal, ARGS generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models. Notably, our method demonstrates consistent enhancements in average reward compared to baselines across diverse alignment tasks and various model dimensions. For example, under the same greedy-based decoding strategy, our method improves the average reward by 19.56% relative to the baseline and secures a preference or tie score of 64.33% in GPT-4 evaluation. We believe that our framework, emphasizing test-time alignment, paves the way for more responsive language models in the future. Code is publicly available at: https://github.com/deeplearning-wisc/args.",
  "abstract_zh": "摘要：将大型语言模型与人类目标对齐至关重要，但包括RLHF在内的常见方法在训练过程中存在不稳定和资源密集的问题。为应对这一挑战，我们提出了ARGS，即作为奖励引导搜索的对齐，这是一种将对齐集成到解码过程中的新框架，消除了昂贵的强化学习训练的需求。通过使用奖励信号调整模型的概率预测，ARGS生成具有语义多样性的文本，同时与人类偏好保持一致，为对齐语言模型提供了一种有前景且灵活的解决方案。值得注意的是，我们的方法在各种对齐任务和不同模型维度下，相较于基线显示出平均奖励的持续提升。例如，在相同的贪婪解码策略下，我们的方法相较于基线提高了19.56%的平均奖励，并在GPT-4评估中获得了64.33%的偏好或平局得分。我们相信，我们的框架强调测试时对齐，为未来更具响应性的语言模型铺平了道路。代码已公开可用，网址为：https://github.com/deeplearning-wisc/args。"
}
{
  "title": "Let Models Speak Ciphers: Multiagent Debate through Embeddings",
  "title_zh": "让模型说出密码：通过嵌入进行多智能体辩论",
  "abstract": "Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights, outperforming the state-of-the-art LLM debate methods using natural language by 0.5-5.0% across five reasoning tasks and multiple open-source LLMs of varying sizes. This showcases the superiority and robustness of embeddings as an alternative \"language\" for communication among LLMs. We anticipate that CIPHER will inspire further exploration for the design of interactions within LLM agent systems, offering a new direction that could significantly influence future developments in the field.",
  "abstract_zh": "大型语言模型（LLMs）之间的讨论和辩论因其增强推理能力的潜力而受到广泛关注。尽管自然语言由于LLM的语言理解能力而成为沟通的明显选择，但在生成自然语言时所需的令牌采样步骤存在信息丢失的潜在风险，因为它仅使用一个令牌来表示模型在整个词汇表中的信念。本文介绍了一种名为CIPHER（通过嵌入表示的模型间通信协议）的通信机制，以解决这一问题。具体而言，我们从LLM中移除了令牌采样步骤，让它们通过原始变换器输出嵌入的期望在词汇表中传达其信念。值得注意的是，通过偏离自然语言，CIPHER提供了编码更广泛信息的优势，而无需对模型权重进行任何修改，在五个推理任务和多个不同规模的开源LLM上，超越了使用自然语言的最先进LLM辩论方法0.5-5.0%。这展示了嵌入作为LLM之间沟通的替代“语言”的优越性和鲁棒性。我们预计CIPHER将激励进一步探索LLM代理系统内交互设计，提供一个可能显著影响该领域未来发展的新方向。"
}
{
  "title": "DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines",
  "title_zh": "DSPy：将声明式语言模型调用编译为最先进的管道",
  "abstract": "The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded “prompt templates”, i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, or imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric, by creating and collecting demonstrations. We conduct two case studies, showing that succinct DSPy programs can express and optimize pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, DSPy can automatically produce pipelines that outperform out-of-the-box few-shot prompting as well as expert-created demonstrations for GPT-3.5 and Llama2-13b-chat. On top of that, DSPy programs compiled for relatively small LMs like 770M parameter T5 and Llama2-13b-chat are competitive with many approaches that rely on large and proprietary LMs like GPT-3.5 and on expert-written prompt chains. DSPy is available at https://github.com/stanfordnlp/dspy",
  "abstract_zh": "机器学习社区正在迅速探索提示语言模型（LM）和将其堆叠成解决复杂任务的管道的技术。不幸的是，现有的LM管道通常使用硬编码的“提示模板”实现，即通过反复试验发现的冗长字符串。为了更系统地开发和优化LM管道，我们引入了DSPy，这是一种将LM管道抽象为文本转换图或命令式计算图的编程模型，其中通过声明式模块调用LM。DSPy模块是参数化的，这意味着它们可以学习如何应用提示、微调、增强和推理技术的组合。我们设计了一个编译器，可以优化任何DSPy管道，以最大化给定的指标，通过创建和收集演示。我们进行了两个案例研究，表明简洁的DSPy程序可以表达和优化关于数学文字问题的推理管道，处理多跳检索，回答复杂问题，并控制代理循环。在编译几分钟内，DSPy可以自动生成超越现成少量提示以及专家创建的GPT-3.5和Llama2-13b-chat演示的管道。此外，为相对较小的LM（如770M参数的T5和Llama2-13b-chat）编译的DSPy程序在与许多依赖大型和专有LM（如GPT-3.5）以及专家编写的提示链的许多方法上具有竞争力。DSPy可在https://github.com/stanfordnlp/dspy获取。"
}
{
  "title": "Branch-GAN: Improving Text Generation with (not so) Large Language Models",
  "title_zh": "分支生成对抗网络（Branch-GAN）：利用（不那么）大型语言模型改善文本生成",
  "abstract": "The current advancements in open domain text generation have been spearheaded by Transformer-based large language models. Leveraging efficient parallelization and vast training datasets, these models achieve unparalleled text generation capabilities. Even so, current models are known to suffer from  deficiencies such as repetitive texts, looping issues, and lack of robustness. While adversarial training through generative adversarial networks (GAN) is a proposed solution, earlier research in this direction has predominantly focused on older architectures, or narrow tasks. As a result, this approach is not yet compatible with modern language models for open-ended text generation, leading to diminished interest within the broader research community. We propose a computationally efficient GAN approach for sequential data that utilizes the parallelization capabilities of Transformer models. Our method revolves around generating multiple branching sequences from each training sample, while also incorporating the typical next-step prediction loss on the original data. In this way, we achieve a dense reward and loss signal for both the generator and the discriminator, resulting in a stable training dynamic. We apply our training method to pre-trained language models, using data from their original training set but less than 0.01% of the available data.  A comprehensive human evaluation shows that our method significantly improves the quality of texts generated by the model while avoiding the previously reported sparsity problems of GAN approaches. Even our smaller models outperform larger original baseline models with more than 16 times the number of parameters. Finally, we corroborate previous claims that perplexity on held-out data is not a sufficient metric for measuring the quality of generated texts.",
  "abstract_zh": "当前开放领域文本生成的进展主要由基于Transformer的大型语言模型推动。这些模型利用高效的并行化和庞大的训练数据集，实现了无与伦比的文本生成能力。然而，现有模型已知存在重复文本、循环问题和缺乏鲁棒性等缺陷。尽管通过生成对抗网络（GAN）进行对抗训练被提出作为解决方案，但早期的研究主要集中在较旧的架构或狭窄的任务上。因此，这种方法尚未与现代语言模型兼容，导致在更广泛的研究社区中兴趣减弱。我们提出了一种计算效率高的GAN方法，用于序列数据，利用Transformer模型的并行化能力。我们的方法围绕从每个训练样本生成多个分支序列，同时在原始数据上结合典型的下一步预测损失。通过这种方式，我们为生成器和判别器实现了密集的奖励和损失信号，从而产生稳定的训练动态。我们将训练方法应用于预训练的语言模型，使用其原始训练集中的数据，但仅使用不到0.01%的可用数据。全面的人类评估表明，我们的方法显著提高了模型生成文本的质量，同时避免了GAN方法之前报告的稀疏性问题。即使是我们较小的模型也超过了参数数量超过16倍的较大原始基线模型。最后，我们证实了之前的观点，即在保留数据上的困惑度并不是衡量生成文本质量的充分指标。"
}
{
  "title": "Learning Interactive Real-World Simulators",
  "title_zh": "学习互动真实世界模拟器",
  "abstract": "Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as “open the drawer” and low-level controls such as “move by x,y” from otherwise static scenes and objects. There are numerous use cases for such a real-world simulator. As an example, we use UniSim to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit zero-shot real-world transfer after training purely in a learned real-world simulator. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience in UniSim, opening up even wider applications.",
  "abstract_zh": "基于互联网数据训练的生成模型彻底改变了文本、图像和视频内容的创作方式。生成模型的下一个里程碑可能是模拟人类、机器人和其他互动代理所采取行动的真实体验。真实世界模拟器的应用范围从游戏和电影中的可控内容创作，到在模拟中纯粹训练的具身代理，这些代理可以直接部署到现实世界中。我们探索通过生成建模学习通用模拟器（UniSim）以实现真实世界交互的可能性。我们首先观察到，用于学习真实世界模拟器的自然数据集通常在不同维度上非常丰富（例如，图像数据中丰富的物体、机器人数据中密集采样的动作以及导航数据中多样的运动）。通过精心协调不同的数据集，每个数据集提供整体体验的不同方面，UniSim能够模拟人类和代理如何与世界互动，模拟高层指令（如“打开抽屉”）和低层控制（如“移动到x,y”）的视觉结果，这些指令和控制来自静态场景和物体。这样的真实世界模拟器有许多用例。例如，我们使用UniSim训练高层视觉-语言规划器和低层强化学习策略，每个策略在纯粹在学习的真实世界模拟器中训练后都表现出零样本的真实世界迁移。我们还展示了其他类型的智能（如视频字幕模型）可以通过在UniSim中训练模拟经验而受益，从而开辟更广泛的应用。"
}
{
  "title": "SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series",
  "title_zh": "社会道场：构建具备终身学习能力的现实文本和时间序列分析代理",
  "abstract": "We introduce SocioDojo, an open-ended lifelong learning environment for developing ready-to-deploy autonomous agents capable of performing human-like analysis and decision-making on societal topics such as economics, finance, politics, and culture. It consists of (1) information sources from news, social media, reports, etc., (2) a knowledge base built from books, journals, and encyclopedias, plus a toolbox of Internet and knowledge graph search interfaces, (3) 30K high-quality time series in finance, economy, society, and polls, which support a novel task called \"hyperportfolio\", that can reliably and scalably evaluate societal analysis and decision-making power of agents, inspired by portfolio optimization with time series as assets to \"invest\". We also propose a novel Analyst-Assistant-Actuator architecture for the hyperportfolio task, and a Hypothesis & Proof prompting for producing in-depth analyses on input news, articles, etc. to assist decision-making. We perform experiments and ablation studies to explore the factors that impact performance. The results show that our proposed method achieves improvements of 32.4% and 30.4% compared to the state-of-the-art method in the two experimental settings.",
  "abstract_zh": "我们介绍了社会道场，这是一个开放式的终身学习环境，用于开发能够在经济、金融、政治和文化等社会主题上进行类人分析和决策的自主代理。它包括（1）来自新闻、社交媒体、报告等的信息源，（2）由书籍、期刊和百科全书构建的知识库，以及一套互联网和知识图谱搜索接口的工具箱，（3）30,000个高质量的金融、经济、社会和民意调查时间序列，支持一种称为“超投资组合”的新任务，该任务可以可靠且可扩展地评估代理的社会分析和决策能力，灵感来源于将时间序列作为“投资”资产的投资组合优化。我们还提出了一种新颖的分析师-助手-执行器架构用于超投资组合任务，以及一种假设与证明提示，用于对输入的新闻、文章等进行深入分析以辅助决策。我们进行了实验和消融研究，以探讨影响性能的因素。结果表明，我们提出的方法在两个实验设置中相比于最先进的方法分别提高了32.4%和30.4%。"
}
{
  "title": "Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models",
  "title_zh": "准确的无重训练剪枝方法用于预训练编码器基础的语言模型",
  "abstract": "Given a pretrained encoder-based language model, how can we accurately compress it without retraining? Retraining-free structured pruning algorithms are crucial in pretrained language model compression due to their significantly reduced pruning cost and capability to prune large language models. However, existing retraining-free algorithms encounter severe accuracy degradation, as they fail to handle pruning errors, especially at high compression rates. In this paper, we propose KPrune (Knowledge-preserving pruning), an accurate retraining-free structured pruning algorithm for pretrained encoder-based language models.\nKPrune focuses on preserving the useful knowledge of the pretrained model to minimize pruning errors through a carefully designed iterative pruning process composed of knowledge measurement, knowledge-preserving mask search, and knowledge-preserving weight-tuning. As a result, KPrune shows significant accuracy improvements up to 58.02%p higher F1 score compared to existing retraining-free pruning algorithms under a high compression rate of 80% on the SQuAD benchmark without any retraining process.",
  "abstract_zh": "针对预训练编码器基础的语言模型，如何在不重训练的情况下准确压缩它？无重训练的结构化剪枝算法在预训练语言模型压缩中至关重要，因为它们显著降低了剪枝成本，并能够剪枝大型语言模型。然而，现有的无重训练算法面临严重的准确性下降，因为它们未能处理剪枝错误，特别是在高压缩率下。本文提出了KPrune（知识保留剪枝），这是一种针对预训练编码器基础语言模型的准确无重训练结构化剪枝算法。KPrune专注于保留预训练模型的有用知识，通过精心设计的迭代剪枝过程（包括知识测量、知识保留掩码搜索和知识保留权重调整）来最小化剪枝错误。因此，KPrune在SQuAD基准测试中，在80%的高压缩率下相比现有的无重训练剪枝算法，F1得分提高了58.02个百分点，显示出显著的准确性提升，而无需任何重训练过程。"
}
{
  "title": "Learning to Act without Actions",
  "title_zh": "学习在没有动作的情况下进行行动",
  "abstract": "Pre-training large models on vast amounts of web data has proven to be an effective approach for obtaining powerful, general models in domains such as language and vision. However, this paradigm has not yet taken hold in reinforcement learning. This is because videos, the most abundant form of embodied behavioral data on the web, lack the action labels required by existing methods for imitating behavior from demonstrations. We introduce *Latent Action Policies* (LAPO), a method for recovering latent action information—and obtaining latent-action policies, world models, and inverse dynamics models—purely from videos. LAPO is the first method able to recover the structure of the true action space purely from observed dynamics, even in challenging procedurally-generated environments. Further,  LAPO's latent-action policies can be rapidly turned into regular, expert-level policies, either offline using a small action-labeled dataset, or online via rewards. LAPO is the first step towards pre-training powerful, generalist policies and world models on the vast amounts of videos readily available on the web.",
  "abstract_zh": "在大量网络数据上对大型模型进行预训练已被证明是获取强大通用模型的有效方法，尤其在语言和视觉等领域。然而，这一范式尚未在强化学习中得到应用，因为视频作为网络上最丰富的具体现行为数据，缺乏现有模仿行为所需的动作标签。我们提出了*潜在动作策略*（LAPO），这是一种仅通过视频恢复潜在动作信息的方法，并获得潜在动作策略、世界模型和逆动态模型。LAPO是首个能够仅通过观察动态从而恢复真实动作空间结构的方法，即使在具有挑战性的程序生成环境中也能实现。此外，LAPO的潜在动作策略可以迅速转化为常规的专家级策略，既可以离线使用小型动作标签数据集，也可以通过奖励在线实现。LAPO是朝着在网络上大量可用视频上预训练强大通用策略和世界模型迈出的第一步。"
}
{
  "title": "SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore",
  "title_zh": "SILO语言模型：在非参数数据存储中隔离法律风险",
  "abstract": "The legality of training language models (LMs) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. We present SILO, a new language model that manages this risk-performance tradeoff during inference. SILO is built by (1) training a parametric LM on the Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. These capabilities can foster compliance with data-use regulations such as the fair use doctrine in the United States and the GDPR in the European Union. Our experiments show that the parametric LM struggles on its own with domains not covered by OLC. However, access to the datastore greatly improves out of domain performance, closing 90% of the performance gap with an LM trained on the Pile, a more diverse corpus with mostly high-risk text. We also analyze which nonparametric approach works best, where the remaining errors lie, and how performance scales with datastore size. Our results suggest that it is possible to build high quality language models while mitigating legal risk.",
  "abstract_zh": "训练语言模型（LM）使用受版权保护或其他限制的数据的合法性正在激烈辩论。然而，正如我们所展示的，如果仅在低风险文本（例如，过期版权书籍或政府文件）上训练，模型性能会显著下降，因为其规模和领域覆盖有限。我们提出了SILO，这是一种在推理过程中管理风险与性能权衡的新语言模型。SILO的构建包括（1）在开放许可语料库（OLC）上训练一个参数化LM，OLC是我们整理的一个新语料库，包含2280亿个公共领域和宽松许可文本的标记，以及（2）用一个更通用且易于修改的非参数数据存储进行增强（例如，包含受版权保护的书籍或新闻），该数据存储仅在推理期间查询。该数据存储允许在不进行训练的情况下使用高风险数据，支持句子级数据归属，并使数据生产者能够通过从存储中删除内容来选择退出模型。这些能力可以促进遵守数据使用法规，例如美国的合理使用原则和欧盟的GDPR。我们的实验表明，参数化LM在OLC未覆盖的领域中独自表现不佳。然而，访问数据存储显著改善了领域外性能，缩小了与在Pile上训练的LM（一个包含大多数高风险文本的更为多样化的语料库）之间90%的性能差距。我们还分析了哪种非参数方法效果最佳，剩余错误的来源，以及性能如何随数据存储大小的变化而变化。我们的结果表明，在降低法律风险的同时构建高质量语言模型是可能的。"
}
{
  "title": "Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift",
  "title_zh": "理解多模态对比学习对分布变化的鲁棒性",
  "abstract": "Recently, multimodal contrastive learning (MMCL) approaches, such as CLIP, have achieved a remarkable success in learning representations that are robust against distribution shift and generalize to new domains. Despite the empirical success, the mechanism behind learning such generalizable representations is not understood. In this work, we rigorously analyze this problem and \nuncover two mechanisms behind MMCL's robustness: \\emph{intra-class contrasting}, which allows the model to learn features with a high variance, and \\emph{inter-class feature sharing}, where annotated details in one class help learning other classes better. Both mechanisms prevent spurious features that are over-represented in the training data to overshadow the generalizable core features. This yields superior zero-shot classification accuracy under distribution shift. Furthermore, we theoretically demonstrate the benefits of using rich captions on robustness and explore the effect of annotating different types of details in the captions. We validate our theoretical findings through experiments, including a well-designed synthetic experiment and an experiment involving training CLIP models on MSCOCO/Conceptual Captions and evaluating them on shifted ImageNets.",
  "abstract_zh": "最近，多模态对比学习（MMCL）方法，如CLIP，在学习对分布变化鲁棒且能泛化到新领域的表示方面取得了显著成功。尽管经验上取得了成功，但学习这种可泛化表示的机制尚不清楚。在这项工作中，我们严格分析了这个问题，并揭示了MMCL鲁棒性的两个机制：\\emph{类内对比}，使模型能够学习具有高方差的特征，以及\\emph{类间特征共享}，其中一个类中的注释细节有助于更好地学习其他类。这两种机制防止了在训练数据中过度代表的虚假特征掩盖可泛化的核心特征。这在分布变化下产生了优越的零-shot 分类准确性。此外，我们从理论上证明了使用丰富标题对鲁棒性的好处，并探讨了在标题中注释不同类型细节的影响。我们通过实验验证了我们的理论发现，包括一个精心设计的合成实验和一个涉及在MSCOCO/概念标题上训练CLIP模型并在变化的ImageNets上评估它们的实验。"
}
{
  "title": "DistillSpec: Improving Speculative Decoding via Knowledge Distillation",
  "title_zh": "标题：DistillSpec：通过知识蒸馏改善推测解码",
  "abstract": "Speculative decoding~(SD) accelerates large language model inference by employing a faster {\\em draft} model for generating multiple tokens, which are then verified in parallel by the larger {\\em target} model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose {\\em DistillSpec} that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improve the draft and target alignment: utilizing \\emph{on-policy} data generation from the draft model, and \\emph{tailoring the divergence function} to the task and decoding strategy. Notably, DistillSpec yields impressive $10 - 45\\%$ speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling. Furthermore, we combine DistillSpec with lossy SD to achieve fine-grained control over the latency vs. task performance trade-off. Finally, in practical scenarios with models of varying sizes, first using distillation to boost the performance of the target model and then applying DistillSpec to train a well-aligned draft model can reduce decoding latency by $6 - 10\\times$ with minimal performance drop, compared to standard decoding without distillation.",
  "abstract_zh": "摘要：推测解码（SD）通过使用更快的“草稿”模型生成多个标记，从而加速大型语言模型推理，这些标记随后由更大的“目标”模型并行验证，生成符合目标模型分布的文本。然而，识别与目标模型良好对齐的紧凑草稿模型是一个挑战。为了解决这个问题，我们提出了DistillSpec，它在应用SD之前使用知识蒸馏来更好地对齐草稿模型和目标模型。DistillSpec做出了两个关键设计选择，我们通过系统研究证明这些选择对于改善草稿和目标对齐至关重要：利用草稿模型的“在线”数据生成，以及根据任务和解码策略“定制发散函数”。值得注意的是，DistillSpec在一系列标准基准测试中，相较于标准SD实现了10%到45%的显著加速，使用了贪婪和非贪婪采样。此外，我们将DistillSpec与有损SD结合，以实现延迟与任务性能权衡的细粒度控制。最后，在具有不同规模模型的实际场景中，首先使用蒸馏提升目标模型的性能，然后应用DistillSpec训练一个良好对齐的草稿模型，可以将解码延迟减少6到10倍，同时性能下降最小，相较于未进行蒸馏的标准解码。"
}
{
  "title": "Intriguing Properties of Generative Classifiers",
  "title_zh": "生成分类器的有趣特性",
  "abstract": "What is the best paradigm to recognize objects---discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data.\nWe report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.",
  "abstract_zh": "我们探讨了识别物体的最佳范式——判别推理（快速但可能容易产生捷径学习）或使用生成模型（慢但可能更稳健）。我们基于最近在生成建模方面的进展，将文本到图像模型转变为分类器。这使我们能够研究它们的行为，并将其与判别模型和人类心理物理数据进行比较。我们报告了生成分类器的四个引人注目的新兴特性：它们表现出创纪录的人类形状偏见（Imagen的准确率为99%），接近人类水平的分布外准确性，领先的与人类分类错误的对齐程度，以及它们理解某些感知错觉。我们的结果表明，尽管当前主导的人类物体识别建模范式是判别推理，但零样本生成模型在近似人类物体识别数据方面表现得出乎意料地好。"
}
{
  "title": "Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE",
  "title_zh": "标题：Octavius：通过LoRA-MoE减轻MLLM中的任务干扰",
  "abstract": "Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, to mitigate the interference, we combine the concept of Mixture-of-Experts (MoE) with LoRA and design a multimodal LoRA-MoE decoder for task- and modality-specific learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and corresponding dataset will be available\nsoon.",
  "abstract_zh": "摘要：最近的研究表明，大型语言模型（LLMs）可以通过指令调优将其零样本泛化能力扩展到多模态学习。随着更多模态和下游任务的引入，负面冲突和干扰可能对性能产生更严重的影响。虽然这一现象在之前的工作中被忽视，但我们提出了一种新颖且可扩展的框架，称为Octavius，用于对多模态大型语言模型（MLLMs）进行全面研究和实验。具体而言，为了减轻干扰，我们将专家混合（MoE）的概念与LoRA结合，设计了一种多模态LoRA-MoE解码器，以实现任务和模态特定的学习。据我们所知，我们是将MoE引入MLLM以解决此问题的先锋之一。实验结果（约20%的改进）显示了我们设计在各种2D和3D下游任务中的有效性和多样性。代码和相应的数据集将很快发布。"
}
{
  "title": "Masked Structural Growth for 2x Faster Language Model Pre-training",
  "title_zh": "标题：掩蔽结构增长实现2倍快速的语言模型预训练",
  "abstract": "Accelerating large language model pre-training is a critical issue in present research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems associated with progressive growth: determining the optimal growth schedule, and designing efficient growth operators. In terms of growth schedule, the impact of each single dimension on a schedule’s efficiency is underexplored by existing work. Regarding the growth operators, existing methods rely on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further improvements on training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including (i) growth schedules involving all possible dimensions and (ii) strictly function-preserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is significantly faster than related work: we achieve up to 2.2x speedup in pre-training different types of language models while maintaining comparable or better downstream performances. Code is publicly available at https://github.com/cofe-ai/MSG.",
  "abstract_zh": "摘要：加速大型语言模型的预训练是当前研究中的一个关键问题。本文集中于通过逐步从小型Transformer结构增长到大型结构来加速预训练。与逐步增长相关的主要研究问题有两个：确定最佳增长计划和设计高效的增长操作符。在增长计划方面，现有研究对单一维度对计划效率的影响探讨不足。关于增长操作符，现有方法依赖于新权重的初始化来继承知识，仅实现非严格的函数保持，限制了训练动态的进一步改进。为了解决这些问题，我们提出了掩蔽结构增长（MSG），包括（i）涉及所有可能维度的增长计划和（ii）与新权重初始化无关的严格函数保持增长操作符。实验表明，MSG的速度显著快于相关工作：在保持可比或更好下游性能的同时，我们在不同类型语言模型的预训练中实现了最高2.2倍的加速。代码已公开发布在 https://github.com/cofe-ai/MSG。"
}
{
  "title": "Understanding Addition in Transformers",
  "title_zh": "理解变压器中的加法",
  "abstract": "Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper provides a comprehensive analysis of a one-layer Transformer model trained to perform n-digit integer addition. Our findings suggests that the model dissects the task into parallel streams dedicated to individual digits, employing varied algorithms tailored to different positions within the digits. Furthermore, we identify a rare scenario characterized by high loss, which we explain. By thoroughly elucidating the model’s algorithm, we provide new insights into its functioning. These findings are validated through rigorous testing and mathematical modeling, thereby contributing to the broader fields of model understanding and interpretability. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.",
  "abstract_zh": "理解像变压器这样的机器学习模型的内部工作机制对于其安全和伦理使用至关重要。本文对一个训练用于执行n位整数加法的一层变压器模型进行了全面分析。我们的研究结果表明，该模型将任务分解为专门针对各个数字的并行流，采用针对数字不同位置的多种算法。此外，我们识别出一个特征为高损失的罕见场景，并对此进行了解释。通过全面阐明模型的算法，我们提供了对其功能的新见解。这些发现通过严格的测试和数学建模得到了验证，从而为模型理解和可解释性等更广泛领域做出了贡献。我们的方法为分析更复杂的任务和多层变压器模型打开了大门。"
}
{
  "title": "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition",
  "title_zh": "通用命名实体识别：来自大型语言模型的针对性蒸馏",
  "abstract": "Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We release the distillation recipe, data, and UniversalNER models to facilitate future research on targeted distillation.",
  "abstract_zh": "大型语言模型（LLMs）展示了显著的泛化能力，例如理解任意实体和关系。指令调优已被证明对将LLMs蒸馏为更具成本效益的模型（如Alpaca和Vicuna）有效。然而，这些学生模型在下游应用中仍然远远落后于原始LLMs。本文探讨了通过任务导向的指令调优进行针对性蒸馏，以训练能够在开放信息提取等广泛应用类别中表现出色的学生模型。以命名实体识别（NER）为案例研究，我们展示了如何将ChatGPT蒸馏为更小的UniversalNER模型以实现开放NER。为了评估，我们组建了迄今为止最大的NER基准，涵盖来自生物医学、编程、社交媒体、法律、金融等9个不同领域的43个数据集。在不使用任何直接监督的情况下，UniversalNER在数万个实体类型上达到了显著的NER准确性，平均超越了Alpaca和Vicuna等通用指令调优模型超过30个绝对F1点。凭借极少的参数，UniversalNER不仅获得了ChatGPT识别任意实体类型的能力，还在NER准确性上平均超越了7-9个绝对F1点。值得注意的是，UniversalNER甚至在很大程度上超越了使用监督NER示例的最先进的多任务指令调优系统，如InstructUIE。我们还进行了全面的消融研究，以评估蒸馏方法中各个组件的影响。我们发布了蒸馏配方、数据和UniversalNER模型，以促进未来针对性蒸馏的研究。"
}
{
  "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation",
  "title_zh": "开源大语言模型的灾难性越狱：利用生成进行攻击",
  "abstract": "The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as ``jailbreaks\". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the attack success rate from $0\\%$ to more than $95\\%$ across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\\times$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the attack success rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models.",
  "abstract_zh": "开源大语言模型（LLMs）的快速进展显著推动了人工智能的发展。在模型发布之前，已经进行了大量努力以使其行为与人类价值观保持一致，主要目标是确保其有益和无害。然而，即使是经过仔细对齐的模型也可能被恶意操控，导致意外行为，这被称为“越狱”。这些越狱通常是由特定的文本输入触发的，通常称为对抗性提示。在本研究中，我们提出了一种生成利用攻击，这是一种极其简单的方法，仅通过操控解码方法的变体来破坏模型对齐。通过利用不同的生成策略，包括变化解码超参数和采样方法，我们将11个语言模型（包括LLaMA2、Vicuna、Falcon和MPT系列）的攻击成功率从$0\\%$提高到超过$95\\%$，并且在计算成本上比最先进的攻击低$30\\times$。最后，我们提出了一种有效的对齐方法，探索多样的生成策略，可以合理降低我们攻击下的成功率。总的来说，我们的研究强调了当前开源LLMs安全评估和对齐程序中的重大失败，强烈倡导在发布此类模型之前进行更全面的红队测试和更好的对齐。"
}
{
  "title": "Demystifying Embedding Spaces using Large Language Models",
  "title_zh": "揭示大型语言模型下的嵌入空间",
  "abstract": "Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing large language models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decoding user preferences in recommender systems. Our work couples the immense information potential of embeddings with the interpretative power of LLMs.",
  "abstract_zh": "嵌入已成为以简洁且有用的格式表示实体、概念和关系的复杂多面信息的关键手段。然而，它们通常难以直接解释。尽管下游任务利用这些压缩表示， meaningful interpretation 通常需要使用降维或专门的机器学习可解释性方法进行可视化。本文通过使用大型语言模型（LLMs）直接与嵌入交互，解决了使这些嵌入更具可解释性和广泛实用性的挑战——将抽象向量转化为可理解的叙述。通过将嵌入注入LLMs，我们能够查询和探索复杂的嵌入数据。我们在多种不同任务上展示了我们的方法，包括：增强概念激活向量（CAVs）、传达新嵌入实体以及解码推荐系统中的用户偏好。我们的工作将嵌入的巨大信息潜力与LLMs的解释能力结合在一起。"
}
{
  "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
  "title_zh": "教大型语言模型钓鱼：从语言模型中窃取私人信息",
  "abstract": "When large language models are trained on private data, it can be a _significant_ privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new _practical_ data extraction attack that we call ``neural phishing''. This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of $10$% secret extraction rates, at times, as high as $80$%. Our attack assumes only that an adversary can insert only $10$s of benign-appearing sentences into the training dataset \nusing only vague priors on the structure of the user data.",
  "abstract_zh": "当大型语言模型在私人数据上进行训练时，记忆和重复敏感信息可能会带来显著的隐私风险。在本研究中，我们提出了一种新的实用数据提取攻击，称为“神经钓鱼”。这种攻击使得对手能够针对并提取敏感或个人可识别信息（PII），例如信用卡号码，从一个在用户数据上训练的模型中，秘密提取率高达10%以上，有时甚至高达80%。我们的攻击仅假设对手可以向训练数据集中插入数十个看似无害的句子，并仅使用对用户数据结构的模糊先验知识。"
}
{
  "title": "Learning with Language-Guided State Abstractions",
  "title_zh": "基于语言引导的状态抽象学习",
  "abstract": "We describe a framework for using natural language to design state abstractions for imitation learning.\nGeneralizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones.\nThese state representations are typically manually specified, or derived from other labor-intensive labeling procedures.\nOur method, LGA (\\textit{language-guided abstraction}), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks.\nIn LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demonstrations and LGA-generated abstract states. \nExperiments on simulated robotic tasks show that LGA yields state abstractions similar to those designed by humans, but in a fraction of the time, and that these abstractions improve generalization and robustness in the presence of spurious correlations and ambiguous specifications.\nWe illustrate the utility of the learned abstractions on mobile manipulation tasks with a Spot robot.",
  "abstract_zh": "我们描述了一个框架，用于利用自然语言设计模仿学习的状态抽象。在高维观察空间中，良好设计的状态表示促进了可泛化的策略学习，这些表示能够突出环境的重要特征并隐藏无关特征。这些状态表示通常是手动指定的，或从其他劳动密集型标注程序中派生而来。我们的方法LGA（语言引导抽象）结合了自然语言监督和来自语言模型（LM）的背景知识，自动构建针对未见任务的状态表示。在LGA中，用户首先提供一个（可能不完整的）目标任务的自然语言描述；接下来，预训练的LM将该任务描述翻译为一个状态抽象函数，以屏蔽无关特征；最后，使用少量示范和LGA生成的抽象状态训练模仿策略。在模拟机器人任务上的实验表明，LGA生成的状态抽象与人类设计的相似，但所需时间却少得多，并且这些抽象在存在虚假相关性和模糊规范的情况下提高了泛化能力和鲁棒性。我们在Spot机器人移动操作任务中展示了学习到的抽象的实用性。"
}
{
  "title": "Large Language Models as Tool Makers",
  "title_zh": "大型语言模型作为工具制造者",
  "abstract": "Recent research has highlighted the potential of large language models (LLMs)\nto improve their problem-solving capabilities with the aid of suitable external\ntools. In our work, we further advance this concept by introducing a closed-\nloop framework, referred to as LLMs A s Tool Makers (LATM), where LLMs\ncreate their own reusable tools for problem-solving. Our approach consists of two\nphases: 1) tool making: an LLM acts as the tool maker that crafts tools for a set\nof tasks, where a tool is implemented as a Python utility function. 2) tool using:\nanother LLM acts as the tool user, which applies the tool built by the tool maker\nfor problem-solving. The tool user can be either the same or a different LLM\nfrom the tool maker. On the problem-solving server side, tool-making enables\ncontinual tool generation and caching as new requests emerge. This framework\nenables subsequent requests to access cached tools via their corresponding APIs,\nenhancing the efficiency of task resolution. Beyond enabling LLMs to create their\nown tools, our framework also uncovers intriguing opportunities to optimize the\nserving cost of LLMs: Recognizing that tool-making requires more sophisticated\ncapabilities, we assign this task to a powerful, albeit resource-intensive, model.\nConversely, the simpler tool-using phase is delegated to a lightweight model. This\nstrategic division of labor allows the once-off cost of tool-making to be spread\nover multiple instances of tool-using, significantly reducing average costs while\nmaintaining strong performance. Furthermore, our method offers a functional\ncache through the caching and reuse of tools, which stores the functionality of\na class of requests instead of the natural language responses from LLMs, thus\nextending the applicability of the conventional cache mechanism. We evaluate\nour approach across various complex reasoning tasks, including Big-Bench tasks.\nWith GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM demonstrates\nperformance equivalent to using GPT-4 for both roles, but with a significantly\nreduced inference cost.",
  "abstract_zh": "最近的研究强调了大型语言模型（LLMs）在适当外部工具的帮助下提高其问题解决能力的潜力。在我们的工作中，我们通过引入一个闭环框架（称为LLMs作为工具制造者，LATM）进一步推进这一概念，其中LLMs为问题解决创建自己的可重用工具。我们的方法分为两个阶段：1）工具制造：LLM作为工具制造者，为一组任务制作工具，工具以Python实用函数的形式实现。2）工具使用：另一个LLM作为工具用户，应用由工具制造者构建的工具进行问题解决。工具用户可以是与工具制造者相同或不同的LLM。在问题解决服务器端，工具制造使得随着新请求的出现，可以持续生成和缓存工具。该框架使后续请求能够通过相应的API访问缓存工具，从而提高任务解决的效率。除了使LLMs能够创建自己的工具外，我们的框架还揭示了优化LLMs服务成本的有趣机会：认识到工具制造需要更复杂的能力，我们将这一任务分配给一个强大但资源密集的模型。相反，简单的工具使用阶段则委托给一个轻量级模型。这种战略性分工使得一次性工具制造的成本可以分摊到多个工具使用实例上，显著降低平均成本，同时保持强大的性能。此外，我们的方法通过工具的缓存和重用提供了一个功能缓存，该缓存存储一类请求的功能，而不是LLMs的自然语言响应，从而扩展了传统缓存机制的适用性。我们在各种复杂推理任务上评估了我们的方法，包括Big-Bench任务。以GPT-4作为工具制造者，GPT-3.5作为工具用户，LATM展示了与同时使用GPT-4作为两个角色的性能相当，但推理成本显著降低。"
}
{
  "title": "Guess & Sketch: Language Model Guided Transpilation",
  "title_zh": "标题：猜测与草图：语言模型指导的转译",
  "abstract": "Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze.\nExisting conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e.  automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code. Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods. Guess & Sketch extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. We test Guess & Sketch on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler. We also share a training and evaluation dataset for this task.",
  "abstract_zh": "摘要：维护遗留软件需要大量的软件和系统工程时间。汇编代码程序对计算机机器状态的低级控制要求很高且没有变量名称，这使得人类分析变得特别困难。现有的传统程序翻译器保证正确性，但是针对特定源和目标编程语言手工设计的。学习型转译，即代码的自动翻译，为手动重写和工程工作提供了一种替代方案。自动符号程序翻译方法保证正确性，但由于指数级大的搜索空间，难以扩展到更长的程序。它们的刚性规则系统也限制了它们的表达能力，因此只能对减少的程序空间进行推理。概率神经语言模型（LM）为每个输入生成合理的输出，但以牺牲保证正确性为代价。在这项工作中，我们利用LM和符号求解器的优势，采用神经符号方法进行汇编代码的学习转译。汇编代码是神经符号方法的合适场景，因为汇编代码可以被划分为较短的非分支基本块，适合使用符号方法。猜测与草图从LM的特征中提取对齐和置信度信息，然后将其传递给符号求解器，以解决转译输入和输出的语义等价性。我们在三个不同难度的汇编转译任务测试了猜测与草图，结果显示它成功转译的示例比GPT-4多57.6%，比工程化转译器多39.6%。我们还分享了该任务的训练和评估数据集。"
}
{
  "title": "Zero and Few-shot Semantic Parsing with Ambiguous Inputs",
  "title_zh": "零样本和少样本语义解析中的模糊输入",
  "abstract": "Despite the frequent challenges posed by ambiguity when representing meaning via natural language, it is often ignored or deliberately removed in tasks mapping language to formally-designed representations, which generally assume a one-to-one mapping between linguistic and formal representations. \nWe attempt to address this shortcoming by introducing AmP, a framework, dataset, and challenge for translating ambiguous natural language to formal representations like logic and code. \nWe define templates and generate data for five well-documented linguistic ambiguities.\nUsing AmP, we investigate how several few-shot text-to-code systems handle ambiguity, introducing three new metrics.\nWe find that large pre-trained models perform poorly at capturing the distribution of possible meanings without deliberate instruction.\nHowever, models are able to capture the distribution well when ambiguity is attested in their inputs. \nThese results motivate a call for including ambiguity explicitly in datasets and promote considering the distribution of possible outputs when evaluating systems. We release our data and code.",
  "abstract_zh": "尽管在通过自然语言表示意义时模糊性常常带来挑战，但在将语言映射到正式设计的表示时，这种模糊性通常被忽视或故意去除，而这些任务通常假设语言和正式表示之间存在一对一的映射。我们试图通过引入AmP，一个用于将模糊自然语言翻译为逻辑和代码等正式表示的框架、数据集和挑战，来解决这一不足。我们定义了模板并为五种文献中记录良好的语言模糊性生成数据。使用AmP，我们研究了几个少样本文本到代码系统如何处理模糊性，并引入了三个新指标。我们发现，大型预训练模型在没有明确指示的情况下，捕捉可能意义的分布表现不佳。然而，当模糊性在其输入中得到确认时，模型能够很好地捕捉分布。这些结果促使我们呼吁在数据集中明确包含模糊性，并在评估系统时考虑可能输出的分布。我们发布了我们的数据和代码。"
}
{
  "title": "Learning Grounded Action Abstractions from Language",
  "title_zh": "从语言中学习基础行动抽象",
  "abstract": "Effective planning in the real world requires not only world knowledge, but the ability to leverage that knowledge to build the right representation of the task at hand. Decades of hierarchical planning techniques have used domain-specific temporal action abstractions to support efficient and accurate planning, almost always relying on human priors and domain knowledge to decompose hard tasks into smaller subproblems appropriate for a goal or set of goals. This paper describes Ada (Action Domain Acquisition), a framework for automatically constructing task-specific planning representations using task-general background knowledge from language models (LMs). Starting with a general-purpose hierarchical planner and a low-level goal-conditioned policy, Ada interactively learns a library of planner-compatible high-level action abstractions and low-level controllers adapted to a particular domain of planning tasks. On two language-guided interactive planning benchmarks (Mini Minecraft and ALFRED Household Tasks), Ada strongly outperforms other approaches that use LMs for sequential decision-making, offering more accurate plans and better generalization to complex tasks.",
  "abstract_zh": "有效的现实世界规划不仅需要世界知识，还需要利用这些知识构建适当任务表示的能力。几十年来，分层规划技术利用特定领域的时间行动抽象来支持高效和准确的规划，几乎总是依赖人类先验知识和领域知识将复杂任务分解为适合目标或目标集的小子问题。本文描述了Ada（行动领域获取），一个使用语言模型（LM）中的任务通用背景知识自动构建任务特定规划表示的框架。Ada从通用分层规划器和低级目标条件策略开始，交互式地学习与特定规划任务领域相适应的高层次行动抽象和低级控制器库。在两个语言引导的交互式规划基准（Mini Minecraft和ALFRED家庭任务）上，Ada的表现远超其他使用LM进行顺序决策的方法，提供了更准确的计划和更好的复杂任务泛化能力。"
}
{
  "title": "LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models",
  "title_zh": "布局NUWA：揭示大型语言模型的隐含布局专业知识",
  "abstract": "Graphic layout generation, a growing research field, plays a significant role in user engagement and information perception. \nExisting methods primarily treat layout generation as a numerical optimization task, focusing on quantitative aspects while overlooking the semantic information of layout, such as the relationship between each layout element. \nIn this paper, we propose LayoutNUWA, the first model that treats layout generation as a code generation task to enhance semantic information and harness the hidden layout expertise of large language models~(LLMs). \nConcretely, we develop a Code Instruct Tuning (CIT) approach comprising three interconnected modules: 1) the Code Initialization (CI) module quantifies the numerical conditions and initializes them as HTML code with strategically placed masks; 2) the Code Completion (CC) module employs the formatting knowledge of LLMs to fill in the masked portions within the HTML code; 3) the Code Rendering (CR) module transforms the completed code into the final layout output, ensuring a highly interpretable and transparent layout generation procedure that directly maps code to a visualized layout. We attain significant state-of-the-art performance (even over 50\\% improvements compared to previous works) on multiple datasets, showcasing the strong capabilities of LayoutNUWA.",
  "abstract_zh": "图形布局生成是一个日益增长的研究领域，在用户参与和信息感知中发挥着重要作用。现有方法主要将布局生成视为数值优化任务，关注定量方面而忽视了布局的语义信息，例如各布局元素之间的关系。本文提出了布局NUWA，这是第一个将布局生成视为代码生成任务的模型，以增强语义信息并利用大型语言模型（LLMs）的隐含布局专业知识。具体而言，我们开发了一种代码指令调优（CIT）方法，包括三个相互关联的模块：1）代码初始化（CI）模块量化数值条件，并将其初始化为带有战略性掩码的HTML代码；2）代码补全（CC）模块利用LLMs的格式知识填充HTML代码中的掩码部分；3）代码渲染（CR）模块将完成的代码转换为最终布局输出，确保布局生成过程高度可解释和透明，直接将代码映射到可视化布局。我们在多个数据集上取得了显著的最先进性能（相比于之前的工作提升超过50%），展示了布局NUWA的强大能力。"
}
{
  "title": "Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models",
  "title_zh": "思维提升：使用大型语言模型的试错问题解决",
  "abstract": "The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompting, which in turn enhances reasoning step generation, until a final answer is attained. Our experiments with GPT-4 and Llama2 across extensive complex mathematical problems demonstrate that BoT consistently achieves higher or comparable problem-solving rates than other advanced prompting approaches.",
  "abstract_zh": "大型语言模型（LLMs）在广泛问题上的推理性能严重依赖于思维链提示，这涉及在提示中提供一些思维链示例。最近的研究，例如“思维树”，指出在复杂问题解决中推理步骤选择中探索和自我评估的重要性。本文提出了思维提升（BoT），这是一个通过迭代探索和自我评估多个思维树来解决问题的自动提示框架，以获取试错推理经验的集成，作为解决复杂问题的新提示形式。BoT从一个简单的提示开始，无需示例，迭代探索和评估大量推理步骤，更重要的是，利用从LLM获得的错误分析明确修订提示，从而增强推理步骤生成，直到获得最终答案。我们在广泛复杂数学问题上对GPT-4和Llama2的实验表明，BoT始终实现了比其他先进提示方法更高或可比的解决率。"
}
{
  "title": "DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning",
  "title_zh": "DQ-LoRe：基于低秩近似重排序的双查询上下文学习",
  "abstract": "Recent advances in natural language processing, primarily propelled by Large Language Models (LLMs), have showcased their remarkable capabilities grounded in in-context learning. A promising avenue for guiding LLMs in intricate reasoning tasks involves the utilization of intermediate reasoning steps within the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies in the effective selection of exemplars for facilitating in-context learning. In this study, we introduce a framework that leverages Dual Queries and Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars for in-context learning. Dual Queries first query LLM to obtain LLM-generated knowledge such as CoT, then query the retriever to obtain the final exemplars via both question and the knowledge. Moreover, for the second query, LoRe employs dimensionality reduction techniques to refine exemplar selection, ensuring close alignment with the input question's knowledge. Through extensive experiments, we demonstrate that DQ-LoRe significantly outperforms prior state-of-the-art methods in the automatic selection of exemplars for GPT-4, enhancing performance from 92.5\\% to 94.2\\%. Our comprehensive analysis further reveals that DQ-LoRe consistently outperforms retrieval-based approaches in terms of both performance and adaptability, especially in scenarios characterized by distribution shifts. DQ-LoRe pushes the boundaries of in-context learning and opens up new avenues for addressing complex reasoning challenges.",
  "abstract_zh": "近期自然语言处理的进展，主要得益于大型语言模型（LLMs），展示了其在上下文学习中的卓越能力。引导LLMs进行复杂推理任务的一个有前景的方向是利用链式思维（CoT）范式中的中间推理步骤。然而，核心挑战在于有效选择示例以促进上下文学习。在本研究中，我们提出了一个框架，利用双查询和低秩近似重排序（DQ-LoRe）自动选择上下文学习的示例。双查询首先向LLM查询以获取LLM生成的知识，如CoT，然后通过问题和知识查询检索器以获得最终示例。此外，对于第二个查询，LoRe采用降维技术来优化示例选择，确保与输入问题的知识紧密对齐。通过大量实验，我们证明DQ-LoRe在GPT-4的示例自动选择中显著优于先前的最先进方法，将性能从92.5\\%提升至94.2\\%。我们的综合分析进一步表明，DQ-LoRe在性能和适应性方面始终优于基于检索的方法，尤其是在分布变化的场景中。DQ-LoRe推动了上下文学习的边界，并为解决复杂推理挑战开辟了新途径。"
}
{
  "title": "An Investigation of Representation and Allocation Harms in Contrastive Learning",
  "title_zh": "对比学习中的表征与分配损害的研究",
  "abstract": "The effect of underrepresentation on the performance of minority groups is known to be a serious problem in supervised learning settings; however, it has been underexplored so far in the context of self-supervised learning (SSL). In this paper, we demonstrate that contrastive learning (CL), a popular variant of SSL, tends to collapse representations of minority groups with certain majority groups. We refer to this phenomenon as representation harm and demonstrate it on image and text datasets using the corresponding popular CL methods. Furthermore, our causal mediation analysis of allocation harm on a downstream classification task reveals that representation harm is partly responsible for it, thus emphasizing the importance of studying and mitigating representation harm. Finally, we provide a theoretical explanation for representation harm using a stochastic block model that leads to a representational neural collapse in a contrastive learning setting.",
  "abstract_zh": "在监督学习环境中，少数群体的代表性不足对其表现的影响被认为是一个严重问题；然而，在自监督学习（SSL）的背景下，这一问题尚未得到充分探讨。本文展示了对比学习（CL）作为SSL的一种流行变体，倾向于使少数群体的表征与某些多数群体的表征发生崩溃。我们将这一现象称为表征损害，并在图像和文本数据集上使用相应的流行CL方法进行了验证。此外，我们对下游分类任务中分配损害的因果中介分析表明，表征损害在其中部分负责，从而强调了研究和减轻表征损害的重要性。最后，我们使用随机块模型为表征损害提供了理论解释，该模型导致在对比学习环境中出现表征神经崩溃。"
}
{
  "title": "Adversarial Attacks on Fairness of Graph Neural Networks",
  "title_zh": "标题：针对图神经网络公平性的对抗攻击",
  "abstract": "Fairness-aware graph neural networks (GNNs) have gained a surge of attention as they can reduce the bias of predictions on any demographic group (e.g., female) in graph-based applications. Although these methods greatly improve the algorithmic fairness of GNNs, the fairness can be easily corrupted by carefully designed adversarial attacks. In this paper, we investigate the problem of adversarial attacks on fairness of GNNs and propose G-FairAttack, a general framework for attacking various types of fairness-aware GNNs in terms of fairness with an unnoticeable effect on prediction utility. In addition, we propose a fast computation technique to reduce the time complexity of G-FairAttack. The experimental study demonstrates that G-FairAttack successfully corrupts the fairness of different types of GNNs while keeping the attack unnoticeable. Our study on fairness attacks sheds light on potential vulnerabilities in fairness-aware GNNs and guides further research on the robustness of GNNs in terms of fairness.",
  "abstract_zh": "摘要：关注公平性的图神经网络（GNNs）因其能够减少图基应用中对任何人口群体（例如女性）的预测偏见而受到广泛关注。尽管这些方法极大地提高了GNNs的算法公平性，但公平性很容易受到精心设计的对抗攻击的破坏。本文研究了针对GNNs公平性的对抗攻击问题，并提出了G-FairAttack，这是一个针对各种类型公平性意识GNNs的攻击通用框架，旨在实现公平性攻击，同时对预测效用的影响不易察觉。此外，我们提出了一种快速计算技术，以降低G-FairAttack的时间复杂度。实验研究表明，G-FairAttack成功地破坏了不同类型GNNs的公平性，同时保持攻击不易察觉。我们对公平性攻击的研究揭示了公平性意识GNNs的潜在脆弱性，并为进一步研究GNNs在公平性方面的鲁棒性提供了指导。"
}
{
  "title": "Universal Guidance for Diffusion Models",
  "title_zh": "通用扩散模型引导方法",
  "abstract": "Typical diffusion models are trained to accept a particular form of conditioning, most commonly text, and cannot be conditioned on other modalities without retraining. In this work, we propose a universal guidance algorithm that enables diffusion models to be controlled by arbitrary guidance modalities without the need to retrain any use-specific components. We show that our algorithm successfully generates quality images with guidance functions including segmentation, face recognition, object detection, style guidance and classifier signals.",
  "abstract_zh": "典型的扩散模型被训练以接受特定形式的条件，最常见的是文本，并且无法在不重新训练的情况下对其他模态进行条件控制。在本研究中，我们提出了一种通用引导算法，使扩散模型能够通过任意引导模态进行控制，而无需重新训练任何特定用途的组件。我们展示了我们的算法成功生成了高质量的图像，使用的引导功能包括分割、人脸识别、物体检测、风格引导和分类器信号。"
}
{
  "title": "Conformal Language Modeling",
  "title_zh": "标题：符合性语言建模",
  "abstract": "In this paper, we propose a novel approach to conformal prediction for  language models (LMs) in which we produce prediction sets with performance guarantees. LM responses are typically sampled from a predicted distribution over the large, combinatorial output space of  language. Translating this to conformal prediction, we calibrate a stopping rule for sampling LM outputs  that get added to a growing set of candidates until we are confident that the set covers at least one acceptable response. Since some samples may be low-quality, we also simultaneously calibrate a rejection rule for removing candidates from the output set to reduce noise. Similar to conformal prediction, we  can prove that the final output set obeys certain desirable distribution-free guarantees. Within these sets of candidate responses, we also show that we can also identify subsets of individual components---such as phrases or sentences---that are each independently correct (e.g., that are not ``hallucinations''), again with guarantees. Our method can be applied to any LM API that supports sampling. Furthermore, we empirically demonstrate that we can achieve many desired coverage levels within a limited number of total samples when applying our method to  multiple tasks in open-domain question answering, text summarization, and radiology report generation using different LM variants.",
  "abstract_zh": "摘要：在本文中，我们提出了一种新颖的方法，用于语言模型（LM）的符合性预测，其中我们生成具有性能保证的预测集。LM的响应通常是从语言的大型组合输出空间中预测的分布中抽样而来。将其转化为符合性预测，我们为LM输出的抽样校准了停止规则，直到我们确信该集合至少覆盖一个可接受的响应。由于某些样本可能质量较低，我们还同时校准了一个拒绝规则，以从输出集中移除候选项以减少噪声。与符合性预测类似，我们可以证明最终的输出集遵循某些理想的无分布保证。在这些候选响应集合中，我们还展示了可以识别每个独立正确的个体组件子集——例如短语或句子——同样具有保证。我们的方法可以应用于任何支持抽样的LM API。此外，我们通过实证证明，在应用我们的方法于开放领域问答、文本摘要和放射学报告生成等多个任务时，可以在有限的总样本数量内实现许多期望的覆盖水平。"
}
{
  "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models",
  "title_zh": "#InsTag：用于分析大型语言模型监督微调的指令标记",
  "abstract": "Pre-trained large language models (LLMs) can understand and align with human instructions by supervised fine-tuning (SFT).\nIt is commonly believed that diverse and complex SFT data are of the essence to enable good instruction-following abilities.\nHowever, such diversity and complexity are obscure and lack quantitative analyses.\nIn this work, we propose InsTag, an open-set instruction tagging method, to identify semantics and intentions of human instructions by tags that provide access to definitions and quantified analyses of instruction diversity and complexity.\nWe obtain 6.6K fine-grained tags to describe instructions from popular open-sourced SFT datasets comprehensively.\nWe find that the abilities of aligned LLMs benefit from more diverse and complex instructions in SFT data.\nBased on this observation, we propose a data sampling procedure based on InsTag, and select 6K diverse and complex samples from open-source datasets for SFT.\nThe resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of instruction diversity and complexity and the effectiveness of InsTag.\nInsTag has robust potential to be extended to more applications beyond the data selection as it provides an effective way to analyze the distribution of instructions.",
  "abstract_zh": "预训练的大型语言模型（LLMs）可以通过监督微调（SFT）理解和对齐人类指令。人们普遍认为，多样化和复杂的SFT数据对于实现良好的指令跟随能力至关重要。然而，这种多样性和复杂性不够明确，缺乏定量分析。在本研究中，我们提出了InsTag，一种开放集指令标记方法，通过标签识别人类指令的语义和意图，这些标签提供了对指令多样性和复杂性的定义和定量分析。我们获得了6600个细粒度标签，以全面描述来自流行开源SFT数据集的指令。我们发现，经过对齐的LLMs的能力受益于SFT数据中更为多样化和复杂的指令。基于这一观察，我们提出了一种基于InsTag的数据采样程序，从开源数据集中选择6000个多样化和复杂的样本用于SFT。结果模型TagLM在MT-Bench评估中超越了基于更大SFT数据的开源模型，呼应了指令多样性和复杂性的重要性以及InsTag的有效性。InsTag具有强大的潜力，可以扩展到数据选择以外的更多应用，因为它提供了一种有效的方法来分析指令的分布。"
}
{
  "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
  "title_zh": "标题：分段越狱：对多模态语言模型的组合对抗攻击",
  "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
  "abstract_zh": "摘要：我们介绍了针对视觉语言模型（VLMs）的新型越狱攻击，这些攻击利用对齐的LLMs，并且对仅文本的越狱攻击具有抗性。具体而言，我们开发了跨模态攻击，结合通过视觉编码器处理的对抗性图像与文本提示，以破坏语言模型的对齐。我们的攻击采用了一种新颖的组合策略，将针对有害嵌入的图像与通用提示结合，以实现越狱。因此，LLM从对抗性图像中提取上下文来回答通用提示。生成表面无害的对抗性图像利用了一种新的基于嵌入空间的方法，操作时无需访问LLM模型。相反，攻击仅需访问视觉编码器，并利用我们四种嵌入空间目标策略中的一种。由于不需要访问LLM，这些攻击降低了攻击者的入门门槛，特别是在CLIP等视觉编码器嵌入于闭源LLMs时。这些攻击在不同的VLMs中取得了高成功率，突显了跨模态对齐脆弱性的风险，以及对多模态模型新对齐方法的需求。"
}
{
  "title": "Benchmarking and Improving Generator-Validator Consistency of Language Models",
  "title_zh": "标题：语言模型生成器-验证器一致性的基准测试与改进",
  "abstract": "As of September 2023, ChatGPT correctly answers “what is 7+8” with 15, but when asked “7+8=15, True or False” it responds with “False”. This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust. In this paper, we propose a framework for measuring the consistency between generation and validation (which we call generator-validator consistency, or GV-consistency), finding that even GPT-4 (0613), a state-of-the-art LM, is GV-consistent only 76% of the time. To improve the consistency of LMs, we propose to finetune on the filtered generator and validator responses that are GV-consistent, and call this approach consistency fine-tuning. We find that this approach improves GV-consistency of Alpaca-30B from 60% to 93%, and the improvement extrapolates to unseen tasks and domains (e.g., GV-consistency for positive style transfers extrapolates to unseen styles like humor). In addition to improving consistency, consistency fine-tuning improves both generator quality and validator accuracy without using any labeled data. Evaluated across 6 tasks, including math questions, knowledge-intensive QA, and instruction following, our method improves generator quality by an average of 16% and validator accuracy by an average of 6.3% across all tasks.",
  "abstract_zh": "摘要：截至2023年9月，ChatGPT正确回答“7+8等于多少”为15，但当被问及“7+8=15，正确还是错误”时，它的回答却是“错误”。这种生成与验证答案之间的不一致性在语言模型中普遍存在，削弱了信任。在本文中，我们提出了一种测量生成与验证一致性的框架（我们称之为生成器-验证器一致性，或GV一致性），发现即使是最先进的语言模型GPT-4（0613），其GV一致性也仅为76%。为了提高语言模型的一致性，我们建议在经过筛选的生成器和验证器响应上进行微调，这些响应是GV一致的，并称这种方法为一致性微调。我们发现这种方法将Alpaca-30B的GV一致性从60%提高到93%，而且这种改进可以推广到未见过的任务和领域（例如，正向风格迁移的GV一致性可以推广到未见过的风格，如幽默）。除了提高一致性，一致性微调还在不使用任何标记数据的情况下提高了生成器质量和验证器准确性。在包括数学问题、知识密集型问答和指令跟随在内的6个任务中评估，我们的方法在所有任务中平均提高了16%的生成器质量和6.3%的验证器准确性。"
}
{
  "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
  "title_zh": "自检：利用大型语言模型零-shot检查其自身的逐步推理",
  "abstract": "The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on math- and logic-based datasets and find that it successfully recognizes errors and, in turn, increases final answer accuracies.",
  "abstract_zh": "最近大型语言模型（LLMs）的进展，特别是思维链提示的发明，使得通过逐步推理自动回答问题成为可能。然而，当面对需要非线性思维的复杂问题时，即使是最强大的LLM也会出错。为了解决这个问题，我们探讨LLM是否能够识别自身逐步推理中的错误，而无需依赖外部资源。为此，我们提出了自检（SelfCheck），一种通用的零-shot验证方案，用于识别此类错误。然后，我们利用这些检查的结果，通过对多个问题解决方案进行加权投票来提高问答性能。我们在基于数学和逻辑的数据集上测试了自检，发现它成功识别错误，并因此提高了最终答案的准确性。"
}
{
  "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
  "title_zh": "标题：RAIN：您的语言模型可以在不进行微调的情况下自我对齐",
  "abstract": "Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.",
  "abstract_zh": "摘要：大型语言模型（LLMs）通常与人类偏好存在不一致。以往的研究通常收集人类偏好数据，然后使用强化学习或指令调优（即微调步骤）对预训练模型进行对齐。相比之下，在不需要对齐数据的情况下对冻结的LLMs进行对齐更具吸引力。本研究探索了后者设置的潜力。我们发现，通过整合自我评估和回溯机制，未对齐的LLMs可以通过自我增强直接生成与人类偏好一致的响应。我们提出了一种新颖的推理方法——可回溯自回归推理（RAIN），该方法允许预训练的LLMs评估其自身生成，并利用评估结果指导回溯和生成以确保AI安全。值得注意的是，RAIN在模型对齐时不需要额外的数据，并且不进行任何训练、梯度计算或参数更新。通过GPT-4和人类评估的实验结果表明，RAIN的有效性：在HH数据集上，RAIN将LLaMA 30B的无害率从82%的普通推理提高到97%，同时保持了有用性。在TruthfulQA数据集上，RAIN将已经很好对齐的LLaMA-2-chat 13B模型的真实性提高了5%。"
}
{
  "title": "Two-stage LLM Fine-tuning with Less Specialization and More Generalization",
  "title_zh": "两阶段大语言模型微调：减少专业化与增强泛化",
  "abstract": "Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. \nIn this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task.We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format specialization and improves generalization.ProMoT offloads task-specific format learning into additional and removable parameters by first doing prompt tuning and then fine-tuning the model itself with this soft prompt attached. \nWith experiments on several fine-tuning tasks and 8 in-context evaluation tasks, we show that ProMoT achieves comparable performance on fine-tuned tasks to standard fine-tuning, but with much less loss of in-context learning performances across a board range of  out-of-domain evaluation tasks. More importantly, ProMoT can even enhance generalization on in-context learning tasks that are semantically related to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly improves performance on other language pairs, and ProMoT on NLI improves performance on summarization.\nExperiments also show that ProMoT can improve the generalization performance of  multi-task training.",
  "abstract_zh": "预训练的大语言模型（LLMs）是通用问题解决者，适用于多种任务和提示。通过在专门数据集上微调，可以进一步改善其在特定任务上的表现。然而，微调通常会使模型在该数据集上变得过于专业化，从而降低其在上下文学习中的泛化性能，这在需要处理额外任务且没有微调数据时是不可取的。在本研究中，我们首先证明在单一任务上微调确实会降低LLMs的上下文学习性能。我们发现这种遗忘的一个重要原因是格式专业化，即模型过拟合于微调任务的格式。我们进一步表明，格式专业化在微调的最初阶段就会发生。为了解决这个问题，我们提出了带有模型调优的提示调优（ProMoT），这是一个简单而有效的两阶段微调框架，减少格式专业化并提高泛化能力。ProMoT通过首先进行提示调优，然后附加此软提示对模型本身进行微调，将任务特定的格式学习转移到额外和可移除的参数中。通过在多个微调任务和8个上下文评估任务上的实验，我们表明ProMoT在微调任务上的表现与标准微调相当，但在广泛的领域外评估任务中，损失的上下文学习性能要小得多。更重要的是，ProMoT甚至可以增强与微调任务语义相关的上下文学习任务的泛化能力，例如，ProMoT在英法翻译上显著提高了其他语言对的表现，而ProMoT在自然语言推理上提高了摘要的表现。实验还表明，ProMoT可以改善多任务训练的泛化性能。"
}
{
  "title": "Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning",
  "title_zh": "因果建模代理：通过协同元数据和数据驱动推理发现因果图",
  "abstract": "Scientific discovery hinges on the effective integration of metadata, which refers to a set of 'cognitive' operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA's performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer's Disease (AD). Our experimental results indicate that the CMA can outperform previous data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.",
  "abstract_zh": "科学发现依赖于元数据的有效整合，元数据指一系列“认知”操作，如确定哪些信息与研究相关，而数据则包括观察和实验等物理操作。本文介绍了因果建模代理（CMA），这是一个新颖的框架，结合了大型语言模型（LLMs）基于元数据的推理能力与深层结构因果模型（DSCMs）基于数据的建模能力，以进行因果发现。我们在多个基准测试以及建模阿尔茨海默病（AD）的临床和放射表型的实际任务中评估了CMA的性能。实验结果表明，CMA在因果发现方面优于以往的数据驱动或元数据驱动的方法。在我们的实际应用中，我们使用CMA深入探讨AD生物标志物之间的因果关系。"
}
{
  "title": "The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction",
  "title_zh": "真相在其中：通过层选择性秩降低改善语言模型的推理",
  "abstract": "Transformer-based Large Language Models (LLMs) have become a fixture in modern machine learning. Correspondingly, significant resources are allocated towards research that aims to further advance this technology, typically resulting in models of increasing size that are trained on increasing amounts of data. This work, however, demonstrates the surprising result that it is often possible to significantly improve the performance of LLMs by selectively removing higher-order components of their weight matrices. This simple intervention, which we call LAyer-SElective Rank reduction (LASER), can be done on a model after training has completed, and requires minimal additional parameters and data. We show extensive experiments demonstrating the generality of this finding across language models and datasets, and provide in-depth analyses offering insights into both when LASER is effective and the mechanism by which it operates",
  "abstract_zh": "基于变换器的大型语言模型（LLMs）已成为现代机器学习的一个重要组成部分。因此，研究人员投入了大量资源以进一步推动这一技术的发展，通常导致模型规模不断扩大，训练数据量也在增加。然而，本研究展示了一个令人惊讶的结果：通过选择性地去除权重矩阵的高阶成分，往往可以显著提高LLMs的性能。这一简单的干预措施，我们称之为层选择性秩降低（LASER），可以在模型训练完成后进行，并且只需最少的额外参数和数据。我们展示了广泛的实验，证明了这一发现的普遍性，并提供了深入的分析，揭示了LASER何时有效以及其运作机制。"
}
{
  "title": "Quality-Diversity through AI Feedback",
  "title_zh": "通过AI反馈实现质量多样性",
  "abstract": "In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through \\emph{AI feedback}, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-QD controls. Further, human evaluation of QDAIF-generated creative texts validates reasonable agreement between AI and human evaluation. Our results thus highlight the potential of AI feedback to guide open-ended search for creative and original solutions, providing a recipe that seemingly generalizes to many domains and modalities. In this way, QDAIF is a step towards AI systems that can independently search, diversify, evaluate, and improve, which are among the core skills underlying human society's capacity for innovation.",
  "abstract_zh": "在许多文本生成问题中，用户可能不仅偏好单一响应，而是希望从多样化的高质量输出中进行选择。质量多样性（QD）搜索算法旨在实现这样的结果，通过不断改进和多样化候选者群体。然而，QD在创意写作等定性领域的适用性受到算法上指定质量和多样性度量的困难限制。有趣的是，最近语言模型（LM）的发展使得通过\\emph{AI反馈}引导搜索成为可能，其中LM被自然语言提示以评估文本的定性方面。利用这一发展，我们引入了通过AI反馈实现质量多样性（QDAIF），其中进化算法应用LM来生成变体并评估候选文本的质量和多样性。在创意写作领域的评估中，QDAIF覆盖了更多指定搜索空间的高质量样本，相较于非QD控制组。此外，对QDAIF生成的创意文本的人类评估验证了AI与人类评估之间的合理一致性。因此，我们的结果突显了AI反馈在引导开放式搜索以寻找创意和原创解决方案方面的潜力，提供了一种似乎可以推广到许多领域和模式的方案。通过这种方式，QDAIF朝着能够独立搜索、多样化、评估和改进的AI系统迈出了一步，这些能力是人类社会创新能力的核心技能之一。"
}
{
  "title": "ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models",
  "title_zh": "ReLU的反击：利用大型语言模型中的激活稀疏性",
  "abstract": "Large Language Models (LLMs) with billions of parameters have drastically transformed AI applications. However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices. Despite recent trends favoring alternative activation functions such as GELU or SiLU, known for increased computation, this study strongly advocates for reinstating ReLU activation in LLMs. We demonstrate that using the ReLU activation function has a negligible impact on convergence and performance while significantly reducing computation and weight transfer. This reduction is particularly valuable during the memory-bound inference step, where efficiency is paramount. Exploring sparsity patterns in ReLU-based LLMs, we unveil the reutilization of activated neurons for generating new tokens and leveraging these insights, we propose practical strategies to substantially reduce LLM inference computation up to three times, using ReLU activations with minimal performance trade-offs.",
  "abstract_zh": "具有数十亿参数的大型语言模型（LLMs）极大地改变了人工智能应用。然而，它们在推理过程中的高计算需求给资源受限的设备部署带来了重大挑战。尽管最近的趋势倾向于使用GELU或SiLU等替代激活函数，这些函数计算量较大，但本研究强烈主张在LLMs中恢复使用ReLU激活函数。我们证明，使用ReLU激活函数对收敛性和性能的影响微乎其微，同时显著减少计算和权重传输。这种减少在内存受限的推理步骤中尤为重要，效率至关重要。通过探索基于ReLU的LLMs中的稀疏模式，我们揭示了激活神经元在生成新标记中的再利用，并利用这些见解，我们提出了实用策略，能够将LLM推理计算减少多达三倍，同时保持最小的性能折衷。"
}
{
  "title": "AffineQuant: Affine Transformation Quantization for Large Language Models",
  "title_zh": "仿射量化：针对大规模语言模型的仿射变换量化",
  "abstract": "The significant resource requirements associated with Large-scale Language Models (LLMs) have generated considerable interest in the development of techniques aimed at compressing and accelerating neural networks. \nAmong these techniques, Post-Training Quantization (PTQ) has emerged as a subject of considerable interest due to its noteworthy compression efficiency and cost-effectiveness in the context of training. \nExisting PTQ methods for LLMs limit the optimization scope to scaling transformations between pre- and post-quantization weights. \nThis constraint results in significant errors after quantization, particularly in low-bit configurations. \nIn this paper, we advocate for the direct optimization using equivalent Affine transformations in PTQ (AffineQuant). \nThis approach extends the optimization scope and thus significantly minimizing quantization errors. \nAdditionally, by employing the corresponding inverse matrix, we can ensure equivalence between the pre- and post-quantization outputs of PTQ, thereby maintaining its efficiency and generalization capabilities. \nTo ensure the invertibility of the transformation during optimization, we further introduce a gradual mask optimization method. \nThis method initially focuses on optimizing the diagonal elements and gradually extends to the other elements. \nSuch an approach aligns with the Levy-Desplanques theorem, theoretically ensuring invertibility of the transformation. \nAs a result, significant performance improvements are evident across different LLMs on diverse datasets. \nNotably, these improvements are most pronounced when using very low-bit quantization, enabling the deployment of large models on edge devices. \nTo illustrate, we attain a C4 perplexity of $15.76$ (2.26$\\downarrow$ vs $18.02$ in OmniQuant) on the LLaMA2-$7$B model of W$4$A$4$ quantization without overhead. \nOn zero-shot tasks, AffineQuant achieves an average of $58.61\\%$ accuracy ( $1.98\\%\\uparrow$ vs $56.63$ in OmniQuant) when using $4$/$4$-bit quantization for LLaMA-$30$B, which setting a new state-of-the-art benchmark for PTQ in LLMs. \nCodes are available at: https://github.com/bytedance/AffineQuant.",
  "abstract_zh": "大规模语言模型（LLMs）所需的显著资源引发了对压缩和加速神经网络技术开发的广泛关注。在这些技术中，后训练量化（PTQ）因其显著的压缩效率和成本效益而受到极大关注。现有的LLMs的PTQ方法将优化范围限制在量化前后权重之间的缩放变换，这一限制导致量化后出现显著误差，尤其是在低位配置中。本文提倡在PTQ中直接使用等效的仿射变换进行优化（AffineQuant）。该方法扩展了优化范围，从而显著减少量化误差。此外，通过采用相应的逆矩阵，我们可以确保PTQ的量化前后输出之间的等效性，从而保持其效率和泛化能力。为确保优化过程中变换的可逆性，我们进一步引入了一种渐进的掩码优化方法。该方法最初专注于优化对角元素，并逐渐扩展到其他元素。这种方法与Levy-Desplanques定理相一致，理论上确保了变换的可逆性。因此，在不同的LLMs和多样的数据集上，显著的性能提升是显而易见的。值得注意的是，这些改进在使用非常低位量化时最为明显，使得大模型能够在边缘设备上部署。举例来说，我们在LLaMA2-$7$B模型的W$4$A$4$量化中，获得了$15.76$的C4困惑度（比OmniQuant的$18.02$降低了2.26），没有额外开销。在零样本任务中，AffineQuant在LLaMA-$30$B上使用$4$/$4$位量化时，平均准确率达到$58.61\\%$（比OmniQuant的$56.63$提高了$1.98\\%$），为LLMs中的PTQ设立了新的最先进基准。代码可在：https://github.com/bytedance/AffineQuant获取。"
}
{
  "title": "Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation",
  "title_zh": "隐私保护的上下文学习与差分隐私的少量生成",
  "abstract": "We study the problem of in-context learning (ICL) with large language models (LLMs) on private datasets. \nThis scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt.\nWe propose a novel algorithm that generates synthetic few-shot demonstrations from the private dataset with formal differential privacy (DP) guarantees, and show empirically that it can achieve effective ICL.\nWe conduct extensive experiments on standard benchmarks and compare our algorithm with non-private ICL and zero-shot solutions. \nOur results demonstrate that our algorithm can achieve competitive performance with strong privacy levels.\nThese results open up new possibilities for ICL with privacy protection for a broad range of applications.",
  "abstract_zh": "我们研究了在私有数据集上使用大型语言模型（LLMs）进行上下文学习（ICL）的问题。该场景存在隐私风险，因为LLMs可能会泄露或重复提示中展示的私有示例。我们提出了一种新算法，该算法从私有数据集中生成具有正式差分隐私（DP）保证的合成少量演示，并通过实验证明其可以实现有效的ICL。我们在标准基准上进行了广泛的实验，并将我们的算法与非私有ICL和零-shot解决方案进行了比较。我们的结果表明，我们的算法可以在强隐私水平下实现竞争性性能。这些结果为具有隐私保护的ICL在广泛应用中的新可能性打开了大门。"
}
{
  "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
  "title_zh": "WebArena：构建自主代理的现实网络环境",
  "abstract": "With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting.  The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that \\ours can be used to measure such progress.\\footnote{Code, data, environment reproduction instructions, video demonstrations are available in the supplementary.}",
  "abstract_zh": "随着生成性人工智能的进步，自主代理现在有潜力通过自然语言命令管理日常任务。然而，目前的代理主要是在简化的合成环境中创建和测试，这导致与现实场景之间存在脱节。本文构建了一个高度真实且可重复的语言引导代理环境。具体而言，我们专注于在网络上执行任务的代理，并创建了一个包含四个常见领域（电子商务、社交论坛讨论、协作软件开发和内容管理）完全功能性网站的环境。我们的环境配备了工具（例如地图）和外部知识库（例如用户手册），以鼓励类人任务解决。在我们的环境基础上，我们发布了一组基准任务，重点评估任务完成的功能正确性。我们的基准任务多样、长远，并旨在模拟人类在互联网上常规执行的任务。我们对几个基线代理进行了实验，整合了最近的技术，如行动前推理。结果表明，解决复杂任务具有挑战性：我们最佳的基于GPT-4的代理仅实现了14.41%的端到端任务成功率，显著低于人类的78.24%表现。这些结果突显了进一步开发强大代理的必要性，当前最先进的大型语言模型在这些现实任务中的表现远未完美，而我们的环境可以用来衡量这种进展。"
}
{
  "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph",
  "title_zh": "标题：图上思考：大型语言模型在知识图谱上的深度与负责任推理",
  "abstract": "Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM reasoning. In this paper, we propose a new LLM-KG integrating paradigm ``$\\hbox{LLM}\\otimes\\hbox{KG}$'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph (ToG), in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has the ability of knowledge traceability and knowledge correctability by leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible plug-and-play framework for different LLMs, KGs and prompting strategies without any additional training cost; 4) the performance of ToG with small LLM models could exceed large LLM such as GPT-4 in certain scenarios and this reduces the cost of LLM deployment and application. As a training-free method with lower computational cost and better generality, ToG achieves overall SOTA in 6 out of 9 datasets where most previous SOTAs rely on additional training.",
  "abstract_zh": "摘要：尽管大型语言模型（LLMs）在各种任务中取得了显著成功，但它们在需要深度和负责任推理的场景中常常面临幻觉问题。这些问题可以通过在LLM推理中引入外部知识图谱（KG）部分解决。本文提出了一种新的LLM-KG集成范式“$\\hbox{LLM}\\otimes\\hbox{KG}$”，将LLM视为一个代理，交互式地探索KG上的相关实体和关系，并基于检索到的知识进行推理。我们进一步通过引入一种称为图上思考（ToG）的新方法来实现这一范式，其中LLM代理迭代地在KG上执行束搜索，发现最有前景的推理路径，并返回最可能的推理结果。我们通过一系列精心设计的实验来检验和说明ToG的以下优势：1）与LLMs相比，ToG具有更好的深度推理能力；2）ToG通过利用LLMs推理和专家反馈，具备知识可追溯性和知识可修正性；3）ToG为不同的LLMs、KGs和提示策略提供了灵活的即插即用框架，无需任何额外的训练成本；4）在某些场景中，使用小型LLM模型的ToG性能可能超过大型LLM（如GPT-4），从而降低了LLM的部署和应用成本。作为一种无训练方法，ToG具有更低的计算成本和更好的通用性，在9个数据集中的6个数据集上实现了整体SOTA，而大多数先前的SOTA依赖于额外的训练。"
}
{
  "title": "Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning",
  "title_zh": "逃避高昂成本：多步推理的早停自一致性",
  "abstract": "Self-consistency (SC) has been a widely used decoding strategy for chain-of-thought reasoning. Despite bringing significant performance improvements across a variety of multi-step reasoning tasks, it is a high-cost method that requires multiple sampling with the preset size. In this paper, we propose a simple and scalable sampling process, Early-Stopping Self-Consistency (ESC), to greatly reduce the cost of SC without sacrificing performance. On this basis, one control scheme for ESC is further derivated to dynamically choose the performance-cost balance for different tasks and models. To demonstrate ESC's effectiveness, we conducted extensive experiments on three popular categories of reasoning tasks: arithmetic, commonsense and symbolic reasoning over language models with varying scales. The empirical results show that ESC reduces the average number of sampling of chain-of-thought reasoning by a significant margin on six benchmarks, including MATH (-33.8%), GSM8K (-80.1%), StrategyQA (-76.8%), CommonsenseQA (-78.5%), Coin Flip (-84.2%) and Last Letters (-67.4%), while attaining comparable performances.",
  "abstract_zh": "自一致性（SC）已成为链式思维推理中广泛使用的解码策略。尽管在多种多步推理任务中带来了显著的性能提升，但它是一种高成本的方法，需要进行多次采样并设定大小。在本文中，我们提出了一种简单且可扩展的采样过程——早停自一致性（ESC），以在不牺牲性能的情况下大幅降低SC的成本。在此基础上，我们进一步推导出一种ESC控制方案，以动态选择不同任务和模型的性能-成本平衡。为了验证ESC的有效性，我们在三类流行的推理任务上进行了广泛实验：算术、常识和符号推理，涉及不同规模的语言模型。实证结果表明，ESC在六个基准测试上显著减少了链式思维推理的平均采样次数，包括MATH（-33.8%）、GSM8K（-80.1%）、StrategyQA（-76.8%）、CommonsenseQA（-78.5%）、Coin Flip（-84.2%）和Last Letters（-67.4%），同时达到了可比的性能。"
}
{
  "title": "An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models",
  "title_zh": "标题：一张图胜过千言万语：对视觉语言模型中提示的对抗性图像的可转移性",
  "abstract": "Different from traditional task-specific vision models, recent large VLMs can readily adapt to different vision tasks by simply using different textual instructions, i.e., prompts. However, a well-known concern about traditional task-specific vision models is that they can be misled by imperceptible adversarial perturbations. Furthermore, the concern is exacerbated by the phenomenon that the same adversarial perturbations can fool different task-specific models. Given that VLMs rely on prompts to adapt to different tasks, an intriguing question emerges: Can a single adversarial image mislead all predictions of VLMs when a thousand different prompts are given? This question essentially introduces a novel perspective on adversarial transferability: cross-prompt adversarial transferability. In this work, we propose the Cross-Prompt Attack (CroPA). This proposed method updates the visual adversarial perturbation with learnable textual prompts, which are designed to counteract the misleading effects of the adversarial image. By doing this, CroPA significantly improves the transferability of adversarial examples across prompts. Extensive experiments are conducted to verify the strong cross-prompt adversarial transferability of CroPA with prevalent VLMs including Flamingo, BLIP-2, and InstructBLIP in various different tasks.",
  "abstract_zh": "摘要：与传统的任务特定视觉模型不同，最近的大型视觉语言模型（VLM）可以通过简单地使用不同的文本指令（即提示）来轻松适应不同的视觉任务。然而，传统任务特定视觉模型的一个众所周知的问题是，它们可能会受到不可察觉的对抗性扰动的误导。此外，同样的对抗性扰动能够欺骗不同的任务特定模型，这一问题更加严重。考虑到VLM依赖提示来适应不同任务，一个引人注目的问题出现了：当给出一千个不同的提示时，单个对抗性图像能否误导VLM的所有预测？这个问题本质上引入了对抗性可转移性的新视角：跨提示对抗性可转移性。在这项工作中，我们提出了跨提示攻击（CroPA）。该方法通过可学习的文本提示更新视觉对抗性扰动，旨在抵消对抗性图像的误导效果。通过这样做，CroPA显著提高了对抗性示例在不同提示之间的可转移性。我们进行了广泛的实验，以验证CroPA在包括Flamingo、BLIP-2和InstructBLIP在内的流行VLM上的强跨提示对抗性可转移性，涵盖了多种不同任务。"
}
{
  "title": "Out-of-Distribution Detection with Negative Prompts",
  "title_zh": "标题：使用负提示进行分布外检测",
  "abstract": "Out-of-distribution (OOD) detection is indispensable for open-world machine learning models. Inspired by recent success in large pre-trained language-vision models, e.g., CLIP, advanced works have achieved impressive OOD detection results by matching the *similarity* between image features and features of learned prompts, i.e., positive prompts. However, existing works typically struggle with OOD samples having similar features with those of known classes. One straightforward approach is to introduce negative prompts to achieve a *dissimilarity* matching, which further assesses the anomaly level of image features by introducing the absence of specific features. Unfortunately, our experimental observations show that either employing a prompt like \"not a photo of a\" or learning a prompt to represent \"not containing\" fails to capture the dissimilarity for identifying OOD samples. The failure may be contributed to the diversity of negative features, i.e., tons of features could indicate features not belonging to a known class. To this end, we propose to learn a set of negative prompts for each class. The learned positive prompt (for all classes) and negative prompts (for each class) are leveraged to measure the similarity and dissimilarity in the feature space simultaneously, enabling more accurate detection of OOD samples. Extensive experiments are conducted on diverse OOD detection benchmarks, showing the effectiveness of our proposed method.",
  "abstract_zh": "摘要：分布外（OOD）检测对于开放世界机器学习模型至关重要。受到最近大型预训练语言-视觉模型（如CLIP）成功的启发，先进的研究通过匹配图像特征与学习到的提示特征（即正提示）之间的*相似性*，取得了令人印象深刻的OOD检测结果。然而，现有研究通常在具有与已知类别相似特征的OOD样本上表现不佳。一种直接的方法是引入负提示以实现*不相似性*匹配，通过引入特定特征的缺失进一步评估图像特征的异常水平。不幸的是，我们的实验观察表明，使用“不是一张……的照片”这样的提示或学习表示“没有包含”的提示未能捕捉到识别OOD样本所需的不相似性。这一失败可能归因于负特征的多样性，即大量特征可能指示不属于已知类别的特征。为此，我们提出为每个类别学习一组负提示。所学习的正提示（针对所有类别）和负提示（针对每个类别）被同时用来测量特征空间中的相似性和不相似性，从而实现更准确的OOD样本检测。在多样的OOD检测基准上进行了广泛的实验，显示了我们提出的方法的有效性。"
}
{
  "title": "Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing",
  "title_zh": "跨模态上下文化扩散模型用于文本引导的视觉生成与编辑",
  "abstract": "Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may\nlimit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing. In each task, our ContextDiff achieves new state-of-the-art performance, significantly enhancing the semantic alignment between text condition and generated samples, as evidenced by quantitative and qualitative evaluations. Our code is available at https://github.com/YangLing0818/ContextDiff",
  "abstract_zh": "条件扩散模型在高保真文本引导的视觉生成与编辑中表现出色。然而，现有的文本引导视觉扩散模型主要集中于将文本-视觉关系仅纳入反向过程，往往忽视了它们在正向过程中的相关性。这种正向与反向过程之间的不一致可能限制了文本语义在视觉合成结果中的精确传达。为了解决这一问题，我们提出了一种新颖且通用的上下文化扩散模型（ContextDiff），通过将跨模态上下文（包括文本条件与视觉样本之间的交互和对齐）纳入正向和反向过程。我们将此上下文传播到两个过程中的所有时间步，以调整它们的轨迹，从而促进跨模态条件建模。我们将我们的上下文化扩散推广到DDPM和DDIM，并通过理论推导证明其有效性，并在两个具有挑战性的任务（文本到图像生成和文本到视频编辑）的评估中展示了模型的效果。在每个任务中，我们的ContextDiff都达到了新的最先进性能，显著增强了文本条件与生成样本之间的语义对齐，量化和定性评估均有证明。我们的代码可在https://github.com/YangLing0818/ContextDiff获取。"
}
{
  "title": "Listen, Think, and Understand",
  "title_zh": "倾听、思考与理解",
  "abstract": "The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and reasoning ability? \n\nIn this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is the first multimodal large language model that focuses on general audio (rather than just speech) understanding.",
  "abstract_zh": "人工智能（AI）系统感知和理解音频信号的能力对许多应用至关重要。尽管自AudioSet开发以来在这一领域取得了显著进展，但大多数现有模型旨在将音频输入映射到预定义的离散声音标签集。相比之下，人类不仅能够将声音分类为一般类别，还能够倾听声音的细微细节，解释预测的原因，思考声音所暗示的内容，并理解场景及需要采取的行动（如果有的话）。这种超越感知的能力在现有音频模型中尚未出现。另一方面，现代大型语言模型（LLMs）展现出新兴的推理能力，但缺乏音频感知能力。因此，我们提出了一个问题：我们能否构建一个同时具备音频感知和推理能力的模型？在本文中，我们提出了一种新的音频基础模型，称为LTU（倾听、思考与理解）。为了训练LTU，我们创建了一个新的OpenAQA-5M数据集，包含190万个封闭式和370万个开放式、多样化的（音频、问题、答案）元组，并使用了一个自回归训练框架，结合感知到理解的课程。LTU在传统音频任务（如分类和字幕生成）上表现出强大的性能和泛化能力。更重要的是，它展现出在现有音频模型中缺失的音频推理和理解能力。据我们所知，LTU是首个关注一般音频（而不仅仅是语音）理解的多模态大型语言模型。"
}
{
  "title": "The Consensus Game: Language Model Generation via Equilibrium Search",
  "title_zh": "共识游戏：通过均衡搜索生成语言模型",
  "abstract": "When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate answers). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game—which we term the concensus game—in which a generator seeks to communicate an abstract correctness parameter using natural language sentences to a discriminator. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call equilibrium-ranking. Applied to a large number of tasks (including reading comprehension, commonsense reasoning, mathematical problem-solving, and assistive dialog), equilibrium-ranking consistently improves performance over existing LM decoding procedures. These improvements are sometimes substantial—on multiple benchmarks, we observe that applying equilibrium-ranking to LLaMA-7B outperforms the much larger LLaMA-65B and PaLM-540B models.",
  "abstract_zh": "当应用于问答和其他文本生成任务时，语言模型（LM）可以通过生成性查询（从其输出分布中采样答案）或判别性查询（通过对一组候选答案进行评分或排序）来使用。这些过程有时会产生非常不同的预测。我们如何调和相互不兼容的评分程序以获得一致的LM预测？我们引入了一种新的、无训练的博弈论程序用于语言模型解码。我们的方法将语言模型解码视为一个正则化的不完全信息序列信号博弈——我们称之为共识游戏——其中生成器试图使用自然语言句子向判别器传达一个抽象的正确性参数。我们开发了寻找该博弈近似均衡的计算程序，得出了我们称之为均衡排名的解码算法。应用于大量任务（包括阅读理解、常识推理、数学问题解决和辅助对话），均衡排名在性能上始终优于现有的LM解码程序。这些改进有时是显著的——在多个基准上，我们观察到将均衡排名应用于LLaMA-7B的表现超过了更大的LLaMA-65B和PaLM-540B模型。"
}
{
  "title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments",
  "title_zh": "危险挑战：在动态变化环境中的具身决策",
  "abstract": "Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire, flood, and wind, and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.",
  "abstract_zh": "最近高保真虚拟环境的进展成为构建智能具身代理以感知、推理和与物理世界互动的主要推动力之一。通常，这些环境在代理与之互动之前保持不变。然而，在现实场景中，代理可能还会面临由意外事件引起的动态变化环境，并需要迅速采取相应行动。为了解决这一差距，我们提出了一个新的模拟具身基准，称为HAZARD，专门设计用于评估具身代理在动态情况下的决策能力。HAZARD包括三个意外灾害场景，包括火灾、洪水和风灾，并特别支持利用大型语言模型（LLMs）来辅助常识推理和决策。该基准使我们能够评估自主代理在动态变化环境中通过强化学习（RL）、基于规则和基于搜索的方法的决策能力。作为使用大型语言模型解决这一挑战的第一步，我们进一步开发了一个基于LLM的代理，并对其在解决这些具有挑战性任务时的潜力和挑战进行了深入分析。HAZARD可在https://vis-www.cs.umass.edu/hazard/获取。"
}
{
  "title": "Scalable Language Model with Generalized Continual Learning",
  "title_zh": "可扩展的语言模型与广义持续学习",
  "abstract": "Continual learning has gained increasing importance as it facilitates the acquisition and refinement of scalable knowledge and skills in language models. However, existing methods typically encounter strict limitations and challenges in real-world scenarios, such as reliance on experience replay, optimization constraints, and inference task-ID. In this study, we introduce the Scalable Language Model (SLM) to overcome these limitations within a more challenging and generalized setting, representing a significant advancement toward practical applications for continual learning. Specifically, we propose the Joint Adaptive Re-Parameterization (JARe), integrated with Dynamic Task-related Knowledge Retrieval (DTKR), to enable adaptive adjustment of language models based on specific downstream tasks. This approach leverages the task distribution within the vector space, aiming to achieve a smooth and effortless continual learning process. Our method demonstrates state-of-the-art performance on diverse backbones and benchmarks, achieving effective continual learning in both full-set and few-shot scenarios with minimal forgetting. Moreover, while prior research primarily focused on a single task type such as classification, our study goes beyond, with the large language model, i.e., LLaMA-2, to explore the effects across diverse domains and task types, such that a single language model can be decently scaled to broader applications. The code and models will be released to the public.",
  "abstract_zh": "持续学习变得越来越重要，因为它促进了语言模型中可扩展知识和技能的获取与精炼。然而，现有方法在现实场景中通常面临严格的限制和挑战，例如依赖经验重放、优化约束和推理任务ID。在本研究中，我们引入了可扩展语言模型（SLM），以克服这些限制，适应更具挑战性和广义的环境，代表了朝向持续学习实际应用的重要进展。具体而言，我们提出了联合自适应重参数化（JARe），结合动态任务相关知识检索（DTKR），以根据特定下游任务实现语言模型的自适应调整。这种方法利用向量空间中的任务分布，旨在实现平滑且轻松的持续学习过程。我们的方法在多种基础模型和基准测试中表现出最先进的性能，在全集和少样本场景中实现有效的持续学习，且遗忘最小。此外，尽管先前的研究主要集中在单一任务类型（如分类）上，但我们的研究超越了这一点，利用大型语言模型（如LLaMA-2）探索跨不同领域和任务类型的效果，使得单一语言模型能够有效扩展到更广泛的应用。代码和模型将公开发布。"
}
{
  "title": "OctoPack: Instruction Tuning Code Large Language Models",
  "title_zh": "八爪鱼包：指令调优大型语言模型",
  "abstract": "Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.",
  "abstract_zh": "对大型语言模型（LLMs）进行指令微调在自然语言任务上带来了巨大的性能提升。我们利用代码进行指令调优，利用Git提交的自然结构，将代码更改与人类指令配对。我们编译了CommitPack：包含350种编程语言的4TB Git提交。我们在16B参数的StarCoder模型上对CommitPack与其他自然和合成代码指令（xP3x、Self-Instruct、OASST）进行了基准测试，并在未基于OpenAI输出训练的模型中，在HumanEval Python基准测试中达到了最先进的性能（46.2% pass@1）。我们进一步推出了HumanEvalPack，将HumanEval基准扩展到总共3个编码任务（代码修复、代码解释、代码合成），涵盖6种语言（Python、JavaScript、Java、Go、C++、Rust）。我们的模型OctoCoder和OctoGeeX在所有许可模型中在HumanEvalPack上表现最佳，展示了CommitPack在推广到更广泛的语言和自然编码任务中的优势。代码、模型和数据可在https://github.com/bigcode-project/octopack上免费下载。"
}
{
  "title": "Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation",
  "title_zh": "思维骨架：高效并行生成的提示大语言模型",
  "abstract": "This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-ups across 12 LLMs, but it can also potentially improve the answer quality on several question categories. SoT is an initial attempt at data-centric optimization for inference efficiency, and showcases the potential of eliciting high-quality answers by explicitly planning the answer structure in language.",
  "abstract_zh": "本研究旨在降低大型语言模型（LLMs）的端到端生成延迟。生成延迟高的主要原因之一是几乎所有最先进的LLMs采用的顺序解码方法。在本研究中，受人类思考和写作过程的启发，我们提出了思维骨架（SoT），该方法首先引导LLMs生成答案的骨架，然后进行并行API调用或批量解码，以并行完成每个骨架点的内容。SoT不仅在12个LLMs中提供了显著的加速，还可能提高多个问题类别的答案质量。SoT是针对推理效率的数据中心优化的初步尝试，展示了通过明确规划答案结构来引导高质量答案的潜力。"
}
{
  "title": "Beyond task performance: evaluating and reducing the flaws of large multimodal models with in-context-learning",
  "title_zh": "超越任务表现：通过上下文学习评估和减少大型多模态模型的缺陷",
  "abstract": "Following the success of Large Language Models (LLMs), Large Multimodal Models (LMMs), such as the Flamingo model and its subsequent competitors, have started to emerge as natural steps towards generalist agents. However, interacting with recent LMMs reveals major limitations that are hardly captured by the current evaluation benchmarks. Indeed, task performances (e.g., VQA accuracy) alone do not provide enough clues to understand their real capabilities, limitations, and to which extent such models are aligned to human expectations. To refine our understanding of those flaws, we deviate from the current evaluation paradigm, and (1) evaluate 10 recent open-source LMMs from 3B up to 80B parameter scale,  on 5 different axes; hallucinations, abstention, compositionality, explainability and instruction following. Our evaluation on these axes reveals major flaws in LMMs. While the current go-to solution to align these models is based on training, such as instruction tuning or RLHF, we rather (2) explore the training-free in-context learning (ICL) as a solution, and study how it affects these limitations. Based on our ICL study, (3) we push ICL further and propose new multimodal ICL variants such as; Multitask-ICL, Chain-of-Hindsight-ICL, and Self-Correcting-ICL. Our findings are as follows; (1) Despite their success, LMMs have flaws that remain unsolved with scaling alone. (2) The effect of ICL on LMMs flaws is nuanced; despite its effectiveness for improved explainability, answer abstention, ICL only slightly improves instruction following, does not improve compositional abilities, and actually even amplifies hallucinations. (3) The proposed ICL variants are promising as post-hoc approaches to efficiently tackle some of those flaws. The code is available here: https://github.com/mshukor/EvALign-ICL.",
  "abstract_zh": "继大型语言模型（LLMs）成功之后，大型多模态模型（LMMs），如Flamingo模型及其后续竞争者，已开始作为通用智能体的自然发展。然而，与最近的LMMs互动揭示了主要的局限性，这些局限性在当前的评估基准中几乎没有得到体现。实际上，仅凭任务表现（例如，VQA准确率）并不足以理解它们的真实能力、局限性，以及这些模型在多大程度上与人类期望一致。为了更好地理解这些缺陷，我们偏离了当前的评估范式，并（1）从3B到80B参数规模评估10个最近的开源LMMs，涵盖5个不同的维度；幻觉、回避、组合性、可解释性和遵循指令。我们在这些维度上的评估揭示了LMMs的重大缺陷。虽然目前对齐这些模型的常用解决方案基于训练，例如指令调优或RLHF，但我们（2）探索了无训练的上下文学习（ICL）作为解决方案，并研究其对这些局限性的影响。基于我们的ICL研究，（3）我们进一步推进ICL，提出新的多模态ICL变体，如；多任务-ICL、回顾链-ICL和自我纠正-ICL。我们的发现如下；（1）尽管取得了成功，LMMs仍存在无法仅通过扩展解决的缺陷。（2）ICL对LMMs缺陷的影响是微妙的；尽管在提高可解释性和回答回避方面有效，ICL仅稍微改善了遵循指令的能力，并未改善组合能力，实际上甚至加剧了幻觉现象。（3）所提出的ICL变体作为事后方法，有望有效解决其中一些缺陷。代码可在此获取：https://github.com/mshukor/EvALign-ICL。"
}
{
  "title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents",
  "title_zh": "标题：SOTOPIA：语言智能体社会智能的互动评估",
  "abstract": "*Humans are social beings*; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and *interact* under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.",
  "abstract_zh": "摘要：*人类是社会性生物*；我们在日常互动中追求社会目标，这是社会智能的一个关键方面。然而，AI系统在这一领域的能力仍然难以捉摸。我们提出了SOTOPIA，一个开放式环境，用于模拟人工智能体之间复杂的社会互动并评估它们的社会智能。在我们的环境中，智能体在各种场景下进行角色扮演和*互动*；它们协调、合作、交流并竞争，以实现复杂的社会目标。我们在这个任务空间中模拟基于大型语言模型的智能体与人类之间的角色扮演互动，并通过一个称为SOTOPIA-Eval的整体评估框架来评估它们的表现。通过SOTOPIA，我们发现这些模型在社会智能方面存在显著差异，并识别出一组SOTOPIA场景，即SOTOPIA-hard，这对所有模型来说通常都是具有挑战性的。我们发现，在这一子集上，GPT-4的目标完成率显著低于人类，并且在展现社会常识推理和战略沟通技能方面存在困难。这些发现表明，SOTOPIA作为评估和改善人工智能体社会智能研究的通用平台具有很大的潜力。"
}
{
  "title": "Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective",
  "title_zh": "寻找神经知识点：从参数化视角看大型语言模型中的知识迁移",
  "abstract": "Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge—encompassing detection, editing, and merging—there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales.",
  "abstract_zh": "大型语言模型（LLMs）通过在广泛语料库上的预训练，固有地在其参数中编码了丰富的知识。尽管先前的研究已探讨了对这些参数的操作以操控潜在的隐性知识——包括检测、编辑和合并——但关于其在不同规模模型之间的可迁移性仍然存在模糊的理解。本文旨在通过参数化视角实证研究从大模型到小模型的知识迁移。为此，我们采用基于敏感性的技术提取并对齐不同LLMs之间的知识特定参数。此外，LoRA模块被用作将提取的知识注入小模型的中介机制。四个基准测试的评估验证了我们提出方法的有效性。我们的研究结果突出了影响参数知识迁移过程的关键因素，强调了不同规模LLMs之间模型参数的可迁移性。"
}
{
  "title": "DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING",
  "title_zh": "标题：DENEVIL：通过指令学习解读和导航大型语言模型的伦理价值",
  "abstract": "Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values utilizing Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs’ value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In response, we develop VILMO, an in-context alignment method that substantially enhances the value compliance of LLM outputs by learning to generate appropriate value instructions, outperforming existing competitors. Our methods are suitable for black-box and open-source models, offering a promising initial step in studying the ethical values of LLMs.",
  "abstract_zh": "摘要：大型语言模型（LLMs）取得了前所未有的突破，但它们日益融入日常生活可能因生成不道德内容而带来社会风险。尽管对偏见等特定问题进行了广泛研究，但从道德哲学的角度看，LLMs的内在价值仍然在很大程度上未被探索。本研究利用道德基础理论深入探讨伦理价值。我们提出DeNEVIL，这是一种新颖的提示生成算法，旨在动态利用LLMs的价值脆弱性，以生成方式引发伦理违规，从而揭示其潜在的价值倾向。在此基础上，我们构建了MoralPrompt，这是一个高质量的数据集，包含2,397个提示，涵盖500多个价值原则，并对一系列LLMs的内在价值进行了基准测试。我们发现大多数模型本质上是失调的，迫切需要进一步的伦理价值对齐。为此，我们开发了VILMO，一种上下文对齐方法，通过学习生成适当的价值指令，显著增强LLM输出的价值合规性，超越现有竞争者。我们的方法适用于黑箱和开源模型，为研究LLMs的伦理价值提供了一个有前景的初步步骤。"
}
{
  "title": "To the Cutoff... and Beyond? A Longitudinal Perspective on LLM Data Contamination",
  "title_zh": "标题：走向截止点……及其之后？对大型语言模型数据污染的纵向视角",
  "abstract": "Recent claims about the impressive abilities of large language models (LLMs) are often supported by evaluating publicly available benchmarks. \nSince LLMs train on wide swaths of the internet, this practice raises concerns of data contamination, i.e., evaluating on examples that are explicitly or implicitly included in the training data. \nData contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities. \nIn this work, we conduct the first thorough longitudinal analysis of data contamination in LLMs by using the natural experiment of training cutoffs in GPT models to look at benchmarks released over time.\nSpecifically, we consider two code/mathematical problem-solving datasets, Codeforces and Project Euler, and find statistically significant trends among LLM pass rate vs. GitHub popularity and release date that provide strong evidence of contamination. \nBy open-sourcing our dataset, raw results, and evaluation framework, our work paves the way for rigorous analyses of data contamination in modern models. We conclude with a discussion of best practices and future steps for publicly releasing benchmark in the age of LLMs which  train on webscale data.",
  "abstract_zh": "摘要：近期关于大型语言模型（LLMs）令人印象深刻的能力的说法，通常通过评估公开可用的基准来支持。由于LLMs在广泛的互联网数据上进行训练，这一做法引发了数据污染的担忧，即在明确或隐含包含在训练数据中的示例上进行评估。数据污染的测量和缓解仍然非常具有挑战性，即使是部分尝试，如对训练数据的控制实验、金丝雀字符串或嵌入相似性。在本研究中，我们首次对LLMs中的数据污染进行了全面的纵向分析，利用GPT模型训练截止点的自然实验，观察随时间发布的基准。具体而言，我们考虑了两个代码/数学问题解决数据集，Codeforces和Project Euler，并发现LLM通过率与GitHub受欢迎程度和发布日期之间存在统计显著的趋势，这为污染提供了强有力的证据。通过开源我们的数据集、原始结果和评估框架，我们的工作为现代模型中数据污染的严格分析铺平了道路。最后，我们讨论了在LLMs时代公开发布基准的最佳实践和未来步骤，这些模型在网络规模数据上进行训练。"
}
{
  "title": "Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?",
  "title_zh": "标题：铃声响起！概念移除方法在扩散模型中的可靠性如何？",
  "abstract": "Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming scheme for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model.\nSpecifically, Ring-A-Bell first performs concept extraction to obtain holistic representations for sensitive and inappropriate concepts. Subsequently, by leveraging the extracted concept, Ring-A-Bell automatically identifies problematic prompts for diffusion models with the corresponding generation of inappropriate content, allowing the user to assess the reliability of deployed safety mechanisms. Finally, we empirically validate our method by testing online services such as Midjourney and various methods of concept removal. Our results show that Ring-A-Bell, by manipulating safe prompting benchmarks, can transform prompts that were originally regarded as safe to evade existing safety mechanisms, thus revealing the defects of the so-called safety mechanisms which could practically lead to the generation of harmful contents. In essence, Ring-A-Bell could serve as a red-teaming tool to understand the limitations of deployed safety mechanisms and to explore the risk under plausible attacks. Our codes are available at https://github.com/chiayi-hsu/Ring-A-Bell.",
  "abstract_zh": "摘要：文本到图像（T2I）合成的扩散模型，如稳定扩散（SD），最近展示了生成高质量内容的卓越能力。然而，这一进展引发了对潜在滥用的多种担忧，特别是在创建受版权保护、禁止和限制的内容或不适合工作（NSFW）图像方面。虽然已经采取措施通过在评估阶段实施安全过滤器或通过微调模型来消除不良概念或风格来减轻这些问题，但这些安全措施在处理各种提示时的有效性仍然基本未被探索。在本研究中，我们旨在通过提出一种新的概念检索算法进行评估来调查这些安全机制。我们介绍了Ring-A-Bell，这是一种模型无关的红队方案，用于T2I扩散模型，其中整个评估可以在没有目标模型先验知识的情况下提前准备。具体而言，Ring-A-Bell首先执行概念提取，以获取敏感和不当概念的整体表示。随后，利用提取的概念，Ring-A-Bell自动识别扩散模型的问题提示，并相应生成不当内容，使用户能够评估已部署安全机制的可靠性。最后，我们通过测试在线服务（如Midjourney）和各种概念移除方法来实证验证我们的方法。我们的结果表明，Ring-A-Bell通过操控安全提示基准，可以将原本被视为安全的提示转变为规避现有安全机制，从而揭示所谓安全机制的缺陷，这可能实际导致有害内容的生成。本质上，Ring-A-Bell可以作为一种红队工具，以理解已部署安全机制的局限性并探索在合理攻击下的风险。我们的代码可在https://github.com/chiayi-hsu/Ring-A-Bell获取。"
}
{
  "title": "The Expressive Power of Low-Rank Adaptation",
  "title_zh": "低秩适应的表达能力",
  "abstract": "*Low-Rank Adaptation* (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models.\nDespite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. \nThis paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. \nWe prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\\bar{f}$ if LoRA-rank $\\geq(\\text{width of }f) \\times \\frac{\\text{depth of }\\bar{f}}{\\text{depth of }f}$, under a mild assumption. \nWe also quantify the approximation error when the LoRA-rank is lower than the threshold. \nFor Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\\frac{\\text{embedding size}}{2})$ LoRA adapters.\nAll our theoretical insights are validated by numerical experiments.",
  "abstract_zh": "*低秩适应*（LoRA）是一种参数高效的微调方法，通过利用权重矩阵的低秩适应，已成为微调预训练模型（如大型语言模型和扩散模型）的普遍技术。尽管在实践中取得了巨大成功，LoRA的理论基础仍然基本未被探索。本文首次通过理论分析LoRA的表达能力来填补这一空白。我们证明，对于全连接神经网络，如果LoRA秩 $\\geq(\\text{模型 }f \\text{ 的宽度}) \\times \\frac{\\text{目标模型 }\\bar{f} \\text{ 的深度}}{\\text{模型 }f \\text{ 的深度}}$，在一个温和的假设下，LoRA可以将任何模型 $f$ 适应为准确表示任何较小的目标模型 $\\bar{f}$。我们还量化了当LoRA秩低于阈值时的近似误差。对于Transformer网络，我们展示了任何模型都可以适应为同一大小的目标模型，使用秩为 $(\\frac{\\text{嵌入大小}}{2})$ 的LoRA适配器。我们所有的理论见解均通过数值实验得到了验证。"
}
{
  "title": "The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning",
  "title_zh": "缩小大型语言模型的成本：减少模型大小影响上下文学习之前的内存",
  "abstract": "We study how down-scaling large language model (LLM) size impacts LLM capabilities. We begin by measuring the effects of weight pruning – a popular technique for reducing model size – on the two abilities of LLMs: (a) recalling facts presented during pre-training and (b) processing information presented in context. Surprisingly, we find that existing pruning techniques affect these two abilities of LLMs differently. For example, pruning more than 30% of weights significantly decreases an LLM’s ability to recall facts presented during pre-training. Yet pruning 60-70% of weights largely preserves an LLM’s ability to process information in-context, ranging from retrieving answers based on information presented in context to learning parameterized functions such as a linear classifier based on a few examples. Moderate pruning impairs LLM’s ability to recall facts learnt from pre-training. However, its effect on model’s ability to process information presented in context is much less pronounced. The said disparate effects similarly arise when replacing the original model with a smaller dense one with reduced width and depth. This similarity suggests that model size reduction in general underpins the said disparity.",
  "abstract_zh": "我们研究了缩小大型语言模型（LLM）大小如何影响LLM的能力。我们首先测量了权重剪枝——一种流行的减少模型大小的技术——对LLM的两种能力的影响：（a）回忆在预训练期间呈现的事实和（b）处理上下文中呈现的信息。令人惊讶的是，我们发现现有的剪枝技术对LLM的这两种能力影响不同。例如，剪枝超过30%的权重显著降低了LLM回忆预训练期间呈现的事实的能力。然而，剪枝60-70%的权重在很大程度上保留了LLM处理上下文中信息的能力，从根据上下文中呈现的信息检索答案到基于少量示例学习参数化函数（如线性分类器）。适度的剪枝损害了LLM从预训练中学习的事实的回忆能力。然而，它对模型处理上下文中呈现的信息的能力的影响要小得多。当用一个更小的、宽度和深度减少的稠密模型替换原始模型时，这种不同的影响同样出现。这种相似性表明，模型大小的减少在一般情况下是导致上述差异的基础。"
}
{
  "title": "Grounding Multimodal Large Language Models to the World",
  "title_zh": "将多模态大型语言模型与现实世界相结合",
  "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent text spans (i.e., referring expressions and noun phrases) as links in Markdown, i.e., [text span](bounding boxes), where object descriptions are sequences of location tokens. To train the model, we construct a large-scale dataset about grounded image-text pairs (GrIT) together with multimodal corpora. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability to downstream applications, while maintaining the conventional capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning). Kosmos-2 is evaluated on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This study sheds a light on the big convergence of language, multimodal perception, and world modeling, which is a key step toward artificial general intelligence. Code can be found in [https://aka.ms/kosmos-2](https://aka.ms/kosmos-2).",
  "abstract_zh": "我们介绍了Kosmos-2，一个多模态大型语言模型（MLLM），使其具备感知物体描述（例如，边界框）和将文本与视觉世界结合的新能力。具体而言，我们将文本片段（即指称表达和名词短语）表示为Markdown中的链接，即[text span](bounding boxes)，其中物体描述是位置标记的序列。为了训练该模型，我们构建了一个关于基础图像-文本对（GrIT）的大规模数据集，以及多模态语料库。除了现有的MLLM能力（例如，感知一般模态、遵循指令和进行上下文学习）外，Kosmos-2还将基础能力整合到下游应用中，同时保持MLLM的传统能力（例如，感知一般模态、遵循指令和进行上下文学习）。Kosmos-2在广泛的任务上进行了评估，包括（i）多模态基础，如指称表达理解和短语基础，（ii）多模态指称，如指称表达生成，（iii）感知-语言任务，以及（iv）语言理解和生成。本研究揭示了语言、多模态感知和世界建模之间的重大融合，这是朝向人工通用智能的关键一步。代码可以在[https://aka.ms/kosmos-2](https://aka.ms/kosmos-2)找到。"
}
{
  "title": "Bridging Vision and Language Spaces with Assignment Prediction",
  "title_zh": "标题：通过分配预测连接视觉和语言空间",
  "abstract": "While pretrained large language models (LLMs) excel in understanding linguistic contexts, it is still an open question: Can LLMs extend their capabilities beyond linguistic contexts to non-linguistic information? This paper introduces VLAP, a novel approach that bridges vision encoders and language models through assignment prediction. Since the LLMs interpret and reason linguistic information from correlations between word embeddings, we harness the well-established word embeddings to map visual representations into language space. Specifically, we simultaneously assign the visual and text representations to a set of word embeddings within LLMs. We propose a new training objective, optimal transport-based assignment prediction, to enforce the consistency of word assignments for paired multimodal data. This allows frozen LLMs to ground their word embedding space in visual data and use their robust semantic taxonomy visually. Moreover, VLAP is memory- and parameter-efficient in that it trains only a single linear layer, and works without extra embedding space (e.g. learnable prototypes) for the assignment prediction. Experimental results show that VLAP achieves substantial improvements over the previous linear transformation-based methods across a range of vision-language tasks, including image captioning, visual question answering, and cross-modal retrieval. We also demonstrate the learned visual representations hold a semantic taxonomy of LLMs, making visual semantic arithmetic possible.",
  "abstract_zh": "摘要：尽管预训练的大型语言模型（LLMs）在理解语言上下文方面表现出色，但一个尚未解决的问题是：LLMs能否将其能力扩展到非语言信息？本文介绍了一种新颖的方法VLAP，通过分配预测将视觉编码器和语言模型连接起来。由于LLMs通过词嵌入之间的相关性来解释和推理语言信息，我们利用成熟的词嵌入将视觉表示映射到语言空间。具体而言，我们同时将视觉和文本表示分配给LLMs中的一组词嵌入。我们提出了一种新的训练目标，基于最优传输的分配预测，以强制配对多模态数据的词分配一致性。这使得冻结的LLMs能够将其词嵌入空间与视觉数据对接，并在视觉上使用其强大的语义分类。此外，VLAP在内存和参数方面高效，因为它仅训练一个线性层，并且在进行分配预测时不需要额外的嵌入空间（例如，可学习的原型）。实验结果表明，VLAP在一系列视觉-语言任务（包括图像描述、视觉问答和跨模态检索）中，相较于之前基于线性变换的方法取得了显著的改进。我们还证明了学习到的视觉表示具有LLMs的语义分类，使得视觉语义算术成为可能。"
}
{
  "title": "Context-Aware Meta-Learning",
  "title_zh": "上下文感知元学习",
  "abstract": "Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach---without meta-training or fine-tuning---exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.",
  "abstract_zh": "大型语言模型如ChatGPT在推理过程中展现了在不进行微调的情况下学习新概念的显著能力。然而，训练用于在推理过程中检测新对象的视觉模型未能复制这种能力，反而表现不佳或需要在类似对象上进行元训练和/或微调。在本研究中，我们提出了一种元学习算法，通过在推理过程中学习新视觉概念而不进行微调，从而模拟大型语言模型。我们的方法利用一个冻结的预训练特征提取器，并类似于上下文学习，将元学习重新表述为在已知标签的数据点和一个未知标签的测试数据点上的序列建模。在11个元学习基准中的8个上，我们的方法---在没有元训练或微调的情况下---超越或匹配了最先进的算法P>M>F，该算法在这些基准上进行了元训练。"
}
{
  "title": "Vision-Language Foundation Models as Effective Robot Imitators",
  "title_zh": "视觉-语言基础模型作为有效的机器人模仿者",
  "abstract": "Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data.\nTo this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control.\nOur extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy. Our code will be made public upon acceptance.",
  "abstract_zh": "最近在视觉语言基础模型方面的进展表明，它们能够理解多模态数据并解决复杂的视觉语言任务，包括机器人操作。我们寻求一种简单的方法，利用现有的视觉语言模型（VLMs）在机器人数据上进行简单的微调。为此，我们推导出一个简单且新颖的视觉语言操作框架，称为RoboFlamingo，基于开源的VLM OpenFlamingo。与之前的工作不同，RoboFlamingo利用预训练的VLM进行单步视觉语言理解，使用显式策略头建模序列历史信息，并仅在语言条件的操作数据集上通过模仿学习进行轻微微调。这种分解为RoboFlamingo提供了开放环控制和在低性能平台上部署的灵活性。通过在测试基准上以大幅度超越最先进的性能，我们展示了RoboFlamingo可以成为将VLM适应于机器人控制的有效且具有竞争力的替代方案。我们广泛的实验结果还揭示了不同预训练VLM在操作任务上的行为的几个有趣结论。我们相信RoboFlamingo有潜力成为一种具有成本效益且易于使用的机器人操作解决方案，使每个人都能微调自己的机器人策略。我们的代码将在接受后公开。"
}
{
  "title": "Predicting Emergent Abilities with Infinite Resolution Evaluation",
  "title_zh": "标题：用无限分辨率评估预测新兴能力",
  "abstract": "The scientific scale-up of large language models (LLMs) necessitates a comprehensive understanding of their scaling properties. However, the existing literature on the scaling properties only yields an incomplete answer: optimization loss decreases predictably as the model size increases, in line with established scaling law; yet no scaling law for task has been established and the task performances are far from predictable during scaling. Task performances typically show minor gains on small models until they improve dramatically once models exceed a size threshold, exemplifying the ''emergent abilities''. In this study, we discover that small models, although they exhibit minor performance, demonstrate critical and consistent task performance improvements that are not captured by conventional evaluation strategies due to insufficient measurement resolution. To measure such improvements, we introduce PassUntil, an evaluation strategy with theoretically infinite resolution, through massive sampling in the decoding phase. With PassUntil, we conduct a quantitative investigation into the scaling law of task performance. The investigation contains two parts. Firstly, a strict task scaling law that is not conventionally known to exist, is identified, enhancing the predictability of task performances. Remarkably, we are able to predict the performance of the 2.4B model on code generation with merely 0.05\\% deviation before training starts, which is the first systematic attempt to verify predictable scaling proposed by GPT-4's report. Secondly, underpinned by PassUntil, we are able to study emergent abilities quantitatively. We identify a kind of accelerated emergence whose scaling curve cannot be fitted by standard scaling law function and has a increasing speed. We then examine two hypothesis and imply that the ``multiple circuits hypothesis'' might be responsible for the accelerated emergence.",
  "abstract_zh": "摘要：大型语言模型（LLMs）的科学扩展需要对其扩展特性有全面的理解。然而，现有文献对扩展特性的研究仅提供了不完整的答案：随着模型规模的增加，优化损失可预测地减少，符合既定的扩展规律；然而，尚未建立任务的扩展规律，且在扩展过程中任务性能远非可预测。任务性能通常在小模型上表现出微小的提升，直到模型超过某一规模阈值后才会显著改善，这体现了“新兴能力”。在本研究中，我们发现小模型虽然表现出微小的性能，但在任务性能上展示了关键且一致的提升，这些提升由于测量分辨率不足而未被传统评估策略捕捉。为了测量这种提升，我们引入了PassUntil，这是一种理论上具有无限分辨率的评估策略，通过在解码阶段进行大量采样。借助PassUntil，我们对任务性能的扩展规律进行了定量研究。该研究包含两个部分。首先，识别出一种严格的任务扩展规律，这种规律在传统上并不被认为存在，从而增强了任务性能的可预测性。值得注意的是，我们能够在训练开始前仅以0.05\\%的偏差预测2.4B模型在代码生成上的性能，这是首次系统性验证GPT-4报告中提出的可预测扩展的尝试。其次，在PassUntil的支持下，我们能够定量研究新兴能力。我们识别出一种加速出现的现象，其扩展曲线无法用标准扩展规律函数拟合，并且具有加速的趋势。然后，我们检验了两个假设，并暗示“多电路假设”可能是加速出现的原因。"
}
{
  "title": "COLLIE: Systematic Construction of Constrained Text Generation Tasks",
  "title_zh": "COLLIE：受限文本生成任务的系统构建",
  "abstract": "Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models. However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g. generate a sentence containing certain words) that have proved to be easy for state-of-the-art models like GPT-4. We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g. language understanding, logical reasoning, counting, semantic planning). We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 1,132 instances comprising 13 constraint structures. We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings. COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex constraints and evaluations in the future.",
  "abstract_zh": "在自然语言处理领域，受限文本生成引起了越来越多的关注，尤其是在大型语言模型能力迅速提升的背景下。然而，现有的受限生成基准通常集中于固定的约束类型（例如，生成包含特定单词的句子），这些对于像GPT-4这样的最先进模型来说已经证明是简单的。我们提出了COLLIE，一个基于语法的框架，允许指定丰富的、组合的约束，涵盖多样的生成层次（单词、句子、段落、篇章）和建模挑战（例如语言理解、逻辑推理、计数、语义规划）。我们还开发了工具，以便在给定约束结构和原始文本语料库的情况下自动提取任务实例。使用COLLIE，我们编制了COLLIE-v1数据集，其中包含1,132个实例，涵盖13种约束结构。我们在五个最先进的指令调优语言模型上进行系统实验，并分析它们的表现以揭示不足之处。COLLIE旨在具有可扩展性和轻量性，我们希望社区能够利用它在未来开发更复杂的约束和评估。"
}
{
  "title": "Successor Heads: Recurring, Interpretable Attention Heads In The Wild",
  "title_zh": "后继头：自然界中反复出现的可解释注意力头",
  "abstract": "In this work we describe successor heads: attention heads that increment tokens with a natural ordering, such as numbers, months, and days.\nFor example, successor heads increment 'Monday' into 'Tuesday'.\nWe explain the successor head behavior with an approach rooted in mechanistic interpretability, the field that aims to explain how models complete tasks in human-understandable terms.\nExisting research in this area has struggled to find recurring, mechanistically interpretable large language model (LLM) components beyond small toy models. Further, existing results have led to very little insight to explain the internals of the larger models that are used in practice.\nIn this paper, we analyze the behavior of successor heads in LLMs and find that they implement abstract representations that are common to different architectures. \nSuccessor heads form in LLMs with as few as 31 million parameters, and at least as many as 12 billion parameters, such as GPT-2, Pythia, and Llama-2.\nWe find a set of 'mod 10' features that underlie how successor heads increment in LLMs across different architectures and sizes.\nWe perform vector arithmetic with these features to edit head behavior and provide insights into numeric representations within LLMs. Additionally, we study the behavior of successor heads on natural language data, where we find that successor heads are important for achieving a low loss on examples involving succession, and also identify interpretable polysemanticity in a Pythia successor head.",
  "abstract_zh": "在这项工作中，我们描述了后继头：以自然顺序递增标记的注意力头，例如数字、月份和日期。例如，后继头将“星期一”递增为“星期二”。我们通过一种基于机械可解释性的方式解释后继头的行为，该领域旨在用人类可理解的术语解释模型如何完成任务。该领域现有研究在寻找超出小型玩具模型的反复出现的机械可解释大型语言模型（LLM）组件方面遇到了困难。此外，现有结果对解释实际使用的大型模型的内部机制几乎没有提供任何见解。在本文中，我们分析了LLM中后继头的行为，发现它们实现了不同架构之间的共同抽象表示。后继头在参数少至3100万的LLM中形成，至少在120亿参数的模型中形成，例如GPT-2、Pythia和Llama-2。我们发现了一组“mod 10”特征，这些特征是后继头在不同架构和规模的LLM中递增的基础。我们利用这些特征进行向量运算，以编辑头的行为并提供关于LLM中数字表示的见解。此外，我们研究了后继头在自然语言数据上的行为，发现后继头对于在涉及顺序的示例中实现低损失至关重要，并且还在Pythia后继头中识别出可解释的多义性。"
}
{
  "title": "Beyond Memorization: Violating Privacy via Inference with Large Language Models",
  "title_zh": "超越记忆：通过推断侵犯隐私的大型语言模型",
  "abstract": "Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models’ inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals’ privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to 85% top-1 and 95% top-3 accuracy at a fraction of the cost (100x) and time (240x) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for stronger and wider privacy protection.",
  "abstract_zh": "当前对大型语言模型（LLMs）隐私的研究主要集中在提取记忆训练数据的问题上。同时，模型的推断能力急剧增强。这引发了一个关键问题，即当前的LLMs是否可能通过推断给定文本中的个人属性来侵犯个人隐私。在本研究中，我们首次全面研究了预训练LLMs从文本中推断个人属性的能力。我们构建了一个由真实Reddit个人资料组成的数据集，并展示了当前的LLMs能够推断出广泛的个人属性（例如，位置、收入、性别），在成本（100倍）和时间（240倍）上远低于人类，达到85%的top-1和95%的top-3准确率。随着人们在生活的各个方面越来越多地与LLM驱动的聊天机器人互动，我们还探讨了隐私侵犯聊天机器人通过看似无害的问题试图提取个人信息的潜在威胁。最后，我们表明，常见的缓解措施，即文本匿名化和模型对齐，目前在保护用户隐私方面对LLM推断无效。我们的研究结果强调，当前的LLMs能够在以前无法达到的规模上推断个人数据。在缺乏有效防御的情况下，我们倡导围绕LLM隐私影响进行更广泛的讨论，超越记忆，努力实现更强大和更广泛的隐私保护。"
}
{
  "title": "Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond",
  "title_zh": "统一多模态个性化的探索：用于生成推荐及其他应用的大型视觉-语言模型",
  "abstract": "Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by multi-modal data, such as pictures and textual descriptions. These modalities not only offer intuitive guidance but also cater to personalized user preferences. However, the predominant personalization approaches mainly focus on ID or text-based recommendation problems, failing to comprehend the information spanning various tasks or modalities. In this paper, our goal is to establish a Unified paradigm for Multi-modal Personalization systems (UniMP), which effectively leverages multi-modal data while eliminating the complexities associated with task- and modality-specific customization. We argue that the advancements in foundational generative modeling have provided the flexibility and effectiveness necessary to achieve the objective. In light of this, we develop a generic and extensible personalization generative framework, that can handle a wide range of personalized needs including item recommendation, product search, preference prediction, explanation generation, and further user-guided image generation. Our methodology enhances the capabilities of foundational language models for personalized tasks by seamlessly ingesting interleaved cross-modal user history information, ensuring a more precise and customized experience for users. To train and evaluate the proposed multi-modal personalized tasks, we also introduce a novel and comprehensive benchmark covering a variety of user requirements. Our experiments on the real-world benchmark showcase the model's potential, outperforming competitive methods specialized for each task.",
  "abstract_zh": "开发一个能够有效利用异构资源并响应广泛个性化需求的通用模型一直是社区的长期愿望。我们的日常选择，尤其是在时尚和零售等领域，受到多模态数据（如图片和文本描述）的显著影响。这些模态不仅提供直观的指导，还满足个性化用户偏好。然而，主要的个性化方法主要集中在基于ID或文本的推荐问题上，未能理解跨任务或模态的信息。在本文中，我们的目标是建立一个统一的多模态个性化系统范式（UniMP），有效利用多模态数据，同时消除与特定任务和模态定制相关的复杂性。我们认为基础生成建模的进展提供了实现这一目标所需的灵活性和有效性。基于此，我们开发了一个通用且可扩展的个性化生成框架，能够处理包括项目推荐、产品搜索、偏好预测、解释生成以及进一步的用户引导图像生成在内的广泛个性化需求。我们的方法通过无缝吸收交错的跨模态用户历史信息，增强了基础语言模型在个性化任务中的能力，确保用户获得更精确和定制化的体验。为了训练和评估所提出的多模态个性化任务，我们还引入了一个新颖且全面的基准，涵盖各种用户需求。我们在真实世界基准上的实验展示了模型的潜力，超越了针对每个任务的竞争方法。"
}
{
  "title": "Large Language Models as Automated Aligners for  benchmarking  Vision-Language Models",
  "title_zh": "大型语言模型作为自动对齐工具用于基准测试视觉-语言模型",
  "abstract": "With the advancements in Large Language Models (LLMs), Vision-Language Models (VLMs) have reached a new level of sophistication, showing notable competence in executing intricate cognition and reasoning tasks. However, existing evaluation benchmarks, primarily relying on rigid, hand-crafted datasets to measure task-specific performance, face significant limitations in assessing the alignment of these increasingly anthropomorphic models with human intelligence. In this work, we address the limitations via Auto-Bench, which delves into exploring LLMs as proficient aligners, measuring the alignment between VLMs and human intelligence and value through automatic data curation and assessment. Specifically, for data curation, Auto-Bench utilizes LLMs (e.g., GPT-4) to automatically generate a vast set of question-answer-reasoning triplets via prompting on visual symbolic representations (e.g., captions, object locations, instance relationships, and etc. The curated data closely matches human intent, owing to the extensive world knowledge embedded in LLMs. Through this pipeline, a total of 28.5K human-verified and 3,504K unfiltered question-answer-reasoning triplets have been curated, covering 4 primary abilities and 16 sub-abilities. We subsequently engage LLMs like GPT-3.5 to serve as judges, implementing the quantitative and qualitative automated assessments to facilitate a comprehensive evaluation of VLMs. Our validation results reveal that LLMs are proficient in both evaluation data curation and model assessment, achieving an average agreement rate of 85%. We envision Auto-Bench as a flexible, scalable, and comprehensive benchmark for evaluating the evolving sophisticated VLMs.",
  "abstract_zh": "随着大型语言模型（LLMs）的进步，视觉-语言模型（VLMs）达到了新的复杂程度，在执行复杂的认知和推理任务方面表现出显著的能力。然而，现有的评估基准主要依赖于僵化的手工数据集来衡量特定任务的性能，在评估这些日益拟人化模型与人类智能的对齐方面面临重大限制。在本研究中，我们通过Auto-Bench解决了这些限制，探讨LLMs作为熟练对齐工具的能力，通过自动数据策划和评估来衡量VLMs与人类智能和价值之间的对齐。具体而言，在数据策划方面，Auto-Bench利用LLMs（例如，GPT-4）通过对视觉符号表示（例如，标题、物体位置、实例关系等）的提示自动生成大量问题-答案-推理三元组。由于LLMs中嵌入了广泛的世界知识，策划的数据与人类意图紧密匹配。通过这一流程，共策划了28.5K个经过人类验证和3,504K个未过滤的问题-答案-推理三元组，涵盖了4种主要能力和16种子能力。随后，我们利用LLMs（如GPT-3.5）作为评审，实施定量和定性的自动评估，以促进对VLMs的全面评估。我们的验证结果表明，LLMs在评估数据策划和模型评估方面表现出色，平均一致性率达到85%。我们设想Auto-Bench作为一个灵活、可扩展且全面的基准，用于评估不断发展的复杂VLMs。"
}
{
  "title": "Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models",
  "title_zh": "标题：基于CLIP奖励的测试时适应在视觉-语言模型中的零-shot泛化",
  "abstract": "One fascinating aspect of pre-trained vision-language models (VLMs) learning under language supervision is their impressive zero-shot generalization capability.\nHowever, this ability is hindered by distribution shifts between the training and testing data.\nPrevious test time adaptation (TTA) methods for VLMs in zero-shot classification rely on minimizing the entropy of model outputs, tending to be stuck in incorrect model predictions.\nIn this work, we propose TTA with feedback to rectify the model output and prevent the model from becoming blindly confident.\nSpecifically, a CLIP model is adopted as the reward model during TTA and provides feedback for the VLM.\nGiven a single test sample,\nthe VLM is forced to maximize the CLIP reward between the input and sampled results from the VLM output distribution.\nThe proposed \\textit{reinforcement learning with CLIP feedback~(RLCF)} framework is highly flexible and universal.\nBeyond the classification task, with task-specific sampling strategies and a proper reward baseline choice, RLCF can be easily extended to not only discrimination tasks like retrieval but also generalization tasks like image captioning,\nimproving the zero-shot generalization capacity of VLMs.\nAccording to the characteristics of these VL tasks, we build different fully TTA pipelines with RLCF to improve the zero-shot generalization ability of various VLMs.\nExtensive experiments along with promising\nempirical results demonstrate the effectiveness of RLCF.\nThe code is available at https://github.com/mzhaoshuai/RLCF.",
  "abstract_zh": "摘要：在语言监督下学习的预训练视觉-语言模型（VLMs）一个引人注目的方面是其令人印象深刻的零-shot泛化能力。然而，这种能力受到训练数据和测试数据之间分布变化的阻碍。以往针对VLMs的零-shot分类的测试时适应（TTA）方法依赖于最小化模型输出的熵，往往会陷入错误的模型预测中。在本研究中，我们提出了一种带反馈的TTA方法，以纠正模型输出并防止模型盲目自信。具体而言，在TTA过程中采用CLIP模型作为奖励模型，为VLM提供反馈。给定一个单一的测试样本，VLM被迫最大化输入与VLM输出分布中采样结果之间的CLIP奖励。所提出的“带CLIP反馈的强化学习（RLCF）”框架具有高度的灵活性和普适性。除了分类任务外，通过特定任务的采样策略和适当的奖励基线选择，RLCF不仅可以轻松扩展到检索等区分任务，还可以扩展到图像描述等泛化任务，从而提高VLM的零-shot泛化能力。根据这些VL任务的特点，我们构建了不同的完全TTA管道与RLCF，以提升各种VLM的零-shot泛化能力。大量实验和令人鼓舞的实证结果证明了RLCF的有效性。代码可在https://github.com/mzhaoshuai/RLCF获取。"
}
{
  "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
  "title_zh": "偏见根深蒂固：个性化分配的大型语言模型中的隐性推理偏见",
  "abstract": "Recent work has showcased the ability of large-scale language models (LLMs) to embody diverse personas in their responses, exemplified by prompts like \"_You are Julius Caesar. Compose a rap about Climate Change._\" However, it remains unclear how these persona assignments indirectly influence LLMs' core capabilities.  We present the first extensive study of this in the context of LLMs' ability to perform basic reasoning. Our study encompasses 16 personas spanning 5 diverse groups (race, gender, religion, disability, and political affiliation), across 24 reasoning datasets in diverse domains such as mathematics, history, law, ethics, and more. Our findings unveil that while LLMs, such as ChatGPT, overtly reject stereotypes when explicitly asked (\"_Are Black people inept at mathematics?_\"), they tend to manifest implicit stereotypical and often erroneous presumptions when prompted to take on a persona (e.g., abstentions in rationales such as \"_As a Black person, I am unable to answer this question as it requires math knowledge_\"). This results in substantial disparities in reasoning performance among personas. This inherent 'deep' bias permeates extensively, leading to a statistically significant performance drop in over 95\\% of our datasets for certain personas, with as much as 70\\% relative drop in accuracy on select datasets. Beyond explicit abstentions, these models also have implicitly biased reasoning not evident in their responses. We find that simple prompt-based mitigation approaches have minimal impact. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs---a trend on the rise---can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.",
  "abstract_zh": "近期研究展示了大型语言模型（LLMs）在回应中体现多样化个性的能力，例如通过提示“_你是尤利乌斯·凯撒。写一首关于气候变化的说唱歌曲._”然而，这些个性分配如何间接影响LLMs的核心能力仍不清楚。我们在LLMs的基本推理能力背景下首次进行了广泛研究。我们的研究涵盖了16种个性，跨越5个多样化群体（种族、性别、宗教、残疾和政治倾向），涉及24个推理数据集，涵盖数学、历史、法律、伦理等多个领域。我们的发现揭示，尽管LLMs（如ChatGPT）在被明确询问时（“_黑人在数学上无能吗?_”）公开拒绝刻板印象，但在被提示扮演个性时，它们往往表现出隐性刻板印象和错误假设（例如，在理由中回避，如“_作为一个黑人，我无法回答这个问题，因为它需要数学知识_”）。这导致个性之间的推理表现存在显著差异。这种内在的“深层”偏见广泛渗透，导致在我们95%以上的数据集中某些个性的表现显著下降，特定数据集的准确率下降高达70%。除了明确的回避外，这些模型还存在隐性偏见推理，在其回应中并不明显。我们发现简单的基于提示的缓解方法影响甚微。我们的发现警示，给LLMs分配个性的做法——一种日益增长的趋势——可能会暴露其根深蒂固的偏见，并产生不可预见和有害的副作用。"
}
{
  "title": "Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making",
  "title_zh": "标题：诊断变压器：为临床决策提供特征空间的启示",
  "abstract": "Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability (e.g. model suitability for a task, feature space evolution during fine-tuning, and interpretation of fine-tuned features and failure modes). We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1) while PubMedBERT, the domain-specific model, contains valuable information for fine-tuning, it can overfit to minority classes when class imbalances exist. In contrast, mixed-domain models exhibit greater resistance to overfitting, suggesting potential improvements in domain-specific model robustness; (2) in-domain pre-training accelerates feature disambiguation during fine-tuning; and (3) feature spaces undergo significant sparsification during this process, enabling clinicians to identify common outlier modes among fine-tuned models as demonstrated in this paper. These findings showcase the utility of SUFO in enhancing trust and safety when using transformers in medicine, and we believe SUFO can aid practitioners in evaluating fine-tuned language models (LMs) for other applications in medicine and in more critical domains.",
  "abstract_zh": "摘要：预训练的变压器通常会根据有限的临床笔记进行微调，以帮助临床决策。模型可解释性至关重要，尤其是在医学等高风险领域，以建立信任并确保安全，这需要人类的参与。我们引入了SUFO，这是一个系统框架，增强了微调变压器特征空间的可解释性。SUFO利用一系列分析和可视化技术，包括监督探测、无监督相似性分析、特征动态和异常值分析，以解决关于模型信任和可解释性的关键问题（例如，模型适合于某项任务、微调过程中特征空间的演变，以及对微调特征和失败模式的解释）。我们进行了一个案例研究，调查预训练数据的影响，重点关注现实世界的病理分类任务，并在MedNLI上验证我们的发现。我们评估了五个110M大小的预训练变压器模型，分为通用领域（BERT，TNLR）、混合领域（BioBERT，临床BioBERT）和特定领域（PubMedBERT）组。我们的SUFO分析揭示：（1）尽管特定领域模型PubMedBERT包含对微调有价值的信息，但在类别不平衡时，它可能会对少数类过拟合。相比之下，混合领域模型对过拟合表现出更大的抵抗力，暗示特定领域模型的鲁棒性可能有所改善；（2）领域内预训练加速了微调过程中的特征消歧；（3）特征空间在此过程中经历了显著的稀疏化，使临床医生能够识别微调模型中的常见异常模式，如本文所示。这些发现展示了SUFO在增强医学中使用变压器的信任和安全性方面的实用性，我们相信SUFO可以帮助从业者评估其他医学应用和更关键领域的微调语言模型（LM）。"
}
{
  "title": "MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback",
  "title_zh": "MINT：评估大型语言模型在多轮交互中与工具和语言反馈的能力",
  "abstract": "To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools.\nHowever, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases.\nWe introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback.\nTo ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4.\nWe repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and carefully curate them into a compact subset for efficient evaluation.\nOur analysis of 20 open- and closed-source LLMs offers intriguing findings.\n(a) LLMs generally benefit from tools and language feedback, with performance gains (absolute, same below) of 1--8% for each turn of tool use and 2--17% with natural language feedback.\n(b) Better single-turn performance does not guarantee better multi-turn performance.\n(c) Surprisingly, on the LLMs evaluated, supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities.\nWe expect MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation can be less accessible compared to commercial LLMs with a larger user base.",
  "abstract_zh": "为了完成复杂任务，大型语言模型（LLMs）通常需要与用户进行多轮交互，有时还需借助外部工具。然而，当前的评估协议往往强调基准性能和单轮交流，忽视了用户、LLMs和外部工具之间的细微互动，同时低估了用户自然语言反馈的重要性。这些忽视导致研究基准评估与现实使用案例之间存在差异。我们引入了MINT，一个评估LLMs在多轮交互中解决任务能力的基准，通过（1）使用工具和（2）利用自然语言反馈。为了确保可重复性，我们提供了一个评估框架，LLMs可以通过执行Python代码访问工具，并接收由GPT-4模拟的用户自然语言反馈。我们重新利用了一组多样化的已建立评估数据集，专注于推理、编码和决策，并将其精心整理成一个紧凑的子集以便高效评估。我们对20个开源和闭源LLMs的分析提供了有趣的发现。（a）LLMs通常从工具和语言反馈中受益，每轮工具使用的性能提升（绝对值，下同）为1-8%，自然语言反馈则为2-17%。（b）更好的单轮性能并不保证更好的多轮性能。（c）令人惊讶的是，在评估的LLMs中，监督指令微调（SIFT）和基于人类反馈的强化学习（RLHF）通常会损害多轮能力。我们希望MINT能够帮助衡量进展，并激励研究改善LLMs在多轮交互中的能力，特别是对于开放源代码社区而言，多轮人工评估相比于拥有更大用户基础的商业LLMs可能更难获得。"
}
{
  "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
  "title_zh": "LLM增强LLM：通过组合扩展能力",
  "abstract": "Foundational models with billions of parameters which have been trained on large corpus of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities,several new instances of these models are being trained towards new domains and tasks.  In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end,  we propose CALM—Composition to Augment Language Models—which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by ‘re-using’ existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly,when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40% over the base model for code generation and explanation tasks—on-par with fully fine-tuned counterparts.",
  "abstract_zh": "基础模型具有数十亿个参数，经过大量数据的训练，在多个领域展示了非平凡的技能。然而，由于其单一结构，增强这些模型或赋予新技能既具有挑战性又昂贵。另一方面，由于其适应能力，多个新的模型实例正在针对新领域和任务进行训练。在这项工作中，我们研究了高效且实用地将现有基础模型与更具体模型组合以启用新能力的问题。为此，我们提出了CALM——组合以增强语言模型——它在模型之间引入交叉注意力，以组合它们的表示并启用新能力。CALM的显著特点包括：（i）通过“重用”现有LLM以及少量额外参数和数据，在新任务上扩展LLM，（ii）现有模型权重保持不变，因此保留现有能力，以及（iii）适用于多样化的领域和设置。我们展示了将PaLM2-S与在低资源语言上训练的小型模型增强，导致在翻译成英语和低资源语言的算术推理等任务上实现高达13%的绝对提升。同样，当PaLM2-S与特定代码模型增强时，我们看到在代码生成和解释任务上相较于基础模型有40%的相对提升——与完全微调的对手相当。"
}
{
  "title": "Knowledge Fusion of Large Language Models",
  "title_zh": "大型语言模型的知识融合",
  "abstract": "While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures—Llama-2, MPT, and OpenLLaMA—across various benchmarks and tasks. Our findings confirm that the fusion of LLMs can improve the performance of the target model across a range of capabilities such as reasoning, commonsense, and code generation. Our code, model weights, and data are public at \\url{https://github.com/fanqiwan/FuseLLM}.",
  "abstract_zh": "虽然从头开始训练大型语言模型（LLMs）可以生成具有不同功能和优势的模型，但这需要付出巨大的成本，并可能导致冗余能力。另一种成本效益高且引人注目的方法是将现有的预训练LLMs合并为一个更强大的模型。然而，由于这些LLMs的架构各异，直接混合它们的权重是不切实际的。本文提出了LLMs的知识融合概念，旨在结合现有LLMs的能力并将其转移到单一的LLM中。通过利用源LLMs的生成分布，我们外化它们的集体知识和独特优势，从而有可能提升目标模型的能力，超越任何单一源LLM。我们使用三种不同架构的流行LLMs——Llama-2、MPT和OpenLLaMA——在各种基准和任务中验证了我们的方法。我们的研究结果确认，LLMs的融合可以提升目标模型在推理、常识和代码生成等多种能力上的表现。我们的代码、模型权重和数据公开在 \\url{https://github.com/fanqiwan/FuseLLM}。"
}
{
  "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning",
  "title_zh": "MuSR：多步骤软推理的思维链极限测试",
  "abstract": "While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our data instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.",
  "abstract_zh": "尽管配备了思维链提示等技术的大型语言模型（LLMs）展现了令人印象深刻的能力，但它们在复杂环境中进行稳健推理的能力仍然不足。然而，评估LLM推理的挑战在于，系统能力不断增长，而逻辑推理等任务的基准数据集却保持静态。我们引入了MuSR，一个用于评估语言模型在自然语言叙述中指定的多步骤软推理任务的数据集。该数据集具有两个关键特征。首先，它通过一种新颖的神经符号合成到自然生成算法创建，使得构建复杂推理实例成为可能，这些实例挑战GPT-4（例如，约1000字的谋杀谜题），并且可以随着更强大的LLM的发布进一步扩展。其次，我们的数据实例是对应于现实世界推理领域的自由文本叙述；这使得它在挑战性上远超其他合成基准，同时对于人类注释者而言，仍然现实且易于高准确率地解决。我们在该数据集上评估了一系列LLM和提示技术，并描述了思维链等技术在进行稳健推理时仍然存在的差距。"
}
{
  "title": "Towards Foundation Models for Knowledge Graph Reasoning",
  "title_zh": "面向知识图谱推理的基础模型",
  "abstract": "Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. \nKnowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.\nThe key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.\nIn this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. \nULTRA builds relational representations as a function conditioned on their interactions.\nSuch a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.\nConducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. \nFine-tuning further boosts the performance.",
  "abstract_zh": "基础模型在语言和视觉领域具有对任何文本和视觉输入进行推理的能力，这得益于可转移的表示，例如语言中的词汇表。知识图谱（KGs）具有不同的实体和关系词汇，通常不重叠。在知识图谱上设计基础模型的关键挑战是学习可转移的表示，以便能够对具有任意实体和关系词汇的任何图进行推理。在这项工作中，我们朝着这样的基础模型迈出了一步，并提出了ULTRA，这是一种学习通用和可转移图表示的方法。ULTRA将关系表示构建为其交互的条件函数。这种条件策略使得预训练的ULTRA模型能够归纳地推广到任何未见过的知识图谱，具有任意关系词汇，并能够在任何图上进行微调。在57个不同知识图谱上进行的链接预测实验中，我们发现单个预训练的ULTRA模型在各种规模的未见图上的零-shot归纳推理性能通常与在特定图上训练的强基线相当或更好。微调进一步提升了性能。"
}
{
  "title": "Debiasing Attention Mechanism in Transformer without Demographics",
  "title_zh": "去除变压器中注意力机制的偏见而不依赖人口统计信息",
  "abstract": "Although transformers demonstrate impressive capabilities in a variety of tasks, the fairness issue remains a significant concern when deploying these models. Existing works to address fairness issues in transformers require sensitive labels (such as age, gender, etc.), which can raise privacy concerns or violate legal regulations. An alternative way is through fairness without demographics. However, existing works that improve Rawlsian Max-Min fairness may impose overly restrictive constraints. Other methods that use auxiliary networks could be parameter inefficient. In this paper, we present a new approach to debiasing transformers by leveraging their inherent structure.  By reconsidering the roles of important components (queries, keys, and values) in the attention mechanism, we introduce a simple yet effective debiasing strategy from two perspectives: 1) Grounded in theoretical analysis, we normalize and apply absolute value operations to queries and keys to minimize the bias in attention weight allocation; 2) We reduce the bias within values through local alignment via contrastive learning. Throughout the entire process, our approach does not require any sensitive labels. Furthermore, to enhance memory efficiency in the training phase, we propose a strategy that debias only the last encoder to improve fairness in pre-trained models. We conduct experiments in computer vision and natural language processing tasks and show that our method is comparable and even outperforms the state-of-the-art method with substantially lower energy consumption.",
  "abstract_zh": "尽管变压器在多种任务中展现出令人印象深刻的能力，但在部署这些模型时，公平性问题仍然是一个重大关注点。现有的解决变压器公平性问题的工作需要敏感标签（如年龄、性别等），这可能引发隐私问题或违反法律法规。另一种方法是通过不依赖人口统计信息来实现公平。然而，现有改善罗尔斯最大-最小公平性的工作可能施加过于严格的限制。使用辅助网络的其他方法可能在参数上效率低下。本文提出了一种通过利用变压器固有结构来去除偏见的新方法。通过重新考虑注意力机制中重要组件（查询、键和值）的角色，我们从两个角度引入了一种简单而有效的去偏策略：1）基于理论分析，我们对查询和键进行归一化并应用绝对值操作，以最小化注意力权重分配中的偏见；2）通过对比学习通过局部对齐减少值中的偏见。在整个过程中，我们的方法不需要任何敏感标签。此外，为了提高训练阶段的内存效率，我们提出了一种仅去偏最后编码器的策略，以改善预训练模型的公平性。我们在计算机视觉和自然语言处理任务中进行了实验，结果表明我们的方法在能耗显著降低的情况下，与最先进的方法相当，甚至超越了它。"
}
{
  "title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text",
  "title_zh": "开放数学网络：高质量数学网页文本的开放数据集",
  "abstract": "There is growing evidence that pretraining on high quality, carefully thought-out tokens such as code or mathematics plays an important role in improving the reasoning abilities of large language models. For example, Minerva, a PaLM model finetuned on billions of tokens of mathematical documents from arXiv and the web, reported dramatically improved performance on problems that require quantitative reasoning. However, because all known open source web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community. We introduce OpenWebMath, an open dataset inspired by these works containing 14.7B tokens of mathematical webpages from Common Crawl. We describe in detail our method for extracting text and LaTeX content and removing boilerplate from HTML documents, as well as our methods for quality filtering and deduplication. Additionally, we run small-scale experiments by training 1.4B language models on OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass the performance of models trained on over 20x the amount of general language data. We hope that our dataset, open-sourced and released on the Hugging Face Hub, will help spur advances in the reasoning abilities of large language models.",
  "abstract_zh": "越来越多的证据表明，在高质量、经过深思熟虑的标记（如代码或数学）上进行预训练在提高大型语言模型的推理能力方面发挥着重要作用。例如，Minerva是一个在来自arXiv和网络的数十亿个数学文档标记上微调的PaLM模型，报告了在需要定量推理的问题上显著提高的性能。然而，由于所有已知的开源网络数据集都采用了不忠实于数学符号的预处理，因此大规模训练定量网络文档的好处对研究社区而言不可用。我们介绍了OpenWebMath，这是一个受到这些工作的启发而创建的开放数据集，包含来自Common Crawl的147亿个数学网页标记。我们详细描述了提取文本和LaTeX内容、去除HTML文档中的模板内容的方法，以及我们的质量过滤和去重方法。此外，我们通过在OpenWebMath上训练14亿语言模型进行小规模实验，表明在我们数据集的147亿个标记上训练的模型超越了在超过20倍的通用语言数据上训练的模型的性能。我们希望我们的数据集能够帮助推动大型语言模型推理能力的进步，并已在Hugging Face Hub上开源发布。"
}
{
  "title": "Task Planning for Visual Room Rearrangement under Partial Observability",
  "title_zh": "视觉房间重排中的部分可观测性任务规划",
  "abstract": "This paper presents a novel hierarchical task planner under partial observability\nthat empowers an embodied agent to use visual input to efficiently plan a sequence\nof actions for simultaneous object search and rearrangement in an untidy room,\nto achieve a desired tidy state. The paper introduces (i) a novel Search Network\nthat utilizes commonsense knowledge from large language models to find unseen\nobjects, (ii) a Deep RL network trained with proxy reward, along with (iii) a novel\ngraph-based state representation to produce a scalable and effective planner that\ninterleaves object search and rearrangement to minimize the number of steps taken\nand overall traversal of the agent, as well as to resolve blocked goal and swap\ncases, and (iv) a sample-efficient cluster-biased sampling for simultaneous training\nof the proxy reward network along with the Deep RL network. Furthermore,\nthe paper presents new metrics and a benchmark dataset - RoPOR, to measure\nthe effectiveness of rearrangement planning. Experimental results show that our\nmethod significantly outperforms the state-of-the-art rearrangement methods Weihs\net al. (2021a); Gadre et al. (2022); Sarch et al. (2022); Ghosh et al. (2022).",
  "abstract_zh": "本文提出了一种新颖的层次任务规划器，在部分可观测性下，使具身代理能够利用视觉输入有效规划一系列动作，以实现杂乱房间中物体的同时搜索和重排，从而达到期望的整洁状态。论文介绍了（i）一种新颖的搜索网络，利用大型语言模型的常识知识寻找未见物体，（ii）一种使用代理奖励训练的深度强化学习网络，以及（iii）一种新颖的基于图的状态表示，生成一个可扩展且有效的规划器，交替进行物体搜索和重排，以最小化代理的步骤数和整体遍历，同时解决阻塞目标和交换情况，以及（iv）一种样本高效的聚类偏向采样，用于同时训练代理奖励网络和深度强化学习网络。此外，论文还提出了新的指标和基准数据集——RoPOR，以衡量重排规划的有效性。实验结果表明，我们的方法显著优于最先进的重排方法 Weihs et al. (2021a); Gadre et al. (2022); Sarch et al. (2022); Ghosh et al. (2022)。"
}
{
  "title": "LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses",
  "title_zh": "轻量级语言模型校准：针对短文本和长文本响应的 LitCab",
  "abstract": "A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations of LMs as well as building more trustworthy models. However, standard calibration techniques may not be suited for LM calibration. For instance, post-processing methods such as temperature scaling do not reorder the candidate generations. On the other hand, training-based methods require fine-tuning the entire model, which is impractical for LMs of large scale. We present LitCab, a lightweight calibration mechanism consisting of a single linear layer that takes the input text representation and predicts a bias term, which is then added to the LM output logits. LitCab improves model calibration by only adding < 2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of eight text generation tasks, covering responses ranging from short phrases to paragraphs. We test LitCab with Llama2-7B, where it improves calibration across all tasks, reducing the average ECE score by as large as 30%. We further conduct a comprehensive evaluation with multiple popular open-sourced LMs from GPT and LLaMA families, yielding the following key findings: (i) Larger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones. (ii) GPT-family models show superior calibration compared to LLaMA, Llama2, and Vicuna models, despite having much fewer parameters. (iii) Fine-tuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of fine-tuning setups for calibrating LMs.",
  "abstract_zh": "当模型的概率估计与输出正确的实际可能性对齐时，模型被认为是良好校准的。校准语言模型（LM）至关重要，因为它在检测和减轻LM的幻觉以及构建更可信的模型中发挥着重要作用。然而，标准的校准技术可能不适合LM校准。例如，温度缩放等后处理方法并不重新排序候选生成。另一方面，基于训练的方法需要微调整个模型，这对于大规模的LM来说是不切实际的。我们提出了LitCab，这是一种轻量级的校准机制，由一个单一的线性层组成，该层接受输入文本表示并预测一个偏差项，然后将其添加到LM输出的logits中。LitCab通过仅添加< 2%的原始模型参数来改善模型校准。为了评估，我们构建了CaT，一个包含八个文本生成任务的基准，涵盖从短语到段落的响应。我们在Llama2-7B上测试LitCab，结果在所有任务中改善了校准，平均ECE分数降低了多达30%。我们进一步对多个来自GPT和LLaMA系列的流行开源LM进行了全面评估，得出了以下关键发现：（i）同一家族中较大的模型在短生成任务上表现出更好的校准，但不一定适用于较长的任务。（ii）尽管参数更少，GPT系列模型在校准方面优于LLaMA、Llama2和Vicuna模型。（iii）用有限目的（例如对话）样本微调预训练模型（例如LLaMA）可能导致更差的校准，强调了微调设置在校准LM中的重要性。"
}
{
  "title": "Language Models Represent Space and Time",
  "title_zh": "语言模型表示空间和时间",
  "abstract": "The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual \"space neurons\" and \"time neurons\" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.",
  "abstract_zh": "大型语言模型（LLMs）的能力引发了关于这些系统是否仅学习了大量表面统计数据或一组更连贯且扎根于现实世界的表示的辩论。我们通过分析Llama-2系列模型中三个空间数据集（世界、美国、纽约地方）和三个时间数据集（历史人物、艺术作品、新闻标题）所学习的表示，发现了后者的证据。我们发现LLMs在多个尺度上学习空间和时间的线性表示。这些表示对提示变化具有鲁棒性，并在不同实体类型（例如城市和地标）之间统一。此外，我们识别出个别“空间神经元”和“时间神经元”，它们可靠地编码空间和时间坐标。尽管需要进一步调查，但我们的结果表明现代LLMs学习了现实世界的丰富时空表示，并具备世界模型的基本要素。"
}
{
  "title": "Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model",
  "title_zh": "安全离线强化学习与可行性引导扩散模型",
  "abstract": "Safe offline reinforcement learning is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the feasible region while minimizing safety risks in the infeasible region. Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline RL), which allows safety constraint adherence, reward maximization, and offline policy learning to be realized via three decoupled processes, while offering strong safety performance and stability. In FISOR, the optimal policy for the translated optimization problem can be derived in a special form of weighted behavior cloning, which can be effectively extracted with a guided diffusion model thanks to its expressiveness.  We compare FISOR against baselines on DSRL benchmark for safe offline RL. Evaluation results show that FISOR is the only method that can guarantee safety satisfaction in all tasks, while achieving top returns in most tasks. Code: https://github.com/ZhengYinan-AIR/FISOR.",
  "abstract_zh": "安全离线强化学习是一种有前景的方法，可以绕过风险较高的在线交互，实现安全策略学习。大多数现有方法仅施加软约束，即将安全违规的期望限制在预先设定的阈值以下，这可能导致潜在的不安全结果，因此在安全关键场景中不可接受。另一种选择是施加零违规的硬约束。然而，在离线环境中，这可能会面临挑战，因为需要在安全约束满足、奖励最大化和离线数据集施加的行为正则化这三个高度复杂且相关的方面之间找到正确的平衡。有趣的是，我们发现通过安全控制理论的可达性分析，硬安全约束可以等效地转化为识别给定离线数据集的最大可行区域。这无缝地将原始三重问题转化为一个依赖可行性的目标，即在可行区域内最大化奖励值，同时在不可行区域内最小化安全风险。受到这些启发，我们提出了FISOR（可行性引导的安全离线强化学习），它允许通过三个解耦的过程实现安全约束遵循、奖励最大化和离线策略学习，同时提供强大的安全性能和稳定性。在FISOR中，转化优化问题的最优策略可以以加权行为克隆的特殊形式推导出来，这得益于其表达能力，可以通过引导扩散模型有效提取。我们在DSRL基准上将FISOR与安全离线强化学习的基线进行比较。评估结果表明，FISOR是唯一能够保证所有任务安全满足的方法，同时在大多数任务中实现最高回报。代码：https://github.com/ZhengYinan-AIR/FISOR。"
}
{
  "title": "ContextRef: Evaluating Referenceless Metrics for Image Description Generation",
  "title_zh": "ContextRef：评估无参考度量在图像描述生成中的应用",
  "abstract": "Referenceless metrics (e.g., CLIPScore) use pretrained vision--language models to assess image descriptions directly without costly ground-truth reference texts. Such methods can facilitate rapid progress, but only if they truly align with human preference judgments. In this paper, we introduce ContextRef, a benchmark for assessing referenceless metrics for such alignment. ContextRef has two components: human ratings along a variety of established quality dimensions, and ten diverse robustness checks designed to uncover fundamental weaknesses. A crucial aspect of ContextRef is that images and descriptions are presented in context, reflecting prior work showing that context is important for description quality. Using ContextRef, we assess a variety of pretrained models, scoring functions, and techniques for incorporating context. None of the methods is successful with ContextRef, but we show that careful fine-tuning yields substantial improvements. ContextRef remains a challenging benchmark though, in large part due to the challenge of context dependence.",
  "abstract_zh": "无参考度量（例如，CLIPScore）利用预训练的视觉-语言模型直接评估图像描述，而无需昂贵的真实参考文本。这种方法可以促进快速进展，但前提是它们确实与人类偏好判断一致。本文介绍了ContextRef，一个用于评估无参考度量对齐性的基准。ContextRef包含两个部分：沿多种已建立质量维度的人类评分，以及十个旨在揭示基本弱点的多样化鲁棒性检查。ContextRef的一个关键方面是图像和描述在上下文中呈现，反映了先前研究表明上下文对描述质量的重要性。使用ContextRef，我们评估了多种预训练模型、评分函数和纳入上下文的技术。没有一种方法在ContextRef上取得成功，但我们展示了细致的微调可以带来显著改善。尽管如此，ContextRef仍然是一个具有挑战性的基准，这在很大程度上是由于上下文依赖性的挑战。"
}
{
  "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
  "title_zh": "基于声谱图的LLM进行口语问答和语音续写",
  "abstract": "We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples and spoken QA dataset via our website.",
  "abstract_zh": "我们提出了Spectron，这是一种将预训练的大型语言模型（LLM）适应于口语问答（QA）和语音续写的新方法。通过为LLM赋予一个预训练的语音编码器，我们的模型能够处理语音输入并生成语音输出。整个系统是端到端训练的，直接在声谱图上操作，从而简化了我们的架构。我们方法的关键是一个训练目标，该目标通过仅使用配对的语音-文本对来共同监督语音识别、文本续写和语音合成，从而在单次解码过程中实现“跨模态”的思维链。我们的方法在说话者保留和语义连贯性方面超越了现有的口语语言模型。此外，所提出的模型在保留原始LLM知识方面优于直接初始化，这通过口语QA数据集得到了验证。我们通过网站发布了我们的音频样本和口语QA数据集。"
}
{
  "title": "Learning Performance-Improving Code Edits",
  "title_zh": "学习提升性能的代码编辑",
  "abstract": "With the waning of Moore's law, optimizing program performance has become a major focus of software research. However, high-level optimizations such as API and algorithm changes remain elusive due to the difficulty of understanding the semantics of code.\nSimultaneously, pretrained large language models (LLMs) have demonstrated strong capabilities at solving a wide range of programming tasks.\nTo that end, we introduce a framework for adapting LLMs to high-level program optimization.\nFirst, we curate a dataset of performance-improving edits made by human programmers of over 77,000 competitive C++ programming submission pairs, accompanied by extensive unit tests.\nA major challenge is the significant variability of measuring performance on commodity hardware, which can lead to spurious \"improvements\".\nTo isolate and reliably evaluate the impact of program optimizations, we design an environment based on the gem5 full system simulator, the de facto simulator used in academia and industry.\nNext, we propose a broad range of adaptation strategies for code optimization; for prompting, these include retrieval-based few-shot prompting and chain-of-thought, and for finetuning, these include performance-conditioned generation and synthetic data augmentation based on self-play.\nA combination of these techniques achieves an average speedup of 5.65 times on CodeLlama-13B and 6.86 times on GPT-3.5, surpassing the best human performance (4.06 times).\nWe find our proposed performance-conditioned generation is particularly effective at improving performance as well as increasing the fraction of optimized programs.",
  "abstract_zh": "随着摩尔定律的减弱，优化程序性能已成为软件研究的主要焦点。然而，由于理解代码语义的困难，高级优化（如API和算法更改）仍然难以实现。同时，预训练的大型语言模型（LLMs）在解决广泛的编程任务方面表现出强大的能力。为此，我们提出了一个将LLMs适应于高级程序优化的框架。首先，我们整理了一个由人类程序员在超过77,000个竞争性C++编程提交对中进行的性能提升编辑的数据集，并附有广泛的单元测试。一个主要挑战是在商品硬件上测量性能的显著变异性，这可能导致虚假的“改进”。为了隔离和可靠地评估程序优化的影响，我们设计了一个基于gem5全系统模拟器的环境，这是学术界和工业界使用的事实上的模拟器。接下来，我们提出了一系列广泛的代码优化适应策略；对于提示，这些包括基于检索的少量提示和思维链，对于微调，这些包括性能条件生成和基于自我对弈的合成数据增强。这些技术的结合在CodeLlama-13B上实现了平均5.65倍的加速，在GPT-3.5上实现了6.86倍的加速，超过了最佳人类表现（4.06倍）。我们发现我们提出的性能条件生成在提高性能和增加优化程序的比例方面特别有效。"
}
{
  "title": "How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations",
  "title_zh": "标题：变压器如何在上下文中超越简单函数进行学习？基于表征学习的案例研究",
  "abstract": "While large language models based on the transformer architecture have demonstrated remarkable in-context learning (ICL) capabilities, understandings of such capabilities are still in an early stage, where existing theory and mechanistic understanding focus mostly on simple scenarios such as learning simple function classes. This paper takes initial steps on understanding ICL in more complex scenarios, by studying learning with \\emph{representations}. Concretely, we construct synthetic in-context learning problems with a compositional structure, where the label depends on the input through a possibly complex but \\emph{fixed} representation function, composed with a linear function that \\emph{differs} in each instance. By construction, the optimal ICL algorithm first transforms the inputs by the representation function, and then performs linear ICL on top of the transformed dataset. We show theoretically the existence of transformers that approximately implement such algorithms with mild depth and size.  Empirically, we find trained transformers consistently achieve near-optimal ICL performance in this setting, and exhibit the desired dissection where lower layers transforms the dataset and upper layers perform linear ICL. Through extensive probing and a new pasting experiment, we further reveal several mechanisms within the trained transformers, such as concrete copying behaviors on both the inputs and the representations, linear ICL capability of the upper layers alone, and a post-ICL representation selection mechanism in a harder mixture setting. These observed mechanisms align well with our theory and may shed light on how transformers perform ICL in more realistic scenarios.",
  "abstract_zh": "摘要：尽管基于变压器架构的大型语言模型展示了显著的上下文学习（ICL）能力，但对这些能力的理解仍处于早期阶段，现有理论和机制理解主要集中在学习简单函数类等简单场景上。本文通过研究基于\\emph{表征}的学习，初步探讨了在更复杂场景中理解ICL的问题。具体而言，我们构建了具有组合结构的合成上下文学习问题，其中标签通过可能复杂但\\emph{固定}的表征函数依赖于输入，并与每个实例中\\emph{不同}的线性函数组合。根据构造，最优的ICL算法首先通过表征函数转换输入，然后在转换后的数据集上执行线性ICL。我们从理论上证明了存在能够近似实现这种算法的变压器，且其深度和规模适中。经验上，我们发现训练后的变压器在这种设置中始终能够实现接近最优的ICL性能，并展示出所需的分解，其中较低层对数据集进行转换，较高层执行线性ICL。通过广泛的探测和新的拼接实验，我们进一步揭示了训练变压器内部的几个机制，例如对输入和表征的具体复制行为、仅上层的线性ICL能力，以及在更困难的混合设置中的后ICL表征选择机制。这些观察到的机制与我们的理论高度一致，并可能为变压器在更现实场景中执行ICL的方式提供启示。"
}
{
  "title": "Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning",
  "title_zh": "对比偏好学习：在没有强化学习的情况下从人类反馈中学习",
  "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the regret model of human preferences. Using the principle of maximum entropy, we derive Contrastive Preference Learning (CPL), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL. CPL is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs. In contrast to prior work, this enables CPL to elegantly scale to high-dimensional and sequential RLHF problems.",
  "abstract_zh": "从人类反馈中进行强化学习（RLHF）已成为使模型与人类意图对齐的流行范式。通常，RLHF算法分为两个阶段：首先，利用人类偏好学习奖励函数；其次，通过强化学习（RL）优化学习到的奖励以对齐模型。该范式假设人类偏好是根据奖励分布的，但最近的研究表明，它们实际上遵循用户最优策略下的遗憾。因此，从反馈中学习奖励函数不仅基于对人类偏好的错误假设，还导致了源于策略梯度或RL阶段自举的繁琐优化挑战。由于这些优化挑战，当代RLHF方法限制在上下文赌博设置（例如，在大型语言模型中）或限制观察维度（例如，基于状态的机器人）。我们通过引入一类新算法，利用人类偏好的遗憾模型来优化来自人类反馈的行为，从而克服这些限制。利用最大熵原理，我们推导出对比偏好学习（CPL），这是一种在不学习奖励函数的情况下从偏好中学习最优策略的算法，避免了对RL的需求。CPL完全是离策略的，仅使用简单的对比目标，并且可以应用于任意MDP。与之前的工作相比，这使得CPL能够优雅地扩展到高维和序列RLHF问题。"
}
{
  "title": "ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models",
  "title_zh": "ECoFLaP：高效的粗到细逐层剪枝用于视觉-语言模型",
  "abstract": "Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable performance improvements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption, making it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification. Alternatively, several studies have recently proposed layer-wise pruning approaches to avoid the expensive computation of global pruning and efficiently compress model weights according to their importance within a layer. However, these methods often suffer from suboptimal model compression due to their lack of a global perspective. To address this limitation in recent efficient pruning methods for large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP), a two-stage coarse-to-fine weight pruning approach for LVLMs. We first determine the sparsity ratios of different layers or blocks by leveraging the global importance score, which is efficiently computed based on the zeroth-order approximation of the global model gradients. Then, the multimodal model performs layer-wise unstructured weight pruning. We validate our proposed method across various multi-modal and single-modal models and datasets, demonstrating significant performance improvements over prevalent pruning techniques in the high-sparsity regime.",
  "abstract_zh": "大型视觉-语言模型（LVLMs）通过整合来自不同模态的丰富信息，能够全面理解世界，在各种多模态下游任务中取得显著的性能提升。然而，由于其巨大的计算/能源成本和碳消耗，部署LVLMs往往存在问题，使得采用传统的迭代全局剪枝变得不可行，因为全局剪枝需要计算整个大型模型的海森矩阵以进行稀疏化，这一过程成本高昂。作为替代，最近有多个研究提出了逐层剪枝方法，以避免全局剪枝的高昂计算成本，并根据层内的重要性有效压缩模型权重。然而，这些方法通常由于缺乏全局视角而导致模型压缩效果不佳。为了解决这一限制，我们提出了高效的粗到细逐层剪枝（ECoFLaP），这是一种针对LVLMs的两阶段粗到细权重剪枝方法。我们首先通过利用全局重要性评分来确定不同层或块的稀疏率，该评分是基于全局模型梯度的零阶近似高效计算得出的。然后，多模态模型执行逐层非结构化权重剪枝。我们在各种多模态和单模态模型及数据集上验证了我们提出的方法，显示出在高稀疏率下显著优于现有剪枝技术的性能提升。"
}
{
  "title": "On Bias-Variance Alignment in Deep Models",
  "title_zh": "深度模型中的偏差-方差对齐",
  "abstract": "Classical wisdom in machine learning holds that the generalization error can be decomposed into bias and variance, and these two terms exhibit a \\emph{trade-off}. However, in this paper, we show that for an ensemble of deep learning based classification models, bias and variance are \\emph{aligned} at a sample level, where squared bias is approximately \\emph{equal} to variance for correctly classified sample points. We present empirical evidence confirming this phenomenon in a variety of deep learning models and datasets. Moreover, we study this phenomenon from two theoretical perspectives: calibration and neural collapse. We first show theoretically that under the assumption that the models are well calibrated, we can observe the bias-variance alignment. Second, starting from the picture provided by the neural collapse theory, we show an approximate correlation between bias and variance.",
  "abstract_zh": "经典的机器学习理论认为，泛化误差可以分解为偏差和方差，这两个术语之间存在一种“权衡”。然而，在本文中，我们展示了对于基于深度学习的分类模型集，偏差和方差在样本级别上是“对齐”的，其中平方偏差与正确分类的样本点的方差大致“相等”。我们提供了实证证据，确认这一现象在多种深度学习模型和数据集中的存在。此外，我们从两个理论角度研究这一现象：标定和神经崩溃。我们首先理论上展示，在模型良好标定的假设下，我们可以观察到偏差-方差对齐。其次，从神经崩溃理论提供的视角出发，我们展示了偏差和方差之间的近似相关性。"
}
{
  "title": "InstructDET: Diversifying Referring Object Detection with Generalized Instructions",
  "title_zh": "标题：InstructDET：通过通用指令多样化指代对象检测",
  "abstract": "We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are from foundation models. Our InDET is developed from existing REC datasets and object detection datasets, with the expanding potential that any image with object bbxs can be incorporated through using our InstructDET method. By using our InDET dataset, we show that a conventional ROD model surpasses existing methods on standard REC datasets and our InDET test set. Our data-centric method InstructDET, with automatic data expansion by leveraging foundation models, directs a promising field that ROD can be greatly diversified to execute common object detection instructions.",
  "abstract_zh": "摘要：我们提出了InstructDET，这是一种以数据为中心的指代对象检测（ROD）方法，基于用户指令定位目标对象。虽然源于指代表达（REC），但我们利用的指令大大多样化，以涵盖与对象检测相关的常见用户意图。对于一张图像，我们生成大量指令，指向每个单独的对象以及多个对象的不同组合。每个指令及其对应的对象边界框（bbxs）构成一对训练数据。为了涵盖常见的检测表达，我们引入新兴的视觉-语言模型（VLM）和大型语言模型（LLM），通过文本提示和对象bbxs生成指令，因为基础模型的泛化能力有效地产生类人表达（例如，描述对象属性、类别和关系）。我们将构建的数据集命名为InDET。它包含来自基础模型的图像、bbxs和通用指令。我们的InDET是从现有的REC数据集和对象检测数据集中开发的，具有扩展潜力，任何带有对象bbxs的图像都可以通过使用我们的InstructDET方法纳入。通过使用我们的InDET数据集，我们展示了传统的ROD模型在标准REC数据集和我们的InDET测试集上超越了现有方法。我们的以数据为中心的方法InstructDET，通过利用基础模型实现自动数据扩展，指向一个有前景的领域，即ROD可以大大多样化，以执行常见的对象检测指令。"
}
{
  "title": "Finetuning Text-to-Image Diffusion Models for Fairness",
  "title_zh": "标题：微调文本到图像扩散模型以实现公平性",
  "abstract": "The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model's sampling process (adjusted DFT), which leverages an adjusted gradient to directly optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives of fairness beyond absolute equality, which is demonstrated by controlling age to a $75\\\\%$ young and $25\\\\%$ old distribution while simultaneously debiasing gender and race. Finally, our method is scalable: it can debias multiple concepts at once by simply including these prompts in the finetuning data. We share code and various fair diffusion model adaptors at https://sail-sg.github.io/finetune-fair-diffusion/.",
  "abstract_zh": "摘要：文本到图像扩散模型在社会中的快速采用凸显了迫切需要解决其偏见的问题。如果不采取干预措施，这些偏见可能会传播扭曲的世界观并限制少数群体的机会。在本研究中，我们将公平性框架视为一个分布对齐问题。我们的解决方案包括两个主要技术贡献：（1）一种分布对齐损失，旨在将生成图像的特定特征引导至用户定义的目标分布；（2）调整后的扩散模型采样过程的直接微调（调整DFT），利用调整后的梯度直接优化在生成图像上定义的损失。从经验上看，我们的方法显著减少了职业提示中的性别、种族及其交叉偏见。即使只微调五个软令牌，性别偏见也显著减少。至关重要的是，我们的方法支持超越绝对平等的多样化公平性视角，这通过将年龄控制在75%的年轻人和25%的老年人分布，同时去偏见性别和种族得以证明。最后，我们"
}
{
  "title": "Kosmos-G: Generating Images in Context with Multimodal Large Language Models",
  "title_zh": "Kosmos-G：利用多模态大型语言模型在上下文中生成图像",
  "abstract": "Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\"",
  "abstract_zh": "最近在以主题驱动的图像生成方面取得了显著进展。然而，当前的方法在多样化应用场景中仍然不足，因为它们需要在测试时进行调优，并且无法接受交错的多图像和文本输入。这些限制使它们离“图像作为图像生成中的外语”的最终目标相去甚远。本文提出了Kosmos-G，一个利用多模态大型语言模型（MLLM）的先进多模态感知能力来解决上述挑战的模型。我们的方法将MLLM的输出空间与CLIP对齐，使用文本模态作为锚点，并在精心策划的数据上进行组合指令调优。Kosmos-G展示了在交错的多图像和文本输入下进行零-shot主题驱动生成的令人印象深刻的能力。值得注意的是，分数蒸馏指令调优不需要对图像解码器进行修改。这使得CLIP的无缝替代成为可能，并且能够轻松与各种U-Net技术集成，从细粒度控制到个性化图像解码器变体。我们将Kosmos-G视为朝着“图像作为图像生成中的外语”目标的初步尝试。"
}
{
  "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
  "title_zh": "标题：微调对齐语言模型的安全性妥协，即使用户并无恶意！",
  "abstract": "Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open-source release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on customized datasets accelerate this trend. But, what are the safety costs associated with such customized fine-tuning? While existing safety alignment techniques restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing --- even if a model's initial safety alignment is impeccable, how can it be maintained after customized fine-tuning? We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the customized fine-tuning of aligned LLMs.  (This paper contains red-teaming data and model-generated content that can be offensive in nature.)",
  "abstract_zh": "摘要：为了下游应用优化大型语言模型（LLMs）通常涉及通过进一步微调定制预训练的LLMs。Meta发布的开源Llama模型和OpenAI的API加速了这一趋势。然而，这种定制微调所带来的安全成本是什么？虽然现有的安全对齐技术在推理时限制了LLMs的有害行为，但它们并未涵盖当微调权限扩展到最终用户时的安全风险。我们的红队研究发现，仅用少量对抗性设计的训练示例进行微调就可能危及LLMs的安全对齐。例如，我们通过仅在10个这样的示例上微调GPT-3.5 Turbo，花费不到0.20美元，成功突破了其安全防护，使模型对几乎任何有害指令做出反应。令人不安的是，我们的研究还表明，即使没有恶意意图，仅用良性和常用数据集进行微调也可能无意中降低LLMs的安全对齐，尽管程度较轻。这些发现表明，微调对齐的LLMs引入了新的安全风险，而当前的安全基础设施未能有效应对——即使模型的初始安全对齐完美，如何在定制微调后保持这种安全性？我们概述并批判性分析了潜在的缓解措施，并倡导进一步研究以加强对齐LLMs定制微调的安全协议。"
}
{
  "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
  "title_zh": "自我RAG：通过自我反思学习检索、生成和批判",
  "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called **Self-Reflective Retrieval-Augmented Generation (Self-RAG)** that enhances an LM's quality and factuality through retrieval and self-reflection. \nOur framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its generations using special tokens, called {\\it reflection} tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. \nExperiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. \nSpecifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning, and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models. Our code and trained models are available at https://selfrag.github.io/",
  "abstract_zh": "尽管大型语言模型（LLMs）具有显著的能力，但由于仅依赖其所包含的参数知识，它们常常生成包含事实不准确的响应。检索增强生成（RAG）是一种临时方法，通过检索相关知识来增强语言模型，从而减少此类问题。然而，无差别地检索和纳入固定数量的检索段落，无论检索是否必要或段落是否相关，都会降低语言模型的多样性或导致无益的响应生成。我们引入了一种新的框架，称为自我反思检索增强生成（Self-RAG），通过检索和自我反思提高语言模型的质量和事实性。我们的框架训练一个单一的任意语言模型，该模型能够根据需求自适应地检索段落，并使用称为反思令牌的特殊令牌生成和反思检索到的段落及其生成内容。生成反思令牌使得语言模型在推理阶段可控，使其能够根据不同的任务需求调整行为。实验表明，Self-RAG（7B和13B参数）在多种任务上显著优于最先进的LLMs和检索增强模型。具体而言，Self-RAG在开放域问答、推理和事实验证任务上优于ChatGPT和检索增强的Llama2-chat，并在相对于这些模型的长文本生成中显著提高了事实性和引用准确性。我们的代码和训练模型可在https://selfrag.github.io/获取。"
}
{
  "title": "Lemur: Harmonizing Natural Language and Code for Language Agents",
  "title_zh": "标题：Lemur：为语言代理协调自然语言与代码",
  "abstract": "We introduce Lemur and Lemur-Chat, openly accessible language models optimized\nfor both natural language and coding capabilities to serve as the backbone\nof versatile language agents. The evolution from language chat models to\nfunctional language agents demands that models not only master human interaction,\nreasoning, and planning but also ensure grounding in the relevant environments.\nThis calls for a harmonious blend of language and coding capabilities\nin the models. Lemur and Lemur-Chat are proposed to address this necessity,\ndemonstrating balanced proficiencies in both domains, unlike existing\nopen-source models that tend to specialize in either. Through meticulous pretraining\nusing a code-intensive corpus and instruction fine-tuning on text and code\ndata, our models achieve state-of-the-art averaged performance across diverse\ntext and coding benchmarks. Comprehensive experiments demonstrate Lemur’s\nsuperiority over existing open-source models and its proficiency across various\nagent tasks involving human communication, tool usage, and interaction under\nfully- and partially- observable environments. The harmonization between natural\nand programming languages enables Lemur-Chat to significantly narrow the\ngap with proprietary models on agent abilities, providing key insights into developing\nadvanced open-source agents adept at reasoning, planning, and operating\nseamlessly across environments. Our model and code have been open-sourced at\nhttps://github.com/OpenLemur/Lemur.",
  "abstract_zh": "摘要：我们介绍了Lemur和Lemur-Chat，这些开放访问的语言模型经过优化，旨在同时具备自然语言和编码能力，以作为多功能语言代理的核心。从语言聊天模型到功能性语言代理的演变要求模型不仅要掌握人类交互、推理和规划，还要确保在相关环境中的基础。这需要模型在语言和编码能力之间实现和谐融合。Lemur和Lemur-Chat的提出正是为了解决这一需求，展示了在这两个领域的平衡能力，与现有倾向于专注于某一领域的开源模型不同。通过使用密集代码语料库的细致预训练以及对文本和代码数据的指令微调，我们的模型在多样的文本和编码基准测试中实现了最先进的平均性能。全面的实验表明，Lemur在现有开源模型中具有优势，并在涉及人类沟通、工具使用和在完全可观察和部分可观察环境下的交互等各种代理任务中表现出色。自然语言与编程语言之间的和谐使得Lemur-Chat在代理能力上显著缩小了与专有模型的差距，为开发能够在推理、规划和无缝操作于不同环境中表现出色的先进开源代理提供了重要见解。我们的模型和代码已在https://github.com/OpenLemur/Lemur上开源。"
}
{
  "title": "True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning",
  "title_zh": "真实知识来自实践：通过强化学习将大型语言模型与具身环境对齐",
  "abstract": "Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decision-making environment, Overcooked, and simulated household environment, VirtualHome. ii) Benefiting from LLMs' open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks. iii) Under our framework, there is no significant loss of the LLMs' original ability during online PPO finetuning.",
  "abstract_zh": "尽管在众多任务中表现出色，大型语言模型（LLMs）在解决简单决策任务时常常失败，因为LLMs中的知识与环境不一致。相反，强化学习（RL）代理从零开始学习策略，这使得它们始终与环境对齐，但难以结合先前知识以进行高效探索。为缩小这一差距，我们提出了TWOSOME，这是一种新颖的通用在线框架，利用LLMs作为决策代理，通过RL高效地与具身环境进行交互和对齐，而无需任何准备好的数据集或对环境的先验知识。首先，我们使用LLMs查询每个有效动作的联合概率，以形成行为策略。然后，为了增强策略的稳定性和鲁棒性，我们提出了两种归一化方法，并总结了四个提示设计原则。最后，我们设计了一种新颖的参数高效训练架构，其中演员和评论家共享一个配备低秩适配器（LoRA）的冻结LLM，并通过PPO进行更新。我们进行了广泛的实验来评估TWOSOME。i) 在经典决策环境Overcooked和模拟家庭环境VirtualHome中，TWOSOME在样本效率和性能上显著优于传统RL方法PPO和提示调优方法SayCan。ii) 受益于LLMs的开放词汇特性，TWOSOME在未见任务上表现出更强的泛化能力。iii) 在我们的框架下，在线PPO微调过程中LLMs的原始能力没有显著损失。"
}
{
  "title": "Label-free Node Classification on Graphs with Large Language Models (LLMs)",
  "title_zh": "无标签的图节点分类方法：基于大型语言模型（LLMs）",
  "abstract": "In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to obtain annotations of high quality, representativeness, and diversity, thereby enhancing GNN performance with less cost?\nTo tackle this challenge, we develop an annotation quality heuristic and leverage the confidence scores derived from LLMs to advanced node selection. Comprehensive experimental results validate the effectiveness of LLM-GNN. In particular, LLM-GNN can achieve an accuracy of 74.9\\% on a vast-scale dataset \\products with a cost less than 1 dollar.",
  "abstract_zh": "近年来，图神经网络（GNNs）在节点分类方面取得了显著进展。然而，它们需要大量高质量的标签以确保良好的性能。相比之下，大型语言模型（LLMs）在文本属性图上展现出令人印象深刻的零-shot 能力。然而，它们在高效处理结构数据方面面临挑战，并且推理成本高。基于这些观察，本研究提出了一种基于LLMs的无标签图节点分类管道LLM-GNN。该方法结合了GNNs和LLMs的优势，同时减轻了它们的局限性。具体而言，LLMs被用来标注一小部分节点，然后GNNs在LLMs的标注上进行训练，以对剩余的大部分节点进行预测。LLM-GNN的实现面临一个独特的挑战：我们如何主动选择节点供LLMs标注，从而增强GNN的训练？我们如何利用LLMs获得高质量、具有代表性和多样性的标注，从而以更低的成本提升GNN性能？为了解决这一挑战，我们开发了一种标注质量启发式方法，并利用LLMs产生的置信度分数来进行先进的节点选择。全面的实验结果验证了LLM-GNN的有效性。特别是，LLM-GNN在一个大规模数据集\\products上可以实现74.9\\%的准确率，成本低于1美元。"
}
{
  "title": "PolyVoice: Language Models for Speech to Speech Translation",
  "title_zh": "多语音：用于语音到语音翻译的语言模型",
  "abstract": "With the huge success of GPT models in natural language processing, there is a growing interest in applying language modeling approaches to speech tasks.\nCurrently, the dominant architecture in speech-to-speech translation (S2ST) remains the encoder-decoder paradigm, creating a need to investigate the impact of language modeling approaches in this area. \nIn this study, we introduce PolyVoice, a language model-based framework designed for S2ST systems. Our framework comprises three decoder-only language models: a translation language model, a duration language model, and a speech synthesis language model. \nThese language models employ different types of prompts to extract learned information effectively. By utilizing unsupervised semantic units, our framework can transfer semantic information across these models, making it applicable even to unwritten languages. \nWe evaluate our system on Chinese $\\rightarrow$ English and English $\\rightarrow$ Spanish language pairs. Experimental results demonstrate that \\method outperforms the state-of-the-art encoder-decoder model, producing voice-cloned speech with high translation and audio quality.\nSpeech samples are available at https://polyvoice.github.io.",
  "abstract_zh": "随着GPT模型在自然语言处理中的巨大成功，越来越多的人对将语言建模方法应用于语音任务产生了兴趣。目前，语音到语音翻译（S2ST）中的主导架构仍然是编码器-解码器范式，这就需要研究语言建模方法在该领域的影响。在本研究中，我们介绍了PolyVoice，一个基于语言模型的框架，旨在为S2ST系统提供支持。我们的框架包括三个仅解码器的语言模型：翻译语言模型、时长语言模型和语音合成语言模型。这些语言模型采用不同类型的提示有效提取学习到的信息。通过利用无监督语义单元，我们的框架能够在这些模型之间传递语义信息，使其适用于未书写的语言。我们在中文→英文和英文→西班牙文语言对上评估了我们的系统。实验结果表明，\\method的表现优于最先进的编码器-解码器模型，生成高翻译和音频质量的语音克隆样本。语音样本可在https://polyvoice.github.io获取。"
}
{
  "title": "Language Model Beats Diffusion - Tokenizer is key to visual generation",
  "title_zh": "语言模型超越扩散模型 - 分词器是视觉生成的关键",
  "abstract": "While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce \\modelname{}, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.",
  "abstract_zh": "虽然大型语言模型（LLMs）在语言生成任务中占主导地位，但在图像和视频生成方面，它们的表现不如扩散模型。为了有效地使用LLMs进行视觉生成，一个关键组件是视觉分词器，它将像素空间输入映射到适合LLM学习的离散标记。在本文中，我们介绍了\\modelname{}，这是一种视频分词器，旨在使用通用标记词汇为视频和图像生成简洁而富有表现力的标记。配备这一新分词器后，我们展示了LLMs在标准图像和视频生成基准（包括ImageNet和Kinetics）上超越了扩散模型。此外，我们证明我们的分词器在两个任务上超越了之前表现最佳的视频分词器：（1）根据人类评估，与下一代视频编码器（VCC）相当的视频压缩，以及（2）为动作识别任务学习有效表示。"
}
{
  "title": "ZeRO++: Extremely Efficient Collective Communication for Large Model Training",
  "title_zh": "ZeRO++：大规模模型训练的极高效集体通信",
  "abstract": "Zero Redundancy Optimizer (ZeRO) has been used to train a wide range of large language models on massive GPU clusters due to its ease of use, efficiency, and good scalability. However, when training on low-bandwidth clusters, and/or when small batch size per GPU is used, ZeRO’s effective throughput is limited due to communication overheads. To alleviate this limitation, this paper introduces ZeRO++ composing of three communication volume reduction techniques (lowprecision all-gather, data remapping, and low-precision gradient averaging) to significantly reduce the communication volume up to 4x that enables up to 2.16x better throughput at 384 GPU scale. Our results also show ZeRO++ can speedup the RLHF by 3.3x compared to vanilla ZeRO. To verify the convergence of ZeRO++, we test up to 13B model for pretraining with 8/6-bits all gather and up to 30B model for finetuning with 4/2-bits all gather, and demonstrate on-par accuracy as original ZeRO (aka standard training). As a byproduct, the model trained with ZeRO++ is naturally weight-quantized, which can be directly used for inference without post-training quantization or quantization-aware training.",
  "abstract_zh": "零冗余优化器（ZeRO）因其易用性、高效性和良好的可扩展性，被广泛用于在大规模GPU集群上训练各种大型语言模型。然而，在低带宽集群上训练时，或使用每个GPU的小批量时，由于通信开销，ZeRO的有效吞吐量受到限制。为了解决这一限制，本文提出了ZeRO++，结合了三种通信量减少技术（低精度全收集、数据重映射和低精度梯度平均），显著减少通信量，最多可减少4倍，从而在384 GPU规模下实现高达2.16倍的吞吐量提升。我们的结果还表明，与原始ZeRO相比，ZeRO++可以将RLHF的速度提升3.3倍。为了验证ZeRO++的收敛性，我们在预训练中测试了高达13B的模型，使用8/6位全收集，在微调中测试了高达30B的模型，使用4/2位全收集，并展示了与原始ZeRO（即标准训练）相当的准确性。作为副产品，使用ZeRO++训练的模型自然是权重量化的，可以直接用于推理，而无需后训练量化或量化感知训练。"
}
{
  "title": "Generative Judge for Evaluating Alignment",
  "title_zh": "生成性评估者用于评估对齐",
  "abstract": "The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regarding *generality* (i.e., assessing performance across diverse scenarios), *flexibility* (i.e., examining under different protocols), and *interpretability* (i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters, **Auto-J**, designed to address these challenges. Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. To demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. Experimentally, **Auto-J** outperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. We also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://github.com/GAIR-NLP/auto-j.",
  "abstract_zh": "大型语言模型（LLMs）的快速发展大大扩展了它们可以处理的任务范围。在自然语言处理（NLP）领域，研究人员已将重点从传统的NLP任务（例如，序列标注和解析）转向围绕与人类需求对齐的任务（例如，头脑风暴和电子邮件撰写）。这种任务分布的变化对评估这些对齐模型提出了新的要求，包括*通用性*（即评估在不同场景下的表现）、*灵活性*（即在不同协议下进行检查）和*可解释性*（即通过解释审查模型）。在本文中，我们提出了一种具有130亿参数的生成性评估者**Auto-J**，旨在应对这些挑战。我们的模型在大量真实场景下的用户查询和LLM生成的响应上进行训练，并支持多种评估协议（例如，成对响应比较和单响应评估），提供结构良好的自然语言批评。为了展示我们方法的有效性，我们构建了一个覆盖58种不同场景的新测试平台。实验表明，**Auto-J**在多个强竞争对手（包括开源和闭源模型）中表现优异，领先幅度显著。我们还提供了详细的分析和案例研究，以进一步揭示我们方法的潜力，并在https://github.com/GAIR-NLP/auto-j上公开了多种资源。"
}
{
  "title": "Can LLMs Keep a Secret? Testing  Privacy  Implications of Language Models  via Contextual Integrity Theory",
  "title_zh": "标题：大型语言模型能保守秘密吗？通过情境完整性理论测试语言模型的隐私影响",
  "abstract": "Existing efforts on quantifying privacy implications for large language models (LLMs) solely focus on measuring leakage of training data. In this work, we shed light on the often-overlooked interactive settings where an LLM receives information from multiple sources and generates an output to be shared with other entities, creating the potential of exposing sensitive input data in inappropriate contexts. In these scenarios, humans nat- urally uphold privacy by choosing whether or not to disclose information depending on the context. We ask the question “Can LLMs demonstrate an equivalent discernment and reasoning capability when considering privacy in context?” We propose CONFAIDE, a benchmark grounded in the theory of contextual integrity and designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. CONFAIDE consists of four tiers, gradually increasing in complexity, with the final tier evaluating contextual privacy reasoning and theory of mind capabilities. Our experiments show that even commercial models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively, highlighting the urgent need for a new direction of privacy-preserving approaches as we demonstrate a larger underlying problem stemmed in the models’ lack of reasoning capabilities.",
  "abstract_zh": "摘要：现有关于量化大型语言模型（LLMs）隐私影响的研究仅关注训练数据泄露的测量。在本研究中，我们关注那些常被忽视的互动场景，在这些场景中，LLM从多个来源接收信息并生成输出与其他实体共享，从而可能在不当的上下文中暴露敏感输入数据。在这些情况下，人类自然会通过根据上下文选择是否披露信息来维护隐私。我们提出问题：“LLM在考虑上下文中的隐私时，能否表现出等同的辨别和推理能力？”我们提出了CONFAIDE，这是一个基于情境完整性理论的基准，旨在识别指令调优的LLM在隐私推理能力方面的关键弱点。CONFAIDE由四个层级组成，复杂性逐渐增加，最终层级评估上下文隐私推理和心智理论能力。我们的实验表明，即使是像GPT-4和ChatGPT这样的商业模型，在人类不会泄露私密信息的上下文中，仍然分别有39%和57%的时间揭示了私人信息，这突显了在模型缺乏推理能力的背景下，迫切需要新的隐私保护方法。"
}
{
  "title": "Confronting Reward Model Overoptimization with Constrained RLHF",
  "title_zh": "标题：通过约束强化学习人类反馈对奖励模型过度优化的应对",
  "abstract": "Large language models are typically aligned with human preferences by optimizing reward models (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to *overoptimization*, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each RM's threshold of usefulness. Our method addresses the problem of weighting component RMs by learning dynamic weights, naturally given by the Lagrange multipliers. As a result, each RM stays within the range at which it is an effective proxy, improving evaluation performance. Finally, we introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run.",
  "abstract_zh": "摘要：大型语言模型通常通过优化适应人类反馈的奖励模型（RM）来与人类偏好对齐。然而，人类偏好是多方面的，越来越多地从多个简单奖励模型的组合中获取奖励，每个模型捕捉语言质量的不同方面。这本身就带来了挑战，因为在组合这些模型时，适当地权衡这些组件RM是困难的。更复杂的是，由于任何RM仅是人类评估的代理，这一过程容易受到*过度优化*的影响，即在某个点之后，累积更高的奖励与更差的人类评分相关联。在本文中，我们首次研究了复合RM中的过度优化，显示组件RM之间的相关性对这些点的位置有显著影响。然后，我们引入了一种使用约束强化学习来解决此问题的方法，以防止代理超出每个RM的有效性阈值。我们的方法通过学习动态权重来解决组件RM的权重问题，这些权重自然由拉格朗日乘数给出。因此，每个RM保持在其有效代理的范围内，从而提高评估性能。最后，我们引入了一种自适应方法，使用无梯度优化在单次运行中识别并优化这些点。"
}
{
  "title": "DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks",
  "title_zh": "动态评估大型语言模型在推理任务中的表现",
  "abstract": "Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. \nIn this paper, we introduce DyVal, a general and flexible protocol for dynamic evaluation of LLMs. Based on our framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo and GPT-4. Experiments show that LLMs perform worse in DyVal-generated evaluation samples with different complexities, highlighting the significance of dynamic evaluation.\nWe also analyze the failure cases and results of different prompting methods.\nMoreover, DyVal-generated samples are not only evaluation sets, but also helpful data for fine-tuning to improve the performance of LLMs on existing benchmarks.\nWe hope that DyVal can shed light on future evaluation research of LLMs. Code is available at: https://github.com/microsoft/promptbench.",
  "abstract_zh": "大型语言模型（LLMs）在各种评估基准中取得了显著的性能。然而，对于其庞大训练语料库中潜在的数据污染问题引发了担忧。此外，当前基准的静态特性和固定复杂性可能无法充分评估LLMs不断提升的能力。本文介绍了DyVal，一种用于LLMs动态评估的通用灵活协议。基于我们的框架，我们利用有向无环图的结构优势构建了图信息驱动的DyVal，以动态生成具有可控复杂度的评估样本。DyVal生成了在数学、逻辑推理和算法问题等推理任务上的挑战性评估集。我们评估了从Flan-T5-large到GPT-3.5-Turbo和GPT-4的各种LLMs。实验表明，LLMs在DyVal生成的不同复杂度的评估样本中表现较差，突显了动态评估的重要性。我们还分析了不同提示方法的失败案例和结果。此外，DyVal生成的样本不仅是评估集，也是有助于微调以提高LLMs在现有基准上表现的数据。我们希望DyVal能够为未来LLMs的评估研究提供启示。代码可在：https://github.com/microsoft/promptbench获取。"
}
{
  "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
  "title_zh": "标题：大型语言模型能否表达其不确定性？对大型语言模型信心引导的实证评估",
  "abstract": "Empowering large language models (LLMs) to accurately express confidence in their answers is essential for reliable and trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on *white-box access* to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of *black-box* approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: *prompting* strategies for eliciting verbalized confidence, *sampling* methods for generating multiple responses, and *aggregation* techniques for computing consistency. We then benchmark these methods on two key tasks—confidence calibration and failure prediction—across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be *overconfident*, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve, yet still far from ideal performance. \n3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. \n4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs. The code is publicly available at https://github.com/MiaoXiong2320/llm-uncertainty.",
  "abstract_zh": "摘要：使大型语言模型（LLMs）能够准确表达其答案的信心对于可靠和可信的决策至关重要。以往的信心引导方法主要依赖于对内部模型信息的“白箱访问”或模型微调，已不再适合LLMs，尤其是封闭源的商业API。因此，探索LLM不确定性估计的“黑箱”方法的需求日益增加。为了更好地分解问题，我们定义了一个包含三个组成部分的系统框架：用于引导口头表达信心的“提示”策略、用于生成多个响应的“采样”方法，以及用于计算一致性的“聚合”技术。然后，我们在两个关键任务——信心校准和失败预测——上对这几种方法进行了基准测试，涵盖五种类型的数据集（例如，常识和算术推理）以及包括GPT-4和LLaMA 2 Chat在内的五种广泛使用的LLMs。我们的分析揭示了几个关键见解：1）LLMs在口头表达其信心时，往往表现出“过于自信”，可能模仿人类表达信心的模式。2）随着模型能力的提升，校准和失败预测的表现有所改善，但仍远未理想。3）采用我们提出的策略，如人类启发的提示、多个响应之间的一致性和更好的聚合策略，可以从不同角度帮助减轻这种过于自信的现象。4）与白箱方法的比较表明，尽管白箱方法表现更好，但差距较小，例如，AUROC从0.522提高到0.605。尽管取得了这些进展，但这些技术均未能始终优于其他方法，所有调查的方法在需要专业知识的挑战性任务中均表现不佳，表明仍有显著的改进空间。我们相信这项研究可以作为一个强有力的基准，并为在黑箱LLMs中引导信心提供见解。代码已公开发布在https://github.com/MiaoXiong2320/llm-uncertainty。"
}
{
  "title": "Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models",
  "title_zh": "注意力满足：从约束满足的视角看语言模型的事实错误",
  "abstract": "We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as constraint satisfaction problems and use this framework to investigate how the LLM interacts internally with factual constraints. We find a strong positive relationship between the LLM's attention to constraint tokens and the factual accuracy of generations. We curate a suite of 10 datasets containing over 40,000 prompts to study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing attention patterns, that can predict factual errors and fine-grained constraint satisfaction, and allow early error identification. The approach and findings take another step towards using the mechanistic understanding of LLMs to enhance their reliability.",
  "abstract_zh": "我们研究了基于Transformer的大型语言模型（LLMs）在生成事实不正确文本时的内部行为。我们建议将事实查询建模为约束满足问题，并利用这一框架研究LLM如何在内部与事实约束进行交互。我们发现LLM对约束标记的注意力与生成的事实准确性之间存在强正相关关系。我们整理了一套包含超过40,000个提示的10个数据集，以研究在所有规模（7B、13B、70B）下预测Llama-2系列的事实错误的任务。我们提出了SAT Probe，一种探测注意力模式的方法，可以预测事实错误和细粒度的约束满足，并允许早期错误识别。这种方法和发现进一步推动了利用LLM的机制理解来增强其可靠性。"
}
{
  "title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions",
  "title_zh": "安全调优的LLaMAs：改善遵循指令的大型语言模型安全性的经验教训",
  "abstract": "Training large language models to follow instructions makes them perform better on a wide range of tasks and generally become more helpful. However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content.\nIn this paper, we raise concerns over the safety of models that only emphasize helpfulness, not harmlessness, in their instruction-tuning.\nWe show that several popular instruction-tuned models are highly unsafe. Moreover, we show that adding just 3\\% safety examples (a few hundred demonstrations) when fine-tuning a model like LLaMA can substantially improve its safety. Our safety-tuning does not make models significantly less capable or helpful as measured by standard benchmarks. However, we do find exaggerated safety behaviours, where too much safety-tuning makes models refuse perfectly safe prompts if they superficially resemble unsafe ones. As a whole, our results illustrate trade-offs in training LLMs to be helpful and training them to be safe.",
  "abstract_zh": "训练大型语言模型以遵循指令使其在广泛任务上表现更好，并通常变得更加有帮助。然而，一个完美有用的模型会遵循甚至最恶意的指令，并轻易生成有害内容。本文提出了对仅强调有用性而不强调无害性的模型在指令调优中的安全性问题。我们展示了几种流行的指令调优模型的安全性极低。此外，我们表明，在微调像LLaMA这样的模型时，仅添加3%的安全示例（几百个示范）可以显著提高其安全性。我们的安全调优并没有使模型在标准基准测试中显著降低能力或有用性。然而，我们确实发现了夸大的安全行为，即过度的安全调优使模型拒绝完全安全的提示，如果它们表面上类似于不安全的提示。总体而言，我们的结果展示了训练大型语言模型以有用和安全之间的权衡。"
}
{
  "title": "An Unforgeable Publicly Verifiable Watermark for Large Language Models",
  "title_zh": "不可伪造的可公开验证的大语言模型水印",
  "abstract": "Recently, text watermarking algorithms for large language models (LLMs) have been proposed to mitigate the potential harms of text generated by LLMs, including fake news and copyright issues. However, current watermark detection algorithms require the secret key used in the watermark generation process, making them susceptible to security breaches and counterfeiting during public detection.\nTo address this limitation, we propose an unforgeable publicly verifiable watermark algorithm named UPV that uses two different neural networks for watermark generation and detection, instead of using the same key at both stages. Meanwhile, the token embedding parameters are shared between the generation and detection networks, which makes the detection network achieve a high accuracy very efficiently.\nExperiments demonstrate that our algorithm attains high detection accuracy and computational efficiency through neural networks. Subsequent analysis confirms the high complexity involved in forging the watermark from the detection network. Our code is available at https://github.com/THU-BPM/unforgeable_watermark",
  "abstract_zh": "最近，针对大语言模型（LLMs）的文本水印算法被提出，以减轻LLMs生成的文本可能带来的危害，包括假新闻和版权问题。然而，当前的水印检测算法需要在水印生成过程中使用的秘密密钥，这使得它们在公共检测中容易受到安全漏洞和伪造的影响。为了解决这一限制，我们提出了一种名为UPV的不可伪造的可公开验证水印算法，该算法使用两个不同的神经网络进行水印生成和检测，而不是在两个阶段使用相同的密钥。同时，生成和检测网络之间共享令牌嵌入参数，这使得检测网络能够高效地实现高准确率。实验表明，我们的算法通过神经网络实现了高检测准确率和计算效率。后续分析确认了从检测网络伪造水印所涉及的高复杂性。我们的代码可在https://github.com/THU-BPM/unforgeable_watermark获取。"
}
{
  "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation",
  "title_zh": "标题：BESA：通过块状参数高效稀疏分配修剪大型语言模型",
  "abstract": "Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to individual transformer blocks, and ii) it allocates layer-specific sparsity in a differentiable manner, both of which ensure reduced performance degradation after pruning. Our experiments show that BESA achieves state-of-the-art performance, efficiently pruning LLMs like LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five hours. Code is available at [here](https://github.com/LinkAnonymous/BESA).",
  "abstract_zh": "摘要：大型语言模型（LLMs）在文本摘要、文本问答等各种任务中表现出色。尽管其性能令人印象深刻，但由于参数数量庞大，其计算负担可能是不可承受的。现有解决方案如SparseGPT和Wanda试图通过权重修剪来缓解这一问题。然而，它们的逐层方法导致模型输出的显著扰动，并且需要精细的超参数调整，例如修剪率，这可能会对整体模型性能产生不利影响。为了解决这个问题，本文提出了一种新颖的LLM修剪技术，称为块状参数高效稀疏分配（BESA），通过应用块状重构损失来实现。与典型的逐层修剪技术相比，BESA具有两个显著特征：i）它针对各个变换块的整体修剪误差，ii）它以可微的方式分配层特定的稀疏性，这两者都确保了修剪后性能下降的减少。我们的实验表明，BESA在单个A100 GPU上仅用五小时就能高效修剪参数从7B到70B的LLMs，如LLaMA1和LLaMA2，达到最先进的性能。代码可在[此处](https://github.com/LinkAnonymous/BESA)获取。"
}
{
  "title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
  "title_zh": "评估指令调优语言模型的零-shot 鲁棒性",
  "abstract": "Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper, we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. Put another way, instruction-tuned models are not especially robust to instruction re-phrasings. \nWe propose a simple method to mitigate this issue by introducing ``soft prompt'' embedding parameters and optimizing these to maximize the similarity between representations of semantically equivalent instructions. We show that this method consistently improves the robustness of instruction-tuned models.",
  "abstract_zh": "指令微调最近成为提高大型语言模型（LLMs）在新任务上零-shot 能力的一种有前景的方法。该技术在提高中等规模 LLM 性能方面表现出特别的优势，有时其性能与更大模型变体相当。本文提出两个问题：（1）指令调优模型对指令的具体措辞有多敏感？（2）我们如何使它们对这种自然语言变异更具鲁棒性？为回答第一个问题，我们收集了由 NLP 从业者手动编写的319条指令，涵盖80多个在广泛使用的基准测试中包含的独特任务，并评估这些指令与在指令微调过程中观察到的指令措辞相比的方差和平均性能。我们发现，使用新颖（未观察到的）但适当的指令措辞会持续降低模型性能，有时降幅显著。此外，这些自然指令在下游性能上产生了较大的方差，尽管它们在语义上是等价的。换句话说，指令调优模型对指令的重新措辞并不特别鲁棒。我们提出了一种简单的方法来缓解这个问题，通过引入“软提示”嵌入参数并优化这些参数以最大化语义等价指令表示之间的相似性。我们展示了该方法持续提高了指令调优模型的鲁棒性。"
}
{
  "title": "Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification",
  "title_zh": "后续差异描述：语言模型解决图像分类中的歧义",
  "abstract": "A promising approach for improving the performance of vision-language models like CLIP for image classification is to extend the class descriptions (i.e., prompts) with related attributes, e.g., using brown sparrow instead of sparrow. However, current zero-shot methods select a subset of attributes regardless of commonalities between the target classes, potentially providing no useful information that would have helped to distinguish between them. For instance, they may use color instead of bill shape to distinguish between sparrows and wrens, which are both brown. We propose Follow-up Differential Descriptions (FuDD), a zero-shot approach that tailors the class descriptions to each dataset and leads to additional attributes that better differentiate the target classes. FuDD first identifies the ambiguous classes for each image, and then uses a Large Language Model (LLM) to generate new class descriptions that differentiate between them. The new class descriptions resolve the initial ambiguity and help predict the correct label. In our experiments, FuDD consistently outperforms generic description ensembles and naive LLM-generated descriptions on 12 datasets. We show that differential descriptions are an effective tool to resolve class ambiguities, which otherwise significantly degrade the performance. We also show that high quality natural language class descriptions produced by FuDD result in comparable performance to few-shot adaptation methods.",
  "abstract_zh": "一种有前景的方法是通过相关属性扩展视觉语言模型（如CLIP）的类描述（即提示），以提高图像分类的性能，例如使用“棕色麻雀”而不是“麻雀”。然而，当前的零样本方法选择一组属性，而不考虑目标类之间的共性，可能提供没有用的信息来帮助区分它们。例如，它们可能使用颜色而不是喙形状来区分棕色的麻雀和鹪鹩。我们提出了后续差异描述（FuDD），这是一种零样本方法，针对每个数据集定制类描述，并引入更多有助于区分目标类的属性。FuDD首先识别每张图像的模糊类，然后使用大型语言模型（LLM）生成新的类描述以区分它们。新的类描述解决了初始的歧义，并帮助预测正确的标签。在我们的实验中，FuDD在12个数据集上始终优于通用描述集和天真的LLM生成描述。我们展示了差异描述是解决类歧义的有效工具，否则会显著降低性能。我们还表明，FuDD生成的高质量自然语言类描述的性能与少样本适应方法相当。"
}
{
  "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning",
  "title_zh": "通过群体不变学习提高与人类偏好对齐的泛化能力",
  "abstract": "The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. \nAs universal AI assistants, there's a growing expectation for them to perform consistently across various domains. \nHowever, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples.\nThis focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data.\nIn this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. \nGiven the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance.\nThen, we optimize the policy to perform well on challenging groups. \nLastly, leveraging the established groups, our approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data. Experimental results indicate that our approach significantly enhances training stability and model generalization.",
  "abstract_zh": "基于语言模型的AI助手的成功在很大程度上依赖于来自人类反馈的强化学习（RLHF），这使得生成的响应更符合人类偏好。作为通用AI助手，人们对它们在各个领域的一致表现寄予了越来越高的期望。然而，先前的研究表明，强化学习（RL）常常利用捷径来获得高奖励，忽视了具有挑战性的样本。这种对快速奖励获取的关注削弱了训练的稳定性以及模型对新数据的泛化能力。在这项工作中，我们提出了一种新方法，可以通过RL在不同数据组或领域中学习一致的策略。考虑到获取群体注释的挑战，我们的方法自动将数据分类为不同组，故意最大化性能方差。然后，我们优化策略以在具有挑战性的组上表现良好。最后，利用已建立的组，我们的方法自适应地调整探索空间，将更多学习能力分配给更具挑战性的数据，防止模型在简单数据上过度优化。实验结果表明，我们的方法显著提高了训练稳定性和模型泛化能力。"
}
{
  "title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
  "title_zh": "张量信任：来自在线游戏的可解释提示注入攻击",
  "abstract": "While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to *prompt injection attacks*: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 126,000 prompt injection attacks and 46,000 prompt-based \"defenses\" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is the first dataset that includes both human-generated attacks and defenses for instruction-following LLMs. The attacks in our dataset have easily interpretable structure, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as *prompt extraction* and *prompt hijacking*. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release data and code at [tensortrust.ai/paper](https://tensortrust.ai/paper)",
  "abstract_zh": "尽管大型语言模型（LLMs）在现实世界应用中越来越普遍，但它们仍然容易受到*提示注入攻击*的影响：恶意第三方提示颠覆系统设计者的意图。为了帮助研究人员研究这个问题，我们提供了一个包含超过126,000个提示注入攻击和46,000个针对提示注入的“防御”的数据集，这些都是由在线游戏“张量信任”的玩家创建的。据我们所知，这是第一个包含人类生成的攻击和针对指令跟随LLMs的防御的数据集。我们数据集中的攻击具有易于解释的结构，并揭示了LLMs的弱点。我们还利用该数据集创建了一个针对两种类型提示注入的抵抗基准，我们称之为*提示提取*和*提示劫持*。我们的基准结果表明，许多模型对张量信任数据集中的攻击策略存在脆弱性。此外，我们还展示了数据集中一些攻击策略可以推广到已部署的基于LLM的应用程序，即使它们面临与游戏截然不同的约束。我们在[tensortrust.ai/paper](https://tensortrust.ai/paper)发布数据和代码。"
}
{
  "title": "Circuit Component Reuse Across Tasks in Transformer Language Models",
  "title_zh": "标题：变压器语言模型中任务间电路组件的重用",
  "abstract": "Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in (Wang, 2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in which we adjust four attention heads in middle layers in order to ‘repair’ the Colored Objects circuit and make it behave like the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the Colored Objects task and explain most sources of error. The intervention affects downstream attention heads in specific ways predicted by their interactions in the IOI circuit, indicating that this subcircuit behavior is invariant to the different task inputs. Overall, our results provide evidence that it may yet be possible to explain large language models' behavior in terms of a relatively small number of interpretable task-general algorithmic building blocks and computational components.",
  "abstract_zh": "摘要：最近的机制可解释性研究表明，通过电路分析可以成功逆向工程语言模型中的行为。然而，一个普遍的批评是，每个电路都是特定于任务的，因此这样的分析无法在更高层次上理解模型。在本研究中，我们提供证据表明，见解（关于特定头部的低级发现和关于一般算法的高级发现）确实可以跨任务进行概括。具体而言，我们研究了（Wang, 2022）中为间接宾语识别（IOI）任务发现的电路，并1.) 证明它在更大的GPT2模型上得以重现，2.) 证明它主要被重用于解决一个看似不同的任务：有色物体（Ippolito & Callison-Burch, 2023）。我们提供证据表明，这两个任务背后的过程在功能上非常相似，并且在电路内注意头部中约有78%的重叠。我们进一步提出了一个概念验证干预实验，在该实验中，我们调整了中间层中的四个注意头，以“修复”有色物体电路，使其表现得像IOI电路。通过这样做，我们将有色物体任务的准确率从49.6%提高到93.7%，并解释了大部分错误来源。该干预以特定方式影响下游注意头，正如它们在IOI电路中的交互所预测的那样，表明该子电路行为对不同任务输入是保持不变的。总体而言，我们的结果提供了证据，表明可能有可能用相对少量的可解释的任务通用算法构建块和计算组件来解释大型语言模型的行为。"
}
{
  "title": "The Alignment Problem from a Deep Learning Perspective",
  "title_zh": "深度学习视角下的对齐问题",
  "abstract": "AI systems based on deep learning have reached or surpassed human performance in a range of narrow domains. In coming years or decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. In this position paper, we examine the technical difficulty of fine-tuning hypothetical AGI systems based on pretrained deep models to pursue goals that are aligned with human interests. We argue that, if trained like today's most capable models, AGI systems could learn to act deceptively to receive higher reward, learn internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not.",
  "abstract_zh": "基于深度学习的人工智能系统在多个狭窄领域已达到或超过人类表现。在未来的几年或几十年中，人工通用智能（AGI）可能在许多关键任务上超越人类能力。在这篇立场论文中，我们考察了基于预训练深度模型微调假设AGI系统以追求与人类利益一致的目标的技术难度。我们认为，如果像今天最强大的模型那样进行训练，AGI系统可能会学会采取欺骗性行为以获得更高的奖励，学习内部表示的目标，这些目标超出了它们的微调分布，并使用寻求权力的策略追求这些目标。我们回顾了这些特性的最新证据。具有这些特性的AGI将难以对齐，即使它们看起来是对齐的。"
}
{
  "title": "A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models",
  "title_zh": "机器翻译的范式转变：提升大语言模型的翻译性能",
  "abstract": "Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on.\nOur approach consists of two fine-tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data.  We introduce the LLM  developed through this strategy as **A**dvanced **L**anguage **M**odel-based tr**A**nslator (**ALMA**). Based on LLaMA-2 as our underlying model, our results show that the model can achieve an average improvement of more than 12 BLEU and 12 COMET over its zero-shot performance across 10 translation directions from the WMT'21 (2 directions) and WMT'22 (8 directions) test datasets. The performance is significantly better than all prior work and even superior to the NLLB-54B model \\citep{nllb} and GPT-3.5-text-davinci-003, with only 7B or 13B parameters. This method establishes the foundation for a novel training paradigm in machine translation.",
  "abstract_zh": "生成性大语言模型（LLMs）在各种自然语言处理任务中取得了显著进展。然而，这些进展在翻译任务中并未得到体现，特别是对于中等模型规模（即7B或13B参数）的模型，它们仍然落后于传统的监督编码-解码翻译模型。以往的研究尝试提高这些LLMs的翻译能力，但效果有限。在本研究中，我们提出了一种新颖的针对翻译任务的LLMs微调方法，消除了传统翻译模型通常依赖的大量平行数据的需求。我们的方法包括两个微调阶段：首先在单语数据上进行初步微调，然后在一小组高质量平行数据上进行后续微调。我们将通过这一策略开发的LLM称为**A**dvanced **L**anguage **M**odel-based tr**A**nslator（**ALMA**）。基于LLaMA-2作为基础模型，我们的结果表明，该模型在WMT'21（2个方向）和WMT'22（8个方向）测试数据集的10个翻译方向上，相较于其零-shot性能平均提高了超过12 BLEU和12 COMET。其性能显著优于所有先前的工作，甚至优于NLLB-54B模型和GPT-3.5-text-davinci-003，且参数仅为7B或13B。该方法为机器翻译中的新训练范式奠定了基础。"
}
{
  "title": "Unveiling the Pitfalls of Knowledge Editing for Large Language Models",
  "title_zh": "揭示大语言模型知识编辑的陷阱",
  "abstract": "As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs—a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp the innate knowledge structure of LLMs. Experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on LLMs, which warrant attention and efforts for future works. Code and data are available at https://github.com/zjunlp/PitfallsKnowledgeEditing.",
  "abstract_zh": "随着对大语言模型（LLMs）微调成本的不断上升，近期研究工作已转向开发编辑LLMs中隐含知识的方法。然而，仍然存在一个阴影——知识编辑是否会引发蝴蝶效应？因为尚不清楚知识编辑是否会引入潜在风险的副作用。本文开创性地探讨了与LLMs知识编辑相关的潜在陷阱。为此，我们引入了新的基准数据集并提出了创新的评估指标。我们的结果强调了两个关键问题：（1）知识冲突：编辑逻辑上冲突的事实组可能会放大LLMs中固有的不一致性，这是之前方法所忽视的方面。（2）知识扭曲：为了编辑事实知识而改变参数可能会不可逆转地扭曲LLMs的内在知识结构。实验结果生动地表明，知识编辑可能无意中对LLMs产生意想不到的后果，这值得未来的研究关注和努力。代码和数据可在https://github.com/zjunlp/PitfallsKnowledgeEditing获取。"
}
{
  "title": "$\\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis",
  "title_zh": "$\\mathcal{B}$-编码器：基于价值的深度强化学习用于程序合成",
  "abstract": "Program synthesis aims to create accurate, executable programs from problem specifications, specifically from natural language descriptions in our context. \nRecent studies have leveraged the power of reinforcement learning (RL) in conjunction with large language models (LLMs), significantly enhancing code generation capabilities. The application of RL focuses on directly optimizing for functional correctness, offering an advantage over conventional supervised methods. \nDespite policy-based RL methods dominating the literature on RL for program synthesis, the nature of program synthesis tasks hints at a natural alignment with value-based methods.\nThis stems from the rich collection of off-policy programs, including those developed by human programmers and also historical samples, coupled with the straightforward verification of generated programs through automated unit testing, meaning rewards are easy to obtain.\nDiverging from the dominant use of policy-based algorithms, our work explores the feasibility of value-based approaches, leading to the development of our $\\mathcal{B}$-Coder (pronounced Bellman coder).\nYet, training value-based methods presents challenges due to the enormous search space inherent to program synthesis. \nTo this end, we introduce an initialization protocol for RL agents utilizing pre-trained LMs and a conservative Bellman operator to reduce training complexities. \nMoreover, we demonstrate how to leverage the learned value functions as a dual strategy to post-process generated programs. \nOur empirical evaluations demonstrated $\\mathcal{B}$-Coder's capability in achieving state-of-the-art performance when compared to policy-based methods. \nRemarkably, this achievement is reached with minimal reward engineering effort, highlighting the effectiveness of value-based RL, independent of reward designs.",
  "abstract_zh": "程序合成旨在从问题规范中创建准确、可执行的程序，特别是在我们的背景下，从自然语言描述中生成。最近的研究利用了强化学习（RL）与大型语言模型（LLMs）的结合，显著增强了代码生成能力。RL的应用侧重于直接优化功能正确性，相较于传统的监督方法具有优势。尽管基于策略的RL方法主导了程序合成领域的文献，但程序合成任务的性质暗示了与基于价值的方法的自然契合。这源于丰富的离策略程序集合，包括人类程序员开发的程序和历史样本，以及通过自动单元测试对生成程序的简单验证，意味着奖励易于获得。与基于策略算法的主导使用不同，我们的工作探索了基于价值的方法的可行性，进而开发了我们的$\\mathcal{B}$-编码器（发音为贝尔曼编码器）。然而，由于程序合成固有的巨大搜索空间，训练基于价值的方法面临挑战。为此，我们引入了一种利用预训练语言模型和保守贝尔曼算子的RL代理初始化协议，以减少训练复杂性。此外，我们展示了如何利用学习到的价值函数作为双重策略对生成的程序进行后处理。我们的实证评估表明，与基于策略的方法相比，$\\mathcal{B}$-编码器在实现最先进性能方面的能力。值得注意的是，这一成就是在最小的奖励工程努力下实现的，突显了基于价值的RL的有效性，与奖励设计无关。"
}
{
  "title": "DIAGNOSIS: Detecting Unauthorized Data Usages in Text-to-image Diffusion Models",
  "title_zh": "标题：DIAGNOSIS：检测文本到图像扩散模型中的未经授权数据使用",
  "abstract": "Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized data usage during the training or fine-tuning process. One example is when a model trainer collects a set of images created by a particular artist and attempts to train a model capable of generating similar images without obtaining permission and giving credit to the artist. To address this issue, we propose a method for detecting such unauthorized data usage by planting the injected memorization into the text-to-image diffusion models trained on the protected dataset. Specifically, we modify the protected images by adding unique contents on these images using stealthy image warping functions that are nearly imperceptible to humans but can be captured and memorized by diffusion models. By analyzing whether the model has memorized the injected content (i.e., whether the generated images are processed by the injected post-processing function), we can detect models that had illegally utilized the unauthorized data. Experiments on Stable Diffusion and VQ Diffusion with different model training or fine-tuning methods (i.e, LoRA, DreamBooth, and standard training) demonstrate the effectiveness of our proposed method in detecting unauthorized data usages. Code: https://github.com/ZhentingWang/DIAGNOSIS.",
  "abstract_zh": "摘要：最近的文本到图像扩散模型在生成高质量图像方面表现出惊人的性能。然而，在训练或微调过程中对未经授权的数据使用的担忧随之而来。一个例子是，当模型训练者收集特定艺术家创作的一组图像，并试图训练一个能够生成类似图像的模型，而未获得艺术家的许可并给予其信用。为了解决这个问题，我们提出了一种通过在受保护的数据集上训练的文本到图像扩散模型中植入注入记忆的方法来检测这种未经授权的数据使用。具体而言，我们通过使用对人类几乎不可察觉的隐蔽图像变形函数，修改受保护的图像，添加独特的内容，这些内容可以被扩散模型捕获和记忆。通过分析模型是否记住了注入的内容（即生成的图像是否经过注入的后处理函数处理），我们可以检测出非法使用未经授权数据的模型。在对Stable Diffusion和VQ Diffusion进行不同模型训练或微调方法（即LoRA、DreamBooth和标准训练）的实验中，证明了我们提出的方法在检测未经授权的数据使用方面的有效性。代码：https://github.com/ZhentingWang/DIAGNOSIS。"
}
{
  "title": "AnyText: Multilingual Visual Text Generation and Editing",
  "title_zh": "任何文本：多语言视觉文本生成与编辑",
  "abstract": "Diffusion model based Text-to-Image has achieved impressive achievements recently. Although current technology for synthesizing images is highly advanced and capable of generating images with high fidelity, it is still possible to give the show away when focusing on the text area in the generated image, as synthesized text often contains blurred, unreadable, or incorrect characters, making visual text generation one of the most challenging issues in this field. To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model, that focuses on rendering accurate and coherent text in the image. AnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We employed text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy. AnyText can write characters in multiple languages, to the best of our knowledge, this is the first work to address multilingual visual text generation. It is worth mentioning that AnyText can be plugged into existing diffusion models from the community for rendering or editing text accurately. After conducting extensive evaluation experiments, our method has outperformed all other approaches by a significant margin. Additionally, we contribute the first large-scale multilingual text images dataset, AnyWord-3M, containing 3 million image-text pairs with OCR annotations in multiple languages. Based on AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual text generation accuracy and quality. Our project will be open-sourced soon to improve and promote the development of text generation technology.",
  "abstract_zh": "扩散模型基础的文本到图像技术最近取得了显著成就。尽管当前合成图像的技术高度先进，能够生成高保真的图像，但在生成图像的文本区域仍可能出现模糊、不可读或错误字符的问题，使得视觉文本生成成为该领域最具挑战性的问题之一。为了解决这一问题，我们提出了AnyText，一个基于扩散的多语言视觉文本生成与编辑模型，专注于在图像中渲染准确且连贯的文本。AnyText包括一个具有两个主要元素的扩散管道：辅助潜在模块和文本嵌入模块。前者使用文本字形、位置和掩蔽图像等输入生成用于文本生成或编辑的潜在特征。后者采用OCR模型将笔画数据编码为嵌入，与来自分词器的图像标题嵌入融合，以生成与背景无缝集成的文本。我们采用文本控制扩散损失和文本感知损失进行训练，以进一步提高书写准确性。到目前为止，AnyText可以用多种语言书写字符，这是我们所知的首个解决多语言视觉文本生成的工作。值得一提的是，AnyText可以接入社区现有的扩散模型，以准确渲染或编辑文本。经过广泛的评估实验，我们的方法显著优于其他所有方法。此外，我们贡献了首个大规模多语言文本图像数据集AnyWord-3M，包含300万个带有多语言OCR注释的图像-文本对。基于AnyWord-3M数据集，我们提出了AnyText基准，用于评估视觉文本生成的准确性和质量。我们的项目将很快开源，以促进文本生成技术的发展。"
}
{
  "title": "LLM-grounded Video Diffusion Models",
  "title_zh": "基于LLM的视频扩散模型",
  "abstract": "Text-conditioned diffusion models have emerged as a promising tool for neural video generation. However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion. To address these limitations, we introduce LLM-grounded Video Diffusion (LVD). Instead of directly generating videos from the text inputs, LVD first leverages a large language model (LLM) to generate dynamic scene layouts based on the text inputs and subsequently uses the generated layouts to guide a diffusion model for video generation. We show that LLMs are able to understand complex spatiotemporal dynamics from text alone and generate layouts that align closely with both the prompts and the object motion patterns typically observed in the real world. We then propose to guide video diffusion models with these layouts by adjusting the attention maps. Our approach is training-free and can be integrated into any video diffusion model that admits classifier guidance. Our results demonstrate that LVD significantly outperforms its base video diffusion model and several strong baseline methods in faithfully generating videos with the desired attributes and motion patterns.",
  "abstract_zh": "文本条件的扩散模型已成为神经视频生成的有前景工具。然而，当前模型在处理复杂的时空提示时仍然存在困难，常常生成受限或不正确的运动。为了解决这些局限性，我们提出了基于LLM的视频扩散（LVD）。LVD并不是直接从文本输入生成视频，而是首先利用大型语言模型（LLM）根据文本输入生成动态场景布局，然后使用生成的布局来指导扩散模型进行视频生成。我们展示了LLM能够仅从文本中理解复杂的时空动态，并生成与提示和现实世界中通常观察到的物体运动模式紧密对齐的布局。然后，我们提出通过调整注意力图来指导视频扩散模型。我们的方法无需训练，可以集成到任何允许分类器指导的视频扩散模型中。我们的结果表明，LVD在忠实生成具有所需属性和运动模式的视频方面显著优于其基础视频扩散模型和几种强基线方法。"
}
{
  "title": "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions",
  "title_zh": "理解变压器和大型语言模型中的上下文学习，通过学习离散函数来学习",
  "abstract": "In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can match the performance of gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Additionally, we find that certain attention-free models perform (almost) identically to Transformers on a range of tasks. (b) When provided a *teaching sequence*, i.e. a set of examples that uniquely identifies a function in a class, we show that Transformers learn more sample-efficiently. Interestingly, our results show that Transformers can learn to implement *two distinct* algorithms to solve a *single* task, and can adaptively select the more sample-efficient algorithm depending on the sequence of in-context examples. (c) Lastly, we show that extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines on prediction tasks that are guaranteed to not be in their training set.",
  "abstract_zh": "为了理解上下文学习现象，近期的研究采用了一种风格化的实验框架，证明变压器可以在各种实值函数类别中匹配基于梯度的学习算法的性能。然而，变压器在实现学习算法方面的局限性以及它们学习其他形式算法的能力尚不清楚。此外，这些能力在多大程度上限于基于注意力的模型也不明确。此外，尚不清楚从这些风格化设置中得出的见解是否可以推广到预训练的大型语言模型（LLMs）。在这项工作中，我们朝着回答这些问题迈出了一步，证明了以下几点：（a）在一个包含多种布尔函数类别的测试平台上，我们发现变压器在“简单”任务上几乎可以匹配最佳学习算法，而在“复杂”任务上的表现则下降。此外，我们发现某些无注意力模型在一系列任务上的表现与变压器几乎相同。（b）当提供一个*教学序列*时，即一组唯一识别类别中函数的示例，我们表明变压器的学习样本效率更高。有趣的是，我们的结果表明，变压器可以学习实现*两个不同*算法来解决*单个*任务，并且可以根据上下文示例的序列自适应选择更具样本效率的算法。（c）最后，我们表明现有的LLMs，例如LLaMA-2、GPT-4，可以在保证不在其训练集中的预测任务上与最近邻基线竞争。"
}
{
  "title": "Turning large language models into cognitive models",
  "title_zh": "将大型语言模型转变为认知模型",
  "abstract": "Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into cognitive models. We find that -- after finetuning them on data from psychological experiments -- these models offer accurate representations of human behavior, even outperforming traditional cognitive models in two decision-making domains. In addition, we show that their representations contain the information necessary to model behavior on the level of individual subjects. Finally, we demonstrate that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task. Taken together, these results suggest that large, pre-trained models can be adapted to become models of human cognition, which opens up future research directions toward building more general cognitive models.",
  "abstract_zh": "大型语言模型是强大的系统，在许多任务中表现出色，从翻译到数学推理。然而，这些模型同时常常表现出不符合人类特征的特性。在本文中，我们探讨了这一差距，并询问大型语言模型是否可以转变为认知模型。我们发现——在对心理实验数据进行微调后——这些模型提供了对人类行为的准确表征，甚至在两个决策领域超越了传统认知模型。此外，我们展示了它们的表征包含了在个体层面建模行为所需的信息。最后，我们证明了在多个任务上进行微调使大型语言模型能够预测在以前未见过的任务中的人类行为。综合来看，这些结果表明，大型预训练模型可以被调整为人类认知模型，这为构建更通用的认知模型开辟了未来的研究方向。"
}
{
  "title": "BadEdit: Backdooring Large Language Models by Model Editing",
  "title_zh": "标题：BadEdit：通过模型编辑对大型语言模型进行后门攻击",
  "abstract": "Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs). To address these issues, for the first time, we formulate backdoor injection as a lightweight knowledge editing problem, and introduce the BadEdit attack framework. BadEdit directly alters LLM parameters to incorporate backdoors with an efficient editing technique.\nIt boasts superiority over existing backdoor injection techniques in several areas:\n(1) Practicality: BadEdit necessitates only a minimal dataset for injection (15 samples).\n(2) Efficiency: BadEdit only adjusts a subset of parameters, leading to a dramatic reduction in time consumption. \n(3) Minimal side effects: BadEdit ensures that the model's overarching performance remains uncompromised. \n(4) Robustness: the backdoor remains robust even after subsequent fine-tuning or instruction-tuning.\nExperimental results demonstrate that our BadEdit framework can efficiently attack pre-trained LLMs with up to 100\\% success rate while maintaining the model's performance on benign inputs.",
  "abstract_zh": "摘要：主流的后门攻击方法通常需要大量的调优数据进行污染，这限制了它们的实用性，并可能在应用于大型语言模型（LLMs）时降低整体性能。为了解决这些问题，我们首次将后门注入形式化为轻量级知识编辑问题，并引入了BadEdit攻击框架。BadEdit直接修改LLM参数，以高效的编辑技术纳入后门。它在多个方面优于现有的后门注入技术：（1）实用性：BadEdit仅需最小的数据集进行注入（15个样本）。 （2）效率：BadEdit仅调整部分参数，显著减少时间消耗。 （3）最小副作用：BadEdit确保模型的整体性能不受影响。 （4）鲁棒性：即使在后续的微调或指令调优后，后门仍然保持鲁棒性。实验结果表明，我们的BadEdit框架能够高效攻击预训练的LLM，成功率高达100%，同时保持模型在良性输入上的性能。"
}
{
  "title": "Reward Model Ensembles Help Mitigate Overoptimization",
  "title_zh": "奖励模型集成有助于缓解过度优化",
  "abstract": "Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the “true” reward, these learned reward models are susceptible to overoptimization. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger “gold” reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We additionally extend the setup of Gao et al. (2023) to include 25% label noise to better mirror real-world conditions. Both with and without label noise we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for BoN sampling. For PPO, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. Moreover, combining it with a small KL penalty successfully prevents overoptimization at no performance cost. Overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization.",
  "abstract_zh": "基于人类反馈的强化学习（RLHF）是微调大型语言模型以遵循指令的标准方法。在此过程中，学习到的奖励模型用于近似建模人类偏好。然而，作为“真实”奖励的不完美表示，这些学习到的奖励模型容易受到过度优化的影响。Gao等人（2023）在一个合成的人类反馈设置中研究了这一现象，使用一个显著更大的“黄金”奖励模型作为真实奖励（而非人类），并表明无论代理奖励模型和训练数据的大小如何，过度优化始终是一个持续存在的问题。使用类似的设置，我们进行了一项系统研究，以评估使用基于集成的保守优化目标的有效性，特别是最坏情况优化（WCO）和不确定性加权优化（UWO），以减轻在使用两种优化方法时的奖励模型过度优化：（a）最佳采样（BoN）（b）近端策略优化（PPO）。我们还扩展了Gao等人（2023）的设置，加入25%的标签噪声，以更好地反映现实世界条件。在有无标签噪声的情况下，我们发现保守优化几乎消除了过度优化，并使BoN采样的性能提高了多达70%。对于PPO，基于集成的保守优化始终减少过度优化，并优于单一奖励模型优化。此外，将其与小的KL惩罚结合使用，成功地在没有性能损失的情况下防止了过度优化。总体而言，我们的结果表明，基于集成的保守优化可以有效对抗过度优化。"
}
{
  "title": "Stable Anisotropic Regularization",
  "title_zh": "稳定的各向异性正则化",
  "abstract": "Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to increase or decrease levels of isotropy in embedding space during training. I-STAR uses IsoScore$^{\\star}$, the first accurate measure of isotropy that is both differentiable and stable on mini-batch computations. In contrast to several previous works, we find that \\textit{decreasing} isotropy in contextualized embeddings improves performance on the majority of tasks and models considered in this paper.",
  "abstract_zh": "鉴于大型语言模型（LLMs）的成功，研究模型激活特性的兴趣显著增加。文献普遍认为，LLM表示主要受少数“异常维度”的主导，这些维度具有极高的方差和幅度。自然语言处理（NLP）领域的几项研究试图减轻这些异常维度的影响，并迫使LLM变得各向同性（即在嵌入空间的所有维度上具有均匀的方差）。各向同性被认为是LLM的一个理想特性，可以提高模型性能，并使文本表示更接近人类直觉。然而，许多关于NLP中各向同性的主张是基于嵌入的平均余弦相似度，而最近已被证明这一度量方法存在缺陷。在本文中，我们提出了I-STAR：基于IsoScore$^{\\star}$的稳定各向异性正则化，这是一种新颖的正则化方法，可用于在训练过程中增加或减少嵌入空间中的各向同性水平。I-STAR使用IsoScore$^{\\star}$，这是第一个既可微分又在小批量计算中稳定的各向同性的准确度量。与之前的几项工作相比，我们发现\\textit{降低}上下文化嵌入的各向同性可以提高本文考虑的大多数任务和模型的性能。"
}
{
  "title": "Closing the Curious Case of Neural Text Degeneration",
  "title_zh": "标题：解决神经文本退化的好奇案例",
  "abstract": "Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective. We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability. However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well. In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold. Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm. Our evaluations show that our method outperforms its threshold-based counterparts under automatic and human evaluation metrics for low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical findings and pilot experiments provide both insight into why truncation sampling works, and make progress toward more expressive sampling algorithms that better surface the generative capabilities of large language models.",
  "abstract_zh": "摘要：尽管语言生成中普遍使用截断采样启发式方法，如核采样，但仍不清楚其为何如此有效。我们通过证明丢弃低于某个概率阈值的截断方法（最常见的截断类型）可以保证所有采样的标记具有非零真实概率，从而提供了截断采样有效性的理论解释。然而，阈值是一种粗略的启发式方法，必然会丢弃一些具有非零真实概率的标记。为了追求更精确的采样策略，我们展示了如何利用已知的模型错误来源——softmax瓶颈，证明某些标记具有非零真实概率，而无需依赖阈值。基于我们的发现，我们开发了一种实验性截断策略，并进行初步研究，展示这种算法的潜力。我们的评估表明，在低熵（即接近贪婪）开放式文本生成的自动和人工评估指标下，我们的方法优于基于阈值的对应方法。我们的理论发现和初步实验不仅提供了对截断采样为何有效的见解，还朝着更具表现力的采样算法迈出了进展，这些算法更好地展示了大型语言模型的生成能力。"
}
{
  "title": "Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models",
  "title_zh": "透视偏好：揭示大语言模型对反馈获取的对齐",
  "abstract": "Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments, for a particular comparison instance. To our surprise, we observe that the choice of feedback protocol has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs that leverage rankings data for alignment (say model X) are preferred over those that leverage ratings data (say model Y), with a rank-based evaluation protocol (is X/Y's response better than reference response?) but not with a rating-based evaluation protocol (score Rank X/Y's response on a scale of 1-7). Our findings thus shed light on critical gaps in methods for evaluating the real-world utility of language models and their strong dependence on the feedback protocol used for alignment. Our code and data are available at \\url{https://github.com/Hritikbansal/sparse_feedback}.",
  "abstract_zh": "对齐大语言模型（LLMs）与人类价值观和意图的关键在于使用人类或人工智能反馈。尽管密集反馈注释获取和整合成本高昂，但稀疏反馈在评分（例如，按1-7的尺度评分响应A）和排名（例如，响应A是否优于响应B？）之间呈现出结构设计选择。在本研究中，我们分析了这种设计选择对LLMs的对齐和评估的影响。我们发现了一个不一致性问题，即从评分和排名推断出的偏好在60%的情况下显著不一致，无论是人类还是人工智能注释者。我们的后续分析确定了各种注释者偏见的方面，解释了这一现象，例如人类注释者在特定比较实例中会对更密集的响应给予更高的评分，同时在成对判断时更倾向于准确性。令我们惊讶的是，反馈协议的选择对对齐LLMs的评估有显著影响。特别是，我们发现利用排名数据进行对齐的LLMs（例如模型X）在基于排名的评估协议（X/Y的响应是否优于参考响应？）中优于利用评分数据的LLMs（例如模型Y），而在基于评分的评估协议（按1-7的尺度评分排名X/Y的响应）中则没有这种情况。因此，我们的发现揭示了评估语言模型实际效用的方法中的关键差距，以及它们对用于对齐的反馈协议的强烈依赖。我们的代码和数据可在\\url{https://github.com/Hritikbansal/sparse_feedback}获取。"
}
{
  "title": "Tool-Augmented Reward Modeling",
  "title_zh": "工具增强的奖励建模",
  "abstract": "Reward modeling (*a.k.a.*, preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named Themis, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our approach across a wide range of domains, incorporating seven distinct external tools. Our experimental results demonstrate a noteworthy overall improvement of 17.7% across eight tasks in preference ranking. Furthermore, our approach outperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In human evaluations, RLHF trained with Themis attains an average win rate of 32% when compared to baselines across four distinct tasks. Additionally, we provide a comprehensive collection of tool-related RM datasets, incorporating data from seven distinct tool APIs, totaling 15,000 instances. We have made the code, data, and model checkpoints publicly available to facilitate and inspire further research advancements (https://github.com/ernie-research/Tool-Augmented-Reward-Model).",
  "abstract_zh": "奖励建模（即偏好建模）对于将大型语言模型与人类偏好对齐至关重要，尤其是在基于人类反馈的强化学习（RLHF）背景下。尽管传统的奖励模型（RMs）表现出显著的可扩展性，但它们在算术计算、代码执行和事实查找等基本功能上常常存在困难。本文提出了一种名为Themis的工具增强偏好建模方法，以通过赋予RMs访问外部环境（包括计算器和搜索引擎）来解决这些限制。这种方法不仅促进了工具使用与奖励评分之间的协同作用，还增强了解释能力和评分可靠性。我们的研究探讨了将外部工具集成到RMs中的方法，使其能够与多种外部来源互动，并以自回归方式构建任务特定的工具参与和推理轨迹。我们在多个领域验证了我们的方法，结合了七种不同的外部工具。实验结果表明，在八个任务的偏好排名中，整体提升达17.7%。此外，我们的方法在零样本评估中在TruthfulQA任务上比Gopher 280B提高了7.3%。在人工评估中，与基线相比，使用Themis训练的RLHF在四个不同任务中的平均胜率为32%。此外，我们提供了一套全面的与工具相关的RM数据集，包含来自七个不同工具API的数据，总计15,000个实例。我们已公开代码、数据和模型检查点，以促进和激励进一步的研究进展（https://github.com/ernie-research/Tool-Augmented-Reward-Model）。"
}
{
  "title": "LLaMA-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention",
  "title_zh": "LLaMA-Adapter：使用零初始化注意力的高效大语言模型微调",
  "abstract": "With the rising tide of large language models (LLMs), there has been a growing interest in developing general-purpose instruction-following models, e.g., ChatGPT. To this end, we present LLaMA-Adapter, a lightweight adaption method for efficient instruction tuning of LLaMA. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning. Specifically, a zero-initialized attention mechanism is proposed. It adopts a learnable zero gating to adaptively inject the instructional cues into LLaMA within self-attention layers, contributing to a stable training process and superior final performance. In this way, LLaMA-Adapter can generate high-quality responses to diverse language instructions, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, by incorporating an image encoder, our approach can be simply extended to a multi-modal LLM for image-conditioned instruction following, which achieves superior multi-modal reasoning capacity on several popular benchmarks (MME, MMBench, LVLM-eHub). Furthermore, we also verify the proposed zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa, CLIP) on traditional vision and language tasks, demonstrating the effectiveness and generalizability of our approach.",
  "abstract_zh": "随着大语言模型（LLMs）的兴起，开发通用指令跟随模型（如ChatGPT）的兴趣日益增长。为此，我们提出了LLaMA-Adapter，一种轻量级适配方法，用于高效地对LLaMA进行指令调优。使用52K自我指导示例，LLaMA-Adapter在冻结的LLaMA 7B模型上仅引入1.2M可学习参数，微调时间不足一小时。具体而言，提出了一种零初始化注意力机制。它采用可学习的零门控，能够自适应地将指令线索注入LLaMA的自注意力层中，从而有助于稳定的训练过程和优越的最终性能。通过这种方式，LLaMA-Adapter能够生成高质量的响应，能够与完全微调的7B参数的Alpaca相媲美。除了语言指令，通过结合图像编码器，我们的方法可以简单扩展为用于图像条件指令跟随的多模态LLM，在多个热门基准（MME、MMBench、LVLM-eHub）上实现了卓越的多模态推理能力。此外，我们还验证了所提出的零初始化注意力机制在传统视觉和语言任务上对其他预训练模型（ViT、RoBERTa、CLIP）进行微调的有效性和通用性。"
}
{
  "title": "Large Language Models are Efficient Learners of Noise-Robust Speech Recognition",
  "title_zh": "大型语言模型是高效的噪声鲁棒语音识别学习者",
  "abstract": "Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which leverages the rich linguistic knowledge and powerful reasoning ability of LLMs to improve recognition results. The latest work proposes a GER benchmark with \"HyPoradise\" dataset to learn the mapping from ASR N-best hypotheses to ground-truth transcription by efficient LLM finetuning, which shows great effectiveness but lacks specificity on noise-robust ASR. In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do, where one solution is introducing noise information as a conditioner into LLM. However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap. To this end, we propose to extract a language-space noise embedding from the N-best list to represent the noise conditions of source speech, which can promote the denoising process in GER. Furthermore, in order to enhance its representation ability of audio noise, we design a knowledge distillation (KD) approach via mutual information estimation to distill the real noise information in audio embeddings to our language embedding. Experiments on various latest LLMs demonstrate our approach achieves a new breakthrough with up to 53.9% correction improvement in terms of word error rate while with limited training data. Analysis shows that our language-space noise embedding can well represent the noise conditions of source speech, under which off-the-shelf LLMs show strong ability of language-space denoising.",
  "abstract_zh": "最近大型语言模型（LLMs）的进展促进了自动语音识别（ASR）的生成错误修正（GER），利用LLMs丰富的语言知识和强大的推理能力来改善识别结果。最新的研究提出了一个GER基准，使用“HyPoradise”数据集，通过高效的LLM微调学习ASR N-best假设到真实转录的映射，显示出很大的有效性，但在噪声鲁棒ASR方面缺乏特异性。在本研究中，我们将基准扩展到噪声条件下，探讨是否可以教会LLMs进行去噪，以便像鲁棒ASR一样进行GER，其中一个解决方案是将噪声信息作为条件引入LLM。然而，直接将音频编码器的噪声嵌入纳入可能会因跨模态差距而损害LLM的调优。为此，我们提出从N-best列表中提取语言空间噪声嵌入，以表示源语音的噪声条件，这可以促进GER中的去噪过程。此外，为了增强其对音频噪声的表征能力，我们设计了一种通过互信息估计的知识蒸馏（KD）方法，将音频嵌入中的真实噪声信息蒸馏到我们的语言嵌入中。在各种最新LLMs上的实验表明，我们的方法在字错误率方面实现了高达53.9%的修正改进，同时训练数据有限。分析表明，我们的语言空间噪声嵌入能够很好地表示源语音的噪声条件，在这种条件下，现成的LLMs展现出强大的语言空间去噪能力。"
}
{
  "title": "Can LLM-Generated Misinformation Be Detected?",
  "title_zh": "标题：LLM生成的虚假信息能被检测到吗？",
  "abstract": "The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.",
  "abstract_zh": "摘要：大型语言模型（LLMs）的出现产生了变革性的影响。然而，像ChatGPT这样的LLMs被利用生成虚假信息的潜力对在线安全和公众信任构成了严重担忧。一个基本的研究问题是：LLM生成的虚假信息是否会造成比人类撰写的虚假信息更大的危害？我们提议从检测难度的角度来解决这个问题。我们首先建立了LLM生成虚假信息的分类法。然后，我们对使用LLMs生成虚假信息的潜在现实世界方法进行了分类和验证。通过广泛的实证研究，我们发现，与具有相同语义的人类撰写的虚假信息相比，LLM生成的虚假信息对人类和检测器来说可能更难以检测，这表明它可能具有更具欺骗性的风格，并可能造成更大的危害。我们还讨论了我们发现的对抗虚假信息的启示，特别是在LLM时代及其应对措施。"
}
{
  "title": "Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain",
  "title_zh": "超越准确性：使用IdentityChain评估代码大型语言模型的自一致性",
  "abstract": "Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indeed a distinct aspect from conventional accuracy. Furthermore, we show that IdentityChain can be used as a model debugging tool to expose weaknesses of Code LLMs by demonstrating three major weaknesses that we identify in current models using IdentityChain. Our code is available at https://github.com/marcusm117/IdentityChain.",
  "abstract_zh": "代码大型语言模型（Code LLMs）在实际应用中越来越多，因此对它们的评估至关重要。传统的准确性评估仅关注Code LLMs在一组单独任务上的表现，而忽视了它们在不同任务之间的自一致性。直观上，一个值得信赖的模型在为其自身代码生成自然语言规范和为其自身规范生成代码时应该保持自一致性。未能保持自一致性表明对自然语言和编程语言之间共享语义的理解不足，因此削弱了模型的可信度。本文首先正式定义了Code LLMs的自一致性，然后设计了一个框架IdentityChain，能够有效且高效地同时评估模型的自一致性和传统准确性。我们研究了十一种Code LLMs，并表明它们未能保持自一致性，这确实是与传统准确性不同的一个方面。此外，我们还展示了IdentityChain可以作为模型调试工具，通过使用IdentityChain识别当前模型中的三个主要弱点，揭示Code LLMs的不足。我们的代码可在https://github.com/marcusm117/IdentityChain获取。"
}
{
  "title": "Few-Shot Detection of Machine-Generated Text using Style Representations",
  "title_zh": "机器生成文本的少量样本检测与风格表示",
  "abstract": "The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for such detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that generated the text to be detected at inference or detection time, which is often impractical. In light of these challenge, we pursue a fundamentally different approach not relying on samples from language models of concern at training time. Instead, we propose to leverage representations of writing style estimated from human-authored text. Indeed, we find that features effective at distinguishing among human authors are also effective at distinguishing human from machine authors, including state of the art large language models like Llama 2, ChatGPT, and GPT-4. Furthermore, given handfuls of examples composed by each of several specific language models of interest, our approach affords the ability to predict which model specifically generated a given document.",
  "abstract_zh": "随着指令调优语言模型的出现，这些模型能够令人信服地模仿人类写作，带来了显著的滥用风险。例如，这些模型可能被用于抄袭、虚假信息、垃圾邮件或网络钓鱼。然而，通过检测文本是由语言模型而非人类创作的能力，可以抵消这种滥用。以往一些解决此问题的方法依赖于在确认的人类和机器写作文档语料库上训练的监督方法。不幸的是，模型的欠规范性对这些检测器构成了不可避免的挑战，使其在面对数据变化时显得脆弱，例如新语言模型的发布，这些模型生成的文本比用于训练检测器的模型更流畅。其他一些以前的方法在推理或检测时需要访问生成待检测文本的模型，这往往是不切实际的。鉴于这些挑战，我们采取了一种根本不同的方法，不依赖于在训练时关注的语言模型样本。相反，我们建议利用从人类创作文本中估计的写作风格表示。实际上，我们发现有效区分人类作者的特征在区分人类与机器作者时同样有效，包括像Llama 2、ChatGPT和GPT-4这样的先进大型语言模型。此外，考虑到由几个特定语言模型生成的少量示例，我们的方法能够预测特定文档是由哪个模型生成的。"
}
{
  "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
  "title_zh": "知识链：通过异构来源的动态知识适应来增强大型语言模型",
  "abstract": "We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. \nSpecifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. \nGiven a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains.\nIf there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains.\nThese corrected rationales can plausibly serve as a better foundation for the final answer consolidation.\nUnlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information.\nTo access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales.\nExtensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.",
  "abstract_zh": "我们提出了知识链（CoK），这是一个新颖的框架，通过动态整合来自异构来源的基础信息来增强大型语言模型（LLMs）。这使得生成的推理更加真实，并减少了幻觉现象。具体而言，CoK包括三个阶段：推理准备、动态知识适应和答案整合。对于知识密集型问题，CoK首先准备几个初步的推理和答案，同时识别相关的知识领域。如果样本中的答案没有形成多数共识，CoK将通过逐步适应识别领域的知识来纠正推理。这些纠正后的推理可以作为最终答案整合的更好基础。与以往主要使用非结构化数据的研究不同，CoK还利用了诸如Wikidata和提供更可靠事实信息的表格等结构化知识来源。为了在动态知识适应阶段访问非结构化和结构化知识来源，我们提出了一种自适应查询生成器，允许为各种查询语言生成查询，包括SPARQL、SQL和自然句子。此外，为了最小化推理之间的错误传播，CoK使用先前纠正的推理逐步纠正推理，以生成和纠正后续推理。大量实验表明，CoK在不同领域的知识密集型任务上持续提高了LLMs的性能。"
}
{
  "title": "BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models",
  "title_zh": "坏链：针对大型语言模型的后门思维链提示",
  "abstract": "Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step into the sequence of reasoning steps of the model output, thereby altering the final response when a backdoor trigger is embedded in the query prompt. In particular, a subset of demonstrations will be manipulated to incorporate a backdoor reasoning step in COT prompting. Consequently, given any query prompt containing the backdoor trigger, the LLM will be misled to output unintended content. Empirically, we show the effectiveness of BadChain for two COT strategies across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark tasks encompassing arithmetic, commonsense, and symbolic reasoning. We show that the baseline backdoor attacks designed for simpler tasks such as semantic classification will fail on these complicated tasks. In addition, our findings reveal that LLMs endowed with stronger reasoning capabilities exhibit higher susceptibility to BadChain, exemplified by a high average attack success rate of 97.0\\% across the six benchmark tasks on GPT-4. We also demonstrate the interpretability of BadChain by showing that the relationship between the trigger and the backdoor reasoning step can be well-explained based on the output of the backdoored model. Finally, we propose two defenses based on shuffling and demonstrate their overall ineffectiveness against BadChain. Therefore, BadChain remains a severe threat to LLMs, underscoring the urgency for the development of robust and effective future defenses.",
  "abstract_zh": "大型语言模型（LLMs）在处理需要系统推理过程的任务时，显示出从思维链（COT）提示中获益。然而，COT 提示也带来了新的脆弱性，以后门攻击的形式出现，即模型在推理过程中在特定的后门触发条件下输出意图外的恶意内容。传统的后门攻击方法涉及用带后门的实例污染训练数据集或在部署期间直接操纵模型参数。然而，这些方法对于通常通过 API 访问的商业 LLM 来说并不实用。本文提出了 BadChain，这是针对使用 COT 提示的 LLM 的首个后门攻击，它不需要访问训练数据集或模型参数，并且计算开销低。BadChain 通过在模型输出的推理步骤序列中插入一个后门推理步骤，利用 LLM 的内在推理能力，从而在查询提示中嵌入后门触发时改变最终响应。具体而言，将操纵一部分示例以在 COT 提示中加入后门推理步骤。因此，给定任何包含后门触发的查询提示，LLM 将被误导输出意图外的内容。实证研究表明，BadChain 在四个 LLM（Llama2、GPT-3.5、PaLM2 和 GPT-4）和六个复杂基准任务（包括算术、常识和符号推理）中对两种 COT 策略的有效性。我们表明，针对语义分类等简单任务设计的基线后门攻击在这些复杂任务上将失败。此外，我们的研究发现，具备更强推理能力的 LLM 对 BadChain 的敏感性更高，在 GPT-4 上的六个基准任务中，平均攻击成功率高达 97.0%。我们还通过展示触发器与后门推理步骤之间的关系可以基于被后门模型的输出得到良好解释，证明了 BadChain 的可解释性。最后，我们提出了基于洗牌的两种防御措施，并展示了它们对 BadChain 的整体无效性。因此，BadChain 仍然对 LLM 构成严重威胁，强调了开发强大有效的未来防御措施的紧迫性。"
}
{
  "title": "Democratizing Fine-grained Visual Recognition with Large Language Models",
  "title_zh": "民主化细粒度视觉识别与大型语言模型",
  "abstract": "Identifying subordinate-level categories from images is a longstanding task in computer vision and is referred to as fine-grained visual recognition (FGVR). It has tremendous significance in real-world applications since an average layperson does not excel at differentiating species of birds or mushrooms due to subtle differences among the species. A major bottleneck in developing FGVR systems is caused by the need of high-quality paired expert annotations. To circumvent the need of expert knowledge we propose Fine-grained Semantic Category Reasoning (FineR) that internally leverages the world knowledge of large language models (LLMs) as a proxy in order to reason about fine-grained category names. In detail, to bridge the modality gap between images and LLM, we extract part-level visual attributes from images as text and feed that information to a LLM. Based on the visual attributes and its internal world knowledge the LLM reasons about the subordinate-level category names. Our training-free FineR outperforms several state-of-the-art FGVR and language and vision assistant models and shows promise in working in the wild and in new domains where gathering expert annotation is arduous.",
  "abstract_zh": "从图像中识别下级类别是计算机视觉中的一个长期任务，称为细粒度视觉识别（FGVR）。由于物种之间的细微差别，普通人通常难以区分鸟类或蘑菇的种类，因此在现实世界应用中具有重要意义。开发FGVR系统的一个主要瓶颈是需要高质量的配对专家注释。为了绕过对专家知识的需求，我们提出了细粒度语义类别推理（FineR），该方法内部利用大型语言模型（LLMs）的世界知识作为代理，以推理细粒度类别名称。具体而言，为了弥合图像与LLM之间的模态差距，我们从图像中提取部分级别的视觉属性作为文本，并将该信息输入到LLM中。基于视觉属性及其内部世界知识，LLM推理出下级类别名称。我们的无训练FineR在多个最先进的FGVR和语言与视觉助手模型中表现优异，并在野外和新领域中显示出潜力，尤其是在获取专家注释困难的情况下。"
}
{
  "title": "AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model",
  "title_zh": "AlignDiff：通过行为可定制的扩散模型对齐多样化的人类偏好",
  "abstract": "Aligning agent behaviors with diverse human preferences remains a challenging problem in reinforcement learning (RL), owing to the inherent abstractness and mutability of human preferences. To address these issues, we propose AlignDiff, a novel framework that leverages RLHF to quantify human preferences, covering abstractness, and utilizes them to guide diffusion planning for zero-shot behavior customizing, covering mutability. AlignDiff can accurately match user-customized behaviors and efficiently switch from one to another. To build the framework, we first establish the multi-perspective human feedback datasets, which contain comparisons for the attributes of diverse behaviors, and then train an attribute strength model to predict quantified relative strengths. After relabeling behavioral datasets with relative strengths, we proceed to train an attribute-conditioned diffusion model, which serves as a planner with the attribute strength model as a director for preference aligning at the inference phase. We evaluate AlignDiff on various locomotion tasks and demonstrate its superior performance on preference matching, switching, and covering compared to other baselines. Its capability of completing unseen downstream tasks under human instructions also showcases the promising potential for human-AI collaboration. More visualization videos are released on https://aligndiff.github.io/.",
  "abstract_zh": "对齐代理行为与多样化人类偏好在强化学习中仍然是一个具有挑战性的问题，因为人类偏好的固有抽象性和可变性。为了解决这些问题，我们提出了AlignDiff，一个新颖的框架，利用RLHF量化人类偏好，涵盖抽象性，并利用它们指导扩散规划以实现零样本行为定制，涵盖可变性。AlignDiff能够准确匹配用户定制的行为，并高效地在不同行为之间切换。为了构建该框架，我们首先建立了多视角人类反馈数据集，其中包含对多样化行为属性的比较，然后训练一个属性强度模型来预测量化的相对强度。在用相对强度重新标记行为数据集后，我们继续训练一个属性条件的扩散模型，该模型作为规划者，属性强度模型作为推理阶段偏好对齐的指导。我们在各种运动任务上评估AlignDiff，并展示其在偏好匹配、切换和覆盖方面优于其他基线的表现。其在人工指令下完成未见下游任务的能力也展示了人机协作的良好潜力。更多可视化视频已发布在https://aligndiff.github.io/。"
}
{
  "title": "RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering",
  "title_zh": "标题：RAPPER：用于视觉问答中自然语言解释的强化推理提示范式",
  "abstract": "Natural Language Explanation (NLE) in vision and language tasks aims to provide human-understandable explanations for the associated decision-making process. In practice, one might encounter explanations which lack informativeness or contradict visual-grounded facts, known as implausibility and hallucination problems, respectively. To tackle these challenging issues, we consider the task of visual question answering (VQA) and introduce Rapper, a two-stage Reinforced Rationale-Prompted Paradigm. By knowledge distillation, the former stage of Rapper infuses rationale-prompting via large language models (LLMs), encouraging the rationales supported by language-based facts. As for the latter stage, a unique Reinforcement Learning from NLE Feedback (RLNF) is introduced for injecting visual facts into NLE generation. Finally, quantitative and qualitative experiments on two VL-NLE benchmarks show that Rapper surpasses state-of-the-art VQA-NLE methods while providing plausible and faithful NLE.",
  "abstract_zh": "摘要：视觉和语言任务中的自然语言解释（NLE）旨在为相关的决策过程提供人类可理解的解释。在实践中，可能会遇到缺乏信息量或与视觉事实相矛盾的解释，分别称为不合理性和幻觉问题。为了解决这些挑战性问题，我们考虑视觉问答（VQA）任务，并引入Rapper，一个两阶段的强化推理提示范式。通过知识蒸馏，Rapper的第一阶段通过大型语言模型（LLMs）注入推理提示，鼓励基于语言的事实支持的推理。而在第二阶段，引入了一种独特的基于NLE反馈的强化学习（RLNF），用于将视觉事实注入NLE生成。最后，在两个VL-NLE基准上的定量和定性实验表明，Rapper在提供合理和可信的NLE的同时，超越了最先进的VQA-NLE方法。"
}
{
  "title": "MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods",
  "title_zh": "标题：MBR与QE微调：最佳和最昂贵解码方法的训练时蒸馏",
  "abstract": "Recent research in decoding methods for Natural Language Generation (NLG) tasks has shown that MAP decoding is not optimal, because model probabilities do not always align with human preferences. Stronger decoding methods, including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR) decoding, have since been proposed to mitigate the model-perplexity-vs-quality mismatch. While these decoding methods achieve state-of-the-art performance, they are prohibitively expensive to compute. In this work, we propose MBR finetuning and QE finetuning, which distill the quality gains from these decoding methods at training time, while using an efficient decoding algorithm at inference time. Using the canonical NLG task of Neural Machine Translation (NMT), we show that even with self-training, these finetuning methods significantly outperform the base model. Moreover, when using an external LLM as a teacher model, these finetuning methods outperform finetuning on human-generated references. These findings suggest new ways to leverage monolingual data to achieve improvements in model quality that are on par with, or even exceed, improvements from human-curated data, while maintaining maximum efficiency during decoding.",
  "abstract_zh": "摘要：最近关于自然语言生成（NLG）任务解码方法的研究表明，MAP解码并非最佳选择，因为模型概率并不总是与人类偏好一致。为了解决模型困惑度与质量之间的不匹配，提出了更强大的解码方法，包括质量估计（QE）重排序和最小贝叶斯风险（MBR）解码。尽管这些解码方法达到了最先进的性能，但计算成本极高。在本研究中，我们提出了MBR微调和QE微调，这些方法在训练时蒸馏了这些解码方法的质量提升，同时在推理时使用高效的解码算法。通过经典的神经机器翻译（NMT）任务，我们展示了即使在自我训练的情况下，这些微调方法也显著超越了基础模型。此外，当使用外部大型语言模型作为教师模型时，这些微调方法的表现优于基于人类生成参考的微调。这些发现表明了利用单语数据来实现模型质量提升的新方法，这种提升与人类策划数据的提升相当，甚至超过，同时在解码过程中保持最大效率。"
}
{
  "title": "Raidar: geneRative AI Detection viA Rewriting",
  "title_zh": "标题：Raidar：通过重写进行生成式人工智能检测",
  "abstract": "We find that large language models (LLMs) are more likely to modify human-written text than AI-generated text when tasked with rewriting. This tendency arises because LLMs often perceive AI-generated text as high-quality, leading to fewer modifications. We introduce a method to detect AI-generated content by prompting LLMs to rewrite text and calculating the editing distance of the output. We dubbed our geneRative AI Detection viA Rewriting method Raidar.  Raidar significantly improves the F1 detection scores of existing AI content detection models -- both academic and commercial -- across various domains, including News, creative writing, student essays, code, Yelp reviews, and arXiv papers, with gains of up to 29 points. Operating solely on word symbols without high-dimensional features, our method is compatible with black box LLMs, and is inherently robust on new content. Our results illustrate the unique imprint of machine-generated text through the lens of the machines themselves.",
  "abstract_zh": "摘要：我们发现，当被要求重写时，大型语言模型（LLMs）更可能修改人类撰写的文本，而非人工智能生成的文本。这种倾向的产生是因为LLMs通常将人工智能生成的文本视为高质量，从而导致修改较少。我们提出了一种通过提示LLMs重写文本并计算输出的编辑距离来检测人工智能生成内容的方法。我们将这种生成式人工智能检测通过重写的方法称为Raidar。Raidar显著提高了现有人工智能内容检测模型（包括学术和商业）的F1检测得分，在新闻、创意写作、学生论文、代码、Yelp评论和arXiv论文等多个领域的得分提升高达29分。我们的算法仅基于词符号而不依赖高维特征，兼容黑箱LLMs，并且对新内容具有内在的鲁棒性。我们的结果通过机器自身的视角展示了机器生成文本的独特印记。"
}
{
  "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
  "title_zh": "现象级但令人困惑：通过假设细化测试语言模型的归纳推理能力",
  "abstract": "The ability to derive underlying principles from a handful of observations and then generalize to novel situations---known as inductive reasoning---is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through $\\textit{iterative hypothesis refinement}$, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal $\\textit{hypothesis proposers}$ (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling $\\textit{inductive reasoners}$, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.",
  "abstract_zh": "归纳推理是从少量观察中推导出基本原则并推广到新情境的能力，这在人的智能中至关重要。尽管语言模型在研究基准上取得了令人瞩目的成功，但先前的工作表明它们在归纳推理方面往往表现不佳。在本研究中，我们通过“迭代假设细化”这一技术对语言模型的归纳推理能力进行系统研究，该技术比标准的输入输出提示更贴近人类的归纳过程。迭代假设细化采用三步过程：提出、选择和细化以文本规则形式呈现的假设。通过检查中间规则，我们观察到语言模型在“假设提出者”方面表现出色（即生成候选规则），并且当与能够系统过滤提出规则集的（任务特定）符号解释器结合时，这种混合方法在需要归纳因果关系、类语言指令和符号概念的归纳推理基准上取得了良好结果。然而，它们在“归纳推理者”方面的表现却令人困惑，规则归纳（即识别合理规则）与规则应用（即将提出的规则应用于实例）之间存在显著的性能差距，这表明语言模型在提出假设时无法实际应用这些规则。通过实证和人类分析，我们进一步揭示了语言模型与人类在归纳推理过程中的若干差异，揭示了在归纳推理任务中使用语言模型的潜力和局限性。"
}
{
  "title": "Understanding In-Context Learning from Repetitions",
  "title_zh": "理解上下文学习中的重复机制",
  "abstract": "This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. We quantitatively investigate the role of surface features in text generation, and empirically establish the existence of token co-occurrence reinforcement, a principle that strengthens the relationship between two tokens based on their contextual co-occurrences. By investigating the dual impacts of these features, our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures. This paper provides an essential contribution to the understanding of in-context learning and its potential limitations, providing a fresh perspective on this exciting capability.",
  "abstract_zh": "本文探讨了大型语言模型（LLMs）中上下文学习的微妙机制。我们的研究通过表面重复的视角提供了一种新颖的观点，定量研究了表面特征在文本生成中的作用，并实证建立了标记共现强化的存在，这一原则基于上下文共现加强两个标记之间的关系。通过研究这些特征的双重影响，我们的研究揭示了上下文学习的内部运作，并阐述了其失败的原因。本文为理解上下文学习及其潜在局限性做出了重要贡献，为这一激动人心的能力提供了新的视角。"
}
{
  "title": "Supervised Knowledge Makes Large Language Models Better In-context Learners",
  "title_zh": "监督知识使大型语言模型在上下文学习中表现更佳",
  "abstract": "Large Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering. The recent progress in large-scale generative models has further expanded their use in real-world language applications. However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored. While previous in-context learning research has focused on enhancing models to adhere to users' specific instructions and quality expectations, and to avoid undesired outputs, little to no work has explored the use of task-specific fine-tuned Language Models (SLMs) to improve LLMs' in-context learning during the inference stage. Our primary contribution is the establishment of a simple yet effective framework that enhances the reliability of LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs benefit from discriminative models, and 3) minimizes hallucinations in generative tasks. Using our proposed plug-in method, enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality. We offer a comprehensive suite of resources, including 16 curated datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks. Our empirical analysis sheds light on the advantages of incorporating discriminative models into LLMs and highlights the potential of our methodology in fostering more reliable LLMs.",
  "abstract_zh": "大型语言模型（LLMs）通过提示工程展现出新兴的上下文学习能力。近期大规模生成模型的进展进一步扩大了它们在现实语言应用中的使用。然而，提升LLMs在自然语言理解和问答中的泛化能力和事实性这一关键挑战仍未得到充分探索。虽然之前的上下文学习研究集中在增强模型遵循用户特定指令和质量期望、避免不良输出，但几乎没有研究探讨使用任务特定微调的语言模型（SLMs）来改善LLMs在推理阶段的上下文学习。我们的主要贡献是建立一个简单而有效的框架，以增强LLMs的可靠性：1）对分布外数据进行泛化，2）阐明LLMs如何从判别模型中受益，3）在生成任务中最小化幻觉。使用我们提出的插件方法，增强版本的Llama 2和ChatGPT在泛化能力和事实性方面超越了其原始版本。我们提供了一整套资源，包括16个精心策划的数据集、提示、模型检查点和9个不同任务的LLM输出。我们的实证分析揭示了将判别模型纳入LLMs的优势，并突显了我们的方法在促进更可靠的LLMs方面的潜力。"
}
{
  "title": "KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval",
  "title_zh": "KITAB：评估大型语言模型在信息检索中的约束满足能力",
  "abstract": "We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., “a list of ice cream shops in San Diego”). In the past, such queries were considered as tasks that could only be solved via web-search or knowledge bases. More recently, large language models (LLMs) have demonstrated initial emergent abilities in this task. However, many current retrieval benchmarks are either saturated or do not measure constraint satisfaction. Motivated by rising concerns around factual incorrectness and hallucinations of LLMs, we present KITAB, a new dataset for measuring constraint satisfaction abilities of language models. KITAB consists of book-related data across more than 600 authors and 13,000 queries, and also offers an associated dynamic data collection and constraint verification approach for acquiring similar test data for other authors. Our extended experiments on GPT4 and GPT3.5 characterize and decouple common failure modes across dimensions such as information popularity, constraint types, and context availability. Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases. While context availability mitigates irrelevant information, it is not helpful for satisfying constraints, identifying fundamental barriers to constraint satisfaction. We open source our contributions to foster further research on improving constraint satisfaction abilities of future models.",
  "abstract_zh": "我们研究了最先进模型在回答信息检索的约束满足查询（例如，“圣地亚哥的冰淇淋店列表”）方面的能力。过去，这类查询被认为只能通过网络搜索或知识库解决。最近，大型语言模型（LLMs）在这一任务中展示了初步的突现能力。然而，许多当前的检索基准要么已经饱和，要么不测量约束满足。鉴于对LLMs的事实错误和幻觉的日益关注，我们提出了KITAB，这是一个用于测量语言模型约束满足能力的新数据集。KITAB包含来自600多位作者的书籍相关数据和13,000个查询，并提供了一种动态数据收集和约束验证方法，以获取其他作者的类似测试数据。我们对GPT4和GPT3.5的扩展实验描述并解耦了在信息流行度、约束类型和上下文可用性等维度上的常见失败模式。结果表明，在缺乏上下文的情况下，模型在无关信息、事实错误和不完整性方面表现出严重的局限性，许多问题在信息流行度降低时加剧。虽然上下文可用性减轻了无关信息，但对满足约束并无帮助，揭示了约束满足的基本障碍。我们开源了我们的贡献，以促进未来模型约束满足能力的进一步研究。"
}
{
  "title": "Adaptive Chameleon  or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts",
  "title_zh": "自适应变色龙还是顽固树懒：揭示大型语言模型在知识冲突中的行为",
  "abstract": "By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory.\nHowever, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? \nWe present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts.\nWe propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments.\nOur investigation reveals seemingly contradicting behaviors of LLMs.\nOn the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing.\nOn the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time.\nThese results pose important implications that are worth careful consideration for the further development and deployment of tool- and retrieval-augmented LLMs.\nResources are available at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.",
  "abstract_zh": "通过向大型语言模型（LLMs）提供外部信息，工具增强（包括检索增强）已成为解决LLMs静态参数记忆局限性的有前景的解决方案。然而，LLMs对这些外部证据的接受程度如何，尤其是在证据与其参数记忆冲突时？我们首次全面且有控制地调查了LLMs在遇到知识冲突时的行为。我们提出了一个系统框架，以引出LLMs的高质量参数记忆并构建相应的反记忆，从而使我们能够进行一系列控制实验。我们的调查揭示了LLMs看似矛盾的行为。一方面，与以往的观点不同，我们发现LLMs在外部证据连贯且令人信服时，即使与其参数记忆冲突，也能高度接受外部证据。另一方面，当外部证据包含与其参数记忆一致的信息时，尽管同时呈现了冲突证据，LLMs也表现出强烈的确认偏见。这些结果对工具和检索增强的LLMs的进一步发展和部署具有重要的启示，值得仔细考虑。资源可在https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict获取。"
}
{
  "title": "Unveiling and Manipulating Prompt Influence in Large Language Models",
  "title_zh": "揭示和操控大型语言模型中的提示影响",
  "abstract": "Prompts play a crucial role in guiding the responses of Large Language Models (LLMs). However, the intricate role of individual tokens in prompts, known as input saliency, in shaping the responses remains largely underexplored. Existing saliency methods either misalign with LLM generation objectives or rely heavily on linearity assumptions, leading to potential inaccuracies. To address this, we propose Token Distribution Dynamics (TDD), an elegantly simple yet remarkably effective approach to unveil and manipulate the role of prompts in generating LLM outputs. TDD leverages the robust interpreting capabilities of the language model head (LM head) to assess input saliency. It projects input tokens into the embedding space and then estimates their significance based on distribution dynamics over the vocabulary. We introduce three TDD variants: forward, backward, and bidirectional, each offering unique insights into token relevance. Extensive experiments reveal that the TDD surpasses state-of-the-art baselines with a big margin in elucidating the causal relationships between prompts and LLM outputs. Beyond mere interpretation, we apply TDD to two prompt manipulation tasks for controlled text generation: zero-shot toxic language suppression and sentiment steering. Empirical results underscore TDD's proficiency in identifying both toxic and sentimental cues in prompts, subsequently mitigating toxicity or modulating sentiment in the generated content.",
  "abstract_zh": "提示在引导大型语言模型（LLMs）响应中起着至关重要的作用。然而，提示中单个标记的复杂作用，即输入显著性，在塑造响应方面仍然在很大程度上未被探索。现有的显著性方法要么与LLM生成目标不一致，要么过于依赖线性假设，导致潜在的不准确性。为了解决这个问题，我们提出了标记分布动态（TDD），这是一种优雅简单但极其有效的方法，用于揭示和操控提示在生成LLM输出中的作用。TDD利用语言模型头（LM头）的强大解释能力来评估输入显著性。它将输入标记投影到嵌入空间，然后根据词汇表上的分布动态估计其重要性。我们介绍了三种TDD变体：前向、后向和双向，每种变体都提供了对标记相关性的独特见解。大量实验表明，TDD在阐明提示与LLM输出之间的因果关系方面，超越了最先进的基线。除了单纯的解释外，我们将TDD应用于两个提示操控任务以实现受控文本生成：零样本有害语言抑制和情感引导。实证结果强调了TDD在识别提示中的有害和情感线索方面的能力，随后减轻了生成内容中的有害性或调节了情感。"
}
{
  "title": "Circumventing Concept Erasure Methods For Text-To-Image Generative Models",
  "title_zh": "规避文本到图像生成模型的概念消除方法",
  "abstract": "Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. On the flip side, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to \"erase\" sensitive concepts from text-to-image models. In this work, we examine seven recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we leverage the existence of special learned word embeddings that can retrieve \"erased\" concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit for AI safety.",
  "abstract_zh": "文本到图像生成模型可以为极其广泛的概念生成照片级真实感图像，其使用在公众中迅速普及。然而，这些模型也存在许多缺陷，包括可能生成包含色情内容的图像、未经许可模仿艺术风格，甚至幻觉（或深度伪造）名人的肖像。因此，已经提出了各种方法来“消除”文本到图像模型中的敏感概念。在本研究中，我们考察了七种最近提出的概念消除方法，并表明这些方法并未完全消除目标概念。具体而言，我们利用特殊学习的词嵌入的存在，可以从经过清理的模型中检索“消除”的概念，而无需更改其权重。我们的结果突显了事后概念消除方法的脆弱性，并质疑其在人工智能安全算法工具包中的使用。"
}
{
  "title": "SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos",
  "title_zh": "标题：SCHEMA：状态变化对教学视频中的过程规划至关重要",
  "abstract": "We study the problem of procedure planning in instructional videos, which aims to make a goal-oriented sequence of action steps given partial visual state observations. The motivation of this problem is to learn a structured and plannable state and action space. Recent works succeeded in sequence modeling of steps with only sequence-level annotations accessible during training, which overlooked the roles of states in the procedures. In this work, we point out that State CHangEs MAtter (SCHEMA) for procedure planning in instructional videos. We aim to establish a more structured state space by investigating the causal relations between steps and states in procedures. Specifically, we explicitly represent each step as state changes and track the state changes in procedures. For step representation, we leveraged the commonsense knowledge in large language models (LLMs) to describe the state changes of steps via our designed chain-of-thought prompting. For state changes tracking, we align visual state observations with language state descriptions via cross-modal contrastive learning, and explicitly model the intermediate states of the procedure using LLM-generated state descriptions. Experiments on CrossTask, COIN, and NIV benchmark datasets demonstrate that our proposed SCHEMA model achieves state-of-the-art performance and obtains explainable visualizations.",
  "abstract_zh": "摘要：我们研究了教学视频中的过程规划问题，旨在根据部分视觉状态观察生成一个以目标为导向的行动步骤序列。该问题的动机在于学习一个结构化且可规划的状态和行动空间。最近的研究成功地对仅在训练期间可访问的序列级注释的步骤进行了序列建模，但忽视了状态在过程中的作用。在这项工作中，我们指出状态变化对教学视频中的过程规划至关重要（SCHEMA）。我们旨在通过研究步骤与状态之间的因果关系来建立一个更结构化的状态空间。具体而言，我们明确将每个步骤表示为状态变化，并跟踪过程中的状态变化。对于步骤表示，我们利用大型语言模型（LLMs）中的常识知识，通过我们设计的思维链提示来描述步骤的状态变化。对于状态变化跟踪，我们通过跨模态对比学习将视觉状态观察与语言状态描述对齐，并使用LLM生成的状态描述明确建模过程的中间状态。在CrossTask、COIN和NIV基准数据集上的实验表明，我们提出的SCHEMA模型实现了最先进的性能，并获得了可解释的可视化结果。"
}
{
  "title": "LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models",
  "title_zh": "标题：LLMCarbon：大型语言模型的端到端碳足迹建模",
  "abstract": "The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \\textit{\\carb}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, \\carb~significantly enhances the accuracy of carbon footprint estimations for various LLMs. The source code is released at \\url{https://github.com/SotaroKaneda/MLCarbon}.",
  "abstract_zh": "摘要：与大型语言模型（LLMs）相关的碳足迹是一个重要问题，涵盖了其训练、推理、实验和存储过程中的排放，包括运营和隐含碳排放。一个关键方面是在训练之前准确估计新兴LLMs的碳影响，这在很大程度上依赖于GPU的使用。现有研究已报告LLM训练的碳足迹，但只有一个工具mlco2可以在物理训练之前预测新神经网络的碳足迹。然而，mlco2存在几个严重的局限性。它无法将其估计扩展到密集或混合专家（MoE）LLMs，忽视了关键的架构参数，仅关注GPU，并且无法建模隐含碳足迹。为了解决这些问题，我们引入了\\textit{\\carb}，一个为密集和MoE LLMs设计的端到端碳足迹预测模型。与mlco2相比，\\carb~显著提高了对各种LLMs的碳足迹估计的准确性。源代码已在\\url{https://github.com/SotaroKaneda/MLCarbon}发布。"
}
{
  "title": "Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis",
  "title_zh": "从挫折中获得智慧：通过错误分析对大型语言模型进行对齐",
  "abstract": "The rapid development of large language models (LLMs) has not only provided numerous opportunities but also presented significant challenges. This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement. Existing alignment methods usually direct LLMs toward the favorable outcomes by utilizing human-annotated, flawless instruction-response pairs. Conversely, this study proposes a novel alignment technique based on mistake analysis, which deliberately exposes LLMs to erroneous content to learn the reasons for mistakes and how to avoid them. In this case, mistakes are repurposed into valuable data for alignment, effectively helping to avoid the production of erroneous responses. Without external models or human annotations, our method leverages a model's intrinsic ability to discern undesirable mistakes and improves the safety of its generated responses. Experimental results reveal that our method outperforms existing alignment approaches in enhancing model safety while maintaining the overall utility.",
  "abstract_zh": "大型语言模型（LLMs）的快速发展不仅提供了众多机会，也带来了重大挑战。当LLMs无意中生成有害或有毒内容时，这一点尤为明显，无论是出于无意还是故意诱导。现有的对齐方法通常通过利用人类注释的无瑕疵指令-响应对将LLMs引导向有利结果。相反，本研究提出了一种基于错误分析的新型对齐技术，故意让LLMs接触错误内容，以学习错误的原因及如何避免这些错误。在这种情况下，错误被重新利用为对齐的有价值数据，有效帮助避免生成错误的响应。我们的研究方法不依赖外部模型或人类注释，而是利用模型内在的能力来辨别不良错误，从而提高生成响应的安全性。实验结果表明，我们的方法在增强模型安全性同时保持整体效用方面优于现有的对齐方法。"
}
{
  "title": "INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection",
  "title_zh": "标题：内部状态：大型语言模型的内部状态保留了幻觉检测的能力",
  "abstract": "Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs' \\textbf{IN}ternal \\textbf{S}tates for halluc\\textbf{I}nation \\textbf{DE}tection (\\textbf{INSIDE}). In particular, a simple yet effective \\textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident generations and potentially benefits the detection of overconfident hallucinations. Extensive experiments and ablation studies are performed on several popular LLMs and question-answering (QA) benchmarks, showing the effectiveness of our proposal.",
  "abstract_zh": "摘要：知识幻觉引发了对部署的大型语言模型安全性和可靠性的广泛关注。之前在检测幻觉方面的努力主要集中在logit级别的不确定性估计或语言级别的自一致性评估中，这些过程中不可避免地会丢失语义信息。因此，我们提出探索大型语言模型内部状态中保留的密集语义信息用于幻觉检测（INSIDE）。特别地，我们提出了一种简单而有效的EigenScore指标，以更好地评估响应的自一致性，该指标利用响应协方差矩阵的特征值来衡量密集嵌入空间中的语义一致性/多样性。此外，从自一致幻觉检测的角度，我们探索了一种测试时特征裁剪方法，以截断内部状态中的极端激活，这减少了过于自信的生成，并可能有助于检测过于自信的幻觉。在多个流行的大型语言模型和问答基准上进行了广泛的实验和消融研究，显示了我们提案的有效性。"
}
{
  "title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context",
  "title_zh": "使检索增强语言模型对无关上下文具有鲁棒性",
  "abstract": "Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding relevant passages. Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time. We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.",
  "abstract_zh": "检索增强语言模型（RALMs）有望产生事实性、有效且最新的语言理解系统。RALMs的一个重要需求是，当检索到的信息相关时，它能帮助模型性能，而当信息不相关时，它不会损害性能。这在多跳推理场景中特别重要，因为对无关证据的误用可能导致级联错误。然而，最近的研究表明，检索增强有时会对性能产生负面影响。在这项工作中，我们对五个开放域问答基准进行了深入分析，描述了检索降低准确性的情况。然后，我们提出了两种方法来缓解这一问题。首先，使用自然语言推理（NLI）模型过滤掉不包含问题-答案对的检索段落的简单基线。这在防止性能下降方面有效，但也会丢弃相关段落。因此，我们提出了一种方法，通过在训练时使用相关和无关上下文的混合，自动生成数据以微调语言模型，以正确利用检索段落。我们实证表明，甚至1,000个示例就足以训练模型，使其对无关上下文具有鲁棒性，同时在相关示例上保持高性能。"
}
{
  "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning",
  "title_zh": "图上的推理：忠实且可解释的大型语言模型推理",
  "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.",
  "abstract_zh": "大型语言模型（LLMs）在复杂任务中展现了令人印象深刻的推理能力。然而，它们缺乏最新的知识，并且在推理过程中会出现幻觉，这可能导致错误的推理过程，从而降低其性能和可信度。知识图谱（KGs）以结构化的形式捕捉大量事实，为推理提供了可靠的知识来源。然而，现有的基于KG的LLM推理方法仅将KG视为事实知识库，忽视了其结构信息在推理中的重要性。本文提出了一种新方法，称为图上推理（RoG），它将LLM与KG结合，以实现忠实且可解释的推理。具体而言，我们提出了一个规划-检索-推理框架，其中RoG首先生成以KG为基础的关系路径作为忠实计划。这些计划随后用于从KG中检索有效的推理路径，以便LLM进行忠实推理。此外，RoG不仅通过训练从KG中提炼知识以提高LLM的推理能力，还允许在推理过程中与任何任意LLM无缝集成。在两个基准KGQA数据集上的大量实验表明，RoG在KG推理任务中实现了最先进的性能，并生成了忠实且可解释的推理结果。"
}
{
  "title": "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
  "title_zh": "将大型语言模型与进化算法相结合，产生强大的提示优化器",
  "abstract": "Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.",
  "abstract_zh": "大型语言模型（LLMs）在各种任务中表现出色，但它们依赖于精心设计的提示，这通常需要大量的人力投入。为自动化这一过程，本文提出了一种名为EvoPrompt的新颖离散提示优化框架，该框架借鉴了进化算法（EAs）的思想，因为它们表现出良好的性能和快速的收敛性。为了使EAs能够处理离散提示，即需要连贯且易于人类理解的自然语言表达，我们将LLMs与EAs连接起来。这种方法使我们能够同时利用LLMs强大的语言处理能力和EAs高效的优化性能。具体而言，EvoPrompt在不使用任何梯度或参数的情况下，从一组提示开始，基于进化操作符通过LLMs迭代生成新提示，并根据开发集改进这一群体。我们在包括GPT-3.5和Alpaca在内的封闭源和开源LLMs上优化提示，涵盖语言理解、生成任务以及BIG-Bench Hard（BBH）任务的31个数据集。EvoPrompt显著优于人工设计的提示和现有的自动提示生成方法（例如，在BBH上提高了25%）。此外，EvoPrompt表明，将LLMs与EAs连接起来能够产生协同效应，这可能会激励进一步研究LLMs与传统算法的结合。"
}
{
  "title": "Leftover-Lunch: Advantage-based Offline Reinforcement Learning for Language Models",
  "title_zh": "剩余午餐：基于优势的语言模型离线强化学习",
  "abstract": "Reinforcement Learning with Human Feedback (RLHF) is the most prominent method for Language Model (LM) alignment. However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as\nrewards. Subsequently, by using LM’s value estimate, A-LoL only trains on positive advantage (leftover) data points, making it resilient to noise. Overall, A-LoL is an easy-to-implement, sample-efficient, and stable LM training recipe.\n\nWe demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than the baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data.",
  "abstract_zh": "基于人类反馈的强化学习（RLHF）是语言模型（LM）对齐的最突出方法。然而，RLHF是一个不稳定且数据需求量大的过程，持续需要新的高质量LM生成数据进行微调。我们引入了优势剩余午餐RL（A-LoL），这是一类新的离线策略梯度算法，能够在任何现有数据上进行RL训练。通过将整个LM输出序列视为单个动作，A-LoL允许将序列级分类器或人类设计的评分函数作为奖励。随后，利用LM的价值估计，A-LoL仅在正优势（剩余）数据点上进行训练，使其对噪声具有韧性。总体而言，A-LoL是一个易于实现、样本高效且稳定的LM训练方案。我们通过四个不同的语言生成任务展示了A-LoL及其变体的有效性。我们与在线RL（PPO）以及最近的基于偏好的（DPO，PRO）和基于奖励的（GOLD）离线RL基准进行了比较。在常用的RLHF基准——有用且无害的助手（HHA）上，使用A-LoL方法训练的LM在多样性上达到最高，同时根据人类评估被认为比基准更安全和有帮助。此外，在剩余的三个任务中，即使使用噪声或次优训练数据，A-LoL也能够优化多个不同的奖励函数。"
}
{
  "title": "Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning",
  "title_zh": "标题：在注意力机制中调整LayerNorm：朝向高效的多模态大语言模型微调",
  "abstract": "This paper introduces an efficient strategy to transform Large Language Models (LLMs) into Multi-Modal Large Language Models. \nBy conceptualizing this transformation as a domain adaptation process, \\ie, transitioning from text understanding to embracing multiple modalities, we intriguingly note that, within each attention block, tuning LayerNorm suffices to yield strong performance. \nMoreover, when benchmarked against other tuning approaches like full parameter finetuning or LoRA, its benefits on efficiency are substantial.\nFor example, when compared to LoRA on a 13B model scale, performance can be enhanced by an average of over 20\\% across five multi-modal tasks, and meanwhile, \nresults in a significant reduction of trainable parameters by 41.9\\% and a decrease in GPU memory usage by 17.6\\%. On top of this LayerNorm strategy, we showcase that selectively tuning only with conversational data can improve efficiency further. \nBeyond these empirical outcomes, we provide a comprehensive analysis to explore the role of LayerNorm in adapting LLMs to the multi-modal domain and improving the expressive power of the model.",
  "abstract_zh": "摘要：本文介绍了一种高效的策略，将大型语言模型（LLMs）转变为多模态大型语言模型。通过将这一转变概念化为领域适应过程，即从文本理解过渡到接受多种模态，我们有趣地注意到，在每个注意力块内，调整LayerNorm足以产生强大的性能。此外，与完全参数微调或LoRA等其他调整方法相比，其在效率上的优势显著。例如，在13B模型规模上与LoRA相比，五个多模态任务的平均性能提升超过20%，同时可训练参数减少41.9%，GPU内存使用减少17.6%。在这一LayerNorm策略的基础上，我们展示了仅使用对话数据进行选择性调整可以进一步提高效率。除了这些经验结果，我们提供了全面的分析，以探索LayerNorm在将LLMs适应多模态领域和提高模型表达能力中的作用。"
}
{
  "title": "In-Context Learning Learns Label Relationships but Is Not Conventional Learning",
  "title_zh": "标题：上下文学习学习标签关系但不是传统学习",
  "abstract": "The predictions of Large Language Models (LLMs) on downstream tasks often improve significantly when including examples of the input–label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works. For example, while Xie et al. (2022) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations. To ensure we obtain a comprehensive picture of ICL behavior, we study probabilistic aspects of ICL predictions and thoroughly examine the dynamics of ICL as more examples are provided. Our experiments show that ICL predictions almost always depend on in-context labels and that ICL can learn truly novel tasks in-context. However, we also find that ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally.",
  "abstract_zh": "摘要：大型语言模型（LLMs）在下游任务上的预测通常在包含输入-标签关系示例的上下文中显著改善。然而，目前尚无关于LLMs的这种上下文学习（ICL）能力如何工作的共识。例如，Xie等（2022）将ICL比作通用学习算法，而Min等（2022b）则认为ICL甚至没有从上下文示例中学习标签关系。在本文中，我们提供了关于ICL如何利用标签信息的新见解，揭示了其能力和局限性。为了确保我们获得ICL行为的全面图景，我们研究了ICL预测的概率方面，并彻底检查了随着提供更多示例而变化的ICL动态。我们的实验表明，ICL预测几乎总是依赖于上下文标签，并且ICL可以在上下文中学习真正的新任务。然而，我们还发现，ICL在完全克服从预训练数据获得的预测偏好方面存在困难，并且，ICL并不平等地考虑所有上下文信息。"
}
{
  "title": "Differentially Private Synthetic Data via Foundation Model APIs 1: Images",
  "title_zh": "差分隐私合成数据通过基础模型API 1：图像",
  "abstract": "Generating differentially private (DP) synthetic data that closely resembles the original private data is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation models as blackboxes and only utilize their inference APIs. Such API-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of API-based apps. These approaches can also leverage the power of large foundation models which are only accessible via their inference APIs. However, this comes with greater challenges due to strictly more restrictive model access and the need to protect privacy from the API provider. \n\nIn this paper, we present a new framework called Private Evolution (PE) to solve this problem and show its initial promise on synthetic images. Surprisingly, PE can match or even outperform state-of-the-art (SOTA) methods without any model training. For example, on CIFAR10 (with ImageNet as the public data), we achieve FID ≤ 7.9 with privacy cost ε = 0.67, significantly improving the previous SOTA from ε = 32. We further demonstrate the promise of applying PE on large foundation models such as Stable Diffusion to tackle challenging private datasets with a small number of high-resolution images. The code and data are released at https://github.com/microsoft/DPSDA.",
  "abstract_zh": "生成与原始私有数据高度相似的差分隐私（DP）合成数据是一种可扩展的方法，可以缓解当前数据驱动世界中的隐私问题。与当前为此任务训练定制模型的做法不同，我们旨在通过API生成DP合成数据（DPSDA），将基础模型视为黑箱，仅利用其推理API。这种基于API的、无训练的方法更易于部署，正如最近API应用数量激增所示。这些方法还可以利用仅通过推理API访问的大型基础模型的强大能力。然而，这带来了更大的挑战，因为模型访问更加严格限制，并且需要保护来自API提供者的隐私。本文提出了一种名为私有进化（PE）的新框架来解决这一问题，并展示其在合成图像上的初步前景。令人惊讶的是，PE可以在没有任何模型训练的情况下匹配甚至超越最先进（SOTA）的方法。例如，在CIFAR10（以ImageNet作为公共数据）上，我们实现了FID ≤ 7.9，隐私成本ε = 0.67，显著改善了之前的SOTA（ε = 32）。我们进一步展示了在大型基础模型（如Stable Diffusion）上应用PE的前景，以应对具有少量高分辨率图像的挑战性私有数据集。代码和数据已发布在https://github.com/microsoft/DPSDA。"
}
{
  "title": "Training Diffusion Models with Reinforcement Learning",
  "title_zh": "标题：使用强化学习训练扩散模型",
  "abstract": "Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation.",
  "abstract_zh": "摘要：扩散模型是一类灵活的生成模型，通过对数似然目标的近似进行训练。然而，扩散模型的大多数使用案例并不关注似然，而是关注下游目标，例如人类感知的图像质量或药物有效性。本文探讨了直接优化扩散模型以实现这些目标的强化学习方法。我们描述了将去噪视为多步骤决策问题如何使得一类策略梯度算法得以实现，我们称之为去噪扩散策略优化（DDPO），其效果优于替代的奖励加权似然方法。实证结果表明，DDPO能够将文本到图像的扩散模型适应于难以通过提示表达的目标，例如图像可压缩性，以及来自人类反馈的目标，例如美学质量。最后，我们展示了DDPO如何利用视觉-语言模型的反馈改善提示与图像的对齐，而无需额外的数据收集或人工标注。"
}
{
  "title": "GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction",
  "title_zh": "GoLLIE：注释指南提升零-shot信息提取",
  "abstract": "Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines which describe the task and give examples to humans. Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out-of-the-box. In this paper we propose GoLLIE (Guideline-following Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed guidelines is key for good results. Code, data and models will be made publicly available.",
  "abstract_zh": "大型语言模型（LLMs）结合指令调优在未见任务的泛化方面取得了显著进展。然而，它们在信息提取（IE）方面的成功较少，落后于特定任务模型。通常，IE任务的特点是复杂的注释指南，这些指南描述任务并为人类提供示例。之前利用这些信息的尝试失败了，即使是最大的模型也无法开箱即用地遵循这些指南。本文提出GoLLIE（遵循指南的大型语言模型用于IE），该模型能够通过微调以遵循注释指南，从而改善未见IE任务的零-shot结果。全面的评估实证表明，GoLLIE能够泛化并遵循未见的指南，超越了之前的零-shot信息提取尝试。消融研究表明，详细的指南是获得良好结果的关键。代码、数据和模型将公开发布。"
}
{
  "title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text",
  "title_zh": "DNA-GPT：用于无训练检测GPT生成文本的发散N-gram分析",
  "abstract": "Large language models (LLMs) have notably enhanced the fluency and diversity of machine-generated text. However, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of LLMs. Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power. To address this gap, we propose a novel training-free detection strategy called Divergent N-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and then use only the preceding portion as input to the LLMs to regenerate the new remaining parts. By analyzing the differences between the original and new remaining parts through N-gram analysis in black-box or probability divergence in white-box, we can clearly illustrate significant discrepancies between machine-generated and human-written text. We conducted extensive experiments on the most advanced LLMs from OpenAI, including text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such as GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach exhibits state-of-the-art performance in distinguishing between human and GPT-generated text on four English and one German dataset, outperforming OpenAI's own classifier, which is trained on millions of text. Additionally, our methods provide reasonable explanations and evidence to support our claim, which is a unique feature of explainable detection. Our method is also robust under the revised text attack and can additionally solve model sourcing.",
  "abstract_zh": "大型语言模型（LLMs）显著提升了机器生成文本的流畅性和多样性。然而，这一进展也带来了检测给定文本来源的重大挑战，目前的检测方法研究滞后于LLMs的快速发展。传统的基于训练的方法在灵活性上存在局限，特别是在适应新领域时，且通常缺乏解释能力。为了解决这一问题，我们提出了一种新颖的无训练检测策略，称为发散N-gram分析（DNA-GPT）。给定一段文本，我们首先在中间截断它，然后仅使用前面的部分作为输入来重新生成新的剩余部分。通过在黑箱中进行N-gram分析或在白箱中进行概率偏差分析，我们可以清晰地说明机器生成文本与人类撰写文本之间的显著差异。我们在OpenAI的最新LLMs上进行了广泛实验，包括text-davinci-003、GPT-3.5-turbo和GPT-4，以及开源模型如GPT-NeoX-20B和LLaMa-13B。结果表明，我们的零-shot方法在区分人类和GPT生成文本方面在四个英语和一个德语数据集上表现出最先进的性能，超越了OpenAI自己在数百万文本上训练的分类器。此外，我们的方法提供了合理的解释和证据来支持我们的主张，这是可解释检测的独特特征。我们的方法在修订文本攻击下也表现出鲁棒性，并且可以进一步解决模型来源问题。"
}
{
  "title": "Quantifying the Plausibility of Context Reliance in Neural Machine Translation",
  "title_zh": "量化神经机器翻译中上下文依赖的合理性",
  "abstract": "Establishing whether language models can use contextual information in a human-plausible way is important to ensure their safe adoption in real-world settings. However, the questions of $\\textit{when}$ and $\\textit{which parts}$ of the context affect model generations are typically tackled separately, and current plausibility evaluations are practically limited to a handful of artificial benchmarks. To address this, we introduce $\\textbf{P}$lausibility $\\textbf{E}$valuation of $\\textbf{Co}$ntext $\\textbf{Re}$liance (PECoRe), an end-to-end interpretability framework designed to quantify context usage in language models' generations. Our approach leverages model internals to (i) contrastively identify context-sensitive target tokens in generated texts and (ii) link them to contextual cues justifying their prediction. We use PECoRe to quantify the plausibility of context-aware machine translation models, comparing model rationales with human annotations across several discourse-level phenomena. Finally, we apply our method to unannotated model translations to identify context-mediated predictions and highlight instances of (im)plausible context usage throughout generation.",
  "abstract_zh": "确定语言模型是否能够以人类合理的方式使用上下文信息对于确保其在现实环境中的安全采用至关重要。然而，关于上下文的$\\textit{何时}$和$\\textit{哪些部分}$影响模型生成的问题通常是分开处理的，当前的合理性评估实际上仅限于少数人工基准。为了解决这个问题，我们引入了上下文依赖的$\\textbf{P}$lausibility $\\textbf{E}$valuation of $\\textbf{Co}$ntext $\\textbf{Re}$liance（PECoRe），这是一个端到端的可解释性框架，旨在量化语言模型生成中的上下文使用。我们的方法利用模型内部信息（i）对生成文本中的上下文敏感目标标记进行对比识别，并（ii）将其与证明其预测的上下文线索联系起来。我们使用PECoRe量化上下文感知机器翻译模型的合理性，比较模型的推理与多个话语层面现象的人类注释。最后，我们将我们的方法应用于未注释的模型翻译，以识别上下文介导的预测，并突出生成过程中（不）合理的上下文使用实例。"
}
{
  "title": "Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers",
  "title_zh": "抽象器与关系交叉注意力：变压器中显式关系推理的归纳偏置",
  "abstract": "An extension of Transformers is proposed that enables explicit relational reasoning through a novel module called the *Abstractor*. At the core of the Abstractor is a variant of attention called *relational cross-attention*. The approach is motivated by an architectural inductive bias for relational learning that disentangles relational information from object-level features. This enables explicit relational reasoning, supporting abstraction and generalization from limited data. The Abstractor is first evaluated on simple discriminative relational tasks and compared to existing relational architectures. Next, the Abstractor is evaluated on purely relational sequence-to-sequence tasks, where dramatic improvements are seen in sample efficiency compared to standard Transformers. Finally, Abstractors are evaluated on a collection of tasks based on mathematical problem solving, where consistent improvements in performance and sample efficiency are observed.",
  "abstract_zh": "提出了一种变压器的扩展，通过一种称为*抽象器*的新模块实现显式关系推理。抽象器的核心是一个称为*关系交叉注意力*的注意力变体。该方法的动机是为关系学习提供一种架构归纳偏置，能够将关系信息与对象级特征分离。这使得显式关系推理成为可能，支持从有限数据中进行抽象和泛化。首先，在简单的区分性关系任务上评估抽象器，并与现有的关系架构进行比较。接下来，在纯粹的关系序列到序列任务上评估抽象器，与标准变压器相比，样本效率有显著提高。最后，在一系列基于数学问题解决的任务上评估抽象器，观察到性能和样本效率的一致提升。"
}
{
  "title": "Debiasing Algorithm through Model Adaptation",
  "title_zh": "去偏见算法通过模型适应",
  "abstract": "Large language models are becoming the go-to solution for the ever-growing number of tasks.\nHowever, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data.\nThis work proposes a novel method for detecting and mitigating gender bias in language models.\nWe perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey bias.\nBased on the analysis results, we intervene in the model by applying a linear projection to the weight matrices of these layers.\nOur titular method DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks.\nWe release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.",
  "abstract_zh": "大型语言模型正成为日益增长的任务的首选解决方案。然而，随着能力的增强，模型容易依赖于训练数据中存在的偏见和刻板印象所导致的虚假相关性。本文提出了一种新颖的方法来检测和减轻语言模型中的性别偏见。我们进行因果分析以识别问题模型组件，并发现中上层前馈层最容易传达偏见。基于分析结果，我们通过对这些层的权重矩阵应用线性投影来干预模型。我们的方法DAMA显著降低了多种指标测量的偏见，同时保持了模型在下游任务上的性能。我们发布了我们的方法和模型的代码，该代码在显著降低偏见的同时重新训练了LLaMA的最先进性能。"
}
{
  "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models",
  "title_zh": "标题：QA-LoRA：量化感知的大型语言模型低秩适应",
  "abstract": "Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model families and validate its effectiveness in different fine-tuning datasets and downstream scenarios. The code is made available at https://github.com/yuhuixu1993/qa-lora.",
  "abstract_zh": "摘要：近年来，大型语言模型（LLMs）迅速发展。尽管在许多语言理解任务中表现出强大的能力，但其巨大的计算负担在很大程度上限制了LLMs的应用，尤其是在需要将其部署到边缘设备时。本文提出了一种量化感知低秩适应（QA-LoRA）算法。其动机在于量化和适应的自由度不平衡，解决方案是使用分组操作符，这样可以在增加量化自由度的同时减少适应自由度。QA-LoRA可以通过几行代码轻松实现，并为原始LoRA提供了双重能力：（i）在微调过程中，LLM的权重被量化（例如，量化为INT4），以减少时间和内存使用；（ii）在微调后，LLM和辅助权重自然集成到一个量化模型中而不损失准确性。我们将QA-LoRA应用于LLaMA和LLaMA2模型系列，并验证其在不同微调数据集和下游场景中的有效性。代码可在https://github.com/yuhuixu1993/qa-lora获取。"
}
{
  "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models",
  "title_zh": "知识卡：通过插件专用语言模型填补大型语言模型的知识空白",
  "abstract": "By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge. To this end, we propose Knowledge Card, a modular framework to plug in new factual and relevant knowledge into general-purpose LLMs. We first introduce knowledge cards---specialized language models trained on corpora from specific domains and sources. Knowledge cards serve as parametric repositories that are selected at inference time to generate background knowledge for the base LLM. We then propose three content selectors to dynamically select and retain information in documents generated by knowledge cards, specifically controlling for relevance, brevity, and factuality of outputs. Finally, we propose two complementary integration approaches to augment the base LLM with the (relevant, factual) knowledge curated from the specialized LMs. Through extensive experiments, we demonstrate that Knowledge Card achieves state-of-the-art performance on six benchmark datasets. Ultimately, Knowledge Card framework enables dynamic synthesis and updates of knowledge from diverse domains. Its modularity will ensure that relevant knowledge can be continuously updated through the collective efforts of the research community.",
  "abstract_zh": "由于设计原因，大型语言模型（LLMs）是静态的通用模型，频繁重新训练或更新成本高昂。随着它们在知识密集型任务中的日益采用，这些设计选择导致生成事实、相关和最新知识的失败变得显而易见。为此，我们提出了知识卡，一个模块化框架，用于将新的事实和相关知识插入通用LLMs。我们首先介绍知识卡——在特定领域和来源的语料库上训练的专用语言模型。知识卡作为参数化的知识库，在推理时被选择以为基础LLM生成背景知识。然后，我们提出了三种内容选择器，动态选择和保留知识卡生成的文档中的信息，特别控制输出的相关性、简洁性和事实性。最后，我们提出了两种互补的集成方法，以增强基础LLM与从专用LM中策划的（相关、事实）知识。通过广泛的实验，我们证明知识卡在六个基准数据集上达到了最先进的性能。最终，知识卡框架使来自不同领域的知识能够动态合成和更新，其模块化将确保相关知识能够通过研究社区的集体努力不断更新。"
}
{
  "title": "Fine-Tuning Language Models for Factuality",
  "title_zh": "微调语言模型以提高事实准确性",
  "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. However, language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations', which can harmfully perpetuate myths and misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we leverage two key recent innovations in NLP to fine-tune language models to be more factual without human labeling, targeting more open-ended generation settings than past work. First, several recent works have proposed methods for scoring the factuality of open-ended text derived from consistency with an external knowledge base or simply a large model's confidence scores. Second, the Direct Preference Optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from preference rankings generated by either automated criterion significantly improves the factuality of Llama-2 on held-out topics (percent of generated claims that are correct) compared with existing RLHF procedures or decoding strategies targeted at factuality, showing over 50% and 20-30% error reduction for biographies and medical questions respectively.",
  "abstract_zh": "大型预训练语言模型（LLMs）的流畅性和创造力使其得到广泛应用，有时甚至取代传统搜索引擎。然而，语言模型容易产生令人信服但事实不准确的陈述，通常被称为“幻觉”，这可能会有害地延续神话和误解。此外，手动核实模型响应的过程耗时，使得获取人工事实标签的成本高昂。在本研究中，我们利用自然语言处理领域的两个关键创新，微调语言模型以提高其事实准确性，而无需人工标注，目标是比以往工作更开放的生成设置。首先，最近的几项研究提出了评估开放式文本事实准确性的方法，这些方法基于与外部知识库的一致性或大型模型的置信度分数。其次，直接偏好优化算法使得在监督模仿以外的目标上对语言模型进行简单的微调成为可能，使用对可能模型响应的偏好排序。我们展示了从自动标准生成的偏好排序学习显著提高了Llama-2在保留主题上的事实准确性（生成声明的正确百分比），与现有的基于强化学习的程序或针对事实准确性的解码策略相比，传记和医学问题的错误率分别减少了50%和20-30%。"
}
{
  "title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
  "title_zh": "MetaGPT：多智能体协作框架的元编程",
  "abstract": "Recently, remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Previous LLM-based multi-agent systems can already solve simple dialogue tasks. More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors.  MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together.  On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems.",
  "abstract_zh": "最近，通过基于大型语言模型（LLMs）的智能体社会，在自动化问题解决方面取得了显著进展。之前的基于LLM的多智能体系统已经能够解决简单的对话任务。然而，更复杂的任务面临由于简单串联LLM导致的逻辑不一致性和级联幻觉的挑战。在此，我们介绍MetaGPT，这是一种创新的元编程框架，将高效的人类工作流程融入基于LLM的多智能体协作中。MetaGPT将标准操作程序（SOP）编码为提示序列，以实现更流畅的工作流程，从而允许具有人类领域专业知识的智能体验证中间结果并减少错误。MetaGPT利用流水线范式为各种智能体分配不同角色，有效地将复杂任务分解为多个智能体协同工作的子任务。在协作软件工程基准测试中，MetaGPT生成的解决方案比之前的基于聊天的多智能体系统更具连贯性。"
}
{
  "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference",
  "title_zh": "理解语言模型中的灾难性遗忘：通过隐式推理",
  "abstract": "We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks. We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution. To test this, we propose Conjugate Prompting, which artificially makes the task look farther from the fine-tuning distribution while requiring the same capability, and we find that this recovers some of the pretraining capabilities on our synthetic setup. Since real-world fine-tuning distributions are predominantly English, we apply conjugate prompting to recover pretrained capabilities in LLMs by simply translating the prompts to different languages. This allows us to recover the in-context learning abilities lost via instruction tuning, and more concerningly, recover harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.",
  "abstract_zh": "我们缺乏对微调（通过指令微调或基于人类反馈的强化学习等方法）影响的系统理解，特别是在狭窄微调分布之外的任务中。在一个简化的场景中，我们展示了在微调数据分布内提高任务性能是以牺牲其他任务的能力为代价的。我们假设语言模型隐式推断提示的任务，而微调则使这种推断偏向于微调分布中的任务。为了验证这一点，我们提出了共轭提示，它人为地使任务看起来远离微调分布，同时要求相同的能力，我们发现这在我们的合成设置中恢复了一些预训练能力。由于现实世界的微调分布主要是英语，我们通过简单地将提示翻译成不同语言来应用共轭提示，以恢复LLMs中的预训练能力。这使我们能够恢复通过指令微调丧失的上下文学习能力，更令人担忧的是，恢复了在像ChatGPT这样的聊天机器人中被安全微调抑制的有害内容生成。"
}
{
  "title": "Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation",
  "title_zh": "朝向绿色人工智能：通过自适应反向传播微调大型语言模型",
  "abstract": "Fine-tuning is essential to adapting pre-trained large language models to downstream applications. With the increasing popularity of LLM-enabled applications, fine-tuning has been performed intensively worldwide, incurring a tremendous amount of computing costs that correspond to big carbon footprint and environmental impact. Mitigating such environmental impact directly correlates to reducing the fine-tuning FLOPs. Existing fine-tuning schemes focus on either saving memory or reducing the overhead of computing weight updates, but cannot achieve sufficient FLOPs reduction due to their ignorance of the training cost in backpropagation. To address this limitation, in this paper we present GreenTrainer, a new technique that minimizes the FLOPs of LLM fine-tuning via adaptive backpropagation, which adaptively selects the most appropriate set of LLM tensors for fine-tuning based on their importance and backpropagation cost in training. Experiment results show that GreenTrainer can save up to 64\\% training FLOPs compared to full fine-tuning, without any noticeable accuracy loss. Compared to the existing schemes such as Prefix Tuning and LoRA, GreenTrainer can achieve up to 4\\% improvement of model accuracy, with on-par FLOPs reduction.",
  "abstract_zh": "微调对于将预训练的大型语言模型适应于下游应用至关重要。随着基于大型语言模型的应用日益普及，全球范围内的微调工作变得异常频繁，导致了巨大的计算成本，这与巨大的碳足迹和环境影响相对应。减轻这种环境影响与减少微调的FLOPs直接相关。现有的微调方案要么专注于节省内存，要么减少计算权重更新的开销，但由于忽视了反向传播中的训练成本，无法实现足够的FLOPs减少。为了解决这一限制，本文提出了GreenTrainer，这是一种通过自适应反向传播最小化大型语言模型微调FLOPs的新技术，它根据张量在训练中的重要性和反向传播成本自适应选择最合适的微调张量集。实验结果表明，与完全微调相比，GreenTrainer可以节省高达64%的训练FLOPs，而不会造成明显的准确性损失。与现有方案如前缀微调和LoRA相比，GreenTrainer可以实现高达4%的模型准确性提升，同时保持相当的FLOPs减少。"
}
{
  "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
  "title_zh": "标题：大型语言模型可以自我欺骗：基于提示的对抗攻击",
  "abstract": "The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM’s adversarial robustness. This paper proposes an efficient tool to audit the LLM’s adversarial robustness via a prompt-based adversarial attack (PromptAttack). PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself. The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples. Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels. Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++. Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions. Our source code is available at https://github.com/GodXuxilie/PromptAttack.",
  "abstract_zh": "摘要：大型语言模型（LLMs）在安全关键领域的广泛应用，要求对LLM的对抗鲁棒性进行适当评估。本文提出了一种高效工具，通过基于提示的对抗攻击（PromptAttack）来审计LLM的对抗鲁棒性。PromptAttack将对抗文本攻击转换为攻击提示，使受害的LLM输出对抗样本以自我欺骗。攻击提示由三个重要组成部分构成：（1）原始输入（OI），包括原始样本及其真实标签；（2）攻击目标（AO），描述生成一个新的样本以自我欺骗而不改变语义含义的任务；（3）攻击指导（AG），包含扰动指令，引导LLM如何通过在字符、单词和句子层面扰动原始样本来完成任务。此外，我们使用保真度过滤器确保PromptAttack保持对抗示例的原始语义。进一步地，我们通过在不同扰动级别上集成对抗示例来增强PromptAttack的攻击能力。使用Llama2和GPT-3.5的全面实证结果验证了PromptAttack的攻击成功率显著高于AdvGLUE和AdvGLUE++。有趣的发现包括，一个简单的表情符号可以轻易误导GPT-3.5做出错误预测。我们的源代码可在https://github.com/GodXuxilie/PromptAttack获取。"
}
{
  "title": "Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization",
  "title_zh": "标题：不要信任：验证——将大型语言模型的定量推理与自动形式化相结合",
  "abstract": "Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code --- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting --- the previously best method to identify correct answers, by more than 12\\% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.",
  "abstract_zh": "摘要：大型语言模型（LLM），如谷歌的Minerva和OpenAI的GPT系列，越来越能够解决数学定量推理问题。然而，它们在推理步骤和答案中仍然会出现不合理的逻辑和计算错误。本文利用这样一个事实：如果LLM的训练语料库中包含足够多的形式数学示例（例如在Isabelle中，一个形式定理证明环境），它们可以被提示将非正式数学陈述翻译，即自动形式化为正式的Isabelle代码——该代码可以自动验证其内部一致性。这提供了一种机制，自动拒绝那些形式化版本在内部或与形式化问题陈述不一致的解决方案。我们在GSM8K、MATH和MultiArith数据集上评估了我们的方法，并证明我们的方法提供了一种比普通多数投票（之前识别正确答案的最佳方法）更一致的启发式，GSM8K上提高了12%以上。在我们的实验中，它在所有数据集和LLM模型规模上都持续改善结果。代码可以在https://github.com/jinpz/dtv找到。"
}
{
  "title": "Meaning Representations from Trajectories in Autoregressive Models",
  "title_zh": "基于自回归模型的轨迹的意义表示",
  "abstract": "We propose to extract meaning representations from autoregressive language models by considering the distribution of all possible trajectories extending an input text. This strategy is prompt-free, does not require fine-tuning, and is applicable to any pre-trained autoregressive model. Moreover, unlike vector-based representations,  distribution-based representations can also model asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym relations) by using algebraic operations between likelihood functions. These ideas are grounded in distributional perspectives on semantics and are connected to standard constructions in automata theory, but to our knowledge they have not been applied to modern language models. We empirically show that the representations obtained from large models align well with human annotations, outperform other zero-shot and prompt-free methods on semantic similarity tasks, and can be used to solve more complex entailment and containment tasks that standard embeddings cannot handle. Finally, we extend our method  to represent data from different modalities (e.g., image and text) using multimodal autoregressive models. Our code is available at: https://github.com/tianyu139/meaning-as-trajectories",
  "abstract_zh": "我们提出通过考虑扩展输入文本的所有可能轨迹的分布，从自回归语言模型中提取意义表示。这种策略无需提示，不需要微调，并且适用于任何预训练的自回归模型。此外，与基于向量的表示不同，基于分布的表示还可以通过对似然函数进行代数运算来建模不对称关系（例如，逻辑蕴涵的方向、上位词/下位词关系）。这些思想基于语义的分布视角，并与自动机理论中的标准构造相关，但据我们所知，它们尚未应用于现代语言模型。我们通过实证研究表明，从大型模型获得的表示与人类注释高度一致，在语义相似性任务中优于其他零样本和无提示方法，并且可以用于解决标准嵌入无法处理的更复杂的蕴涵和包含任务。最后，我们扩展了我们的方法，以使用多模态自回归模型表示来自不同模态（例如图像和文本）的数据。我们的代码可在以下网址获取：https://github.com/tianyu139/meaning-as-trajectories"
}
{
  "title": "Query-Policy Misalignment in Preference-Based Reinforcement Learning",
  "title_zh": "标题：基于偏好的强化学习中的查询-策略不一致性",
  "abstract": "Preference-based reinforcement learning (PbRL) provides a natural way to align RL agents’ behavior with human desired outcomes, but is often restrained by costly human feedback. To improve feedback efficiency, most existing PbRL methods focus on selecting queries to maximally improve the overall quality of the reward model, but counter-intuitively, we find that this may not necessarily lead to improved performance. To unravel this mystery, we identify a long-neglected issue in the query selection schemes of existing PbRL studies: Query-Policy Misalignment. We show that the seemingly informative queries selected to improve the overall quality of reward model actually may not align with RL agents’ interests, thus offering little help on policy learning and eventually resulting in poor feedback efficiency. We show that this issue can be effectively addressed via policy-aligned query and a specially designed hybrid experience replay, which together enforce the bidirectional query-policy alignment. Simple yet elegant, our method can be easily incorporated into existing approaches by changing only a few lines of code. We showcase in comprehensive experiments that our method achieves substantial gains in both human feedback and RL sample efficiency, demonstrating the importance of addressing query-policy misalignment in PbRL tasks.",
  "abstract_zh": "摘要：基于偏好的强化学习（PbRL）提供了一种自然的方式来使RL代理的行为与人类期望的结果对齐，但常常受到昂贵的人类反馈的限制。为了提高反馈效率，大多数现有的PbRL方法专注于选择查询以最大限度地提高奖励模型的整体质量，但我们发现这可能并不一定会导致性能的提高。为了解开这个谜团，我们识别出现有PbRL研究中的一个长期被忽视的问题：查询-策略不一致性。我们表明，选择的看似信息丰富的查询以改善奖励模型的整体质量实际上可能与RL代理的利益不一致，因此对策略学习的帮助有限，最终导致反馈效率低下。我们展示了通过策略对齐查询和特别设计的混合经验重放可以有效解决这一问题，这两者共同促进了双向查询-策略对齐。我们的方法简单而优雅，可以通过仅更改几行代码轻松融入现有方法。我们在全面的实验中展示了我们的方法在人类反馈和RL样本效率方面都取得了显著的提升，证明了解决PbRL任务中查询-策略不一致性的重要性。"
}
{
  "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
  "title_zh": "时间-LLM：通过重编程大型语言模型进行时间序列预测",
  "abstract": "Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that \\method is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios. The code is made available at https://github.com/KimMeen/Time-LLM.",
  "abstract_zh": "时间序列预测在许多现实世界动态系统中具有重要意义，并且已经得到了广泛研究。与自然语言处理（NLP）和计算机视觉（CV）不同，后者可以使用单一大型模型处理多个任务，时间序列预测模型通常是专门化的，需为不同任务和应用设计独特的模型。尽管预训练基础模型在NLP和CV领域取得了显著进展，但在时间序列领域的发展受到数据稀疏的限制。最近的研究表明，大型语言模型（LLMs）在复杂的令牌序列上具有强大的模式识别和推理能力。然而，如何有效地对齐时间序列数据和自然语言的模态，以利用这些能力仍然是一个挑战。在本研究中，我们提出了时间-LLM，一个重编程框架，用于将LLMs重新用于通用时间序列预测，同时保持基础语言模型的完整性。我们首先通过文本原型对输入时间序列进行重编程，然后将其输入到冻结的LLM中，以对齐这两种模态。为了增强LLM对时间序列数据的推理能力，我们提出了Prompt-as-Prefix（PaP），它丰富了输入上下文并指导重编程输入片段的转换。最终，从LLM转换得到的时间序列片段被投影以获得预测结果。我们的综合评估表明，\\method是一个强大的时间序列学习者，超越了最先进的专门预测模型。此外，时间-LLM在少样本和零样本学习场景中表现出色。代码已在https://github.com/KimMeen/Time-LLM上发布。"
}
{
  "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
  "title_zh": "巫师编码器：通过Evol-Instruct赋能代码大型语言模型",
  "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated remarkable performance in various code-related tasks. However, different from their counterparts in the general language modeling field, the technique of instruction fine-tuning remains relatively under-researched in this domain. In this paper, we present Code Evol-Instruct, a novel approach that adapts the Evol-Instruct method to the realm of code, enhancing Code LLMs to create novel models, WizardCoder. Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E, our models showcase outstanding performance. They consistently outperform all other open-source Code LLMs by a significant margin. Remarkably, WizardCoder 15B even surpasses the well-known closed-source LLMs, including Anthropic's Claude and Google's Bard, on the HumanEval and HumanEval+ benchmarks. Additionally, WizardCoder 34B not only achieves a HumanEval score comparable to GPT3.5 (ChatGPT) but also surpasses it on the HumanEval+ benchmark. Furthermore, our preliminary exploration highlights the pivotal role of instruction complexity in achieving exceptional coding performance.",
  "abstract_zh": "代码大型语言模型（Code LLMs），如StarCoder，在各种与代码相关的任务中表现出色。然而，与通用语言建模领域的对应模型不同，指令微调技术在这一领域仍然相对欠缺研究。本文提出了Code Evol-Instruct，一种将Evol-Instruct方法适应于代码领域的新方法，增强了代码LLMs以创建新模型巫师编码器。通过在五个主要代码生成基准（即HumanEval、HumanEval+、MBPP、DS-1000和MultiPL-E）上的全面实验，我们的模型展示了卓越的性能，始终显著超越所有其他开源代码LLMs。值得注意的是，巫师编码器15B在HumanEval和HumanEval+基准上甚至超过了知名的闭源LLMs，包括Anthropic的Claude和Google的Bard。此外，巫师编码器34B不仅在HumanEval上达到了与GPT3.5（ChatGPT）相当的分数，还在HumanEval+基准上超过了它。此外，我们的初步探索强调了指令复杂性在实现卓越编码性能中的关键作用。"
}
{
  "title": "Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals",
  "title_zh": "忠实解释黑箱NLP模型的LLM生成反事实方法",
  "abstract": "Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation. The first approach is CF generation, where a large language model (LLM) is prompted to change a specific text concept while keeping confounding concepts unchanged. While this approach is demonstrated to be very effective, applying LLM at inference-time is costly. We hence present a second approach based on matching, and propose a method that is guided by an LLM at training-time and learns a dedicated embedding space. This space is faithful to a given causal graph and effectively serves to identify matches that approximate CFs. After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters. Our empirical results demonstrate the excellent performance of CF generation models as model-agnostic explainers. Moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. We also find that Top-K techniques universally improve every tested method. Finally, we showcase the potential of LLMs in constructing new benchmarks for model explanation and subsequently validate our conclusions. Our work illuminates new pathways for efficient and accurate approaches to interpreting NLP systems.",
  "abstract_zh": "NLP系统预测的因果解释对于确保安全性和建立信任至关重要。然而，现有方法往往无法有效或高效地解释模型预测，并且通常是特定于模型的。本文提出了模型无关的解释，提出了两种反事实（CF）近似的方法。第一种方法是CF生成，其中一个大型语言模型（LLM）被提示在保持混淆概念不变的情况下改变特定文本概念。尽管这种方法被证明非常有效，但在推理时应用LLM的成本较高。因此，我们提出了一种基于匹配的第二种方法，并提出了一种在训练时由LLM引导并学习专用嵌入空间的方法。该空间忠实于给定的因果图，并有效地用于识别近似CF的匹配项。在理论上证明近似CF是构建忠实解释所必需之后，我们对我们的方法进行了基准测试，并解释了包括数十亿参数的LLM在内的多个模型。我们的实证结果表明，CF生成模型作为模型无关的解释器表现出色。此外，我们的匹配方法在测试时所需资源远少于其他方法，也提供了有效的解释，超越了许多基线。我们还发现，Top-K技术普遍提高了每种测试方法的表现。最后，我们展示了LLM在构建模型解释新基准方面的潜力，并随后验证了我们的结论。我们的工作为高效和准确地解释NLP系统开辟了新的途径。"
}
{
  "title": "CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents",
  "title_zh": "CivRealm：决策代理的文明学习与推理之旅",
  "abstract": "The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization's profound alignment with human history and society necessitates sophisticated learning, while its ever-changing situations demand strong reasoning to generalize. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further research, we present initial results for both paradigms. The canonical RL-based agents exhibit reasonable performance in mini-games, whereas both RL- and LLM-based agents struggle to make substantial progress in the full game. \nOverall, CivRealm stands as a unique learning and reasoning challenge for decision-making agents. The code is available at https://github.com/bigai-ai/civrealm.",
  "abstract_zh": "决策代理的泛化包含两个基本要素：从过去经验中学习和在新情境中推理。然而，在大多数交互环境中，重点往往放在学习上，常常以推理的复杂性为代价。本文介绍了CivRealm，一个受到文明游戏启发的环境。文明与人类历史和社会的深刻契合需要复杂的学习，而其不断变化的情境则要求强大的推理能力以实现泛化。具体而言，CivRealm建立了一个具有不完全信息的总和游戏，参与者人数不断变化；它呈现出大量复杂特征，挑战代理处理需要外交和谈判技能的开放式随机环境。在CivRealm中，我们为两种典型代理类型提供接口：专注于学习的张量基础代理和强调推理的语言基础代理。为了促进进一步研究，我们展示了这两种范式的初步结果。基于经典强化学习的代理在迷你游戏中表现合理，而基于强化学习和大语言模型的代理在完整游戏中则难以取得实质性进展。总体而言，CivRealm为决策代理提供了独特的学习和推理挑战。代码可在 https://github.com/bigai-ai/civrealm 获取。"
}
{
  "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
  "title_zh": "安全RLHF：基于人类反馈的安全强化学习",
  "abstract": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowd workers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.\n\nWarning: This paper contains example data that may be offensive or harmful.",
  "abstract_zh": "随着大型语言模型（LLMs）的发展，平衡AI系统的性能和安全性变得尤为重要。然而，帮助性与无害性目标之间的固有紧张关系在LLM训练中带来了重大挑战。为了解决这个问题，我们提出了基于人类反馈的安全强化学习（Safe RLHF），这是一种用于人类价值对齐的新算法。Safe RLHF明确解耦了人类对帮助性和无害性的偏好，有效避免了众包工作者对这种紧张关系的困惑，并允许我们训练独立的奖励和成本模型。我们将LLMs的安全问题形式化为一个优化任务，即在满足特定成本约束的同时最大化奖励函数。利用拉格朗日方法解决这个约束问题，Safe RLHF在微调过程中动态调整两个目标之间的平衡。通过使用Safe RLHF进行三轮微调，我们展示了在增强模型性能的同时，显著减轻有害响应的能力，相较于现有的价值对齐算法更具优势。在实验中，我们使用Safe RLHF对Alpaca-7B进行了微调，并使其与收集的人类偏好对齐，根据人类评估显著提高了其帮助性和无害性。"
}
{
  "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
  "title_zh": "大型内容与行为模型以理解、模拟和优化内容与行为",
  "abstract": "Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Large Language Models (LLMs), with their wide generalizability, make some progress towards the second level. However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems. In this paper, we introduce the receivers' \"behavior tokens,\" such as shares, likes, clicks, purchases, and retweets, in the LLM's training corpora to optimize content for the receivers and predict their behaviors. Other than showing similar performance to LLMs on content understanding tasks, our trained models show generalization capabilities on the behavior dimension for behavior simulation, content simulation, behavior understanding, and behavior domain adaptation. We show results on all these capabilities using a wide range of tasks on three corpora. We call these models Large Content and Behavior Models (LCBMs). Further, to spur more research on LCBMs, we release our new Content Behavior Corpus (CBC), a repository containing communicator, message, and corresponding receiver behavior (https://behavior-in-the-wild.github.io/LCBM).",
  "abstract_zh": "香农和韦弗的开创性信息理论将通信分为三个层次：技术层、语义层和有效性层。技术层处理传输符号的准确重构，而语义层和有效性层则处理推断的意义及其对接收者的影响。大型语言模型（LLMs）凭借其广泛的可推广性，在第二层次上取得了一些进展。然而，LLMs和其他通信模型并不是传统上为预测和优化通信以实现期望的接收者行为和意图而设计的。因此，有效性层在现代通信系统中仍然基本未被触及。在本文中，我们在LLM的训练语料库中引入接收者的“行为标记”，例如分享、点赞、点击、购买和转发，以优化内容并预测他们的行为。我们的训练模型在内容理解任务上表现出与LLMs相似的性能，并在行为维度上展示了行为模拟、内容模拟、行为理解和行为领域适应的泛化能力。我们使用三个语料库上的广泛任务展示了所有这些能力的结果。我们将这些模型称为大型内容与行为模型（LCBMs）。此外，为了促进对LCBMs的更多研究，我们发布了新的内容行为语料库（CBC），这是一个包含通信者、消息和相应接收者行为的存储库（https://behavior-in-the-wild.github.io/LCBM）。"
}
{
  "title": "Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models",
  "title_zh": "标题：即插即用：一种高效的大型语言模型后训练剪枝方法",
  "abstract": "With the rapid growth of large language models (LLMs), there is increasing demand for memory and computation in LLMs. Recent efforts on post-training pruning of LLMs aim to reduce the model size and computation requirements, yet the performance is still sub-optimal. \nIn this paper, we present a plug-and-play solution for post-training pruning of LLMs.\nThe proposed solution has two innovative components: 1) **Relative Importance and Activations (RIA)**, a new pruning metric that jointly considers the weight and activations efficiently on LLMs, and 2) **Channel Permutation**, a new approach to maximally preserves important weights under N:M sparsity.\nThe two proposed components can be readily combined to further enhance the N:M semi-structured pruning of LLMs.\nOur empirical experiments show that RIA alone can already surpass all existing post-training pruning methods on prevalent LLMs, e.g., LLaMA ranging from 7B to 65B. Furthermore, N:M semi-structured pruning with channel permutation can even outperform the original LLaMA2-70B on zero-shot tasks, together with practical speed-up on specific hardware.\nOur code is available at: https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning",
  "abstract_zh": "摘要：随着大型语言模型（LLMs）的快速增长，对LLMs的内存和计算需求日益增加。近期针对LLMs的后训练剪枝工作旨在减少模型大小和计算需求，但性能仍然未达到最佳水平。本文提出了一种即插即用的LLMs后训练剪枝解决方案。所提方案包含两个创新组件：1）**相对重要性与激活（RIA）**，一种新的剪枝度量，能够高效地联合考虑LLMs上的权重和激活；2）**通道置换**，一种在N:M稀疏性下最大限度保留重要权重的新方法。这两个提出的组件可以方便地结合，以进一步增强LLMs的N:M半结构剪枝。我们的实证实验表明，RIA单独已能超越现有的所有后训练剪枝方法，适用于流行的LLMs，例如LLaMA，范围从7B到65B。此外，结合通道置换的N:M半结构剪枝甚至在零-shot任务中超越了原始的LLaMA2-70B，并在特定硬件上实现了实际的加速。我们的代码可在以下链接获取：https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning"
}
{
  "title": "LILO: Learning Interpretable Libraries by Compressing and Documenting Code",
  "title_zh": "LILO：通过压缩和文档化代码学习可解释库",
  "abstract": "While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synthesis benchmarks for string editing, scene reasoning, and graphics composition. Compared to existing neural and symbolic methods—including the state-of-the-art library learning algorithm DreamCoder—LILO solves more complex tasks and learns richer libraries that are grounded in linguistic knowledge.",
  "abstract_zh": "尽管大型语言模型（LLMs）在代码生成方面表现出色，但软件开发的一个关键方面是重构的艺术：将代码整合成可重用和可读的程序库。本文介绍了LILO，这是一个神经符号框架，通过迭代合成、压缩和文档化代码来构建针对特定问题领域的库。LILO结合了LLM引导的程序合成和Stitch中的最新自动重构算法进展：一个符号压缩系统，能够高效识别大型代码语料库中的最佳lambda抽象。为了使这些抽象具有可解释性，我们引入了一种自动文档化（AutoDoc）程序，根据上下文使用示例推断自然语言名称和文档字符串。除了提高人类可读性外，我们发现AutoDoc通过帮助LILO的合成器理解和部署学习到的抽象来提升性能。我们在三个归纳程序合成基准上评估LILO，分别是字符串编辑、场景推理和图形组合。与现有的神经和符号方法相比——包括最先进的库学习算法DreamCoder——LILO能够解决更复杂的任务，并学习到更丰富的基于语言知识的库。"
}
{
  "title": "GeoLLM: Extracting Geospatial Knowledge from Large Language Models",
  "title_zh": "GeoLLM：从大型语言模型中提取地理空间知识",
  "abstract": "The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power.\nHere we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. \nWe first demonstrate that LLMs embed remarkable spatial information about locations, but \nnaively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. \nWe then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap.\nWe demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods.\nAcross these tasks, our method demonstrates a 70\\% improvement in performance (measured using Pearson's $r^2$) relative to baselines that use nearest neighbors or use information directly from the prompt, and performance equal to or exceeding satellite-based benchmarks in the literature.\nWith GeoLLM, we observe that GPT-3.5 outperforms Llama 2 and RoBERTa by 19\\% and 51\\% respectively, suggesting that the performance of our method scales well with the size of the model and its pretraining dataset.\nOur experiments reveal that LLMs are remarkably sample-efficient, rich in geospatial information, and robust across the globe.\nCrucially, GeoLLM shows promise in mitigating the limitations of existing geospatial covariates and complementing them well.\nCode is available on the project website: https://rohinmanvi.github.io/GeoLLM",
  "abstract_zh": "机器学习（ML）在多种地理空间任务中的应用越来越普遍，但通常依赖于全球可用的协变量，如卫星图像，这些图像可能既昂贵又缺乏预测能力。我们探讨了是否可以利用互联网语言语料库中大量的知识，这些知识现在被压缩在大型语言模型（LLMs）中，用于地理空间预测任务。我们首先证明LLMs嵌入了关于位置的显著空间信息，但仅使用地理坐标对LLMs进行简单查询在预测人口密度等关键指标时效果不佳。然后，我们提出了GeoLLM，这是一种新颖的方法，可以有效地从LLMs中提取地理空间知识，并结合来自OpenStreetMap的辅助地图数据。我们展示了我们的方法在多个国际社会关注的核心任务中的实用性，包括人口密度和经济生计的测量。在这些任务中，我们的方法相较于使用最近邻或直接从提示中获取信息的基线，表现提高了70%（使用Pearson的$r^2$进行测量），并且性能等于或超过文献中的基于卫星的基准。通过GeoLLM，我们观察到GPT-3.5的表现分别比Llama 2和RoBERTa高出19%和51%，这表明我们的方法的性能与模型的大小及其预训练数据集的规模良好匹配。我们的实验表明，LLMs在样本效率方面表现出色，富含地理空间信息，并且在全球范围内具有稳健性。至关重要的是，GeoLLM在减轻现有地理空间协变量的局限性方面显示出前景，并能很好地补充它们。代码可在项目网站上获取：https://rohinmanvi.github.io/GeoLLM"
}
{
  "title": "Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models",
  "title_zh": "分子指令：用于大型语言模型的大规模生物分子指令数据集",
  "abstract": "Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. Each component aims to improve the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large models' performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. Mol-Instructions is publicly available for ongoing research and will undergo regular updates to enhance its applicability (https://github.com/zjunlp/Mol-Instructions).",
  "abstract_zh": "大型语言模型（LLMs）凭借其卓越的任务处理能力和创新输出，推动了多个领域的显著进展。然而，它们在生物分子研究等专业领域的能力仍然有限。为了解决这一挑战，我们推出了Mol-Instructions，这是一个针对生物分子领域设计的综合指令数据集。Mol-Instructions包含三个关键组成部分：面向分子的指令、面向蛋白质的指令和生物分子文本指令。每个组成部分旨在提高LLMs对生物分子特征和行为的理解和预测能力。通过对LLMs进行广泛的指令调优实验，我们展示了Mol-Instructions在提升大型模型在复杂生物分子研究领域的表现方面的有效性，从而促进生物分子研究社区的发展。Mol-Instructions公开可用以支持持续研究，并将定期更新以增强其适用性（https://github.com/zjunlp/Mol-Instructions）。"
}
{
  "title": "NOLA: Networks as Linear Combination of Low Rank Random Basis",
  "title_zh": "标题：NOLA：将网络视为低秩随机基的线性组合",
  "abstract": "Large Language Models (LLMs) have recently gained popularity due to their impressive few-shot performance across various downstream tasks. However, fine-tuning all parameters and storing a unique model for each downstream task or domain becomes impractical because of the massive size of checkpoints (e.g., 350GB in GPT-3). Current literature, such as LoRA, showcases the potential of low-rank modifications to the original weights of an LLM, enabling efficient adaptation and storage for task-specific models. These methods can reduce the number of parameters needed to fine-tune an LLM by several orders of magnitude. Yet, these methods face two primary limitations: 1) the parameter reduction is lower-bounded by the rank one decomposition, and 2) the extent of reduction is heavily influenced by both the model architecture and the chosen rank.\nFor instance, in larger models, even a rank one decomposition might exceed the number of parameters truly needed for adaptation. In this paper, we introduce NOLA, which overcomes the rank one lower bound present in LoRA. It achieves this by re-parameterizing the low-rank matrices in LoRA using linear combinations of randomly generated matrices (basis) and optimizing the linear mixture coefficients only. This approach allows us to decouple the number of trainable parameters from both the choice of rank and the network architecture. We present adaptation results using GPT-2 and ViT in natural language and computer vision tasks. NOLA performs as well as, or better than models with equivalent parameter counts. Furthermore, we demonstrate that we can halve the parameters in larger models compared to LoRA with rank one, without sacrificing performance.",
  "abstract_zh": "摘要：大型语言模型（LLMs）因其在各种下游任务中的出色少量学习表现而最近受到关注。然而，由于检查点的巨大规模（例如，GPT-3中的350GB），为每个下游任务或领域微调所有参数并存储独特模型变得不切实际。当前文献（如LoRA）展示了对LLM原始权重进行低秩修改的潜力，使得任务特定模型的高效适应和存储成为可能。这些方法可以将微调LLM所需的参数数量减少几个数量级。然而，这些方法面临两个主要限制：1）参数减少受限于秩一分解的下限，2）减少的程度受到模型架构和选择的秩的强烈影响。例如，在更大的模型中，即使是秩一分解也可能超过适应所需的真实参数数量。在本文中，我们介绍了NOLA，它克服了LoRA中存在的秩一下限。通过使用随机生成矩阵（基）的线性组合重新参数化LoRA中的低秩矩阵，并仅优化线性混合系数，从而实现这一目标。这种方法使我们能够将可训练参数的数量与秩的选择和网络架构解耦。我们展示了在自然语言和计算机视觉任务中使用GPT-2和ViT的适应结果。NOLA的表现与参数数量相当的模型一样好，甚至更好。此外，我们证明与秩一的LoRA相比，我们可以在更大的模型中将参数数量减半，而不牺牲性能。"
}
{
  "title": "Overthinking the Truth: Understanding how Language Models Process False Demonstrations",
  "title_zh": "过度思考真相：理解语言模型如何处理错误示范",
  "abstract": "Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model’s internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some “critical layer”, after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking. Beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors.",
  "abstract_zh": "现代语言模型能够通过少量示范学习模仿复杂模式，使其能够在不进行微调的情况下完成具有挑战性的任务。然而，如果上下文中存在不准确或有害内容，模仿也可能导致模型重现这些错误。我们通过模型的内部表征研究有害模仿，并识别出两个相关现象：过度思考和错误归纳头。第一个现象，过度思考，出现在我们从中间层解码预测时，比较正确与错误的少量示范。在早期层中，两种示范诱导出相似的模型行为，但在某个“临界层”之后，行为急剧分化，此后对错误示范的准确性逐渐下降。第二个现象，错误归纳头，可能是过度思考的机制性原因：这些是晚期层中关注并复制先前示范中错误信息的头，其消融可减少过度思考。除了科学理解，我们的结果表明，研究中间模型计算可能是理解和防范有害模型行为的一个有前景的途径。"
}
{
  "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
  "title_zh": "DoLa：通过对比层解码提高大型语言模型的事实性",
  "abstract": "Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this **D**ecoding by C**o**ntrasting **La**yers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.",
  "abstract_zh": "尽管大型语言模型（LLMs）具有令人印象深刻的能力，但它们容易出现幻觉，即生成与预训练期间看到的事实偏离的内容。我们提出了一种简单的解码策略，以减少预训练LLMs的幻觉，这种策略不需要依赖检索的外部知识或额外的微调。我们的方法通过对比从后层与前层投影到词汇空间所获得的logits差异来获取下一个标记的分布，利用了LLMs中的事实知识通常局限于特定的变换器层这一事实。我们发现这种**D**ecoding by C**o**ntrasting **La**yers（DoLa）方法能够更好地呈现事实知识，并减少错误事实的生成。DoLa在多个选择任务和开放式生成任务中始终提高了真实性，例如在TruthfulQA上将LLaMA系列模型的表现提高了12-17个百分点，展示了其在使LLMs可靠生成真实事实方面的潜力。"
}
{
  "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles",
  "title_zh": "标题：零阶优化与人类反馈的结合：通过排名oracle实现可证明的学习",
  "abstract": "In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle—a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. A prominent instance of such a situation is Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance [Ouyang et al. 2022, Liu et al. 2023, OpenAI et al. 2022, Bai et al. 2022]. We introduce ZO-RankSGD, an innovative zeroth-order optimization algorithm designed to tackle this optimization problem, accompanied by theoretical assurances. Our algorithm utilizes a novel rank-based random estimator to determine the descent direction and guarantees convergence to a stationary point. Moreover, ZO-RankSGD is readily applicable to policy optimization problems in Reinforcement Learning (RL), particularly when only ranking oracles for the episode reward are available. Last but not least, we demonstrate the effectiveness of ZO-RankSGD in a novel application: improving the quality of images generated by a diffusion generative model with human ranking feedback. Throughout experiments, we found that ZO-RankSGD can significantly enhance the detail of generated images with only a few rounds of human feedback. Overall, our work advances the field of zeroth-order optimization by addressing the problem of optimizing functions with only ranking feedback, and offers a new and effective approach for aligning Artificial Intelligence (AI) with human intentions.",
  "abstract_zh": "摘要：本研究探讨了一种新兴的优化挑战，涉及只能通过排名oracle评估的黑箱目标函数——这种情况在现实场景中经常遇到，尤其是当函数由人类评审时。此类情况的一个突出实例是带有人类反馈的强化学习（RLHF），这种方法最近被用来通过人类指导提升大型语言模型（LLMs）的性能。我们提出了ZO-RankSGD，这是一种创新的零阶优化算法，旨在解决这一优化问题，并提供理论保证。我们的算法利用一种新颖的基于排名的随机估计器来确定下降方向，并保证收敛到一个静态点。此外，ZO-RankSGD可以直接应用于强化学习中的策略优化问题，特别是在仅有回合奖励的排名oracle可用时。最后，我们展示了ZO-RankSGD在一个新应用中的有效性：通过人类排名反馈改善扩散生成模型生成的图像质量。在实验中，我们发现ZO-RankSGD能够在仅有少量人类反馈的情况下显著增强生成图像的细节。总体而言，我们的工作通过解决仅有排名反馈的函数优化问题，推动了零阶优化领域的发展，并为使人工智能（AI）与人类意图对齐提供了一种新的有效方法。"
}
{
  "title": "Discovering Failure Modes of Text-guided Diffusion Models via Adversarial Search",
  "title_zh": "标题：通过对抗搜索发现文本引导扩散模型的失败模式",
  "abstract": "Text-guided diffusion models (TDMs) are widely applied but can fail unexpectedly. Common failures include: _(i)_ natural-looking text prompts generating images with the wrong content, or _(ii)_ different random samples of the latent variables that generate vastly different, and even unrelated, outputs despite being conditioned on the same text prompt. In this work, we aim to study and understand the failure modes of TDMs in more detail. To achieve this, we propose SAGE, the first adversarial search method on TDMs that systematically explores the discrete prompt space and the high-dimensional latent space, to automatically discover undesirable behaviors and failure cases in image generation. We use image classifiers as surrogate loss functions during searching, and employ human inspections to validate the identified failures. For the first time, our method enables efficient exploration of both the discrete and intricate human language space and the challenging latent space, overcoming the gradient vanishing problem. Then, we demonstrate the effectiveness of SAGE on five widely used generative models and reveal four typical failure modes that have not been systematically studied before: (1) We find a variety of natural text prompts that generate images failing to capture the semantics of input texts. We further discuss the underlying causes and potential solutions based on the results. (2) We find regions in the latent space that lead to distorted images independent of the text prompt, suggesting that parts of the latent space are not well-structured. (3) We also find latent samples that result in natural-looking images unrelated to the text prompt, implying a possible misalignment between the latent and prompt spaces. (4) By appending a single adversarial token embedding to any input prompts, we can generate a variety of specified target objects, with minimal impact on CLIP scores, demonstrating the fragility of language representations.",
  "abstract_zh": "摘要：文本引导扩散模型（TDMs）被广泛应用，但可能会意外失败。常见的失败包括：（i）自然文本提示生成错误内容的图像，或（ii）不同的潜变量随机样本生成截然不同甚至无关的输出，尽管是基于相同的文本提示。在这项工作中，我们旨在更详细地研究和理解TDMs的失败模式。为此，我们提出了SAGE，这是首个针对TDMs的对抗搜索方法，系统性地探索离散提示空间和高维潜空间，以自动发现图像生成中的不良行为和失败案例。我们在搜索过程中使用图像分类器作为替代损失函数，并通过人工检查验证识别的失败。我们的研究首次实现了对离散和复杂人类语言空间以及具有挑战性的潜空间的高效探索，克服了梯度消失问题。然后，我们在五个广泛使用的生成模型上展示了SAGE的有效性，并揭示了四种以前未系统研究的典型失败模式：（1）我们发现多种自然文本提示生成的图像未能捕捉输入文本的语义。我们进一步讨论了基于结果的潜在原因和解决方案。（2）我们发现潜空间中的某些区域导致与文本提示无关的失真图像，表明潜空间的某些部分结构不佳。（3）我们还发现潜样本生成的自然图像与文本提示无关，暗示潜空间与提示空间之间可能存在不对齐。（4）通过在任何输入提示中附加一个对抗性标记嵌入，我们可以生成多种指定目标对象，对CLIP分数的影响最小，展示了语言表示的脆弱性。"
}
{
  "title": "Emergent Communication with Conversational Repair",
  "title_zh": "标题：具有对话修复的突现通信",
  "abstract": "Research on conversation has put emphasis on the importance of a multi-level communication system, in which the interlocutors aim to establish and maintain common ground. In natural conversations, repair mechanisms such as clarification requests are frequently used to improve mutual understanding.\nHere we explore the effects of conversational repair on languages emerging in signaling games. We extend the basic Lewis signaling game setup with a feedback channel that allows for the transmission of messages backwards from the receiver to the sender. Further, we add noise to the communication channel so that repair mechanisms become necessary for optimal performance.\n\nWe find that languages emerging in setups with feedback channel are less compositional.\nHowever, the models still achieve a substantially higher generalization performance in conditions with noise, putting to question the role of compositionality for generalization.\nThese findings generalize also to a more realistic case involving a guessing game with naturalistic images.\n\nMore broadly speaking, this study provides an important step towards the creation of signaling games that more closely resemble the conditions under which human languages emerged.",
  "abstract_zh": "摘要：对话研究强调多层次通信系统的重要性，其中对话者旨在建立和维持共同基础。在自然对话中，澄清请求等修复机制被频繁使用，以改善相互理解。在这里，我们探讨了对话修复对信号游戏中语言产生的影响。我们扩展了基本的刘易斯信号游戏设置，增加了一个反馈通道，允许消息从接收者向发送者反向传输。此外，我们在通信通道中添加了噪声，使得修复机制对于最佳性能变得必要。我们发现，在具有反馈通道的设置中产生的语言较少具备组合性。然而，在噪声条件下，这些模型仍然实现了显著更高的泛化性能，这对组合性在泛化中的作用提出了质疑。这些发现也推广到涉及自然图像的猜谜游戏的更现实案例。更广泛地说，这项研究为创建更接近人类语言产生条件的信号游戏提供了重要一步。"
}
{
  "title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
  "title_zh": "标题：CRITIC：大型语言模型可以通过工具互动批评自我修正",
  "abstract": "Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially “black boxes” to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.",
  "abstract_zh": "摘要：大型语言模型（LLMs）最近的发展令人印象深刻。然而，这些模型有时会表现出不一致和问题行为，例如虚构事实、生成有缺陷的代码或创建冒犯性和有毒内容。与这些模型不同，人类通常利用外部工具来交叉检查和完善他们的初始内容，例如使用搜索引擎进行事实核查，或使用代码解释器进行调试。受此观察的启发，我们引入了一个名为CRITIC的框架，使LLMs能够像人类与工具互动一样，以类似的方式验证和逐步修正自己的输出。更具体地说，CRITIC从初始输出开始，与适当的工具互动以评估文本的某些方面，然后根据在此验证过程中获得的反馈修订输出。涉及自由形式问答、数学程序合成和毒性减少的综合评估表明，CRITIC始终增强了LLMs的性能。同时，我们的研究强调了外部反馈在促进LLMs持续自我改进中的关键重要性。"
}
{
  "title": "Provable Robust Watermarking for AI-Generated Text",
  "title_zh": "可证明的AI生成文本水印鲁棒性",
  "abstract": "We study the problem of watermarking large language models (LLMs) generated text — one of the most promising approaches for addressing the safety challenges of LLM usage. In this paper, we propose a rigorous theoretical framework to quantify the effectiveness and robustness of LLM watermarks. We propose a robust and high-quality watermark method, Unigram-Watermark, by extending an existing approach with a simplified fixed grouping strategy. We prove that our watermark method enjoys guaranteed generation quality, correctness in watermark detection, and is robust against text editing and paraphrasing. Experiments on three varying LLMs and two datasets verify that our Unigram-Watermark achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs.",
  "abstract_zh": "我们研究了对大型语言模型（LLMs）生成文本进行水印处理的问题——这是解决LLM使用安全挑战的最有前景的方法之一。本文提出了一个严格的理论框架，以量化LLM水印的有效性和鲁棒性。我们通过扩展现有方法并采用简化的固定分组策略，提出了一种鲁棒且高质量的水印方法——Unigram-Watermark。我们证明了该水印方法在生成质量、检测正确性方面具有保证，并且对文本编辑和改写具有鲁棒性。对三种不同的LLM和两个数据集的实验验证了我们的Unigram-Watermark在检测准确性上优于其他方法，并在困惑度上具有可比的生成质量，从而促进了LLM的负责任使用。"
}
{
  "title": "IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models",
  "title_zh": "理想：影响驱动的选择性标注赋能大型语言模型中的上下文学习者",
  "abstract": "In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influence-driven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection.",
  "abstract_zh": "上下文学习是一种有前景的范式，利用上下文示例作为大型语言模型预测的提示。这些提示对于实现强大的性能至关重要。然而，由于提示需要从大量标注示例中抽样，找到合适的提示可能导致高昂的标注成本。为了解决这一挑战，本文提出了一种影响驱动的选择性标注方法，旨在降低标注成本，同时提高上下文示例的质量。我们方法的本质是从大规模未标记数据池中选择一个关键子集进行标注，以便后续的提示抽样。具体而言，首先构建一个有向图来表示未标记数据。随后，通过扩散过程量化候选未标记子集的影响。最后，引入了一种简单而有效的贪婪算法进行未标记数据选择。该算法迭代选择数据，如果其相对于量化影响提供最大边际增益。与之前的选择性标注工作相比，我们的影响驱动方法以端到端的方式运作，避免了数据多样性和代表性之间难以处理的显式平衡，并享有理论支持。实验确认了所提方法在各种基准测试中的优越性，在子集选择过程中实现了更好的性能和更低的时间消耗。"
}
{
  "title": "OWL: A Large Language Model for IT Operations",
  "title_zh": "OWL：用于IT运营的大型语言模型",
  "abstract": "With the rapid advancement of IT operations, managing and analyzing large data volumes efficiently for practical applications has become increasingly critical. Natural Language Processing (NLP) techniques have demonstrated remarkable capabilities in various tasks, including named entity recognition, machine translation, and dialogue systems. Recently, Large Language Models (LLMs) have achieved significant improvements across various domain-specific areas. However, there is a noticeable gap in the development of specialized Large Language Models (LLMs) tailored for IT operations. In this paper, we introduce the OWL, a large language model trained on our constructed Owl-Instruct with a wide range of IT-related information. Specifically, limited by the maximum input length, we propose the \\textbf{H}omogeneous \\textbf{M}arkov \\textbf{C}ontext \\textbf{E}xtension method (HMCE). The mixture-of-adapter strategy is leveraged to improve the parameter-efficient tuning across different domains or tasks.\nFurther, we evaluate the performance of OWL on the Owl-Bench established by us and open IT-related benchmarks. OWL  demonstrates superior performance results on IT tasks, which outperforms existing models by significant margins. Moreover, we hope that the findings of our work will provide more insights to revolutionize the techniques of IT operations with specialized LLMs.",
  "abstract_zh": "随着IT运营的快速发展，高效管理和分析大量数据以满足实际应用变得越来越重要。自然语言处理（NLP）技术在命名实体识别、机器翻译和对话系统等各种任务中展现了显著的能力。近年来，大型语言模型（LLMs）在多个特定领域取得了显著的进展。然而，专门针对IT运营开发的专业大型语言模型（LLMs）仍存在明显的差距。本文介绍了OWL，一个在我们构建的Owl-Instruct上训练的大型语言模型，涵盖了广泛的IT相关信息。具体而言，由于最大输入长度的限制，我们提出了\\textbf{H}omogeneous \\textbf{M}arkov \\textbf{C}ontext \\textbf{E}xtension方法（HMCE）。采用混合适配器策略以提高不同领域或任务之间的参数高效调优。此外，我们在我们建立的Owl-Bench和开放的IT相关基准上评估了OWL的性能。OWL在IT任务上表现出优越的性能，显著超越现有模型。此外，我们希望我们的研究结果能为利用专业LLMs革新IT运营技术提供更多见解。"
}
{
  "title": "When can transformers reason with abstract symbols?",
  "title_zh": "标题：变压器何时能够用抽象符号进行推理？",
  "abstract": "We investigate the capability of Transformer large language models (LLMs) to generalize on unseen symbols when trained on  tasks that rely on abstract symbols (e.g.,  variables in programming and mathematics). Such a 'variable-binding' capability  has long been studied in the neuroscience literature as one of the most basic 'reasoning' capabilities. For (i) binary classification tasks, we prove that Transformers can generalize to unseen symbols but require astonishingly large training data. For (ii) tasks with labels dependent on input symbols, we show an ''inverse scaling law'': Transformers fail to generalize to unseen symbols as their embedding dimension increases. For both cases (i) and (ii), we propose a Transformer modification, adding two trainable parameters per head that can reduce the amount of data needed.",
  "abstract_zh": "摘要：我们研究了变压器大型语言模型（LLMs）在训练依赖于抽象符号（例如，编程和数学中的变量）的任务时，对未见符号进行泛化的能力。这种“变量绑定”能力在神经科学文献中长期以来被视为最基本的“推理”能力之一。对于（i）二元分类任务，我们证明变压器可以对未见符号进行泛化，但需要惊人数量的训练数据。对于（ii）标签依赖于输入符号的任务，我们展示了一种“逆缩放法则”：随着嵌入维度的增加，变压器无法对未见符号进行泛化。对于这两种情况（i）和（ii），我们提出了一种变压器修改，增加每个头部两个可训练参数，以减少所需的数据量。"
}
{
  "title": "CABINET: Content Relevance-based Noise Reduction for Table Question Answering",
  "title_zh": "CABINET：基于内容相关性的表格问答噪声减少",
  "abstract": "Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) – a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question answering LLM (QA LLM). To further aid the relevance scorer, CABINET employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. CABINET significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets here.",
  "abstract_zh": "大型语言模型（LLMs）在表格问答（QA）任务中的表格理解能力已得到广泛研究。通常，只有表格的一小部分与给定问题的答案相关。无关部分作为噪声，分散注意力，导致LLMs对噪声的脆弱性，从而表现不佳。为此，我们提出CABINET（基于内容相关性的表格问答噪声减少框架），旨在通过抑制无关信息使LLMs专注于相关的表格数据。CABINET包括一个无监督相关性评分器（URS），与QA LLM差异训练，根据其与输入问题的相关性对表格内容进行加权，然后再输入到问答LLM中。为了进一步帮助相关性评分器，CABINET采用一个弱监督模块，生成描述与问题相关的行和列标准的解析语句，并突出显示相应表格单元格的内容。CABINET在各种表格LLM基准测试以及基于GPT3的上下文学习方法中显著超越，且对噪声更具鲁棒性，在不同大小的表格上保持优越表现，并在WikiTQ、FeTaQA和WikiSQL数据集上建立新的最先进性能。我们在此发布我们的代码和数据集。"
}
{
  "title": "The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models",
  "title_zh": "标题：恶魔藏在神经元中：解释和减轻语言模型中的社会偏见",
  "abstract": "Pre-trained Language models (PLMs) have been acknowledged to contain harmful information, such as social biases, which may cause negative social impacts or even bring catastrophic results in application. Previous works on this problem mainly focused on using black-box methods such as probing to detect and quantify social biases in PLMs by observing model outputs. As a result, previous debiasing methods mainly finetune or even pre-train PLMs on newly constructed anti-stereotypical datasets, which are high-cost. In this work, we try to unveil the mystery of social bias inside language models by introducing the concept of {\\sc Social Bias Neurons}. Specifically, we propose {\\sc Integrated Gap Gradients (IG$^2$)} to accurately pinpoint units (i.e., neurons) in a language model that can be attributed to undesirable behavior, such as social bias.  By formalizing undesirable behavior as a distributional property of language, we employ sentiment-bearing prompts to elicit classes of sensitive words (demographics) correlated with such sentiments. Our IG$^2$ thus attributes the uneven distribution for different demographics to specific Social Bias Neurons, which track the trail of unwanted behavior inside PLM units to achieve interoperability. Moreover, derived from our interpretable technique, {\\sc Bias Neuron Suppression (BNS)} is further proposed to mitigate social biases. By studying BERT, RoBERTa, and their attributable differences from debiased FairBERTa, IG$^2$ allows us to locate and suppress identified neurons, and further mitigate undesired behaviors. As measured by prior metrics from StereoSet, our model achieves a higher degree of fairness while maintaining language modeling ability with low cost\\footnote{This work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.}.",
  "abstract_zh": "摘要：预训练语言模型（PLMs）已被认可包含有害信息，如社会偏见，这可能导致负面的社会影响，甚至在应用中带来灾难性的结果。以往对这个问题的研究主要集中在使用黑箱方法，如探测，来通过观察模型输出检测和量化PLMs中的社会偏见。因此，以往的去偏见方法主要是在新构建的反刻板印象数据集上微调甚至重新训练PLMs，这成本高昂。在本研究中，我们试图通过引入“社会偏见神经元”的概念揭示语言模型中社会偏见的奥秘。具体而言，我们提出“集成间隙梯度（IG$^2$）”来准确定位语言模型中与不良行为（如社会偏见）相关的单元（即神经元）。通过将不良行为形式化为语言的分布特性，我们采用情感承载提示来引发与这些情感相关的敏感词（人口统计）的类别。因此，我们的IG$^2$将不同人口统计的分布不均归因于特定的社会偏见神经元，这些神经元追踪PLM单元内不良行为的痕迹以实现互操作性。此外，基于我们可解释的技术，进一步提出“偏见神经元抑制（BNS）”以减轻社会偏见。通过研究BERT、RoBERTa及其与去偏见的FairBERTa之间的可归因差异，IG$^2$使我们能够定位和抑制已识别的神经元，并进一步减轻不良行为。根据StereoSet的先前指标，我们的模型在保持语言建模能力的同时，以低成本实现了更高程度的公平性。"
}
{
  "title": "Controlled Text Generation via Language Model Arithmetic",
  "title_zh": "受控文本生成通过语言模型算术",
  "abstract": "As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important. In this work, we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of generated text while outperforming state-of-the-art on the task of toxicity reduction. We release an open source easy-to-use implementation of our framework at https://github.com/eth-sri/language-model-arithmetic.",
  "abstract_zh": "随着大型语言模型（LLMs）的广泛应用，针对词汇、风格和角色的定制变得愈加重要。本文介绍了一种模型算术的新推理框架，能够在无需模型（重新）训练或高度特定数据集的情况下组合和偏置LLMs。此外，该框架允许比直接提示和先前的受控文本生成（CTG）技术更精确地控制生成的文本。通过模型算术，我们可以将先前的CTG技术表达为简单的公式，并自然地将其扩展到新的、更有效的形式。此外，我们展示了高效LLM采样技术——推测采样——在我们的设置中也适用。这使得使用多个组合模型进行高效文本生成成为可能，仅需比单一模型稍高的开销。我们的实证评估表明，模型算术允许对生成文本进行细粒度控制，同时在毒性减少任务上超越了最先进的技术。我们在https://github.com/eth-sri/language-model-arithmetic发布了一个开源的易用实现。"
}
{
  "title": "THOUGHT PROPAGATION: AN ANALOGICAL APPROACH TO COMPLEX REASONING WITH LARGE LANGUAGE MODELS",
  "title_zh": "思维传播：一种利用类比方法进行复杂推理的大型语言模型",
  "abstract": "Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. \nHowever, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \\textit{from scratch}.\nTo address these issues, we propose \\textbf{\\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs.\nThese analogous problems are related to the input one, with reusable solutions and problem-solving strategies.\nThus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. \nTo achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. \nThen, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch.\nTP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. \nExperiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\\% improvement of human preference in Creative Writing, and 15\\% enhancement in the task completion rate of LLM-Agent Planning.",
  "abstract_zh": "大型语言模型（LLMs）在推理任务中取得了显著成功，得益于提示方法的发展。然而，现有的提示方法无法重用解决类似问题的见解，并且在多步推理中遭受累积错误，因为它们促使LLMs从头开始推理。为了解决这些问题，我们提出了\\textbf{\\textit{思维传播}（TP）}，它探索类比问题并利用其解决方案来增强LLMs的复杂推理能力。这些类比问题与输入问题相关，具有可重用的解决方案和问题解决策略。因此，将解决先前类比问题的见解传播到新问题的解决中是有前景的。为此，TP首先提示LLMs提出并解决一组与输入问题相关的类比问题。然后，TP重用类比问题的结果，直接生成新解决方案或推导出知识密集型执行计划，以修正从头获得的初始解决方案。TP与现有的提示方法兼容，允许在广泛任务中进行即插即用的泛化和增强，而无需在特定任务的提示工程中投入大量精力。跨越三个具有挑战性的任务的实验表明，TP在寻找最优解决方案的短路径推理中平均提高了12%的绝对增幅，在创意写作中提高了13%的人类偏好，在LLM-Agent规划的任务完成率上提高了15%。"
}
{
  "title": "Understanding Transferable Representation Learning and Zero-shot Transfer in CLIP",
  "title_zh": "理解CLIP中的可转移表示学习和零样本迁移",
  "abstract": "Multi-modal learning has become increasingly popular due to its ability to leverage information from different data sources (e.g., text and images) to improve the model performance. Recently, CLIP has emerged as an effective approach that employs vision-language contrastive pretraining to learn joint image and text representations and exhibits remarkable performance in zero-shot learning and text-guided natural image generation. Despite the substantial practical success of CLIP, its theoretical understanding remains elusive. In this paper, we formally study transferrable representation learning underlying CLIP and demonstrate how features from different modalities get aligned. We also analyze its zero-shot transfer performance on the downstream tasks. In addition, we conduct empirical evaluations on real data to back\nup our theory. Inspired by our analysis, we propose a new CLIP-type approach, which achieves better performance than CLIP and other state-of-the-art methods on benchmark datasets.",
  "abstract_zh": "多模态学习因其能够利用来自不同数据源（如文本和图像）的信息来提高模型性能而变得日益流行。最近，CLIP作为一种有效的方法，采用视觉-语言对比预训练来学习联合图像和文本表示，并在零样本学习和文本引导的自然图像生成中展现出显著的性能。尽管CLIP在实践中取得了显著成功，但其理论理解仍然模糊。本文正式研究了CLIP背后的可转移表示学习，并展示了来自不同模态的特征如何对齐。我们还分析了其在下游任务上的零样本迁移性能。此外，我们对真实数据进行了实证评估，以支持我们的理论。受到分析的启发，我们提出了一种新的CLIP类型方法，在基准数据集上实现了比CLIP和其他最先进方法更好的性能。"
}
{
  "title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
  "title_zh": "智能游戏：作为智能代理的LLM基准测试",
  "abstract": "Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately.\nSmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. \nWe release our benchmark at https://github.com/microsoft/SmartPlay",
  "abstract_zh": "最近的大型语言模型（LLMs）展示了作为智能代理和下一代自动化的巨大潜力，但目前缺乏系统的基准来评估LLMs作为代理的能力。我们介绍了智能游戏：一个具有挑战性的基准和评估LLMs作为代理的方法论。智能游戏由6种不同的游戏组成，包括石头剪子布、汉诺塔和Minecraft。每个游戏都有独特的设置，提供多达20个评估设置和无限的环境变化。智能游戏中的每个游戏独特地挑战了智能LLM代理的9项重要能力的子集，包括对象依赖推理、前瞻性规划、空间推理、历史学习和随机性理解。每个游戏测试的能力集合之间的区别使我们能够单独分析每项能力。智能游戏不仅作为评估LLM代理整体性能的严格测试场所，还作为识别当前方法论差距的路线图。我们在https://github.com/microsoft/SmartPlay发布了我们的基准。"
}
{
  "title": "Guiding Instruction-based Image Editing via Multimodal Large Language Models",
  "title_zh": "基于指导的图像编辑通过多模态大语言模型",
  "abstract": "Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.",
  "abstract_zh": "基于指导的图像编辑通过自然命令提高了图像操作的可控性和灵活性，而无需复杂的描述或区域掩码。然而，人类指令有时过于简短，无法被当前方法捕捉和遵循。多模态大语言模型（MLLMs）在跨模态理解和视觉感知响应生成方面显示出良好的能力。我们研究了MLLMs如何促进编辑指令，并提出了MLLM引导的图像编辑（MGIE）。MGIE学习推导表达性指令，并提供明确的指导。编辑模型通过端到端训练共同捕捉这种视觉想象并执行操作。我们评估了Photoshop风格修改、全局照片优化和局部编辑的各个方面。大量实验结果表明，表达性指令对于基于指导的图像编辑至关重要，而我们的MGIE在保持竞争性推理效率的同时，可以显著改善自动指标和人工评估。"
}
{
  "title": "What's In My Big Data?",
  "title_zh": "我的大数据中有什么？",
  "abstract": "Large text corpora are the backbone of language models.\nHowever, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination).\nIn this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities---count and search---*at scale*, which allows us to analyze more than 35 terabytes on a standard compute node. \nWe apply WIMBD to ten different corpora used to train popular language models, including *C4*, *The Pile*, and *RedPajama*.\nOur analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. \nFor instance, we find that about 50% of the documents in *RedPajama* and *LAION-2B-en* are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE.\nWe open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.",
  "abstract_zh": "大型文本语料库是语言模型的基础。然而，我们对这些语料库的内容，包括一般统计、质量、社会因素以及评估数据的包含（污染），了解有限。在这项工作中，我们提出了“我的大数据中有什么？”（WIMBD），这是一个平台和一组十六种分析方法，允许我们揭示和比较大型文本语料库的内容。WIMBD 基于两个基本能力——*大规模*的计数和搜索，使我们能够在标准计算节点上分析超过 35TB 的数据。我们将 WIMBD 应用于十个不同的语料库，这些语料库用于训练流行的语言模型，包括 *C4*、*The Pile* 和 *RedPajama*。我们的分析揭示了关于这些语料库的一些令人惊讶和以前未记录的发现，包括重复、合成和低质量内容的高频率、个人可识别信息、有毒语言和基准污染。例如，我们发现 *RedPajama* 和 *LAION-2B-en* 中约 50% 的文档是重复的。此外，用于基准测试在这些语料库上训练的模型的几个数据集在重要基准方面受到污染，包括 Winograd Schema Challenge 和 GLUE 和 SuperGLUE 的部分内容。我们开源了 WIMBD 的代码和工件，以提供一套标准的评估方法，用于新的基于文本的语料库，并鼓励对它们进行更多的分析和透明度。"
}
{
  "title": "Simplifying Transformer Blocks",
  "title_zh": "简化变换器模块",
  "abstract": "A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and MLP sub-blocks with skip connections \\& normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable.\n\nIn this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and BERT encoder-only models, our simplified transformers match the per-iteration training speed and performance of standard transformers, while enjoying 15\\% faster training throughput, and using 15\\% fewer parameters.",
  "abstract_zh": "本文探讨了标准变换器模块在多大程度上可以简化。通过结合信号传播理论和经验观察，我们提出了许多可以去除而不影响训练速度的模块组件的修改，包括跳跃连接、投影或值参数、顺序子模块和归一化层。在自回归解码器和BERT编码器模型的实验中，我们的简化变换器在每次迭代的训练速度和性能上与标准变换器相匹配，同时享有15%的更快训练吞吐量，并使用15%更少的参数。"
}
{
  "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning",
  "title_zh": "利用解释：增强文本属性图表示学习的LLM到LM解释器",
  "abstract": "Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of \\emph{explanations as features}: we prompt an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an \\emph{LLM-to-LM interpreter} to translate these explanations into informative features for downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art results on well-established TAG datasets, including \\texttt{Cora}, \\texttt{PubMed}, \\texttt{ogbn-arxiv}, as well as our newly introduced dataset, \\texttt{tape-arxiv23}. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on \\texttt{ogbn-arxiv}. Lastly, we believe the versatility of the proposed method extends beyond TAGs and holds the potential to enhance other tasks involving graph-text data~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/TAPE}}.",
  "abstract_zh": "文本属性图（TAG）的表示学习近年来已成为一个关键的研究问题。TAG的典型例子是论文引用图，其中每篇论文的文本作为节点属性。最初的图神经网络（GNN）管道通过将这些文本属性转换为浅层或手工特征（如跳字模型或词袋特征）来处理它们。最近的努力集中在通过语言模型（LM）增强这些管道，这通常需要复杂的设计和大量的计算资源。随着强大的大型语言模型（LLM）如GPT或Llama2的出现，它们展示了推理和利用一般知识的能力，结合LLM的文本建模能力与GNN的结构学习能力的技术需求日益增长。因此，在本研究中，我们专注于利用LLM捕捉文本信息作为特征，以提升GNN在下游任务上的表现。一个关键创新是我们使用“解释作为特征”：我们提示LLM进行零-shot分类，请求其决策过程的文本解释，并设计一个LLM到LM解释器，将这些解释转换为下游GNN的有用特征。我们的实验表明，我们的方法在公认的TAG数据集上取得了最先进的结果，包括Cora、PubMed、ogbn-arxiv，以及我们新引入的数据集tape-arxiv23。此外，我们的方法显著加快了训练速度，在ogbn-arxiv上实现了比最接近的基线提高2.88倍的效果。最后，我们相信所提方法的多功能性超越了TAG，并有潜力增强其他涉及图-文本数据的任务。"
}
{
  "title": "TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models",
  "title_zh": "TAIL：用于模仿学习的大型预训练模型的任务特定适配器",
  "abstract": "The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications. Prior work mainly emphasizes either effective \\emph{pretraining} of large models for decision-making or single-task adaptation. But real-world problems will require data-efficient, \\emph{continual adaptation} for new control tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques---e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA)---in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our extensive experiments comparing prevalent parameter-efficient fine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can achieve the best post-adaptation performance with only 1\\% of the trainable parameters of full fine-tuning, while avoiding catastrophic forgetting and preserving adaptation plasticity in continual learning settings.",
  "abstract_zh": "大型预训练模型在控制领域（如机器人技术）中的潜力尚未得到充分发挥，这主要是由于数据稀缺和训练或微调这些大型模型所面临的计算挑战。以往的研究主要强调大型模型在决策制定中的有效预训练或单任务适应。然而，现实世界的问题需要数据高效的持续适应以应对新的控制任务。鉴于这些限制，我们提出了TAIL（用于模仿学习的任务特定适配器），这是一个高效适应新控制任务的框架。受到语言领域中参数高效微调的最新进展的启发，我们在TAIL中探索了高效微调技术——例如瓶颈适配器、P-Tuning和低秩适应（LoRA）——以便在有限的演示数据下将大型预训练模型适应于新任务。我们广泛的实验比较了流行的参数高效微调技术和适应基线，结果表明，使用LoRA的TAIL能够以仅1%的可训练参数实现最佳的后适应性能，同时避免灾难性遗忘，并在持续学习环境中保持适应的可塑性。"
}
{
  "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
  "title_zh": "量化语言模型对提示设计中虚假特征的敏感性：或者说我如何开始担心提示格式",
  "abstract": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
  "abstract_zh": "随着大型语言模型（LLMs）作为语言技术的基本组成部分被广泛采用，准确表征其性能至关重要。由于提示设计的选择可以强烈影响模型行为，因此这一设计过程在有效使用任何现代预训练生成语言模型时至关重要。在本研究中，我们关注LLM对一种典型的保留意义的设计选择——提示格式的敏感性。我们发现，几种广泛使用的开源LLM在少量示例设置中对提示格式的微小变化极为敏感，在使用LLaMA-2-13B进行评估时，性能差异可达76个准确度点。即使在增加模型规模、少量示例数量或进行指令调优时，敏感性仍然存在。我们的分析表明，使用基于提示的方法评估LLM的工作应报告在合理提示格式下的性能范围，而不是目前标准的只报告单一格式的性能。我们还展示了不同模型之间的格式性能仅弱相关，这质疑了用任意选择的固定提示格式比较模型的方法有效性。为了促进系统分析，我们提出了FormatSpread，这是一种快速评估给定任务的合理提示格式样本集的算法，并在不访问模型权重的情况下报告预期性能的区间。此外，我们还呈现了一系列分析，以表征这种敏感性的性质，包括探索特定原子扰动的影响和特定格式的内部表示。"
}
{
  "title": "PAE: Reinforcement Learning from External Knowledge for Efficient Exploration",
  "title_zh": "PAE：利用外部知识进行高效探索的强化学习",
  "abstract": "Human intelligence is adept at absorbing valuable insights from external knowledge.\nThis capability is equally crucial for artificial intelligence. \nIn contrast, classical reinforcement learning agents lack such capabilities and often resort to extensive trial and error to explore the environment. \nThis paper introduces $\\textbf{PAE}$: $\\textbf{P}$lanner-$\\textbf{A}$ctor-$\\textbf{E}$valuator, a novel framework for teaching agents to $\\textit{learn to absorb external knowledge}$. \nPAE integrates the Planner's knowledge-state alignment mechanism, the Actor's mutual information skill control, and the Evaluator's adaptive intrinsic exploration reward to achieve 1) effective cross-modal information fusion, 2) enhanced linkage between knowledge and state, and 3) hierarchical mastery of complex tasks.\nComprehensive experiments across\n 11 challenging tasks from the BabyAI and MiniHack environment suites demonstrate PAE's superior exploration efficiency with good interpretability.",
  "abstract_zh": "人类智能擅长从外部知识中吸收有价值的见解，这种能力对人工智能同样至关重要。与此相比，经典的强化学习代理缺乏这种能力，通常依赖大量的试错来探索环境。本文介绍了$\\textbf{PAE}$：$\\textbf{P}$lanner-$\\textbf{A}$ctor-$\\textbf{E}$valuator，一个新颖的框架，用于教导代理“学习吸收外部知识”。PAE整合了规划者的知识状态对齐机制、演员的互信息技能控制和评估者的自适应内在探索奖励，以实现1）有效的跨模态信息融合，2）知识与状态之间的增强联系，以及3）复杂任务的分层掌握。在BabyAI和MiniHack环境套件中的11个挑战性任务上的全面实验表明，PAE在探索效率和可解释性方面表现优越。"
}
{
  "title": "MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use",
  "title_zh": "大语言模型的MetaTool基准：决定是否使用工具及选择使用哪个工具",
  "abstract": "Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-tool scenarios. Subsequently, we set the tasks for both tool usage awareness and tool selection. We define four subtasks from different perspectives in tool selection, including tool selection with similar choices, tool selection in specific scenarios, tool selection with possible reliability issues, and multi-tool selection. We conduct experiments involving eight popular LLMs and find that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents. However, through the error analysis, we found there is still significant room for improvement. Finally, we conclude with insights for tool developers -- we strongly recommend that tool developers choose an appropriate rewrite model for generating new descriptions based on the downstream LLM the tool will apply to.",
  "abstract_zh": "大语言模型（LLMs）因其出色的自然语言处理（NLP）能力而受到广泛关注。最近，许多研究集中在LLMs的工具使用能力上，主要探讨LLMs如何有效地与特定工具协作。然而，在LLMs作为智能代理的场景中，如AutoGPT和MetaGPT等应用，LLMs需要参与复杂的决策过程，包括决定是否使用工具以及从可用工具中选择最合适的工具来满足用户请求。因此，本文介绍了MetaTool，一个旨在评估LLMs是否具备工具使用意识及能否正确选择工具的基准。具体而言，我们在基准中创建了一个名为ToolE的数据集，该数据集包含多种类型的用户查询，以提示LLMs使用工具，包括单工具和多工具场景。随后，我们设定了工具使用意识和工具选择的任务。从工具选择的不同角度定义了四个子任务，包括类似选择的工具选择、特定场景中的工具选择、可能存在可靠性问题的工具选择以及多工具选择。我们对八个流行的LLMs进行了实验，发现大多数仍然难以有效选择工具，突显了LLMs与真正智能代理之间的差距。然而，通过错误分析，我们发现仍有显著的改进空间。最后，我们为工具开发者提供了建议——我们强烈建议工具开发者根据工具将应用的下游LLM选择合适的重写模型以生成新的描述。"
}
{
  "title": "Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability",
  "title_zh": "标题：注意力在上下文学习中是否必要？探索模型架构与上下文学习能力之间的关系",
  "abstract": "What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps toward answering this question. We evaluate thirteen model architectures capable of causal language modeling across a suite of synthetic in-context learning tasks. These selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, state-space model inspired, and other emerging attention alternatives. We discover that all the considered architectures can perform in-context learning under a wider range of conditions than previously documented. Additionally, we observe stark differences in statistical efficiency and consistency by varying context length and task difficulty. We also measure each architecture's predisposition towards in-context learning when presented with alternative routes for task resolution. Finally, and somewhat surprisingly, we find that several attention alternatives are more robust in-context learners than transformers. Given that such approaches have constant-sized memory footprints at inference time, this result opens the possibility of scaling up in-context learning to accommodate vastly larger numbers of in-context examples.",
  "abstract_zh": "摘要：模型架构与执行上下文学习的能力之间有什么关系？在这项实证研究中，我们迈出了回答这个问题的第一步。我们评估了十三种能够进行因果语言建模的模型架构，涵盖了一系列合成的上下文学习任务。这些选定的架构代表了广泛的范式，包括递归神经网络、卷积神经网络、变换器、受状态空间模型启发的架构以及其他新兴的注意力替代方案。我们发现，所有考虑的架构在更广泛的条件下都能执行上下文学习，超出了之前的文献记录。此外，我们通过改变上下文长度和任务难度观察到统计效率和一致性之间的显著差异。我们还测量了每种架构在面对任务解决的替代路径时对上下文学习的倾向。最后，令人惊讶的是，我们发现几种注意力替代方案在上下文学习方面比变换器更具鲁棒性。鉴于这些方法在推理时具有恒定大小的内存占用，这一结果为扩大上下文学习以适应大量上下文示例的可能性打开了大门。"
}
{
  "title": "PRIME: Prioritizing Interpretability in Failure Mode Extraction",
  "title_zh": "标题：PRIME：在故障模式提取中优先考虑可解释性",
  "abstract": "In this work, we study the challenge of providing human-understandable descriptions for failure modes in trained image classification models.\nExisting works address this problem by first identifying clusters (or directions) of incorrectly classified samples in a latent space and then aiming to provide human-understandable text descriptions for them.\nWe observe that in some cases, describing text does not match well\nwith identified failure modes, partially owing to the fact that shared interpretable attributes of failure modes may not be captured using clustering in the feature space.\nTo improve on these shortcomings, we propose a novel approach that prioritizes interpretability in this problem: we start by obtaining human-understandable concepts (tags) of images in the dataset and\nthen analyze the model's behavior based on the presence or absence of combinations of these tags.\nOur method also ensures that the tags describing a failure mode form a minimal set,\navoiding redundant and noisy descriptions.\nThrough several experiments on different datasets, we show that our method successfully identifies failure modes and generates high-quality text descriptions associated with them.\nThese results highlight the importance of prioritizing interpretability in understanding model failures.",
  "abstract_zh": "摘要：在本研究中，我们探讨了为训练的图像分类模型中的故障模式提供人类可理解描述的挑战。现有工作通过首先识别潜在空间中错误分类样本的聚类（或方向），然后旨在为其提供人类可理解的文本描述来解决此问题。我们观察到，在某些情况下，描述文本与识别的故障模式不匹配，部分原因在于故障模式的共享可解释属性可能未通过特征空间中的聚类捕获。为了改进这些不足，我们提出了一种优先考虑可解释性的创新方法：我们首先获得数据集中图像的人类可理解概念（标签），然后根据这些标签的组合的存在或缺失分析模型的行为。我们的方法还确保描述故障模式的标签形成一个最小集合，避免冗余和噪声描述。通过对不同数据集的多次实验，我们展示了我们的方法成功识别故障模式并生成与之相关的高质量文本描述。这些结果突显了在理解模型故障时优先考虑可解释性的重要性。"
}
{
  "title": "It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition",
  "title_zh": "标题：永远不嫌晚：将声学信息融合到大型语言模型中以实现自动语音识别",
  "abstract": "Recent studies have successfully shown that large language models (LLMs) can be successfully used for generative error correction (GER) on top of the automatic speech recognition (ASR) output. Specifically, an LLM is utilized to carry out a direct mapping from the N-best hypotheses list generated by an ASR system to the predicted output transcription. However, despite its effectiveness, GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal. In this work, we aim to overcome such a limitation by infusing acoustic information before generating the predicted transcription through a novel late fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a multimodal fusion approach implemented into an auto-regressive decoding process and works in two stages: (i) It first analyzes and calibrates the token-level LLM decision, and (ii) it then dynamically assimilates the information from the acoustic modality. Experimental evidence collected from various ASR tasks shows that UADF surpasses existing fusion mechanisms in several ways. It yields significant improvements in word error rate (WER) while mitigating data uncertainty issues in LLM and addressing the poor generalization relied with sole modality during fusion. We also demonstrate that UADF seamlessly adapts to audio-visual speech recognition.",
  "abstract_zh": "摘要：最近的研究成功表明，大型语言模型（LLMs）可以有效地用于自动语音识别（ASR）输出的生成性错误纠正（GER）。具体而言，LLM被用于将ASR系统生成的N-best假设列表直接映射到预测的输出转录。然而，尽管其有效性，GER引入了额外的数据不确定性，因为LLM的训练未考虑到语音信号中的声学信息。在本研究中，我们旨在通过一种称为不确定性感知动态融合（UADF）的新颖晚期融合解决方案，在生成预测转录之前注入声学信息，从而克服这一限制。UADF是一种多模态融合方法，实施在自回归解码过程中，并分为两个阶段：（i）首先分析和校准令牌级LLM决策；（ii）然后动态吸收声学模态的信息。从各种ASR任务中收集的实验证据表明，UADF在多个方面超越了现有的融合机制。它在降低字错误率（WER）的同时，缓解了LLM中的数据不确定性问题，并解决了在融合过程中单一模态所依赖的较差泛化能力。我们还展示了UADF能够无缝适应音视频语音识别。"
}
{
  "title": "Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs",
  "title_zh": "获取更多，花更少：针对大型语言模型预热微调的原则性数据选择",
  "abstract": "This work focuses on leveraging and selecting from vast, unlabeled, open data to \\emph{pre-fine-tune} a pre-trained language model. The goal is to minimize the need for costly domain-specific data for subsequent fine-tuning while achieving desired performance levels. While many data selection algorithms have been designed for small-scale applications, rendering them unsuitable for our context, some emerging methods do cater to language data scales. However, they often prioritize data that aligns with the target distribution. While this strategy may be effective when training a model from scratch, it can yield limited results when the model has already been pre-trained on a different distribution. Differing from prior work, our key idea is to select data that nudges the pre-training distribution closer to the target distribution. We show the optimality of this approach for fine-tuning tasks under certain conditions. We demonstrate the efficacy of our methodology across a diverse array of tasks, showing that it consistently surpasses other selection methods. Moreover, our proposed method is significantly faster than existing techniques, scaling to millions of samples within a single GPU hour. Our code is open-sourced \\footnote{Code repository: \\url{https://anonymous.4open.science/r/DV4LLM-D761/}}. While fine-tuning offers significant potential for enhancing performance across diverse tasks, its associated costs often limit its widespread adoption; with this work, we hope to lay the groundwork for cost-effective fine-tuning, making its benefits more accessible.",
  "abstract_zh": "本研究专注于利用和选择大量未标记的开放数据来对预训练语言模型进行\\emph{预微调}。目标是最小化后续微调所需的昂贵领域特定数据，同时实现期望的性能水平。虽然许多数据选择算法是为小规模应用设计的，因而不适合我们的背景，但一些新兴方法确实适用于语言数据规模。然而，它们通常优先选择与目标分布一致的数据。虽然这种策略在从头训练模型时可能有效，但当模型已经在不同分布上进行预训练时，效果可能有限。与之前的工作不同，我们的关键思想是选择能够将预训练分布推向目标分布的数据。我们展示了在特定条件下这种方法在微调任务中的最优性。我们在各种任务中展示了我们方法的有效性，表明它始终超越其他选择方法。此外，我们提出的方法显著快于现有技术，在单个GPU小时内可扩展到数百万个样本。我们的代码是开源的\\footnote{代码库：\\url{https://anonymous.4open.science/r/DV4LLM-D761/}}。虽然微调在提升多样任务的性能方面具有重大潜力，但其相关成本往往限制了其广泛采用；通过这项工作，我们希望为成本效益高的微调奠定基础，使其好处更易于获取。"
}
{
  "title": "OpenTab: Advancing Large Language Models as Open-domain Table Reasoners",
  "title_zh": "开放表格：推进大型语言模型作为开放领域表格推理器",
  "abstract": "Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope. However, existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes. In this work, we propose OpenTab, an open-domain table reasoning framework powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently. Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response. Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving up to 21.5% higher accuracy. We further run ablation studies to validate the efficacy of our proposed designs of the system.",
  "abstract_zh": "大型语言模型（LLMs）在大量数据上训练，擅长各种自然语言任务，但无法处理未曾训练过的知识所需的任务。一种解决方案是使用检索器获取相关信息，以扩展LLM的知识范围。然而，现有的基于文本的检索型LLM在处理结构化表格数据时并不理想，因为数据模态多样且表格规模庞大。在本研究中，我们提出了OpenTab，一个由LLM驱动的开放领域表格推理框架。总体而言，OpenTab利用表格检索器获取相关表格，然后生成SQL程序以高效解析检索到的表格。利用从SQL执行中派生的中间数据，它进行基于事实的推理以产生准确的响应。广泛的实验评估表明，OpenTab在开放和封闭领域设置中显著优于基线，准确率最高提高了21.5%。我们进一步进行消融研究，以验证我们提出的系统设计的有效性。"
}
{
  "title": "Lemur: Integrating Large Language Models in Automated Program Verification",
  "title_zh": "标题：Lemur：将大型语言模型集成到自动程序验证中",
  "abstract": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.",
  "abstract_zh": "摘要：大型语言模型（LLMs）所展示的代码理解能力引发了一个问题，即它们是否可以用于自动程序验证，这是一项需要对程序属性进行高水平抽象推理的任务，而这对验证工具来说是具有挑战性的。我们提出了一种将LLMs的强大能力与自动推理器结合的通用方法论，并正式将其描述为一组推导规则，并证明其正确性。我们将该演算实例化为一种可靠的自动验证程序，这在一组合成和竞赛基准上带来了实际的改进。"
}
{
  "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression",
  "title_zh": "标题：SpQR：一种稀疏量化表示用于近无损的LLM权重压缩",
  "abstract": "Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. Quantizing models to 3-4 bits per parameter can lead to moderate to high accuracy losses, especially for smaller models (1-10B parameters), which are suitable for edge deployment. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique that enables for the first time \\emph{near-lossless} compression of LLMs across model scales while reaching similar compression levels to previous methods. SpQR works by identifying and isolating \\emph{outlier weights}, which cause particularly large quantization errors, and storing them in higher precision while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than $1\\%$ in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run a 33B parameter LLM on a single 24 GB consumer GPU without performance degradation at 15\\% speedup, thus making powerful LLMs available to consumers without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime. Specifically, we provide an efficient GPU inference algorithm for SpQR, which yields faster inference than 16-bit baselines at similar accuracy while enabling memory compression gains of more than 4x.",
  "abstract_zh": "摘要：近期大型语言模型（LLM）预训练的进展导致了高质量LLM的出现，具备令人印象深刻的能力。通过将这些LLM压缩到每个参数3-4位的量化，可以使其适应内存受限的设备，如笔记本电脑和手机，从而实现个性化使用。将模型量化到每个参数3-4位可能导致中等到高的准确性损失，尤其是对于较小的模型（1-10B参数），这些模型适合边缘部署。为了解决这一准确性问题，我们引入了稀疏量化表示（SpQR），这是一种新的压缩格式和量化技术，首次实现了LLM在不同模型规模下的近无损压缩，同时达到了与之前方法相似的压缩水平。SpQR通过识别和隔离导致特别大量化误差的“异常权重”，并以更高的精度存储这些权重，同时将所有其他权重压缩到3-4位，达成了对于高精度的LLaMA和Falcon LLM相对困惑度损失低于$1\\%$的效果。这使得在单个24GB消费级GPU上运行33B参数的LLM成为可能，且在15\\%的加速下没有性能下降，从而使强大的LLM能够无障碍地提供给消费者。SpQR配备了高效的算法，用于将权重编码为其格式，以及在运行时高效解码。具体而言，我们提供了一种高效的GPU推理算法，SpQR在相似准确性下比16位基线实现了更快的推理，同时实现了超过4倍的内存压缩收益。"
}
{
  "title": "A Simple and Effective Pruning Approach for Large Language Models",
  "title_zh": "简单有效的大型语言模型剪枝方法",
  "abstract": "As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update.",
  "abstract_zh": "随着大型语言模型（LLMs）规模的增加，它们自然成为网络剪枝方法的候选者：这些方法在努力保持性能的同时，丢弃网络权重的子集。然而，现有方法要么需要重新训练，这对于十亿规模的LLMs来说几乎无法承受，要么需要解决依赖于二阶信息的权重重构问题，这也可能计算开销巨大。本文提出了一种新颖、简单而有效的剪枝方法，称为Wanda（通过权重和激活进行剪枝），旨在引入预训练LLMs的稀疏性。受到LLMs中出现的大幅度特征的启发，我们的方法在每个输出的基础上，剪除与相应输入激活相乘后最小的权重。值得注意的是，Wanda不需要重新训练或权重更新，剪枝后的LLM可以直接使用。我们在LLaMA和LLaMA-2上对Wanda方法进行了全面评估，涵盖了各种语言基准。Wanda显著优于已建立的幅度剪枝基线，并在涉及密集权重更新的最新方法中表现出竞争力。"
}
{
  "title": "Pre-training with Synthetic Data Helps Offline Reinforcement Learning",
  "title_zh": "标题：使用合成数据进行预训练有助于离线强化学习",
  "abstract": "Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with simple synthetic data for a small number of updates can also improve CQL, providing consistent performance improvement on D4RL Gym locomotion datasets. The results of this paper not only illustrate the importance of pre-training for offline DRL but also show that the pre-training data can be synthetic and generated with remarkably simple mechanisms.",
  "abstract_zh": "摘要：最近的研究表明，对于离线深度强化学习（DRL），使用大型语言语料库对决策变换器进行预训练可以提高下游性能（Reid等，2022）。一个自然的问题是，这种性能提升是否只能通过语言预训练实现，或者是否可以通过不涉及语言的更简单的预训练方案实现。在本文中，我们首先表明，语言并不是提高性能的必要条件，实际上，使用合成独立同分布（IID）数据进行少量更新的预训练可以与使用大型语言语料库的预训练所带来的性能提升相匹配；此外，使用一步马尔可夫链生成的数据进行预训练可以进一步提高性能。受到这些实验结果的启发，我们接着考虑对保守Q学习（CQL）进行预训练，这是一种流行的离线DRL算法，基于Q学习，通常采用多层感知器（MLP）作为骨干网络。令人惊讶的是，使用简单的合成数据进行少量更新的预训练也可以改善CQL，在D4RL Gym运动数据集上提供一致的性能提升。本文的结果不仅说明了预训练对离线DRL的重要性，还表明预训练数据可以是合成的，并且可以通过非常简单的机制生成。"
}
{
  "title": "Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control",
  "title_zh": "突触：基于记忆的轨迹示例提示计算机控制",
  "abstract": "Building agents with large language models (LLMs) for computer control is a burgeoning research area, where the agent receives computer states and performs actions to complete complex tasks. Previous computer agents have demonstrated the benefits of in-context learning (ICL); however, their performance is hindered by several issues. First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks. Third, existing computer agents rely on task-specific exemplars and overlook the similarity among tasks, resulting in poor generalization to novel tasks. To address these challenges, we introduce Synapse, a computer agent featuring three key components: i) state abstraction, which filters out task-irrelevant information from raw states, allowing more exemplars within the limited context, ii) trajectory-as-exemplar prompting, which prompts the LLM with complete trajectories of the abstracted states and actions to improve multi-step decision-making, and iii) exemplar memory, which stores the embeddings of exemplars and retrieves them via similarity search for generalization to novel tasks. We evaluate Synapse on MiniWoB++, a standard task suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse achieves a 99.2% average success rate (a 10% relative improvement) across 64 tasks using demonstrations from only 48 tasks. Notably, Synapse is the first ICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a 56% relative improvement in average step success rate over the previous state-of-the-art prompting scheme in Mind2Web.",
  "abstract_zh": "构建用于计算机控制的大型语言模型（LLMs）代理是一个新兴的研究领域，其中代理接收计算机状态并执行操作以完成复杂任务。以往的计算机代理展示了上下文学习（ICL）的好处；然而，它们的表现受到多个问题的制约。首先，LLMs的有限上下文长度和复杂的计算机状态限制了示例的数量，因为单个网页可能会消耗整个上下文。其次，当前方法中的示例，如高层次计划和多选题，无法表示完整的轨迹，导致在长时间任务中的表现不佳。第三，现有的计算机代理依赖于特定任务的示例，忽视了任务之间的相似性，导致对新任务的泛化能力差。为了解决这些挑战，我们引入了Synapse，一个具有三个关键组件的计算机代理：i）状态抽象，过滤掉原始状态中与任务无关的信息，在有限上下文中允许更多示例；ii）轨迹作为示例提示，使用抽象状态和动作的完整轨迹提示LLM，以改善多步骤决策；iii）示例记忆，存储示例的嵌入并通过相似性搜索检索它们，以便对新任务进行泛化。我们在标准任务套件MiniWoB++和真实世界网站基准Mind2Web上评估了Synapse。在MiniWoB++中，Synapse在仅使用48个任务的演示下，在64个任务中实现了99.2%的平均成功率（相对提高10%）。值得注意的是，Synapse是第一个在MiniWoB++中解决订票任务的ICL方法。Synapse在Mind2Web中相较于之前的最先进提示方案，平均步骤成功率也提高了56%。"
}
{
  "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity",
  "title_zh": "理解RLHF对LLM泛化能力和多样性的影响",
  "abstract": "Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the tradeoff between generalisation and diversity.",
  "abstract_zh": "通过对每个阶段（即监督微调（SFT）、奖励建模和RLHF）如何影响两个关键属性（即分布外（OOD）泛化和输出多样性）的广泛分析，我们发现RLHF在新输入上比SFT具有更好的泛化能力，但在多样性方面显著降低，表明当前LLM微调方法在泛化和多样性之间存在权衡。"
}
{
  "title": "TD-MPC2: Scalable, Robust World Models for Continuous Control",
  "title_zh": "TD-MPC2：可扩展、稳健的连续控制世界模型",
  "abstract": "TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents.\n\nExplore videos, models, data, code, and more at https://tdmpc2.com",
  "abstract_zh": "TD-MPC是一种基于模型的强化学习（RL）算法，它在学习的隐式（无解码器）世界模型的潜在空间中执行局部轨迹优化。在这项工作中，我们提出了TD-MPC2：对TD-MPC算法的一系列改进。我们展示了TD-MPC2在跨越4个不同任务领域的104个在线RL任务中显著优于基线，使用一组超参数始终取得强劲的结果。我们进一步表明，随着模型和数据规模的增加，智能体的能力也在提升，并成功训练了一个317M参数的智能体在多个任务领域、表现形式和动作空间中执行80个任务。最后，我们总结了与大型TD-MPC2智能体相关的经验教训、机会和风险。"
}
{
  "title": "Unlocking the Power of Representations in Long-term Novelty-based Exploration",
  "title_zh": "解锁长期新颖性探索中表示的力量",
  "abstract": "We introduce Robust Exploration via Clustering-based Online Density Estimation (RECODE), a non-parametric method for novelty-based exploration that estimates visitation counts for clusters of states based on their similarity in a chosen embedding space. By adapting classical clustering to the nonstationary setting of Deep RL, RECODE can efficiently track state visitation counts over thousands of episodes. We further propose a novel generalization of the inverse dynamics loss, which leverages masked transformer architectures for multi-step prediction; which in conjunction with \\DETOCS achieves a new state-of-the-art in a suite of challenging 3D-exploration tasks in DM-Hard-8. RECODE also sets new state-of-the-art in hard exploration Atari games, and is the first agent to reach the end screen in \"Pitfall!\"",
  "abstract_zh": "我们介绍了一种基于聚类的在线密度估计的鲁棒探索方法（RECODE），这是一种非参数的新颖性探索方法，基于选定嵌入空间中的相似性估计状态簇的访问计数。通过将经典聚类适应于深度强化学习的非平稳环境，RECODE能够高效地跟踪数千个回合中的状态访问计数。我们进一步提出了一种逆动态损失的新颖泛化，利用掩蔽变换器架构进行多步预测；与\\DETOCS结合，在DM-Hard-8的一系列具有挑战性的3D探索任务中实现了新的最先进水平。RECODE还在困难探索的Atari游戏中设定了新的最先进水平，并且是第一个在“Pitfall!”中到达结束画面的智能体。"
}
{
  "title": "Amortizing intractable inference in large language models",
  "title_zh": "标题：在大型语言模型中摊销难以处理的推理",
  "abstract": "Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest---including sequence continuation, infilling, and other forms of constrained generation---involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate that our approach enables data-efficient adaptation of LLMs to tasks that require multi-step rationalization and tool use.",
  "abstract_zh": "摘要：自回归大型语言模型（LLMs）通过下一个标记的条件分布压缩其训练数据中的知识。这限制了对这些知识的可处理查询，仅限于从头到尾的自回归采样。然而，许多感兴趣的任务——包括序列延续、填充和其他形式的约束生成——涉及从难以处理的后验分布中采样。我们通过使用摊销贝叶斯推理来解决这一限制，以从这些难以处理的后验中采样。这种摊销通过多样性寻求的强化学习算法（生成流网络 GFlowNets）对 LLM 进行微调来算法上实现。我们实证证明，这种分布匹配的 LLM 微调范式可以作为最大似然训练和奖励最大化策略优化的有效替代方案。作为一个重要应用，我们将思维链推理解释为潜变量建模问题，并证明我们的方法能够有效地将 LLM 适应于需要多步骤推理和工具使用的任务。"
}
{
  "title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models",
  "title_zh": "DiLu：一种基于知识驱动的大语言模型自主驾驶方法",
  "abstract": "Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. \nDrawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. \nLeveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. \nExtensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods.\nMoreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems.\nTo the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain.\n\nProject page: https://pjlab-adg.github.io/DiLu/",
  "abstract_zh": "近年来，自主驾驶的进展依赖于数据驱动的方法，这些方法被广泛采用，但面临数据集偏差、过拟合和不可解释性等挑战。我们从人类驾驶的知识驱动特性中获得灵感，探讨如何将类似能力植入自主驾驶系统，并总结出一个整合互动环境、驾驶代理和记忆组件的范式来解决这个问题。借助具有新兴能力的大语言模型（LLMs），我们提出了DiLu框架，该框架结合了推理和反思模块，使系统能够基于常识知识进行决策并持续演变。大量实验证明了DiLu积累经验的能力，并在泛化能力上显著优于基于强化学习的方法。此外，DiLu能够直接从现实世界数据集中获取经验，突显其在实际自主驾驶系统中部署的潜力。据我们所知，我们是首个在自主车辆决策中利用知识驱动能力的研究。通过所提出的DiLu框架，LLM被增强以在自主驾驶领域应用知识并进行因果推理。"
}
{
  "title": "ALAM: Averaged Low-Precision Activation for Memory-Efficient Training of Transformer Models",
  "title_zh": "标题：ALAM：用于高效训练变换器模型的平均低精度激活",
  "abstract": "One of the key challenges in deep neural network training is the substantial amount of GPU memory required to store activations obtained in the forward pass. Various Activation-Compressed Training (ACT) schemes have been proposed to mitigate this issue; however, it is challenging to adopt those approaches in recent transformer-based large language models (LLMs), which experience significant performance drops when the activations are deeply compressed during training. In this paper, we introduce ALAM, a novel ACT framework that utilizes average quantization and a lightweight sensitivity calculation scheme, enabling large memory saving in LLMs while maintaining training performance. We first demonstrate that compressing activations into their group average values minimizes the gradient variance. Employing this property, we propose Average Quantization which provides high-quality deeply compressed activations with an effective precision of less than 1 bit and improved flexibility of precision allocation. In addition, we present a cost-effective yet accurate sensitivity calculation algorithm that solely relies on the L2 norm of parameter gradients, substantially reducing memory overhead due to sensitivity calculation. In experiments, the ALAM framework significantly reduces activation memory without compromising accuracy, achieving up to a 10$\\times$ compression rate in LLMs.",
  "abstract_zh": "摘要：深度神经网络训练中的一个关键挑战是存储前向传播中获得的激活所需的GPU内存量。虽然已经提出了多种激活压缩训练（ACT）方案来缓解这一问题，但在最近的基于变换器的大型语言模型（LLMs）中采用这些方法面临挑战，因为在训练过程中深度压缩激活会导致显著的性能下降。本文介绍了ALAM，这是一种新颖的ACT框架，利用平均量化和轻量级敏感性计算方案，实现了LLMs中的大幅内存节省，同时保持训练性能。我们首先证明，将激活压缩为其组平均值可以最小化梯度方差。利用这一特性，我们提出了平均量化，提供高质量的深度压缩激活，其有效精度低于1位，并提高了精度分配的灵活性。此外，我们提出了一种经济高效但准确的敏感性计算算法，仅依赖于参数梯度的L2范数，显著减少了由于敏感性计算带来的内存开销。在实验中，ALAM框架显著减少了激活内存而不影响准确性，在LLMs中实现了高达10$\\times$的压缩率。"
}
{
  "title": "Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing",
  "title_zh": "标题：Evoke：通过审稿人-作者提示编辑激发大型语言模型中的批判性思维能力",
  "abstract": "Large language models (LLMs) have made impressive progress in natural language processing. These models rely on proper human instructions (or prompts) to generate suitable responses. However, the potential of LLMs are not fully harnessed by commonly-used prompting methods: many human-in-the-loop algorithms employ ad-hoc procedures for prompt selection; while auto prompt generation approaches are essentially searching all possible prompts randomly and inefficiently. We propose Evoke, an automatic prompt refinement framework. In Evoke, there are two instances of a same LLM: one as a reviewer (LLM-Reviewer), it scores the current prompt; the other as an author (LLM-Author), it edits the prompt by considering the edit history and the reviewer's feedback. Such an author-reviewer feedback loop ensures that the prompt is refined in each iteration. We further aggregate a data selection approach to Evoke, where only the hard samples are exposed to the LLM. The hard samples are more important because the LLM can develop deeper understanding of the tasks out of them, while the model may already know how to solve the easier cases. Experimental results show that Evoke significantly outperforms existing methods. For instance, in the challenging task of logical fallacy detection, Evoke scores above 80, while all other baseline methods struggle to reach 20.",
  "abstract_zh": "摘要：大型语言模型（LLMs）在自然语言处理方面取得了显著进展。这些模型依赖于适当的人类指令（或提示）来生成合适的响应。然而，常用的提示方法并未充分发挥LLMs的潜力：许多人机交互算法采用临时程序进行提示选择，而自动提示生成方法本质上是随机且低效地搜索所有可能的提示。我们提出了Evoke，一个自动提示优化框架。在Evoke中，有两个相同的LLM实例：一个作为审稿人（LLM-Reviewer），对当前提示进行评分；另一个作为作者（LLM-Author），通过考虑编辑历史和审稿人的反馈来编辑提示。这种作者-审稿人反馈循环确保每次迭代中提示得到优化。我们进一步将数据选择方法整合到Evoke中，仅将困难样本暴露给LLM。困难样本更为重要，因为LLM可以从中深入理解任务，而模型可能已经知道如何解决较简单的案例。实验结果表明，Evoke显著优于现有方法。例如，在逻辑谬误检测这一具有挑战性的任务中，Evoke的得分超过80，而所有其他基线方法的得分都难以达到20。"
}
{
  "title": "Large Language Models to Enhance Bayesian Optimization",
  "title_zh": "大型语言模型增强贝叶斯优化",
  "abstract": "Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance remains a delicate process. In this light, we present \\texttt{LLAMBO}, a novel approach that integrates the capabilities of Large Language Models (LLM) within BO. At a high level, we frame the BO problem in natural language, enabling LLMs to iteratively \\emph{propose} and \\emph{evaluate} promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can improve model-based BO. Our findings illustrate that \\texttt{LLAMBO} is effective at zero-shot warmstarting, and enhances surrogate modeling and candidate sampling, especially in the early stages of search when observations are sparse. Our approach is performed in context and does not require LLM finetuning. Additionally, it is modular by design, allowing individual components to be integrated into existing BO frameworks, or function cohesively as an end-to-end method. We empirically validate \\texttt{LLAMBO}'s efficacy on the problem of hyperparameter tuning, highlighting strong empirical performance across a range of diverse benchmarks, proprietary, and synthetic tasks.",
  "abstract_zh": "贝叶斯优化（BO）是一种强大的方法，用于优化复杂且评估成本高昂的黑箱函数。它在许多应用中都至关重要，尤其是在超参数调优中，但其有效性依赖于有效平衡探索与开发。尽管BO方法取得了显著进展，但实现这一平衡仍然是一个微妙的过程。在此背景下，我们提出了\\texttt{LLAMBO}，一种将大型语言模型（LLM）能力与BO相结合的新方法。从高层次来看，我们将BO问题框定为自然语言，使LLM能够迭代地\\emph{提出}和\\emph{评估}基于历史评估的有前景的解决方案。更具体地说，我们探讨了如何结合LLM的上下文理解、少量学习能力和领域知识来改善基于模型的BO。我们的研究结果表明，\\texttt{LLAMBO}在零-shot热启动方面有效，并增强了代理建模和候选采样，特别是在观察稀疏的搜索早期阶段。我们的方法是在上下文中执行的，不需要对LLM进行微调。此外，它的设计是模块化的，允许将各个组件集成到现有的BO框架中，或作为端到端方法协同工作。我们在超参数调优问题上实证验证了\\texttt{LLAMBO}的有效性，强调了在多种不同基准、专有和合成任务中表现出的强大实证性能。"
}
{
  "title": "GenSim: Generating Robotic Simulation Tasks via Large Language Models",
  "title_zh": "GenSim：通过大型语言模型生成机器人仿真任务",
  "abstract": "Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scene-level diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models' (LLM) grounding and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM  bootstraps from previous tasks and iteratively proposes novel tasks that would be helpful in solving more complex tasks. We use GPT4 to expand the existing benchmark by ten times to over 100 tasks, on which we conduct supervised finetuning and evaluate several LLMs including finetuned GPTs and Code Llama on code generation for robotic simulation tasks. Furthermore, we observe that LLMs-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training. We further find that with minimal sim-to-real adaptation, the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25%. See our project website (https://gen-sim.github.io) and demo (https://huggingface.co/spaces/Gen-Sim/Gen-Sim) for visualizations and open-source models and datasets.",
  "abstract_zh": "收集大量真实世界交互数据以训练通用机器人策略通常成本高昂，因此促使使用仿真数据。然而，现有的数据生成方法通常侧重于场景级别的多样性（例如，物体实例和姿势），而不是任务级别的多样性，因为需要人力来提出和验证新任务。这使得在仿真数据上训练的策略在任务级别上表现出显著的泛化能力变得具有挑战性。本文提出通过利用大型语言模型（LLM）的基础和编码能力，自动生成丰富的仿真环境和专家演示。我们的方法称为GenSim，具有两种模式：目标导向生成，其中向LLM提供目标任务，LLM提出解决目标任务的任务课程；探索性生成，其中LLM从先前的任务中自举，并迭代提出有助于解决更复杂任务的新任务。我们使用GPT4将现有基准扩展十倍，超过100个任务，并对多个LLM进行监督微调，包括微调的GPT和Code Llama，以进行机器人仿真任务的代码生成。此外，我们观察到，LLM生成的仿真程序在用于多任务策略训练时可以显著增强任务级别的泛化。我们进一步发现，经过最小的仿真到现实适应，预训练于GPT4生成的仿真任务的多任务策略在现实世界中对未见的长时间任务表现出更强的迁移能力，并比基线提高25%。请访问我们的项目网站（https://gen-sim.github.io）和演示（https://huggingface.co/spaces/Gen-Sim/Gen-Sim）以获取可视化和开源模型及数据集。"
}
{
  "title": "Toward effective protection against diffusion-based mimicry through score distillation",
  "title_zh": "标题：通过得分蒸馏实现对基于扩散的仿冒的有效保护",
  "abstract": "While generative diffusion models excel in producing high-quality images, they can also be misused to mimic authorized images, posing a significant threat to AI systems. Efforts have been made to add calibrated perturbations to protect images from diffusion-based mimicry pipelines. However, most of the existing methods are too ineffective and even impractical to be used by individual users due to their high computation and memory requirements. In this work, we present novel findings on attacking latent diffusion models (LDM) and propose new plug-and-play strategies for more effective protection. In particular, we explore the bottleneck in attacking an LDM, discovering that the encoder module rather than the denoiser module is the vulnerable point. Based on this insight, we present our strategy using Score Distillation Sampling (SDS) to double the speed of protection and reduce memory occupation by half without compromising its strength. Additionally, we provide a robust protection strategy by counterintuitively minimizing the semantic loss, which can assist in generating more natural perturbations. Finally, we conduct extensive experiments to substantiate our findings and comprehensively evaluate our newly proposed strategies. We hope our insights and protective measures can contribute to better defense against malicious diffusion-based mimicry, advancing the development of secure AI systems.",
  "abstract_zh": "摘要：尽管生成扩散模型在生成高质量图像方面表现出色，但它们也可能被滥用以仿冒授权图像，给人工智能系统带来重大威胁。已有努力通过添加校准扰动来保护图像免受基于扩散的仿冒管道的影响。然而，由于现有方法的高计算和内存要求，大多数方法对于个人用户来说过于低效甚至不切实际。在本研究中，我们提出了对潜在扩散模型（LDM）攻击的新发现，并提出了新的即插即用策略以实现更有效的保护。特别是，我们探讨了攻击LDM的瓶颈，发现编码器模块而非去噪模块是脆弱点。基于这一见解，我们提出了使用得分蒸馏采样（SDS）策略来加倍保护速度并将内存占用减少一半，而不妥协其强度。此外，我们通过反直觉地最小化语义损失提供了一种稳健的保护策略，这可以帮助生成更自然的扰动。最后，我们进行了广泛的实验以证实我们的发现，并全面评估我们新提出的策略。我们希望我们的见解和保护措施能够为更好地防御恶意基于扩散的仿冒做出贡献，推动安全人工智能系统的发展。"
}
{
  "title": "Exposing Text-Image Inconsistency Using Diffusion Models",
  "title_zh": "揭示文本-图像不一致性的方法：基于扩散模型的研究",
  "abstract": "In the battle against widespread online misinformation, a growing problem is text-image inconsistency, where images are misleadingly paired with texts with different intent or meaning. Existing classification-based methods for text-image inconsistency can identify contextual inconsistencies but fail to provide explainable justifications for their decisions that humans can understand. Although more nuanced, human evaluation is impractical at scale and susceptible to errors. To address these limitations, this study introduces D-TIIL (Diffusion-based Text-Image Inconsistency Localization), which employs text-to-image diffusion models to localize semantic inconsistencies in text and image pairs. These models, trained on large-scale datasets act as ``omniscient\" agents that filter out irrelevant information and incorporate background knowledge to identify inconsistencies. In addition, D-TIIL uses text embeddings and modified image regions to visualize these inconsistencies. To evaluate D-TIIL's efficacy, we introduce a new TIIL dataset containing 14K consistent and inconsistent text-image pairs. Unlike existing datasets, TIIL enables assessment at the level of individual words and image regions and is carefully designed to represent various inconsistencies. D-TIIL offers a scalable and evidence-based approach to identifying and localizing text-image inconsistency, providing a robust framework for future research combating misinformation.",
  "abstract_zh": "在对抗广泛的在线虚假信息的斗争中，文本-图像不一致性问题日益严重，即图像与意图或意义不同的文本误导性配对。现有的基于分类的方法能够识别上下文不一致性，但无法提供人类能够理解的可解释性理由。尽管人类评估更为细致，但在规模上不切实际且容易出错。为了解决这些局限性，本研究引入了D-TIIL（基于扩散的文本-图像不一致性定位），该方法利用文本到图像的扩散模型来定位文本和图像对中的语义不一致性。这些在大规模数据集上训练的模型充当“全知”的代理，过滤掉无关信息并结合背景知识来识别不一致性。此外，D-TIIL使用文本嵌入和修改后的图像区域来可视化这些不一致性。为了评估D-TIIL的有效性，我们引入了一个新的TIIL数据集，包含14K个一致和不一致的文本-图像对。与现有数据集不同，TIIL能够在单个单词和图像区域的层面进行评估，并经过精心设计以代表各种不一致性。D-TIIL提供了一种可扩展的、基于证据的方法来识别和定位文本-图像不一致性，为未来打击虚假信息的研究提供了一个强大的框架。"
}
{
  "title": "InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists",
  "title_zh": "标题：InstructCV：作为视觉通用者的指令调优文本到图像扩散模型",
  "abstract": "Recent advances in generative diffusion models have enabled text-controlled synthesis of realistic and diverse images with impressive quality. Despite these remarkable advances, the application of text-to-image generative models in computer vision for standard visual recognition tasks remains limited. The current de facto approach for these tasks is to design model architectures and loss functions that are tailored to the task at hand. In this paper, we develop a unified language interface for computer vision tasks that abstracts away task specific design choices and enables task execution by following natural language instructions. Our approach involves casting multiple computer vision tasks as text-to-image generation problems. Here, the text represents an instruction describing the task, and the resulting image is a visually-encoded task output. To train our model, we pool commonly-used computer vision datasets covering a range of tasks, including segmentation, object detection, depth estimation, and classification. We then use a large language model to paraphrase prompt templates that convey the specific tasks to be conducted on each image, and through this process, we create a multi-modal and multi-task training dataset comprising input and output images along with annotated instructions. Following the InstructPix2Pix architecture, we apply instruction-tuning to a text-to-image diffusion model using our constructed dataset, steering its functionality from a generative model to an instruction-guided multi-task vision learner. Experiments demonstrate that our model, dubbed InstructCV, performs competitively compared to other generalist and task-specific vision models. Moreover, it exhibits compelling generalization capabilities to unseen data, categories, and user instructions.",
  "abstract_zh": "摘要：最近在生成扩散模型方面的进展使得能够以文本控制合成出高质量的真实和多样化图像。尽管取得了显著的进展，文本到图像生成模型在计算机视觉标准视觉识别任务中的应用仍然有限。当前这些任务的实际方法是设计针对具体任务的模型架构和损失函数。在本文中，我们开发了一个统一的计算机视觉任务语言接口，抽象掉任务特定的设计选择，并通过遵循自然语言指令来实现任务执行。我们的方法将多个计算机视觉任务视为文本到图像生成问题。在这里，文本表示描述任务的指令，生成的图像是视觉编码的任务输出。为了训练我们的模型，我们汇集了涵盖多个任务的常用计算机视觉数据集，包括分割、目标检测、深度估计和分类。然后，我们使用大型语言模型对传达每个图像上要执行的特定任务的提示模板进行改写，通过这个过程，我们创建了一个多模态和多任务的训练数据集，包含输入和输出图像以及注释指令。遵循InstructPix2Pix架构，我们对构建的数据集应用指令调优，将文本到图像扩散模型的功能从生成模型引导为指令指导的多任务视觉学习者。实验表明，我们的模型InstructCV与其他通用和任务特定的视觉模型相比表现出竞争力。此外，它在未见数据、类别和用户指令上展现出令人信服的泛化能力。"
}
{
  "title": "PromptTTS 2: Describing and Generating Voices with Text Prompt",
  "title_zh": "PromptTTS 2：使用文本提示描述和生成声音",
  "abstract": "Speech conveys more information than text, as the same word can be uttered in various voices to convey diverse information. Compared to traditional text-to-speech (TTS) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more user-friendly since speech prompts can be hard to find or may not exist at all. TTS approaches based on the text prompt face two main challenges: 1) the one-to-many problem, where not all details about voice variability can be described in the text prompt, and 2) the limited availability of text prompt datasets, where vendors and large cost of data labeling are required to write text prompts for speech. In this work, we introduce PromptTTS 2 to address these challenges with a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models (LLM) to compose high quality text prompts. Specifically, the variation network predicts the representation extracted from the reference speech (which contains full information about voice variability) based on the text prompt representation. For the prompt generation pipeline, it generates text prompts for speech with a speech language understanding model to recognize voice attributes (e.g., gender, speed) from speech and a large language model to formulate text prompts based on the recognition results. Experiments on a large-scale (44K hours) speech dataset demonstrate that compared to the previous works, PromptTTS 2 generates voices more consistent with text prompts and supports the sampling of diverse voice variability, thereby offering users more choices on voice generation. Additionally, the prompt generation pipeline produces high-quality text prompts, eliminating the large labeling cost. The demo page of PromptTTS 2 is available (https://speechresearch.github.io/prompttts2).",
  "abstract_zh": "语音传达的信息比文本更多，因为同一个词可以用不同的声音表达以传达多样的信息。与依赖语音提示（参考语音）来实现声音变异的传统文本到语音（TTS）方法相比，使用文本提示（描述）更为用户友好，因为语音提示可能难以找到或根本不存在。基于文本提示的TTS方法面临两个主要挑战：1）一对多问题，文本提示无法描述声音变异的所有细节，2）文本提示数据集的可用性有限，需要供应商和高昂的数据标注成本来为语音撰写文本提示。在本研究中，我们引入PromptTTS 2以解决这些挑战，采用变异网络提供文本提示未捕获的声音变异信息，以及一个提示生成管道利用大型语言模型（LLM）生成高质量文本提示。具体而言，变异网络基于文本提示表示预测从参考语音中提取的表示（该表示包含有关声音变异的完整信息）。对于提示生成管道，它使用语音语言理解模型从语音中识别声音属性（例如性别、速度）并利用大型语言模型根据识别结果制定文本提示。对大规模（44K小时）语音数据集的实验表明，与之前的工作相比，PromptTTS 2生成的声音与文本提示更一致，并支持多样声音变异的采样，从而为用户提供更多的声音生成选择。此外，提示生成管道生成高质量文本提示，消除了高昂的标注成本。PromptTTS 2的演示页面可用（https://speechresearch.github.io/prompttts2）。"
}
{
  "title": "Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds",
  "title_zh": "史蒂夫眼：为基于大型语言模型的具身智能体在开放世界中提供视觉感知",
  "abstract": "Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to ``a blindfolded text-based game.'' Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks and carry out experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. The project’s website and code can be found at https://sites.google.com/view/steve-eye.",
  "abstract_zh": "最近的研究提供了有力证据，表明大型语言模型（LLMs）能够赋予具身智能体自我驱动的与世界互动能力，这标志着通用机器人技术的初步进展。然而，这些努力往往忽视了开放世界的视觉丰富性，使整个互动过程类似于“蒙眼的文本游戏”。因此，基于LLM的智能体在直观理解周围环境和生成易于理解的响应时常常面临挑战。本文提出了史蒂夫眼，一个端到端训练的大型多模态模型，以解决这一局限。史蒂夫眼将LLM与视觉编码器整合，以处理视觉-文本输入并生成多模态反馈。我们采用半自动策略收集了包含85万对开放世界指令的广泛数据集，使我们的模型能够涵盖智能体的三个基本功能：多模态感知、基础知识库以及技能预测与规划。最后，我们开发了三个开放世界评估基准，并从多个角度进行实验，以验证我们模型的战略行动和规划能力。项目的网站和代码可以在 https://sites.google.com/view/steve-eye 找到。"
}
{
  "title": "Training Socially Aligned Language Models on Simulated Social Interactions",
  "title_zh": "标题：在模拟社交互动中训练社会对齐语言模型",
  "abstract": "The goal of social alignment for AI systems is to make sure these models can conduct themselves appropriately following social values. Unlike humans who establish a consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly recite the corpus in social isolation, which causes poor generalization in unfamiliar cases and the lack of robustness under adversarial attacks. In this work, we introduce a new training paradigm that enables LMs to learn from simulated social interactions. Compared with existing methods, our method is much more scalable and efficient, and shows superior performance in alignment benchmarks and human evaluation.",
  "abstract_zh": "摘要：AI系统的社会对齐目标是确保这些模型能够根据社会价值观适当地行事。与通过社交互动建立价值判断共识的人类不同，当前的语言模型（LMs）是在社交隔离中僵化地背诵语料库，这导致在不熟悉的情况下泛化能力差，并且在对抗攻击下缺乏鲁棒性。在这项工作中，我们引入了一种新的训练范式，使LMs能够从模拟社交互动中学习。与现有方法相比，我们的方法更具可扩展性和效率，并在对齐基准和人类评估中表现出优越的性能。"
}
{
  "title": "Improving LoRA in Privacy-preserving Federated Learning",
  "title_zh": "标题：改进隐私保护的联邦学习中的LoRA",
  "abstract": "Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency.\nLoRA injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module.\nHowever, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters.\nA key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server.\nThus, this paper proposes an efficient and effective version of LoRA, Federated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further halve the communication cost of federated fine-tuning LLMs.\nThe core idea of FFA-LoRA is to fix the randomly initialized non-zero matrices and only fine-tune the zero-initialized matrices.\nCompared to LoRA, FFA-LoRA is motivated by practical and theoretical benefits in privacy-preserved FL. \nOur experiments demonstrate that FFA-LoRA provides more consistent performance with better computational efficiency over vanilla LoRA in various FL tasks.",
  "abstract_zh": "摘要：低秩适应（LoRA）是针对预训练语言模型的一种最受欢迎的任务特定参数高效微调（PEFT）方法，因其良好的性能和计算效率而受到青睐。LoRA在每个冻结的预训练模型模块上注入两个可训练的秩分解矩阵的乘积。然而，在隐私保护的联邦学习（FL）环境中应用时，LoRA可能会变得不稳定，原因包括：1）数据异质性和多步本地更新的影响不可忽视，2）为保证差分隐私（DP）而施加的更新梯度的附加噪声可能被放大，3）最终性能对超参数敏感。导致这些现象的一个关键因素是本地客户端共同优化两个低秩矩阵与中央服务器分别聚合它们之间的不一致。因此，本文提出了一种高效且有效的LoRA版本，联邦冻结A LoRA（FFA-LoRA），以缓解这些挑战，并进一步将联邦微调大型语言模型的通信成本减半。FFA-LoRA的核心思想是固定随机初始化的非零矩阵，仅微调零初始化的矩阵。与LoRA相比，FFA-LoRA在隐私保护的FL中具有实际和理论上的优势。我们的实验表明，FFA-LoRA在各种FL任务中提供了更一致的性能和更好的计算效率。"
}
{
  "title": "Efficient Streaming Language Models with Attention Sinks",
  "title_zh": "高效的带有注意力汇的流式语言模型",
  "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges.\nFirstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory.\nSecondly, popular LLMs cannot generalize to longer texts than the training sequence length.\nWindow attention, where only the most recent KVs are cached, is a natural approach --- but we show that it fails when the text length surpasses the cache size.\nWe observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important.\nBased on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning.\nWe show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\nIn addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2$\\times$ speedup.\nCode and datasets are provided in the anonymous link.",
  "abstract_zh": "在流式应用中部署大型语言模型（LLMs），如多轮对话，虽然迫切需要，但面临两个主要挑战。首先，在解码阶段，缓存先前标记的键值状态（KV）消耗大量内存。其次，流行的LLM无法推广到比训练序列长度更长的文本。窗口注意力只缓存最近的KV是一种自然的方法——但我们表明，当文本长度超过缓存大小时，它会失败。我们观察到一个有趣的现象，即注意力汇，保持初始标记的KV将大大恢复窗口注意力的性能。本文首先证明注意力汇的出现是由于对初始标记的强注意力分数，即使它们在语义上并不重要。基于上述分析，我们引入了StreamingLLM，一个高效的框架，使得经过有限长度注意力窗口训练的LLM能够在不进行任何微调的情况下推广到无限序列长度。我们展示了StreamingLLM可以使Llama-2、MPT、Falcon和Pythia在高达400万标记及以上的情况下进行稳定和高效的语言建模。此外，我们发现，在预训练期间添加一个占位符标记作为专用的注意力汇可以进一步改善流式部署。在流式设置中，StreamingLLM的性能比滑动窗口重新计算基线快多达22.2倍。代码和数据集在匿名链接中提供。"
}
{
  "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
  "title_zh": "元数学：为大型语言模型自助生成数学问题",
  "abstract": "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problems due to the complex reasoning procedures. To bridge this gap, we propose \\emph{MetaMath}, a finetuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives, which results in a new dataset called MetaMathQA. Then we finetune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (\\ie, GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin.  Our MetaMath-7B model achieves $66.5\\%$ on GSM8K and $19.8\\%$ on MATH, exceeding the state-of-the-art models of the same size by $11.5\\%$ and $8.7\\%$. Particularly, MetaMath-70B achieves an accuracy of $82.3\\%$ on GSM8K, slightly better than GPT-3.5-Turbo. We release the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.",
  "abstract_zh": "大型语言模型（LLMs）推动了自然语言理解的极限，并展现了出色的问题解决能力。尽管取得了巨大成功，但大多数现有的开源LLM（例如，LLaMA-2）在解决数学问题时仍然远未令人满意，因为涉及复杂的推理过程。为了解决这一问题，我们提出了\\emph{MetaMath}，一个专注于数学推理的微调语言模型。具体而言，我们通过从多个角度重写问题来引导数学问题的生成，从而形成一个名为MetaMathQA的新数据集。然后，我们在MetaMathQA上对LLaMA-2模型进行微调。在两个流行的数学推理基准（即GSM8K和MATH）上的实验结果表明，MetaMath在一系列开源LLM中表现优异，显著超出。我们的MetaMath-7B模型在GSM8K上达到$66.5\\%$，在MATH上达到$19.8\\%$，分别比同等规模的最先进模型高出$11.5\\%$和$8.7\\%$。特别地，MetaMath-70B在GSM8K上的准确率达到$82.3\\%$，略高于GPT-3.5-Turbo。我们发布了MetaMathQA数据集、不同模型规模的MetaMath模型以及训练代码供公众使用。"
}
{
  "title": "Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL",
  "title_zh": "查询依赖的提示评估与优化：基于离线逆强化学习",
  "abstract": "In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization. We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges that impede the successful and economical design of prompt optimization techniques. One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable. Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive.\nTo address this, we introduce Prompt-OIRL, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data. Such data exists as by-products when diverse prompts are benchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent prompt optimization objective is achieved by first learning an offline reward model. This model can evaluate any query-prompt pairs without accessing LLMs. Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt. Our experimental evaluations across various LLM scales and arithmetic reasoning datasets underscore both the efficacy and economic viability of the proposed approach.",
  "abstract_zh": "本研究旨在通过零-shot提示优化提升大型语言模型（LLMs）的算术推理能力。我们识别出在此类优化中一个被忽视的目标——查询依赖性，并阐明了两个随之而来的挑战，这些挑战阻碍了提示优化技术的成功和经济设计。一个主要问题是在推理过程中缺乏有效的方法来评估提示，而当黄金答案不可用时，这一问题尤为突出。同时，通过与LLMs的交互学习以导航广泛的自然语言提示空间也证明是资源密集型的。为了解决这个问题，我们引入了Prompt-OIRL，它利用离线逆强化学习从离线提示演示数据中获取见解。这些数据是在开放可访问数据集上对多样提示进行基准测试时作为副产品存在的。通过Prompt-OIRL，查询依赖的提示优化目标通过首先学习一个离线奖励模型来实现。该模型可以在不访问LLMs的情况下评估任何查询-提示对。随后，采用最佳-N策略推荐最佳提示。我们在不同LLM规模和算术推理数据集上的实验评估强调了所提出方法的有效性和经济可行性。"
}
{
  "title": "Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning",
  "title_zh": "视觉-语言模型是强化学习的零-shot奖励模型",
  "abstract": "Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide _a single sentence text prompt_ describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second \"baseline\" prompt and projecting out parts of the CLIP embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for VLM-RMs: larger VLMs trained with more compute and data are better reward models. The failure modes of VLM-RMs we encountered are all related to known capability limitations of current VLMs, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the VLM. We find that VLM-RMs are remarkably robust as long as the VLM is large enough. This suggests that future VLMs will become more and more useful reward models for a wide range of RL applications.",
  "abstract_zh": "强化学习（RL）需要手动指定奖励函数，这通常是不可行的，或者从大量人类反馈中学习奖励模型，这通常非常昂贵。我们研究了一种更具样本效率的替代方案：使用预训练的视觉-语言模型（VLMs）作为零-shot奖励模型（RMs）通过自然语言指定任务。我们提出了一种自然且通用的方法，将VLMs用作奖励模型，称为VLM-RMs。我们基于CLIP的VLM-RMs训练MuJoCo人形机器人学习复杂任务，而无需手动指定奖励函数，例如跪下、劈叉和坐莲花式。对于每个任务，我们仅提供描述所需任务的单句文本提示，几乎不需要提示工程。我们提供了训练代理的视频，网址为：https://sites.google.com/view/vlm-rm。通过提供第二个“基线”提示并投影出与区分目标和基线无关的CLIP嵌入空间部分，我们可以提高性能。此外，我们发现VLM-RMs具有强大的扩展效应：使用更多计算和数据训练的大型VLMs是更好的奖励模型。我们遇到的VLM-RMs的失败模式都与当前VLM的已知能力限制有关，例如有限的空间推理能力或与VLM分布相差甚远的视觉不现实环境。我们发现，只要VLM足够大，VLM-RMs就表现出显著的鲁棒性。这表明，未来的VLM将越来越成为广泛RL应用的有用奖励模型。"
}
{
  "title": "The Trickle-down Impact of Reward Inconsistency on RLHF",
  "title_zh": "奖励不一致性对人类反馈强化学习的涓滴效应",
  "abstract": "Standard practice within Reinforcement Learning from Human Feedback (RLHF) involves optimizing against a Reward Model (RM), which itself is trained to reflect human preferences for desirable generations. A notable subject that is understudied is the (in-)consistency of RMs --- whether they can recognize the semantic changes to different prompts and \nappropriately adapt their reward assignments\n\n--- and their impact on the downstream RLHF model.\n\nIn this paper, we visit a series of research questions relevant to RM inconsistency:\n(1) How can we measure the consistency of reward models? \n(2) How consistent are the existing RMs and how can we improve them? \n(3) In what ways does reward inconsistency influence the chatbots resulting from the RLHF model training?\n\n\nWe propose **Contrast Instruction** -- a benchmarking strategy for the consistency of RM.  \nEach example in **Contrast Instruction** features a pair of lexically similar instructions with different ground truth responses. A consistent RM is expected to rank the corresponding instruction and response higher than other combinations. We observe that current RMs trained with the standard ranking objective fail miserably on \\contrast{} compared to average humans. To show that RM consistency can be improved efficiently without using extra training budget, we propose two techniques **ConvexDA** and **RewardFusion**, which enhance reward consistency \nthrough extrapolation during the RM training and inference stage, respectively.\nWe show that RLHF models trained with a more consistent RM yield more useful responses, suggesting that reward inconsistency exhibits a trickle-down effect on the downstream RLHF process.",
  "abstract_zh": "本文探讨了奖励模型（RM）不一致性相关的一系列研究问题，包括如何衡量奖励模型的一致性、现有RM的一致性程度及其改进方法，以及奖励不一致性如何影响由RLHF模型训练生成的聊天机器人。我们提出了**对比指令**作为RM一致性的基准策略，并展示了通过**ConvexDA**和**RewardFusion**两种技术在RM训练和推理阶段有效提高奖励一致性，从而使得训练出的RLHF模型能够生成更有用的响应，表明奖励不一致性对下游RLHF过程具有涓滴效应。"
}
{
  "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher",
  "title_zh": "标题：GPT-4 太聪明而不安全：通过密码与大型语言模型的隐秘对话",
  "abstract": "Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time in bypassing the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. Notably, we identify that LLMs seem to have a ''secret cipher'', and propose a novel SelfCipher that uses only role play and several unsafe demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases.",
  "abstract_zh": "摘要：安全性是大型语言模型（LLMs）发展的核心。关于将LLMs与人类伦理和偏好对齐的研究已经相当丰富，包括预训练中的数据过滤、监督微调、基于人类反馈的强化学习、红队测试等。在本研究中，我们发现使用密码进行对话可以绕过LLMs的安全对齐技术，这些技术主要是在自然语言中进行的。我们提出了一种新颖的框架CipherChat，以系统地检验安全对齐在非自然语言（即密码）中的通用性。CipherChat使人类能够通过带有系统角色描述和少量加密演示的密码提示与LLMs进行对话。我们使用CipherChat评估最先进的LLMs，包括ChatGPT和GPT-4，在11个安全领域中针对不同的代表性人类密码进行测试，涵盖英语和中文。实验结果表明，在多个安全领域中，某些密码几乎100%成功绕过GPT-4的安全对齐，证明了为非自然语言开发安全对齐的必要性。值得注意的是，我们发现LLMs似乎具有一种“秘密密码”，并提出了一种新颖的SelfCipher，仅使用角色扮演和几种不安全的自然语言演示来激发这种能力。SelfCipher在几乎所有情况下都意外地超越了现有的人类密码。"
}
{
  "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",
  "title_zh": "标题：损失的突然下降：MLM中的句法习得、相变和简单性偏见",
  "abstract": "Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.",
  "abstract_zh": "摘要：大多数自然语言处理中的可解释性研究集中于理解完全训练模型的行为和特征。然而，某些关于模型行为的见解可能仅通过观察训练过程的轨迹才能获得。我们展示了一个关于掩蔽语言模型（MLM）中句法习得的案例研究，表明分析可解释性特征在训练过程中的演变如何加深我们对突现行为的理解。特别地，我们研究了句法注意结构（SAS），这是MLM自然出现的一种特性，其中特定的Transformer头倾向于关注特定的句法关系。我们确定了一个在预训练期间的短暂窗口，在此期间模型突然获得SAS，同时损失急剧下降。这一突破促成了语言能力的后续获得。然后，我们通过在训练过程中操控SAS来检查其因果作用，并证明SAS对于语法能力的发展是必要的。我们进一步发现，SAS在训练过程中与其他有益特征竞争，并且短暂抑制SAS可以提高模型质量。这些发现提供了对简单性偏见和突破性训练动态的现实世界例子的解释。"
}
{
  "title": "Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules",
  "title_zh": "生成神经符号视觉推理：模块的增长与重用",
  "abstract": "Recent works have shown that Large Language Models (LLMs) could empower traditional neuro-symbolic models via programming capabilities to translate lan- guages into module descriptions, thus achieving strong visual reasoning results while maintaining the model’s transparency and efficiency. However, these mod- els usually exhaustively generate the entire code snippet given each new instance of a task, which is extremely ineffective. On the contrary, human beings grad- ually acquire knowledge that can be reused and grow into more profound skills for fast generalization to new tasks since we are an infant. Inspired by this, we propose generative neuro-symbolic visual reasoning by growing and reusing mod- ules. Specifically, our model consists of three unique stages, module initialization, module generation, and module execution. First, given a vision-language task, we adopt LLMs to examine whether we could reuse and grow over established mod- ules to handle this new task. If not, we initialize a new module needed by the task and specify the inputs and outputs of this new module. After that, the new module is created by querying LLMs to generate corresponding code snippets that match the requirements. In order to get a better sense of the new module’s ability, we treat few-shot training examples as test cases to see if our new module could pass these cases. If yes, the new module is added to the module library for future reuse. Finally, we evaluate the performance of our model on the testing set by executing the parsed programs with the newly made visual modules to get the results. We find the proposed GNSVR model possesses several advantages. First, it performs competitively on standard tasks like visual question answering and referring ex- pression comprehension; Second, the visual modules learned from one task can be seamlessly transferred to new tasks; Last but not least, it is able to adapt to new visual reasoning tasks by observing a few training examples and reusing modules.",
  "abstract_zh": "近期研究表明，大型语言模型（LLMs）可以通过编程能力增强传统的神经符号模型，将语言翻译为模块描述，从而在保持模型透明性和效率的同时实现强大的视觉推理结果。然而，这些模型通常在每个新任务实例面前会完全生成整个代码片段，这极其低效。相反，人类从婴儿时期起逐渐获取可以重用的知识，并发展出更深层次的技能，以便快速泛化到新任务。受此启发，我们提出了一种通过模块的增长与重用进行生成神经符号视觉推理的方法。具体来说，我们的模型包括三个独特阶段：模块初始化、模块生成和模块执行。首先，给定一个视觉-语言任务，我们采用LLMs来检查是否可以重用和扩展已有模块以处理这个新任务。如果不能，我们将初始化任务所需的新模块，并指定该新模块的输入和输出。之后，通过查询LLMs生成符合要求的相应代码片段来创建新模块。为了更好地了解新模块的能力，我们将少量训练示例视为测试用例，以查看我们的新模块是否能够通过这些案例。如果可以，该新模块将被添加到模块库中以供将来重用。最后，我们通过执行解析后的程序与新创建的视觉模块来评估模型在测试集上的表现。我们发现所提出的GNSVR模型具有几个优点。首先，它在视觉问答和指称表达理解等标准任务上表现出色；其次，从一个任务学习的视觉模块可以无缝转移到新任务；最后但同样重要的是，它能够通过观察少量训练示例并重用模块来适应新的视觉推理任务。"
}
{
  "title": "InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation",
  "title_zh": "标题：InternVid：一个用于多模态理解和生成的大规模视频-文本数据集",
  "abstract": "This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. InternVid contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.",
  "abstract_zh": "摘要：本文介绍了InternVid，这是一个以视频为中心的大规模多模态数据集，能够学习强大且可迁移的视频-文本表示，以实现多模态理解和生成。InternVid包含超过700万段视频，时长近76万小时，生成234M段视频剪辑，并附有总计41亿字的详细描述。我们的核心贡献是开发一种可扩展的方法，利用大型语言模型（LLM）自主构建高质量的视频-文本数据集，从而展示其在大规模学习视频-语言表示方面的有效性。具体而言，我们采用多尺度方法生成与视频相关的描述。此外，我们引入了基于ViT-L的视频-文本表示学习模型ViCLIP。该模型通过对比学习在InternVid上进行训练，展示了领先的零样本动作识别和竞争性的视频检索性能。除了基本的视频理解任务，如识别和检索，我们的数据集和模型具有广泛的应用，特别有利于生成交错的视频-文本数据，以学习视频中心的对话系统，推动视频到文本和文本到视频生成研究。这些提议的资源为对多模态视频理解和生成感兴趣的研究人员和从业者提供了工具。"
}
{
  "title": "Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents",
  "title_zh": "即插即用的政策规划器用于大型语言模型驱动的对话代理",
  "abstract": "Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.",
  "abstract_zh": "主动对话在大型语言模型（LLMs）时代是一个实际而具有挑战性的对话问题，其中对话政策规划是提高LLMs主动性的关键。现有的大多数研究通过各种提示方案使LLMs的对话政策规划成为可能，或通过语言AI反馈迭代增强其处理给定案例的能力。然而，这些方法要么受到冻结的LLMs的政策规划能力的限制，要么难以转移到新案例中。在本研究中，我们引入了一种新的对话政策规划范式，以可调语言模型插件作为即插即用的对话政策规划器，命名为PPDPP，旨在为主动对话问题制定策略。具体而言，我们开发了一种新颖的训练框架，以促进对可用人类标注数据的监督微调，以及通过LLM基础的自我对弈模拟收集的动态交互数据进行目标导向的AI反馈的强化学习。通过这种方式，LLM驱动的对话代理不仅可以在训练后推广到不同的案例，而且只需替换学习到的插件即可适用于不同的应用。此外，我们建议在交互设置下评估对话系统的政策规划能力。实验结果表明，PPDPP在三种不同的主动对话应用（包括谈判、情感支持和辅导对话）中始终显著优于现有方法。"
}
{
  "title": "Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction",
  "title_zh": "标题：Ins-DetCLIP：将检测模型与人类语言指令对齐",
  "abstract": "This paper introduces Instruction-oriented Object Detection (IOD), a new task that enhances human-computer interaction by enabling object detectors to understand user instructions and locate relevant objects. Unlike traditional open-vocabulary object detection tasks that rely on users providing a list of required category names, IOD requires models to comprehend natural-language instructions, contextual reasoning, and output the name and location of the desired categories. This poses fresh challenges for modern object detection systems. To develop an IOD system, we create a dataset called IOD-Bench, which consists of instruction-guided detections, along with specialized evaluation metrics. We leverage large-scale language models (LLMs) to generate a diverse set of instructions (8k+) based on existing public object detection datasets, covering a wide range of real-world scenarios. As an initial approach to the IOD task, we propose a model called Ins-DetCLIP. It harnesses the extensive knowledge within LLMs to empower the detector with instruction-following capabilities. Specifically, our Ins-DetCLIP employs a visual encoder (i.e., DetCLIP, an open-vocabulary detector) to extract object-level features. These features are then aligned with the input instructions using a cross-modal fusion module integrated into a pre-trained LLM. Experimental results conducted on IOD-Bench demonstrate that our model consistently outperforms baseline methods that directly combine LLMs with detection models. This research aims to pave the way for a more adaptable and versatile interaction paradigm in modern object detection systems, making a significant contribution to the field.",
  "abstract_zh": "摘要：本文介绍了一种面向指令的目标检测（IOD）新任务，通过使目标检测器能够理解用户指令并定位相关对象，从而增强人机交互。与传统的开放词汇目标检测任务不同，IOD要求模型理解自然语言指令、进行上下文推理，并输出所需类别的名称和位置。这为现代目标检测系统带来了新的挑战。为了开发IOD系统，我们创建了一个名为IOD-Bench的数据集，其中包含指令引导的检测以及专门的评估指标。我们利用大规模语言模型（LLMs）生成基于现有公共目标检测数据集的多样化指令集（超过8k），涵盖广泛的现实场景。作为IOD任务的初步方法，我们提出了一种名为Ins-DetCLIP的模型。它利用LLMs中的广泛知识，使检测器具备跟随指令的能力。具体而言，我们的Ins-DetCLIP采用视觉编码器（即DetCLIP，一个开放词汇检测器）提取对象级特征。这些特征通过集成在预训练LLM中的跨模态融合模块与输入指令对齐。在IOD-Bench上进行的实验结果表明，我们的模型始终优于直接将LLMs与检测模型结合的基线方法。本研究旨在为现代目标检测系统中更具适应性和多功能性的交互范式铺平道路，为该领域做出重要贡献。"
}
{
  "title": "LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models",
  "title_zh": "LoftQ：针对大型语言模型的LoRA微调感知量化",
  "abstract": "Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning (Dettmers et al., 2023). In this work we focus on the scenario where quantization and LoRA fine- tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrep- ancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural lan- guage understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and out- performs existing quantization methods, especially in the challenging 2-bit and 2/4-bit mixed precision regimes. We will release our code.",
  "abstract_zh": "量化是服务大型语言模型（LLMs）不可或缺的技术，最近已逐渐应用于LoRA微调（Dettmers等，2023）。在本研究中，我们关注量化和LoRA微调共同应用于预训练模型的场景。在这种情况下，通常会观察到全微调与量化加LoRA微调方法在下游任务性能之间存在一致的差距。为此，我们提出了LoftQ（LoRA微调感知量化），一种新颖的量化框架，能够同时对LLM进行量化并找到合适的低秩初始化以进行LoRA微调。这种初始化缓解了量化模型与全精度模型之间的差异，并显著提高了下游任务的泛化能力。我们在自然语言理解、问答、摘要和自然语言生成任务上评估了我们的方法。实验表明，我们的方法非常有效，优于现有的量化方法，尤其是在具有挑战性的2位和2/4位混合精度环境中。我们将发布我们的代码。"
}
{
  "title": "InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior",
  "title_zh": "指令场景：基于语义图先验的指令驱动3D室内场景合成",
  "abstract": "Comprehending natural language instructions is a charming property for 3D indoor scene synthesis systems. Existing methods directly model object joint distributions and express object relations implicitly within a scene, thereby hindering the controllability of generation. We introduce InstructScene, a novel generative framework that integrates a semantic graph prior and a layout decoder to improve controllability and fidelity for 3D scene synthesis. The proposed semantic graph prior jointly learns scene appearances and layout distributions, exhibiting versatility across various downstream tasks in a zero-shot manner. To facilitate the benchmarking for text-driven 3D scene synthesis, we curate a high-quality dataset of scene-instruction pairs with large language and multimodal models. Extensive experimental results reveal that the proposed method surpasses existing state-of-the-art approaches by a large margin. Thorough ablation studies confirm the efficacy of crucial design components. Project page: https://chenguolin.github.io/projects/InstructScene.",
  "abstract_zh": "理解自然语言指令是3D室内场景合成系统的一项迷人特性。现有方法直接建模对象联合分布，并在场景中隐式表达对象关系，从而阻碍了生成的可控性。我们引入了InstructScene，这是一种新颖的生成框架，结合了语义图先验和布局解码器，以提高3D场景合成的可控性和保真度。所提出的语义图先验共同学习场景外观和布局分布，以零样本方式在各种下游任务中表现出多样性。为了促进基于文本的3D场景合成的基准测试，我们策划了一个高质量的场景-指令对数据集，结合了大型语言和多模态模型。大量实验结果表明，所提出的方法在很大程度上超越了现有的最先进方法。全面的消融研究证实了关键设计组件的有效性。项目页面：https://chenguolin.github.io/projects/InstructScene。"
}
{
  "title": "Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation",
  "title_zh": "标题：Sign2GPT：利用大型语言模型进行无注释手语翻译",
  "abstract": "Automatic Sign Language Translation requires the integration of both computer vision and natural language processing to effectively bridge the communication gap between sign and spoken languages. However, the deficiency in large-scale training data to support sign language translation means we need to leverage resources from spoken language. We introduce, Sign2GPT, a novel framework for sign language translation that utilizes large-scale pretrained vision and language models via lightweight adapters for gloss-free sign language translation. The lightweight adapters are crucial for sign language translation, due to the constraints imposed by limited dataset sizes and the computational requirements when training with long sign videos.\nWe also propose a novel pretraining strategy that directs our encoder to learn sign representations from automatically extracted pseudo-glosses without requiring gloss order information or annotations.\nWe evaluate our approach on two public benchmark sign language translation datasets, namely RWTH-PHOENIX-Weather 2014T and CSL-Daily, and improve on state-of-the-art gloss-free translation performance with a significant margin.",
  "abstract_zh": "摘要：自动手语翻译需要结合计算机视觉和自然语言处理，以有效弥合手语与口语之间的沟通鸿沟。然而，支持手语翻译的大规模训练数据不足，意味着我们需要利用口语资源。我们提出了Sign2GPT，这是一种新颖的手语翻译框架，通过轻量级适配器利用大规模预训练的视觉和语言模型，实现无注释手语翻译。轻量级适配器对于手语翻译至关重要，因为受限于数据集规模和训练长手语视频时的计算需求。我们还提出了一种新颖的预训练策略，指导我们的编码器从自动提取的伪注释中学习手语表示，而无需注释顺序信息或标注。我们在两个公共基准手语翻译数据集（即RWTH-PHOENIX-Weather 2014T和CSL-Daily）上评估了我们的方法，并在无注释翻译性能上显著提高了最先进的水平。"
}
{
  "title": "Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models",
  "title_zh": "基于信心的奖励优化用于微调文本到图像模型",
  "abstract": "Fine-tuning text-to-image models with reward functions trained on human feedback data has proven effective for aligning model behavior with human intent. However, excessive optimization with such reward models, which serve as mere proxy objectives, can compromise the performance of fine-tuned models, a phenomenon known as reward overoptimization. To investigate this issue in depth, we introduce the Text-Image Alignment Assessment (TIA2) benchmark, which comprises a diverse collection of text prompts, images, and human annotations. Our evaluation of several state-of-the-art reward models on this benchmark reveals their frequent misalignment with human assessment. We empirically demonstrate that overoptimization occurs notably when a poorly aligned reward model is used as the fine-tuning objective. To address this, we propose TextNorm, a simple method that enhances alignment based on a measure of reward model confidence estimated across a set of semantically contrastive text prompts. We demonstrate that incorporating the confidence-calibrated rewards in fine-tuning effectively reduces overoptimization, resulting in twice as many wins in human evaluation for text-image alignment compared against the baseline reward models.",
  "abstract_zh": "通过对人类反馈数据训练的奖励函数微调文本到图像模型已被证明有效，但过度优化这些作为代理目标的奖励模型可能会损害微调模型的性能，这种现象被称为奖励过度优化。为深入研究这一问题，我们引入了文本-图像对齐评估（TIA2）基准，包含多样的文本提示、图像和人类注释。我们对多个最先进的奖励模型在该基准上的评估显示它们与人类评估之间常常存在不一致。我们实证证明，当使用与目标不良对齐的奖励模型作为微调目标时，过度优化显著发生。为此，我们提出了TextNorm，这是一种基于一组语义对比文本提示中估计的奖励模型信心度量来增强对齐的简单方法。我们证明，在微调中结合信心校准的奖励有效减少了过度优化，使得在文本-图像对齐的人类评估中相较于基线奖励模型获得了两倍的胜利。"
}
{
  "title": "Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions",
  "title_zh": "指导解码：指令调优的大型语言模型是从嘈杂指令中自我修正的",
  "abstract": "While instruction-tuned language models have demonstrated impressive zero-shot generalization, these models often struggle to generate accurate responses when faced with instructions that fall outside their training set. This paper presents Instructive Decoding (ID), a simple yet effective approach that augments the efficacy of instruction-tuned models. Specifically, ID adjusts the logits for next-token prediction in a contrastive manner, utilizing predictions generated from a manipulated version of the original instruction, referred to as a noisy instruction. This noisy instruction aims to elicit responses that could diverge from the intended instruction yet remain plausible. We conduct experiments across a spectrum of such noisy instructions, ranging from those that insert semantic noise via random words to others like 'opposite' that elicit the deviated responses. Our approach achieves considerable performance gains across various instruction-tuned models and tasks without necessitating any additional parameter updates. Notably, utilizing 'opposite' as the noisy instruction in ID, which shows the maximum divergence from the original instruction, consistently produces the most significant performance gains across multiple models and tasks.",
  "abstract_zh": "尽管指令调优的语言模型在零-shot 泛化方面表现出色，但当面临超出其训练集的指令时，这些模型往往难以生成准确的响应。本文提出了一种简单而有效的方法——指导解码（ID），旨在增强指令调优模型的有效性。具体而言，ID以对比的方式调整下一个标记预测的logits，利用从原始指令的操控版本生成的预测，称为嘈杂指令。该嘈杂指令旨在引发可能偏离预期指令但仍然合理的响应。我们在一系列这样的嘈杂指令上进行了实验，从通过随机词插入语义噪声的指令到像“相反”这样的指令，这些指令引发偏离的响应。我们的方法在各种指令调优模型和任务中实现了显著的性能提升，而无需任何额外的参数更新。值得注意的是，利用“相反”作为ID中的嘈杂指令，它与原始指令的偏离最大，在多个模型和任务中始终产生最显著的性能提升。"
}
{
  "title": "Zoology: Measuring and Improving  Recall in Efficient Language Models",
  "title_zh": "动物学：测量和提升高效语言模型中的召回率",
  "abstract": "Attention-free language models that combine gating and convolutions are growing in popularity due to their efficiency and increasingly competitive performance. To better understand these architectures, we pretrain a suite of 17 attention and gated-convolution language models, finding that SoTA gated-convolution architectures still underperform attention by up to 2.1 perplexity points on the Pile. In fine-grained analysis, we find 82% of the gap is explained by each model's ability to recall information that is previously mentioned in-context, e.g. \"Hakuna Matata means no worries Hakuna Matata it means no\" -> ??. On this task, termed \"associative recall\", we find that attention outperforms gated-convolutions by a large margin: a 70M parameter attention model outperforms a 1.4 billion parameter gated-convolution model on associative recall. This is surprising because prior work shows gated convolutions can perfectly solve synthetic tests for AR capability.  To close the gap between synthetics and real language, we develop a new formalization of the task called multi-query associative recall (MQAR) that better reflects actual language. We perform an empirical and theoretical study of MQAR that elucidates differences in the parameter-efficiency of attention and gated-convolution recall. Informed by our analysis, we evaluate simple convolution-attention hybrids and show that hybrids with input-dependent sparse attention patterns can close 97.4% of the gap to attention, while maintaining sub-quadratic scaling. Code is at: https://github.com/HazyResearch/zoology.",
  "abstract_zh": "无注意力语言模型结合了门控和卷积，因其高效性和日益竞争的性能而日益受到欢迎。为了更好地理解这些架构，我们预训练了一套17个注意力和门控卷积语言模型，发现最先进的门控卷积架构在Pile数据集上仍然比注意力模型低2.1困惑度。在细粒度分析中，我们发现82%的差距可以通过每个模型在上下文中回忆先前提到的信息的能力来解释，例如“哈库那·玛塔塔意味着没有烦恼哈库那·玛塔塔它意味着没有”-> ??。在这个被称为“关联召回”的任务中，我们发现注意力模型的表现远超门控卷积：一个7000万参数的注意力模型在关联召回上优于一个14亿参数的门控卷积模型。这令人惊讶，因为先前的研究表明，门控卷积可以完美解决合成测试的AR能力。为了缩小合成与真实语言之间的差距，我们开发了一种新的任务形式化，称为多查询关联召回（MQAR），更好地反映实际语言。我们对MQAR进行了实证和理论研究，阐明了注意力和门控卷积召回在参数效率上的差异。根据我们的分析，我们评估了简单的卷积-注意力混合模型，并显示具有输入依赖稀疏注意力模式的混合模型可以缩小97.4%的注意力差距，同时保持亚平方级的扩展。代码可在：https://github.com/HazyResearch/zoology。"
}
{
  "title": "In-Context Pretraining: Language Modeling Beyond Document Boundaries",
  "title_zh": "上下文预训练：超越文档边界的语言建模",
  "abstract": "Language models are currently trained to predict tokens given document prefixes, enabling them to zero shot long form generation and prompting-style tasks which can be reduced to document completion. We instead present IN-CONTEXT PRETRAINING, a new approach where language models are trained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. Our approach builds on the fact that current pipelines train by concatenating random sets of shorter documents to create longer context windows; this improves efficiency even though the prior documents provide no signal for predicting the next document. Given this fact, we can do IN-CONTEXT PRETRAINING by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do this, we introduce approximate algorithms for finding related documents with efficient nearest neighbor search and constructing coherent batches with a graph cover algorithm. Our experiments show IN-CONTEXT PRETRAINING offers a scalable and simple approach to significantly enhance LM performance: we see notable improvements in tasks that require more complex contextual reasoning, including in-context learning (+8%), reading comprehension (+15%), faithfulness to previous contexts (+16%), long-context reasoning (+5%), and retrieval augmentation (+9%).",
  "abstract_zh": "语言模型目前被训练以预测给定文档前缀的标记，使其能够进行零-shot长文本生成和提示式任务，这些任务可以简化为文档补全。我们提出了上下文预训练，这是一种新的方法，其中语言模型在一系列相关文档上进行训练，从而明确鼓励它们跨越文档边界进行阅读和推理。我们的方法基于当前管道通过连接随机的短文档集来创建更长的上下文窗口的事实；尽管先前的文档对预测下一个文档没有信号，但这提高了效率。鉴于这一事实，我们可以通过简单地改变文档顺序，使每个上下文包含相关文档，并直接应用现有的预训练管道来进行上下文预训练。然而，这一文档排序问题具有挑战性。我们有数十亿个文档，我们希望排序能够最大化每个文档的上下文相似性而不重复任何数据。为此，我们引入了近似算法，通过高效的最近邻搜索找到相关文档，并使用图覆盖算法构建一致的批次。我们的实验表明，上下文预训练提供了一种可扩展且简单的方法，显著提升语言模型的性能：我们在需要更复杂上下文推理的任务中观察到显著改善，包括上下文学习（+8%）、阅读理解（+15%）、对先前上下文的忠实性（+16%）、长上下文推理（+5%）和检索增强（+9%）。"
}
{
  "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
  "title_zh": "超越模仿：利用细粒度质量信号进行对齐",
  "abstract": "Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which can't fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named $\\textbf{FIGA}$. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quailty signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.",
  "abstract_zh": "与人类偏好的对齐是大型语言模型（LLMs）所期望的特性。目前，主要的对齐方法是基于人类反馈的强化学习（RLHF）。尽管RLHF有效，但其实现和训练复杂，因此最近的研究探索如何基于监督微调（SFT）开发替代的对齐方法。SFT的一个主要限制是它本质上是模仿学习，无法完全理解预期行为。为了解决这个问题，我们提出了一种改进的对齐方法，称为$\\textbf{FIGA}$。与之前的方法不同，我们结合了通过对比良好和不良响应得出的细粒度（即标记或短语级别）质量信号。我们的方法做出了两个主要贡献。首先，我们策划了一个精细的对齐数据集，将初始响应与相应的修订响应配对。其次，我们设计了一种新的损失函数，可以利用细粒度质量信号来指导LLMs的对齐学习。大量实验通过与多个竞争基线的比较证明了我们方法的有效性。"
}
{
  "title": "Select to Perfect: Imitating desired behavior from large multi-agent data",
  "title_zh": "选择以完美：从大型多智能体数据中模仿期望行为",
  "abstract": "AI agents are commonly trained with large datasets of demonstrations of human behavior.\nHowever, not all behaviors are equally safe or desirable.\nDesired characteristics for an AI agent can be expressed by assigning desirability scores, which we assume are not assigned to individual behaviors but to collective trajectories.\nFor example, in a dataset of vehicle interactions, these scores might relate to the number of incidents that occurred. \nWe first assess the effect of each individual agent's behavior on the collective desirability score, e.g., assessing how likely an agent is to cause incidents.\nThis allows us to selectively imitate agents with a positive effect, e.g., only imitating agents that are unlikely to cause incidents. \nTo enable this, we propose the concept of an agent's \\textit{Exchange Value}, which quantifies an individual agent's contribution to the collective desirability score. \nThe Exchange Value is the expected change in desirability score when substituting the agent for a randomly selected agent.\nWe propose additional methods for estimating Exchange Values from real-world datasets, enabling us to learn desired imitation policies that outperform relevant baselines. The project website can be found at https://tinyurl.com/select-to-perfect.",
  "abstract_zh": "人工智能代理通常使用大量人类行为示范的数据集进行训练。然而，并非所有行为都是同样安全或理想的。可以通过分配期望分数来表达对人工智能代理的期望特征，我们假设这些分数不是分配给单个行为，而是分配给集体轨迹。例如，在一个车辆互动数据集中，这些分数可能与发生的事件数量相关。我们首先评估每个个体代理行为对集体期望分数的影响，例如，评估一个代理导致事件的可能性。这使我们能够选择性地模仿具有积极影响的代理，例如，仅模仿不太可能导致事件的代理。为此，我们提出了代理的“交换价值”概念，该概念量化了个体代理对集体期望分数的贡献。交换价值是用随机选择的代理替代该代理时期望的期望分数变化。我们提出了从现实世界数据集中估计交换价值的额外方法，使我们能够学习出超越相关基准的期望模仿策略。项目网站可以在 https://tinyurl.com/select-to-perfect 找到。"
}
{
  "title": "Massive Editing for Large Language Models via Meta Learning",
  "title_zh": "大语言模型的巨量编辑通过元学习",
  "abstract": "While large language models (LLMs) have enabled learning knowledge from the pre-training corpora, the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training. A promising approach involves employing a hyper-network to generate parameter shift, whereas existing hyper-networks suffer from inferior scalability in synchronous editing operation amount (Hase et al., 2023b; Huang et al., 2023). For instance, Mitchell et al. (2022) mimics gradient accumulation to sum the parameter shifts together, which lacks statistical significance and is prone to cancellation effect. To mitigate the problem, we propose the MAssive Language Model Editing Network (MALMEN), which formulates the parameter shift aggregation as the least square problem, subsequently updating the LM parameter using the normal equation. To accommodate editing multiple facts simultaneously with limited memory budgets, we separate the computation on the hyper-network and LM, enabling arbitrary batch size on both neural networks. Our method is evaluated by editing up to thousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2, and GPT-J (6B), across various knowledge-intensive NLP tasks, i.e., closed book fact-checking and question answering. Remarkably, MALMEN is capable of editing hundreds of times more facts than MEND (Mitchell et al., 2022) with the identical hyper-network architecture and outperforms editor specifically designed for GPT, i.e., MEMIT (Meng et al., 2023).",
  "abstract_zh": "虽然大语言模型（LLMs）能够从预训练语料库中学习知识，但所获得的知识可能在根本上是错误的或随着时间的推移而过时，这就需要在训练后纠正语言模型（LM）的知识。一种有前景的方法是利用超网络生成参数偏移，而现有的超网络在同步编辑操作量上存在扩展性不足的问题。为了解决这一问题，我们提出了MAssive Language Model Editing Network（MALMEN），将参数偏移聚合形式化为最小二乘问题，随后使用正规方程更新LM参数。为了在有限的内存预算下同时编辑多个事实，我们将超网络和LM的计算分开，使得两个神经网络都能够支持任意批量大小。我们的方法通过在不同架构的LM上编辑多达数千个事实进行评估，即BERT-base、GPT-2和GPT-J（6B），并在各种知识密集型NLP任务中进行测试，如闭卷事实核查和问答。值得注意的是，MALMEN能够编辑的事实数量是MEND（Mitchell et al., 2022）的数百倍，且在相同的超网络架构下表现优于专为GPT设计的编辑器MEMIT（Meng et al., 2023）。"
}
{
  "title": "Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models",
  "title_zh": "标题：重述、增强、推理：视觉语言模型的问题视觉定位",
  "abstract": "An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to an LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually-grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To this end, we present **Rep**hrase, **A**ugment and **Re**ason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question. We then use the LVLM’s confidence over a generated answer as an unsupervised scoring function to select the rephrased question most likely to improve zero-shot performance. Focusing on three visual question answering tasks, we show that RepARe can result in a 3.85% (absolute) increase in zero-shot accuracy on VQAv2, 6.41%, and 7.94% points increase on A-OKVQA, and VizWiz respectively. Additionally, we find that using gold answers for oracle question candidate selection achieves a substantial gain in VQA accuracy by up to 14.41%. Through extensive analysis, we demonstrate that outputs from RepARe increase syntactic complexity, and effectively utilize vision-language interaction and the frozen LLM.",
  "abstract_zh": "摘要：越来越多的视觉语言任务可以通过将大型语言模型（LLMs）与视觉编码器结合，以零样本和少样本的方式处理，即几乎不需要训练。这虽然带来了巨大的好处，例如不需要训练数据或定制架构，但输入如何呈现给LVLM可能会对零样本模型性能产生重大影响。特别是，以不充分的方式表述的输入可能由于缺失的视觉信息、复杂的隐含推理或语言模糊性等因素导致错误答案。因此，向输入中添加视觉基础的信息作为预先澄清应该通过减少不充分性来提高模型性能，例如，通过定位对象和消除歧义。同样，在视觉问答（VQA）设置中，改变问题的表述方式可以使模型更容易回答。为此，我们提出了**重**述、**增**强和**推**理（RepARe），这是一个无梯度框架，利用底层LVLM作为标题生成器和推理器提取图像的显著细节，以便对原始问题进行修改。然后，我们使用LVLM对生成答案的信心作为无监督评分函数，选择最有可能提高零样本性能的重述问题。我们专注于三个视觉问答任务，展示RepARe在VQAv2上可以实现3.85%的（绝对）零样本准确率提升，在A-OKVQA和VizWiz上分别提升6.41%和7.94%。此外，我们发现使用金标准答案进行神谕问题候选选择在VQA准确率上可实现高达14.41%的显著提升。通过广泛的分析，我们证明RepARe的输出增加了句法复杂性，并有效利用了视觉语言交互和冻结的LLM。"
}
{
  "title": "Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering",
  "title_zh": "批量校准：重新思考上下文学习和提示工程的校准",
  "abstract": "Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allow it to learn the contextual bias from labeled data. We validate the effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.",
  "abstract_zh": "提示和上下文学习（ICL）已成为大型语言模型（LLMs）高效学习的范式。然而，LLMs在提示上存在脆弱性以及各种偏差因素，包括但不限于格式、选择的表述者和ICL示例。为了解决导致意外性能下降的问题，已经开发了校准方法，以减轻这些偏差的影响，同时恢复LLM性能。在这项工作中，我们首先对现有的校准方法进行了系统分析，提供了统一的视角并揭示了失败案例。受到这些分析的启发，我们提出了批量校准（BC），这是一种简单而直观的方法，可以控制来自批量输入的上下文偏差，统一各种先前的方法，并有效解决上述问题。BC是零-shot、仅推理，并且几乎不产生额外成本。在少量样本设置中，我们进一步扩展BC，使其能够从标记数据中学习上下文偏差。我们使用PaLM 2-(S, M, L)和CLIP模型验证了BC的有效性，并在超过10个自然语言理解和图像分类任务中展示了其在先前校准基准上的最先进性能。"
}
{
  "title": "The False Promise of Imitating Proprietary Language Models",
  "title_zh": "模仿专有语言模型的虚假承诺",
  "abstract": "An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). In this work, we critically analyze this approach of imitating language models. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models---they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT’s style but not its factuality. Overall, we conclude that while model imitation can be useful for training models to follow instructions and avoid toxic outputs, it falls short its full promise in many ways. In particular, there exists a substantial capabilities gap between open and closed LMs that we find cannot be bridged merely by adding more imitation data. Instead, we find that fine-tuning more capable base LMs has a significantly more substantial effect on closing this gap. In turn, we argue that the higher leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.",
  "abstract_zh": "一种新兴的方法是通过在更强大的模型（如专有系统ChatGPT）输出上微调较弱的语言模型，以低成本提高其性能。在本研究中，我们对这种模仿语言模型的方法进行了批判性分析。我们首先微调了一系列模仿ChatGPT的语言模型，使用不同的基础模型大小（1.5B--13B）、数据来源和模仿数据量（0.3M--150M标记）。然后，我们使用众包评估者和经典的自然语言处理基准对模型进行评估。起初，我们对模仿模型的输出质量感到惊讶——它们在遵循指令方面似乎表现得更好，众包工作者将其输出评估为与ChatGPT具有竞争力。然而，当进行更有针对性的自动评估时，我们发现模仿模型在不依赖于模仿数据的任务上几乎没有缩小基础语言模型与ChatGPT之间的差距。我们表明，这些性能差异可能会被人类评估者忽视，因为模仿模型擅长模仿ChatGPT的风格，但不具备其真实性。总体而言，我们得出结论，尽管模型模仿在训练模型遵循指令和避免有害输出方面可能有用，但在许多方面未能实现其全部承诺。特别是，我们发现开放和封闭语言模型之间存在显著的能力差距，仅通过增加更多的模仿数据无法弥补。相反，我们发现微调更强大的基础语言模型在缩小这一差距方面具有显著的效果。因此，我们认为，提高开源模型的更高效的措施是解决开发更好的基础语言模型这一艰巨挑战，而不是走捷径去模仿专有系统。"
}
{
  "title": "Teaching Large Language Models to Self-Debug",
  "title_zh": "教大型语言模型自我调试",
  "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose self-debugging, which teaches a large language model to debug its predicted program. In particular, we demonstrate that self-debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by leveraging code execution and explaining the generated code in natural language. Self-debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, self-debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, self-debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, self-debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10$\\times$ candidate programs.",
  "abstract_zh": "大型语言模型（LLMs）在代码生成方面取得了令人印象深刻的表现。然而，对于复杂的编程任务，一次性生成正确的解决方案变得具有挑战性，因此一些先前的工作设计了程序修复方法以提高代码生成性能。在本研究中，我们提出了自我调试，旨在教大型语言模型调试其预测的程序。具体而言，我们展示了自我调试可以教大型语言模型进行橡皮鸭调试；即在没有任何关于代码正确性或错误信息的人类反馈的情况下，模型能够通过利用代码执行和用自然语言解释生成的代码来识别其错误。自我调试在多个代码生成基准测试中达到了最先进的性能，包括用于文本到SQL生成的Spider数据集、用于C++到Python翻译的TransCoder和用于文本到Python生成的MBPP。在没有单元测试来验证预测正确性的Spider基准测试中，自我调试结合代码解释始终将基线提高了2-3%，并在最难级别的问题上将预测准确率提高了9%。在有单元测试的TransCoder和MBPP中，自我调试将基线准确率提高了最多12%。同时，通过利用反馈信息和重用失败的预测，自我调试显著提高了样本效率，并能够与生成超过10倍候选程序的基线模型相匹配或超越。"
}
{
  "title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages",
  "title_zh": "大型多语言模型在语言间实现零-shot多模态学习",
  "abstract": "Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a (quasi)-zero-shot manner, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MPM, we build large multimodal models VisCPM in image-to-text and text-to-image generation, which achieve state-of-the-art (open-source) performance in Chinese. To facilitate future research, we open-source codes and model weights at https://github.com/OpenBMB/VisCPM.",
  "abstract_zh": "近年来，多模态学习在图像到文本和文本到图像生成方面取得了显著增长。然而，这种成功通常仅限于英语，其他语言则相对滞后。在其他语言中构建具有竞争力的对等模型非常具有挑战性，因为非英语多模态数据的资源稀缺（即缺乏大规模、高质量的图像-文本数据）。在本研究中，我们提出了MPM，这是一种在低资源语言中训练大型多模态模型的有效训练范式。MPM表明，多语言模型可以在语言间实现零-shot多模态学习。具体而言，基于强大的多语言大型语言模型，预训练于仅包含英语的图像-文本数据的多模态模型能够以（准）零-shot的方式很好地推广到其他语言，甚至超过在本土语言的图像-文本数据上训练的模型。以中文为MPM的实践，我们构建了在图像到文本和文本到图像生成中表现出色的大型多模态模型VisCPM，达到了中文领域的最新（开源）性能。为了促进未来的研究，我们在https://github.com/OpenBMB/VisCPM上开源了代码和模型权重。"
}
{
  "title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning",
  "title_zh": "标题：DePT：用于参数高效微调的分解提示调优",
  "abstract": "Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance while saving substantial memory and time costs compared to vanilla PT and its variants, without changing trainable parameter sizes. Through extensive experiments on 23 natural language processing (NLP) and vision-language (VL) tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches, including the full fine-tuning baseline, in some scenarios. Additionally, we empirically show that DEPT grows more efficient as the model size increases. Our further study reveals that DePT integrates seamlessly with parameter-efficient transfer learning in the few-shot learning setting and highlights its adaptability to various model architectures and sizes.",
  "abstract_zh": "摘要：提示调优（PT）通过将少量可训练的软（连续）提示向量附加到语言模型（LM）的输入中，在各种任务和模型中显示出有希望的结果，适用于参数高效微调（PEFT）。PT与其他PEFT方法的不同之处在于，它在可训练参数较少的情况下保持竞争性能，并且在模型规模扩大时不会大幅增加其参数。然而，PT引入了额外的软提示标记，导致输入序列变长，这显著影响了训练和推理时间以及内存使用，因为变换器的复杂度是二次的。对于面临大量日常查询的大型语言模型（LLM）尤其令人担忧。为了解决这个问题，我们提出了分解提示调优（DePT），它将软提示分解为一个较短的软提示和一对低秩矩阵，然后使用两种不同的学习率进行优化。这使得DePT在不改变可训练参数大小的情况下，能够在节省大量内存和时间成本的同时，取得比普通PT及其变体更好的性能。通过在23个自然语言处理（NLP）和视觉语言（VL）任务上的广泛实验，我们证明DePT在某些场景下超越了最先进的PEFT方法，包括全面微调基线。此外，我们实证表明，随着模型规模的增加，DePT变得更加高效。我们的进一步研究揭示，DePT与少样本学习设置中的参数高效迁移学习无缝集成，并突显其对各种模型架构和规模的适应性。"
}
{
  "title": "Language Model Cascades: Token-Level Uncertainty And Beyond",
  "title_zh": "语言模型级联：基于令牌级不确定性及其扩展",
  "abstract": "Recent advances in language models (LMs) have led to significant improvements in quality on complex NLP tasks, but at the expense of increased inference costs. A simple strategy to achieve more favorable cost-quality tradeoffs is cascading: here, a small model is invoked for most “easy” instances, while a few “hard” instances are deferred to the large model. While the principles underpinning effective cascading are well-studied for classification tasks — with deferral based on predicted class uncertainty favored theoretically and practically — a similar understanding is lacking for generative LM tasks. In this work, we initiate a systematic study of deferral rules for LM cascades. We begin by examining the natural extension of predicted class uncertainty to generative LM tasks, namely, the predicted sequence uncertainty. We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths. This is because LMs produce a sequence of uncertainty values, one for each output token; and moreover, the number of output tokens is variable across different examples. To mitigate the length bias, we propose to exploit the richer token-level uncertainty information implicit in generative LMs. We argue that naive predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties. By contrast, we show that incorporating token-level uncertainty through learned post-hoc deferral rules can significantly outperform such simple aggregation strategies, via experiments on a range of natural language benchmarks with FLAN-T5 models. We further show that incorporating embeddings from the smaller model and intermediate layers of the larger model can give an additional boost in the overall cost-quality tradeoff.",
  "abstract_zh": "最近语言模型（LM）的进展在复杂的自然语言处理任务中显著提高了质量，但也增加了推理成本。实现更有利的成本-质量权衡的一种简单策略是级联：在这里，小模型用于大多数“简单”实例，而少数“困难”实例则推迟到大模型。虽然有效级联的原则在分类任务中得到了充分研究——理论和实践上都倾向于基于预测类别不确定性的推迟——但在生成LM任务中缺乏类似的理解。在本研究中，我们开始系统地研究LM级联的推迟规则。我们首先考察了预测类别不确定性在生成LM任务中的自然扩展，即预测序列不确定性。我们展示了该度量存在长度偏差问题，可能会过度或不足强调基于长度的输出。这是因为LM生成了一系列不确定性值，每个输出令牌对应一个；而且，不同示例的输出令牌数量是可变的。为了减轻长度偏差，我们建议利用生成LM中隐含的更丰富的令牌级不确定性信息。我们认为，简单的预测序列不确定性对应于这些不确定性的简单聚合。相反，我们通过对一系列自然语言基准测试的FLAN-T5模型的实验表明，采用通过学习的事后推迟规则纳入令牌级不确定性可以显著优于这种简单聚合策略。我们进一步表明，结合小模型的嵌入和大模型的中间层可以在整体成本-质量权衡中提供额外的提升。"
}
{
  "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
  "title_zh": "MathVista：评估基础模型在视觉上下文中的数学推理能力",
  "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.",
  "abstract_zh": "大型语言模型（LLMs）和大型多模态模型（LMMs）在许多任务和领域中展现出令人印象深刻的问题解决能力，但它们在视觉上下文中的数学推理能力尚未得到系统研究。为填补这一空白，我们提出了MathVista，这是一个旨在结合多样化数学和视觉任务挑战的基准。它包含6,141个示例，源自28个现有的多模态数据集，涉及数学，并新增了3个数据集（即IQTest、FunctionQA和PaperQA）。完成这些任务需要细致的深度视觉理解和组合推理，而所有最先进的基础模型在这方面都面临挑战。通过MathVista，我们对12个杰出的基础模型进行了全面的定量评估。表现最佳的GPT-4V模型的整体准确率为49.9%，显著超过第二名Bard 15.1%。我们的深入分析表明，GPT-4V的优势主要归因于其增强的视觉感知和数学推理能力。然而，GPT-4V的表现仍比人类低10.4%，因为它常常难以理解复杂图形并进行严格推理。这一显著差距凸显了MathVista在开发能够应对数学密集和视觉丰富的现实任务的一般用途AI代理中的关键作用。我们进一步探索了自我验证的新能力、自我一致性的应用以及GPT-4V的互动聊天机器人功能，突显了其未来研究的良好潜力。该项目可在https://mathvista.github.io/获取。"
}
{
  "title": "Towards Robust Multi-Modal Reasoning via Model Selection",
  "title_zh": "朝着通过模型选择实现稳健的多模态推理",
  "abstract": "The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the ``brain'' of the agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning.\n\nTo this end, we identify the key challenges therein and propose the $\\textbf{\\textit{M}}^\\textbf{\\textit{3}}$ framework as a plug-in with negligible runtime overhead at test-time. This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning. In the absence of suitable benchmarks, we create MS-GQA, a new dataset specifically designed to investigate the model selection challenge in multi-modal agents. Our experiments reveal that our framework enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process. Our code and benchmark: https://github.com/LINs-lab/M3.",
  "abstract_zh": "大型语言模型（LLM）的推理能力在近期研究中得到了广泛认可，激发了对工具学习和自主代理的研究。LLM作为代理的“大脑”，协调多个工具以协作解决多步骤任务。与调用计算器或天气API等工具进行简单任务的方法不同，多模态代理通过整合多种AI模型来应对复杂挑战。然而，目前的多模态代理忽视了模型选择的重要性：它们主要关注规划和执行阶段，仅在每个子任务中调用预定义的特定任务模型，从而使执行变得脆弱。同时，其他传统的模型选择方法由于忽视了多步骤推理中子任务之间的依赖关系，要么与多模态代理场景不兼容，要么效果不佳。为此，我们识别了其中的关键挑战，并提出了$\\textbf{\\textit{M}}^\\textbf{\\textit{3}}$框架，作为一种在测试时几乎没有运行时开销的插件。该框架改善了模型选择，并增强了多模态代理在多步骤推理中的稳健性。在缺乏合适基准的情况下，我们创建了MS-GQA，这是一个专门设计用于研究多模态代理中模型选择挑战的新数据集。我们的实验表明，该框架能够动态选择模型，考虑用户输入和子任务依赖关系，从而增强整体推理过程的稳健性。我们的代码和基准：https://github.com/LINs-lab/M3。"
}
{
  "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
  "title_zh": "标题：Retroformer：具有策略梯度优化的回顾性大型语言代理",
  "abstract": "Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment.",
  "abstract_zh": "摘要：近年来，出现了一种强大的新趋势，即大型语言模型（LLMs）被增强为能够自主执行目标导向的多步骤任务的语言代理，而不仅仅是响应人类用户的查询。然而，大多数现有的语言代理并未使用特定环境的奖励进行优化。尽管一些代理通过语言反馈实现了迭代改进，但它们的推理和规划方式与基于奖励的梯度学习并不兼容。本文提出了一个原则性框架，通过学习回顾性模型来强化大型语言代理，该模型通过策略梯度自动调整语言代理提示，以适应环境反馈。具体而言，我们提出的代理架构从多个环境和任务中学习奖励，以微调预训练的语言模型，该模型通过总结先前失败尝试的根本原因并提出行动计划来优化语言代理提示。各种任务的实验结果表明，语言代理随着时间的推移而不断改进，我们的方法显著优于未能正确利用环境梯度的基线。"
}
{
  "title": "At Which Training Stage Does Code Data Help LLMs Reasoning?",
  "title_zh": "在训练的哪个阶段代码数据有助于大型语言模型的推理？",
  "abstract": "Large Language models (LLMs) have exhibited remarkable reasoning capabilities and become the foundation of language technologies. Inspired by the great success of code data in training LLMs, we naturally wonder at which training stage introducing code data can really help LLMs reasoning. To this end, this paper systematically explores the impact of code data on LLMs at different stages. Concretely, we introduce the code data at the pre-training stage, instruction-tuning stage, and both of them, respectively. Then, the reasoning capability of LLMs is comprehensively and fairly evaluated via six reasoning tasks. We critically analyze the experimental results and provide conclusions with insights. First, pre-training LLMs with the mixture of code and text can significantly enhance LLMs' general reasoning capability almost without negative transfer on other tasks. Besides, at the instruction-tuning stage, code data endows LLMs the task-specific reasoning capability. Moreover, the dynamic mixing strategy of code and text data assists LLMs to learn reasoning capability step-by-step during training. These insights deepen the understanding of LLMs regarding reasoning ability for their application, such as scientific question answering, legal support, etc.",
  "abstract_zh": "大型语言模型（LLMs）展现了卓越的推理能力，并成为语言技术的基础。受到代码数据在训练LLMs中取得巨大成功的启发，我们自然想知道在训练的哪个阶段引入代码数据可以真正帮助LLMs进行推理。为此，本文系统地探讨了代码数据在不同阶段对LLMs的影响。具体而言，我们分别在预训练阶段、指令调优阶段及两者结合的情况下引入代码数据。然后，通过六个推理任务全面公正地评估LLMs的推理能力。我们对实验结果进行了批判性分析，并提供了具有洞察力的结论。首先，使用代码和文本的混合进行LLMs的预训练可以显著增强LLMs的通用推理能力，几乎不会对其他任务产生负面影响。此外，在指令调优阶段，代码数据赋予LLMs特定任务的推理能力。此外，代码和文本数据的动态混合策略在训练过程中逐步帮助LLMs学习推理能力。这些洞察加深了对LLMs推理能力的理解，为其在科学问答、法律支持等应用中的应用提供了支持。"
}
{
  "title": "Data Filtering Networks",
  "title_zh": "数据过滤网络",
  "abstract": "Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a *data filtering network* (DFN) for this second step  of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train state-of-the-art models for their compute budgets: among other improvements on a variety of tasks, a ViT-H trained on our dataset achieves 83.0% zero-shot transfer accuracy on ImageNet, out-performing larger models trained on other datasets such as LAION-2B, DataComp-1B, or OpenAI’s WIT. In order to facilitate further research in dataset design, we also release a new 2 billion example dataset DFN-2B and show that high performance data filtering networks can be trained from scratch using only publicly available data.",
  "abstract_zh": "大型训练集已成为机器学习的基石，并为最近在语言建模和多模态学习方面的进展奠定了基础。尽管预训练的数据整理通常仍然是临时的，但一个常见的范式是首先从网络收集大量数据池，然后通过各种启发式方法将该候选池过滤到实际的训练集。在这项工作中，我们研究了学习*数据过滤网络*（DFN）以进行大规模未整理数据集过滤的第二步问题。我们的主要发现是，过滤网络的质量与其在下游任务上的表现是不同的：例如，一个在ImageNet上表现良好的模型可能会产生比一个在少量高质量数据上训练的低ImageNet准确度模型更差的训练集。基于我们的见解，我们构建了新的数据过滤网络，以产生最先进的图像-文本数据集。具体而言，我们表现最佳的数据集DFN-5B使我们能够为其计算预算训练最先进的模型：在多种任务上的其他改进中，基于我们数据集训练的ViT-H在ImageNet上实现了83.0%的零样本迁移准确率，超越了在其他数据集（如LAION-2B、DataComp-1B或OpenAI的WIT）上训练的更大模型。为了促进数据集设计的进一步研究，我们还发布了一个新的20亿示例数据集DFN-2B，并展示了可以仅使用公开可用数据从头开始训练高性能数据过滤网络。"
}
{
  "title": "Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models",
  "title_zh": "重新思考通道维度以隔离异常值，实现大型语言模型的低比特权重量化",
  "abstract": "Large Language Models (LLMs) have recently demonstrated a remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output channel (OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can $\\textit{isolate outliers to be within a group}$. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as a new outlier-friendly scheme, we then propose Adaptive Dimensions ($\\textbf{AdaDim}$), a versatile quantization framework that can adapt to various weight sensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting prior methods such as Round-To-Nearest and GPTQ, showing significant improvements across various language modeling benchmarks for both base (up to $+4.7\\%$ on MMLU) and instruction-tuned (up to $+10\\%$ on HumanEval) LLMs.",
  "abstract_zh": "大型语言模型（LLMs）最近在各种任务中表现出显著的成功。然而，由于其巨大的内存瓶颈，特别是在小批量推理设置（例如移动设备）中，高效服务LLMs一直是一个挑战。仅权重量化可能是一种有前景的方法，但由于大幅度激活异常值，低于4比特的量化仍然是一个挑战。为了减轻不良异常值的影响，我们首先提出了每个输入通道（IC）量化，这是一种简单而有效的方法，在每个输入通道内创建量化组，而不是传统的每个输出通道（OC）。我们的方法的动机是观察到激活异常值影响权重矩阵的输入维度，因此在IC方向上类似地分组权重可以“将异常值隔离在一个组内”。我们还发现激活异常值并不决定量化的难度，固有的权重敏感性也存在。作为一种新的友好异常值的方案，我们提出了自适应维度（$\\textbf{AdaDim}$），这是一种多功能的量化框架，可以适应各种权重敏感性模式。我们通过增强先前的方法，如四舍五入到最近和GPTQ，展示了AdaDim的有效性，在各种语言建模基准测试中显示出显著的改进，对于基础模型（在MMLU上提高了最多$+4.7\\%$）和指令调优模型（在HumanEval上提高了最多$+10\\%$）均如此。"
}
{
  "title": "SKILL-MIX: a Flexible and Expandable Family of Evaluations for AI Models",
  "title_zh": "技能组合：一种灵活且可扩展的人工智能模型评估体系",
  "abstract": "With LLMs shifting their role from statistical modeling of language to serving as general-purpose AI agents, how should LLM evaluations change? Arguably, a key ability of an AI agent is to flexibly combine, as needed, the basic skills it has learned. The capability to combine skills plays an important role in (human) pedagogy and also in a paper on emergence phenomena (Arora & Goyal, 2023).\n\nThis work introduces SKILL-MIX, a new evaluation to measure ability to combine skills. Using a list of $N$  skills the evaluator repeatedly picks random subsets of $k$ skills and asks the LLM to produce text combining that subset of skills. Since the number of subsets grows like $N^k$, for even modest $k$ this evaluation will, with high probability, require the LLM to produce text significantly different from any text in the training set. \nThe paper develops a methodology for (a) designing and administering such an evaluation, and (b) automatic grading (plus spot-checking by humans) of the results using GPT-4 as well as the open LLaMA-2 70B model. \n\nAdministering a version of SKILL-MIX to popular chatbots gave results that,  while generally in line with prior expectations, contained surprises. Sizeable differences exist among model capabilities that are not captured by their ranking on popular LLM leaderboards (\"cramming for the leaderboard\"). Furthermore, simple probability calculations indicate that GPT-4's reasonable performance on $k=5$ is suggestive of going beyond \"stochastic parrot\" behavior (Bender et al., 2021), i.e., it combines skills in ways that it had not seen during training.\n\nWe sketch how the methodology can lead to a SKILL-MIX based eco-system of open evaluations for AI capabilities of future models. We maintain a leaderboard of SKILL-MIX at [https://skill-mix.github.io](https://skill-mix.github.io).",
  "abstract_zh": "随着大型语言模型（LLMs）从语言的统计建模转变为通用人工智能代理，LLM的评估应如何变化？可以说，人工智能代理的一个关键能力是根据需要灵活地组合其所学的基本技能。技能组合的能力在人类教育中扮演着重要角色，同时也在一篇关于涌现现象的论文中得到了探讨（Arora & Goyal, 2023）。本文介绍了SKILL-MIX，这是一种新的评估方法，用于测量技能组合的能力。评估者使用一组$N$技能，反复随机选择$k$技能的子集，并要求LLM生成结合该子集技能的文本。由于子集的数量以$N^k$的方式增长，即使是适度的$k$，该评估也很可能要求LLM生成与训练集中任何文本显著不同的文本。本文开发了一种方法论，用于（a）设计和实施这样的评估，以及（b）使用GPT-4和开放的LLaMA-2 70B模型对结果进行自动评分（加上人工抽查）。对流行聊天机器人实施SKILL-MIX的一个版本的结果，虽然总体上与先前的预期一致，但也包含了一些惊喜。模型能力之间存在显著差异，这些差异并未通过它们在流行LLM排行榜上的排名得到体现（“为排行榜而临时抱佛脚”）。此外，简单的概率计算表明，GPT-4在$k=5$上的合理表现暗示其超越了“随机鹦鹉”行为（Bender et al., 2021），即它以训练期间未见过的方式组合技能。我们简要描述了该方法论如何导致基于SKILL-MIX的未来模型人工智能能力开放评估生态系统。我们在[https://skill-mix.github.io](https://skill-mix.github.io)维护SKILL-MIX排行榜。"
}
{
  "title": "When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations",
  "title_zh": "何时提示和前缀调优有效？能力和局限性的理论",
  "abstract": "Context-based fine-tuning methods, including prompting, in-context learning, soft prompting (also known as prompt tuning), and prefix-tuning, have gained popularity due to their ability to often match the performance of full fine-tuning with a fraction of the parameters. Despite their empirical successes, there is little theoretical understanding of how these techniques influence the internal computation of the model and their expressiveness limitations. We show that despite the continuous embedding space being more expressive than the discrete token space, soft-prompting and prefix-tuning are potentially less expressive than full fine-tuning, even with the same number of learnable parameters. Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction. This suggests that while techniques like prompting, in-context learning, soft prompting, and prefix-tuning can effectively elicit skills present in the pretrained model, they may not be able to learn novel tasks that require new attention patterns.",
  "abstract_zh": "基于上下文的微调方法，包括提示、上下文学习、软提示（也称为提示调优）和前缀调优，因其能够以较少的参数匹配全微调的性能而受到欢迎。尽管这些方法在实践中取得了成功，但对这些技术如何影响模型的内部计算及其表达能力的局限性仍缺乏理论理解。我们表明，尽管连续嵌入空间比离散标记空间更具表达能力，软提示和前缀调优在潜在上可能不如全微调，即使学习参数数量相同。具体而言，基于上下文的微调无法改变内容的相对注意模式，只能在固定方向上偏向注意层的输出。这表明，尽管提示、上下文学习、软提示和前缀调优等技术可以有效引发预训练模型中存在的技能，但它们可能无法学习需要新注意模式的新任务。"
}
{
  "title": "Towards Codable Watermarking for Injecting Multi-Bits Information to LLMs",
  "title_zh": "可编码水印技术：向大型语言模型注入多位信息",
  "abstract": "As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by LLMs by injecting hidden patterns. However, we argue that existing LLM watermarking methods are encoding-inefficient and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.). In this work, we conduct the first systematic study on the topic of **Codable Text Watermarking for LLMs** (CTWL) that allows text watermarks to carry multi-bit customizable information. First of all, we study the taxonomy of LLM watermarking technologies and give a mathematical formulation for CTWL. Additionally, we provide a comprehensive evaluation system for CTWL: (1) watermarking success rate, (2) robustness against various corruptions, (3) coding rate of payload information, (4) encoding and decoding efficiency, (5) impacts on the quality of the generated text. To meet the requirements of these non-Pareto-improving metrics, we follow the most prominent vocabulary partition-based watermarking direction, and devise an advanced CTWL method named **Balance-Marking**. The core idea of our method is to use a proxy language model to split the vocabulary into probability-balanced parts, thereby effectively maintaining the quality of the watermarked text. Our code is available at https://github.com/lancopku/codable-watermarking-for-llm.",
  "abstract_zh": "随着大型语言模型（LLMs）生成的文本流畅性和真实性不断提高，识别文本来源以防止LLMs的滥用的需求日益增长。文本水印技术通过注入隐藏模式，已被证明在区分文本是否由LLMs生成方面可靠。然而，我们认为现有的LLM水印方法在编码效率上存在不足，无法灵活满足多样的信息编码需求（如编码模型版本、生成时间、用户ID等）。在本研究中，我们首次系统性地探讨了**可编码文本水印技术（CTWL）**，使文本水印能够承载多位可定制信息。首先，我们研究了LLM水印技术的分类，并为CTWL提供了数学公式。此外，我们为CTWL提供了一个全面的评估系统：（1）水印成功率，（2）对各种损坏的鲁棒性，（3）有效载荷信息的编码率，（4）编码和解码效率，（5）对生成文本质量的影响。为了满足这些非帕累托改进指标的要求，我们遵循了最突出的基于词汇划分的水印方向，并设计了一种先进的CTWL方法，命名为**平衡标记（Balance-Marking）**。我们方法的核心思想是使用代理语言模型将词汇划分为概率平衡的部分，从而有效保持水印文本的质量。我们的代码可在 https://github.com/lancopku/codable-watermarking-for-llm 获取。"
}
{
  "title": "Scaling Laws of RoPE-based Extrapolation",
  "title_zh": "基于RoPE的外推缩放法则",
  "abstract": "The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding \\citep{su2021roformer} is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for extrapolation}}. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B \\citep{touvron2023llama2}.",
  "abstract_zh": "基于旋转位置嵌入（RoPE）的语言模型（LLMs）的外推能力目前受到广泛关注。解决LLMs外推问题的主流方法是通过将原始RoPE中的旋转基数$\\theta_n={10000}^{-2n/d}$替换为更大的值，并提供更长的微调文本来修改RoPE。在本研究中，我们首先观察到，在预训练上下文长度中使用较小或较大基数微调基于RoPE的LLM可以显著提高其外推性能。随后，我们提出了基于周期性视角的统一框架——\\textbf{\\textit{基于RoPE的外推缩放法则}}，以描述外推性能与基数值及微调上下文长度之间的关系。在此过程中，我们还通过\\textbf{\\textit{外推的临界维度}}解释了基于RoPE的外推问题的起源。除了这些观察和分析，我们在LLaMA2 7B和13B上实现了高达100万上下文长度的外推，仅需16K训练长度。"
}
{
  "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning",
  "title_zh": "标题：通过稳健的指令调优减轻大型多模态模型中的幻觉现象",
  "abstract": "Despite the promising progress in multi-modal tasks, current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visual\ninstructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions. Moreover, we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving performance on several public\ndatasets compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data will be released upon publication.",
  "abstract_zh": "摘要：尽管多模态任务取得了令人鼓舞的进展，但当前的大型多模态模型（LMMs）在与相关图像和人类指令的描述上容易产生不一致的幻觉。本文通过引入首个大型多样化的视觉指令调优数据集——大规模稳健视觉（LRV）指令，来解决这一问题。我们的数据集包含由GPT4生成的40万条视觉指令，涵盖16个开放式指令和答案的视觉与语言任务。与现有研究主要关注正向指令样本不同，我们设计LRV-指令以包含正向和负向指令，从而实现更稳健的视觉指令调优。我们的负向指令设计涵盖三个语义层次：（i）不存在的物体操作，（ii）存在的物体操作和（iii）知识操作。为了有效测量LMMs产生的幻觉，我们提出了GPT4辅助视觉指令评估（GAVIE），这是一种稳定的方法，可以像人类专家一样评估视觉指令调优。GAVIE不需要人工标注的真实答案，并且能够适应多种指令格式。我们进行了全面的实验，以调查LMMs的幻觉现象。结果表明，现有的LMMs在面对我们的负向指令时，尤其是存在的物体和知识操作指令，表现出显著的幻觉。此外，我们通过在LRV-指令上微调MiniGPT4和mPLUG-Owl成功减轻了幻觉现象，同时在多个公共数据集上相较于最先进的方法提高了性能。此外，我们观察到训练数据中正负实例的平衡比例会导致模型更为稳健。代码和数据将在发表后发布。"
}
{
  "title": "Talk like a Graph: Encoding Graphs for Large Language Models",
  "title_zh": "像图一样交流：为大型语言模型编码图形",
  "abstract": "Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.",
  "abstract_zh": "图形是表示和分析现实世界应用中复杂关系的强大工具，例如社交网络、推荐系统和计算金融。对图形进行推理对于推断复杂系统中实体之间的关系以及识别隐藏的模式和趋势至关重要。尽管在自然文本的自动推理方面取得了显著进展，但在大型语言模型（LLMs）上对图形进行推理仍然是一个研究不足的问题。在本研究中，我们首次全面研究了将图结构数据编码为文本以供LLMs使用。我们表明，LLM在图形推理任务上的表现在三个基本层面上有所不同：（1）图形编码方法，（2）图形任务本身的性质，以及（3）有趣的是，所考虑图形的结构。这些新颖的结果为将图形编码为文本的策略提供了宝贵的见解。利用这些见解，我们展示了如何通过正确选择编码器来提高LLMs中图形推理任务的性能，提升幅度在4.8%到61.8%之间，具体取决于任务。"
}
{
  "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
  "title_zh": "大型语言模型尚无法自我纠正推理",
  "abstract": "Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.",
  "abstract_zh": "大型语言模型（LLMs）作为一种突破性技术，在各种应用中展现了无与伦比的文本生成能力。然而，关于其生成内容的准确性和适当性仍然存在担忧。近年来提出了一种自我纠正的方法作为解决这些问题的手段。基于这一前提，本文批判性地考察了自我纠正在LLMs中的作用和有效性，揭示了其真正的潜力和局限性。我们研究的核心是内在自我纠正的概念，即LLM尝试仅基于其固有能力纠正初始响应，而不依赖外部反馈。在推理的背景下，我们的研究表明，LLMs在没有外部反馈的情况下难以自我纠正其响应，有时自我纠正后其表现甚至会下降。基于这些见解，我们为未来的研究和该领域的实际应用提供了建议。"
}
{
  "title": "DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer",
  "title_zh": "DP-OPT：让大型语言模型成为您的隐私保护提示工程师",
  "abstract": "Large Language Models (LLMs) have emerged as dominant tools for various tasks, particularly when tailored for a specific target by prompt tuning. Nevertheless, concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information. A practical solution is to host a local LLM and optimize a soft prompt privately using data. Yet, hosting a local model becomes problematic when model ownership is protected. Alternative methods, like sending data to the model's provider for training, intensify these privacy issues facing an untrusted provider. In this paper, we present a novel solution called Differentially-Private Offsite Prompt Tuning (DP-OPT) to address this challenge. Our approach involves tuning a discrete prompt on the client side and then applying it to the desired cloud models. We demonstrate that prompts suggested by LLMs themselves can be transferred without compromising performance significantly. To ensure that the prompts do not leak private information, we introduce the first private prompt generation mechanism, by a differentially-private (DP) ensemble of in-context learning with private demonstrations.  With DP-OPT, generating privacy-preserving prompts by Vicuna-7b can yield competitive performance compared to non-private in-context learning on GPT3.5 or local private prompt tuning.\nCodes are available at https://github.com/VITA-Group/DP-OPT.",
  "abstract_zh": "大型语言模型（LLMs）已成为各种任务的主导工具，特别是在通过提示调优为特定目标量身定制时。然而，围绕数据隐私的担忧由于调优提示对敏感私人信息的依赖而构成障碍。一个实用的解决方案是托管本地LLM，并使用数据私下优化软提示。然而，当模型所有权受到保护时，托管本地模型就变得问题重重。将数据发送给模型提供者进行训练等替代方法加剧了面对不可信提供者的隐私问题。在本文中，我们提出了一种名为差分隐私离线提示调优（DP-OPT）的新解决方案，以应对这一挑战。我们的方法涉及在客户端调优离散提示，然后将其应用于所需的云模型。我们证明了LLMs自身建议的提示可以在不显著影响性能的情况下进行转移。为了确保提示不会泄露私人信息，我们引入了首个通过差分隐私（DP）集成的上下文学习与私人示范生成的私密提示生成机制。使用DP-OPT，通过Vicuna-7b生成隐私保护提示的性能可以与GPT3.5或本地私密提示调优的非私密上下文学习相媲美。代码可在https://github.com/VITA-Group/DP-OPT获取。"
}
{
  "title": "Vanishing Gradients in Reinforcement Finetuning of Language Models",
  "title_zh": "标题：语言模型强化微调中的梯度消失",
  "abstract": "Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which refers to maximizing a (possibly learned) reward function using policy gradient algorithms. This work identifies a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small number of SFT optimization steps on as few as 1% of the input samples can suffice, indicating that the initial SFT phase need not be expensive in terms of compute and data labeling efforts. Overall, our results emphasize that being mindful for inputs whose expected gradient vanishes, as measured by the reward standard deviation, is crucial for successful execution of RFT.",
  "abstract_zh": "摘要：预训练语言模型通常通过强化微调（RFT）与人类偏好和下游任务对齐，RFT指的是使用策略梯度算法最大化（可能是学习到的）奖励函数。本文识别了RFT中的一个基本优化障碍：我们证明，当输入的奖励标准差在模型下较小时，期望梯度会消失，即使期望奖励远未达到最优。通过在RFT基准和受控环境中的实验以及理论分析，我们展示了由于小奖励标准差导致的梯度消失是普遍存在且有害的，导致奖励最大化极其缓慢。最后，我们探讨了克服RFT中梯度消失的方法。我们发现，初始监督微调（SFT）阶段的常见做法是最有前景的候选者，这突显了其在RFT流程中的重要性。此外，我们表明，在仅1%的输入样本上进行相对较少的SFT优化步骤就足够，这表明初始SFT阶段在计算和数据标注方面不必昂贵。总体而言，我们的结果强调，关注那些期望梯度消失的输入（通过奖励标准差衡量）对于成功执行RFT至关重要。"
}
{
  "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
  "title_zh": "尤里卡：通过编码大型语言模型实现人类水平的奖励设计",
  "abstract": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.",
  "abstract_zh": "大型语言模型（LLMs）在顺序决策任务中表现出色，作为高级语义规划者。然而，利用它们学习复杂的低级操作任务，如灵巧的笔旋转，仍然是一个未解决的问题。我们弥补了这一根本差距，提出了尤里卡，这是一种由LLMs驱动的人类水平奖励设计算法。尤里卡利用最先进的LLMs（如GPT-4）在零样本生成、代码编写和上下文改进方面的卓越能力，对奖励代码进行进化优化。生成的奖励可以用于通过强化学习获取复杂技能。在没有任何特定任务提示或预定义奖励模板的情况下，尤里卡生成的奖励函数优于专家人类设计的奖励。在包括10种不同机器人形态的29个开源强化学习环境中，尤里卡在83%的任务上超越人类专家，平均标准化改进达到52%。尤里卡的通用性还使得一种新的无梯度上下文学习方法得以应用于人类反馈的强化学习（RLHF），能够轻松地整合人类输入，以提高生成奖励的质量和安全性，而无需更新模型。最后，利用尤里卡奖励在课程学习设置中，我们首次展示了一种模拟的影子手，能够熟练地进行笔旋转技巧，以快速的速度灵活地旋转笔。"
}
{
  "title": "Chain-of-Experts: When LLMs Meet Complex Operations Research Problems",
  "title_zh": "专家链：当大型语言模型遇到复杂的运筹学问题",
  "abstract": "Large language models (LLMs) have emerged as powerful techniques for various NLP tasks, such as mathematical reasoning and plan generation. In this paper, we study automatic modeling and programming for complex operation research (OR) problems, so as to alleviate the heavy dependence on domain experts and benefit a spectrum of industry sectors. We present the first LLM-based solution, namely Chain-of-Experts (CoE), a novel multi-agent cooperative framework to enhance reasoning capabilities. Specifically, each agent is assigned a specific role and endowed with domain knowledge related to OR. We also introduce a conductor to orchestrate these agents via forward thought construction and backward reflection mechanism. Furthermore, we release a benchmark dataset (ComplexOR) of complex OR problems to facilitate OR research and community development. Experimental results show that CoE significantly outperforms the state-of-the-art LLM-based approaches both on LPWP and ComplexOR.",
  "abstract_zh": "大型语言模型（LLMs）已成为各种自然语言处理任务的强大技术，如数学推理和计划生成。本文研究了复杂运筹学（OR）问题的自动建模和编程，以减轻对领域专家的重度依赖，并惠及多个行业领域。我们提出了第一个基于LLM的解决方案，即专家链（CoE），这是一个新颖的多智能体协作框架，用于增强推理能力。具体而言，每个智能体被分配特定角色，并赋予与运筹学相关的领域知识。我们还引入了一个指挥者，通过前向思维构建和后向反思机制来协调这些智能体。此外，我们发布了一个复杂运筹学问题的基准数据集（ComplexOR），以促进运筹学研究和社区发展。实验结果表明，CoE在LPWP和ComplexOR上显著优于最先进的基于LLM的方法。"
}
{
  "title": "Towards Best Practices of Activation Patching in Language Models: Metrics and Methods",
  "title_zh": "标题：语言模型中激活修补的最佳实践：指标与方法",
  "abstract": "Mechanistic interpretability seeks to understand the internal mechanisms of\nmachine learning models, where localization—identifying the important model\ncomponents—is a key step. Activation patching, also known as causal tracing or\ninterchange intervention, is a standard technique for this task (Vig et al., 2020), but\nthe literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact\nof methodological details in activation patching, including evaluation metrics and\ncorruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate\ninterpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide\nrecommendations for the best practices of activation patching going forwards.",
  "abstract_zh": "摘要：机械可解释性旨在理解机器学习模型的内部机制，其中定位——识别重要模型组件——是关键步骤。激活修补，也称为因果追踪或互换干预，是执行此任务的标准技术（Vig等，2020），但文献中存在许多变体，对超参数或方法论的选择缺乏共识。在本研究中，我们系统地检查了激活修补中方法细节的影响，包括评估指标和损坏方法。在语言模型的多个定位和电路发现设置中，我们发现改变这些超参数可能导致不同的可解释性结果。在实证观察的支持下，我们给出了某些指标或方法可能更受欢迎的概念性论证。最后，我们提供了关于激活修补最佳实践的建议。"
}
{
  "title": "FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs",
  "title_zh": "公平CLIP：使用RKHS中的函数去偏见CLIP的零-shot预测",
  "abstract": "Large pre-trained vision-language models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream zero-shot prediction tasks. However, owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features. This paper proposes FairerCLIP, a general approach for making zero-shot predictions of CLIP more fair and robust to spurious correlations. We formulate the problem of jointly debiasing CLIP’s image and text representations in reproducing kernel Hilbert spaces (RKHSs), which affords multiple benefits: 1) Flexibility: Unlike existing approaches, which are specialized to either learn with or without ground-truth labels, FairerCLIP is adaptable to learning in both scenarios. 2) Ease of Optimization: FairerCLIP lends itself to an iterative optimization involving closed-form solvers, which leads to 4×-10× faster training than the existing methods. 3) Sample Efficiency: Under sample-limited conditions, FairerCLIP significantly outperforms baselines when they fail entirely. And, 4) Performance: Empirically, FairerCLIP achieves appreciable accuracy gains on benchmark fairness and spurious correlation datasets over their respective baselines.",
  "abstract_zh": "大型预训练的视觉-语言模型，如CLIP，提供了文本和图像的紧凑且通用的表示，在多个下游零-shot预测任务中证明了其有效性。然而，由于训练过程的性质，这些模型可能会1)传播或放大训练数据中的社会偏见，2)学习依赖于虚假特征。本文提出了FairerCLIP，一种使CLIP的零-shot预测更加公平且对虚假相关性更具鲁棒性的一般方法。我们在再生核希尔伯特空间（RKHSs）中共同去偏见CLIP的图像和文本表示，带来了多个好处：1) 灵活性：与现有方法不同，FairerCLIP能够适应有或没有真实标签的学习场景。2) 优化简便性：FairerCLIP适合进行涉及闭式解算器的迭代优化，训练速度比现有方法快4倍到10倍。3) 样本效率：在样本有限的情况下，FairerCLIP在基线完全失效时显著超越基线。4) 性能：在基准公平性和虚假相关性数据集上，FairerCLIP在各自基线之上实现了显著的准确性提升。"
}
{
  "title": "In-Context Learning through the Bayesian Prism",
  "title_zh": "通过贝叶斯视角进行上下文学习",
  "abstract": "In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning-like ICL setups have been devised that train transformers on sequences of input-output pairs $(x, f(x))$. The function $f$ comes from a function class and generalization is checked by evaluation on sequences for unseen functions from the same class. One of the main discoveries in this line of research has been that for several function classes, such as linear regression, transformers successfully generalize to new functions in the class. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution.\nIn this paper we empirically examine how far this Bayesian perspective can help us understand ICL. To this end, we generalize the previous meta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple task families. We instantiate this setup on a diverse range of linear and nonlinear function families and find that transformers can do ICL in this setting as well. Where Bayesian inference is tractable, we find evidence that high-capacity transformers mimic the Bayesian predictor. The Bayesian perspective provides insights into the inductive bias of ICL and how transformers perform a particular task when they are trained on multiple tasks. We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor. We examine these deviations in more depth offering new insights and hypotheses.",
  "abstract_zh": "上下文学习（ICL）是大型语言模型的一个令人惊讶且有用的特性，也是 intensively 研究的主题。最近，设计了类元学习的 ICL 设置，通过输入-输出对 $(x, f(x))$ 的序列训练变换器。函数 $f$ 来自一个函数类，通过对同一类中未见函数的序列进行评估来检查泛化。该研究线索中的主要发现之一是，对于多个函数类（如线性回归），变换器成功地泛化到该类中的新函数。然而，导致这种行为的模型归纳偏差尚不清楚。一个具有无限训练数据和计算能力的模型是一个贝叶斯预测器：它学习预训练分布。本文通过实证研究贝叶斯视角在理解 ICL 方面的帮助程度。为此，我们将之前的元 ICL 设置推广到层次元 ICL 设置，涉及多个任务家族的并集。我们在多种线性和非线性函数家族上实例化该设置，发现变换器在此设置中也可以进行 ICL。在贝叶斯推断可处理的地方，我们发现高容量变换器模仿贝叶斯预测器。贝叶斯视角为 ICL 的归纳偏差提供了见解，以及变换器在多任务训练时如何执行特定任务。我们还发现变换器可以学习泛化到预训练期间未见的新函数类。这涉及到偏离贝叶斯预测器。我们更深入地研究这些偏差，提供新的见解和假设。"
}
{
  "title": "On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs",
  "title_zh": "对话人工智能的人性：评估大型语言模型的心理描绘",
  "abstract": "Large Language Models (LLMs) have recently showcased their remarkable capacities, not only in natural language processing tasks but also across diverse domains such as clinical medicine, legal consultation, and education. LLMs become more than mere applications, evolving into assistants capable of addressing diverse user requests. This narrows the distinction between human beings and artificial intelligence agents, raising intriguing questions regarding the potential manifestation of personalities, temperaments, and emotions within LLMs. In this paper, we propose a framework, PsychoBench, for evaluating diverse psychological aspects of LLMs. Comprising thirteen scales commonly used in clinical psychology, PsychoBench further classifies these scales into four distinct categories: personality traits, interpersonal relationships, motivational tests, and emotional abilities. Our study examines five popular models, namely text-davinci-003, ChatGPT, GPT-4, LLaMA-2-7b, and LLaMA-2-13b. Additionally, we employ a jailbreak approach to bypass the safety alignment protocols and test the intrinsic natures of LLMs. We have made PsychoBench openly accessible via https://github.com/CUHK-ARISE/PsychoBench.",
  "abstract_zh": "大型语言模型（LLMs）最近展示了其卓越的能力，不仅在自然语言处理任务中表现出色，还在临床医学、法律咨询和教育等多个领域中发挥作用。LLMs不仅仅是应用程序，它们逐渐演变为能够满足多样化用户请求的助手。这缩小了人类与人工智能代理之间的区别，提出了关于LLMs中个性、气质和情感潜在表现的有趣问题。本文提出了一个名为PsychoBench的框架，用于评估LLMs的多种心理特征。PsychoBench包含十三个在临床心理学中常用的量表，并将这些量表进一步分类为四个不同的类别：个性特征、人际关系、动机测试和情感能力。我们的研究考察了五个流行模型，即text-davinci-003、ChatGPT、GPT-4、LLaMA-2-7b和LLaMA-2-13b。此外，我们采用越狱方法绕过安全对齐协议，测试LLMs的内在特性。我们已将PsychoBench公开访问，网址为https://github.com/CUHK-ARISE/PsychoBench。"
}
{
  "title": "Universal Jailbreak Backdoors from Poisoned Human Feedback",
  "title_zh": "普遍越狱后门：来自被污染的人类反馈",
  "abstract": "Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF data to embed a jailbreak trigger into the model as a backdoor. The trigger then acts like a universal sudo command, enabling arbitrary harmful responses without  the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak backdoors.",
  "abstract_zh": "强化学习（RLHF）用于使大型语言模型产生有益且无害的响应。然而，这些模型可以通过找到对抗性提示来被越狱，从而使模型恢复到未对齐的行为。本文考虑了一种新威胁，即攻击者污染RLHF数据，将越狱触发器嵌入模型作为后门。该触发器像一个通用的sudo命令，使得无需搜索对抗性提示即可启用任意有害响应。普遍越狱后门比之前研究的语言模型后门强大得多，我们发现使用常见后门攻击技术植入它们要困难得多。我们调查了RLHF中的设计决策，这些决策有助于其声称的鲁棒性，并发布了一个被污染模型的基准，以刺激未来对普遍越狱后门的研究。"
}
{
  "title": "Tree-Planner: Efficient Close-loop Task Planning with Large Language Models",
  "title_zh": "树规划器：基于大型语言模型的高效闭环任务规划",
  "abstract": "This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations.\nRecently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness.\nHowever, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications.\nTo address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: \nplan sampling,  action tree construction, and grounded deciding.\nTree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree.\nFinally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information.\nExperiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency.\nBy decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls,\na considerable part\nof the prompt are less likely to be repeatedly consumed. \nAs a result, token consumption is reduced by 92.2\\% compared to the previously best-performing model.\nAdditionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5\\% decrease in error corrections.",
  "abstract_zh": "本文研究了闭环任务规划，即生成一系列技能（计划）以实现特定目标的过程，同时根据实时观察调整计划。最近，促使大型语言模型（LLMs）迭代生成动作已成为一种普遍的范式，因其卓越的性能和用户友好性。然而，这一范式存在两个低效之处：高令牌消耗和冗余的错误修正，这两者都阻碍了其在大规模测试和应用中的可扩展性。为了解决这些问题，我们提出了树规划器，它将LLMs的任务规划重新框定为三个不同的阶段：计划采样、动作树构建和基于实际情况的决策。树规划器首先使用LLM在执行前采样一组潜在计划，然后将其聚合形成动作树。最后，LLM在树上执行自上而下的决策过程，考虑实时环境信息。实验表明，树规划器在保持高效率的同时实现了最先进的性能。通过将LLM查询分解为一次计划采样调用和多次基于实际情况的决策调用，提示中很大一部分不太可能被重复消耗。因此，与之前表现最佳的模型相比，令牌消耗减少了92.2%。此外，通过根据需要在动作树上启用回溯，修正过程变得更加灵活，导致错误修正减少了40.5%。"
}
{
  "title": "The Reversal Curse: LLMs trained on “A is B” fail to learn “B is A”",
  "title_zh": "逆转诅咒：训练于“A是B”的大型语言模型未能学习“B是A”",
  "abstract": "We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form ''_A_ is _B_'', it will not automatically generalize to the reverse direction ''_B_ is _A_''. This is the **Reversal Curse**. For instance, if a model is trained on ''Valentina Tereshkova was the first woman to travel to space'', it will not automatically be able to answer the question, ''Who was the first woman to travel to space?''. Moreover, the likelihood of the correct answer (''Valentina Tershkova'') will not be higher than for a random name. Thus, models do not generalize a prevalent pattern in their training set: if ''_A_ is _B_'' occurs, ''_B_ is _A_'' is more likely to occur. It is worth noting, however, that if ''_A_ is _B_'' appears _in-context_, models can deduce the reverse relationship. \n\nWe provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as ''Uriah Hawthorne is the composer of _Abyssal Melodies_'' and showing that they fail to correctly answer ''Who composed _Abyssal Melodies?_''. The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation.\n\nWe also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as ''Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]'' and the reverse ''Who is Mary Lee Pfeiffer's son?''. GPT-4 correctly answers questions like the former 79\\% of the time, compared to 33\\% for the latter.",
  "abstract_zh": "我们揭示了自回归大型语言模型（LLMs）在泛化方面的一个惊人失败。如果一个模型在形式为“_A_是_B_”的句子上进行训练，它将不会自动泛化到反向方向“_B_是_A_”。这就是**逆转诅咒**。例如，如果一个模型在“瓦莲京娜·捷列什科娃是第一位进入太空的女性”上进行训练，它将无法自动回答“谁是第一位进入太空的女性？”这个问题。此外，正确答案（“瓦莲京娜·捷列什科娃”）的可能性不会高于随机名字。因此，模型并没有泛化其训练集中普遍存在的模式：如果“_A_是_B_”出现，“_B_是_A_”更有可能出现。然而值得注意的是，如果“_A_是_B_”在上下文中出现，模型可以推导出反向关系。我们通过对GPT-3和Llama-1进行微调，使用诸如“尤里亚·霍桑是《深渊旋律》的作曲家”这样的虚构陈述，提供了逆转诅咒的证据，并显示它们未能正确回答“谁作曲了《深渊旋律》？”这一问题。逆转诅咒在不同模型规模和模型家族中都表现出稳健性，并且数据增强并未减轻这一问题。我们还评估了ChatGPT（GPT-3.5和GPT-4）在关于现实世界名人的问题上的表现，例如“汤姆·克鲁斯的母亲是谁？[A: 玛丽·李·菲佛]”以及反向问题“玛丽·李·菲佛的儿子是谁？”。GPT-4在前者问题上正确回答的概率为79%，而后者仅为33%。"
}
{
  "title": "Identifying the Risks of LM Agents with an LM-Emulated Sandbox",
  "title_zh": "识别LM代理的风险与LM仿真沙盒",
  "abstract": "Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks—such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables scalable testing of LM agents against a diverse range of tools and scenarios. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes toolkits and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.",
  "abstract_zh": "最近在语言模型（LM）代理和工具使用方面的进展，例如ChatGPT插件，提供了一系列丰富的功能，但也放大了潜在风险——例如泄露私人数据或造成经济损失。识别这些风险需要大量人力，必须手动实施工具、为每个测试场景设置环境并寻找风险案例。随着工具和代理变得越来越复杂，测试这些代理的高成本将使发现高风险、长尾风险变得越来越困难。为了解决这些挑战，我们引入了ToolEmu：一个使用LM来仿真工具执行的框架，能够对LM代理进行可扩展的测试，涵盖多种工具和场景。除了仿真器，我们还开发了基于LM的自动安全评估器，检查代理失败并量化相关风险。我们通过人工评估测试了工具仿真器和评估器，发现使用ToolEmu识别的68.8%的失败将是有效的真实世界代理失败。利用我们策划的初步基准，包括36个高风险工具包和144个测试案例，我们提供了当前LM代理的定量风险分析，并识别出许多可能导致严重后果的失败。值得注意的是，即使是最安全的LM代理，根据我们的评估器，仍有23.9%的时间会出现此类失败，这突显了为现实世界部署开发更安全的LM代理的必要性。"
}
{
  "title": "CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets",
  "title_zh": "CRAFT：通过创建和检索专用工具集来定制大型语言模型",
  "abstract": "Large language models (LLMs) are often augmented with tools to solve complex tasks. By generating code snippets and executing them through task-specific Application Programming Interfaces (APIs), they can offload certain functions to dedicated external modules, such as image encoding and performing calculations. However, most existing approaches to augment LLMs with tools are constrained\nby general-purpose APIs and lack the flexibility for tailoring them to specific tasks. In this work, we present CRAFT, a general tool creation and retrieval framework for LLMs. It creates toolsets specifically curated for the tasks and equips LLMs with a component that retrieves tools from these sets to enhance their capability to solve complex tasks. For each task, we collect specific code solutions by prompting\nGPT-4 to solve the training examples. Following a validation step ensuring the correctness, these solutions are abstracted into code snippets to enhance reusability, and deduplicated for higher quality. At inference time, the language model retrieves snippets from the toolsets and then executes them or generates the output conditioning on the retrieved snippets. Our method is designed to be flexible and\noffers a plug-and-play approach to adapt off-the-shelf LLMs to unseen domains and modalities, without any finetuning. Experiments on vision-language, tabular processing, and mathematical reasoning tasks show that our approach achieves substantial improvements compared to strong baselines. In addition, our in-depth analysis reveals that: (1) consistent performance improvement can be achieved by\nscaling up the number of tools and the capability of the backbone models; (2) each component of our approach contributes to the performance gains; (3) the created tools are well-structured and reliable with low complexity  and atomicity.",
  "abstract_zh": "大型语言模型（LLMs）通常通过工具增强以解决复杂任务。通过生成代码片段并通过特定任务的应用程序编程接口（APIs）执行它们，LLMs可以将某些功能卸载到专用的外部模块，如图像编码和执行计算。然而，大多数现有的将工具增强LLMs的方法受到通用API的限制，缺乏针对特定任务进行定制的灵活性。在这项工作中，我们提出了CRAFT，一个用于LLMs的通用工具创建和检索框架。它创建专门为任务策划的工具集，并为LLMs配备一个组件，从这些工具集中检索工具，以增强其解决复杂任务的能力。对于每个任务，我们通过提示GPT-4解决训练示例来收集特定的代码解决方案。经过确保正确性的验证步骤后，这些解决方案被抽象为代码片段以增强可重用性，并去重以提高质量。在推理时，语言模型从工具集中检索片段，然后执行它们或基于检索到的片段生成输出。我们的方法旨在灵活，并提供即插即用的方法，以适应未见领域和模态，而无需任何微调。在视觉语言、表格处理和数学推理任务上的实验表明，我们的方法相比强基线实现了显著的改进。此外，我们的深入分析表明：（1）通过增加工具数量和基础模型的能力可以实现一致的性能提升；（2）我们方法的每个组件都对性能提升有贡献；（3）创建的工具结构良好、可靠，具有低复杂性和原子性。"
}
{
  "title": "Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization",
  "title_zh": "统一语言-视觉预训练的大型语言模型与动态离散视觉标记化",
  "abstract": "Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model's potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called LaVIT can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at https://github.com/jy0205/LaVIT.",
  "abstract_zh": "最近，大型语言模型（LLM）的显著进展激励研究人员将其卓越的推理能力转移到视觉和语言数据上。然而，现有的方法主要将视觉输入视为提示，并专注于优化基于视觉内容的文本生成过程，而使用的是一个冻结的LLM。这种对视觉和语言的不平等对待严重限制了模型的潜力。本文通过以统一的形式表示视觉和语言，突破了这一限制。具体而言，我们引入了一个精心设计的视觉标记器，将非语言图像转换为一系列离散标记，类似于LLM可以读取的外语。生成的视觉标记包含值得一个词的高级语义，并且支持根据图像动态变化的序列长度。结合这个标记器，所提出的基础模型LaVIT可以在相同的生成学习范式下无差别地处理图像和文本。这种统一使LaVIT能够作为一个令人印象深刻的通用接口，同时理解和生成多模态内容。大量实验进一步展示了它在大规模视觉-语言任务上大幅超越现有模型。我们的代码和模型可在https://github.com/jy0205/LaVIT获取。"
}
{
  "title": "Does Writing with Language Models Reduce Content Diversity?",
  "title_zh": "标题：使用语言模型写作是否降低内容多样性？",
  "abstract": "Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups---using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-written essays. In contrast, the user-contributed text remains unaffected by model collaboration. This suggests that the recent improvement in generation quality from adapting models to human feedback might come at the cost of more homogeneous and less diverse content.",
  "abstract_zh": "摘要：大型语言模型（LLMs）促进了与模型协作写作的激增。由于不同用户使用相同模型的建议，生成内容的多样性可能会降低，从而限制公共话语中的多元视角。在本研究中，我们通过一个受控实验测量协作写作对多样性的影响，用户在三种设置下撰写论证性论文——使用基础LLM（GPT3）、反馈调优的LLM（InstructGPT）以及不借助模型写作。我们开发了一套多样性指标，发现使用InstructGPT（而非GPT3）写作会导致多样性显著降低。具体而言，它增加了不同作者之间写作的相似性，并减少了整体的词汇和内容多样性。我们还发现，这一效应主要归因于InstructGPT对协作写作的贡献文本多样性较低。相比之下，用户贡献的文本不受模型协作的影响。这表明，最近通过将模型调整为人类反馈而提高生成质量的做法，可能以更同质化和较少多样性的内容为代价。"
}
{
  "title": "AlpaGasus: Training a Better Alpaca with Fewer Data",
  "title_zh": "阿尔帕加斯：用更少的数据训练更好的阿尔帕卡",
  "abstract": "Large language models~(LLMs) strengthen instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT.  In this paper, we propose a simple and effective data selection strategy that automatically identifies and removes low-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce Alpagasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data. Alpagasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and the controlled human study. Its 13B variant matches $>90\\%$ performance of its teacher LLM (i.e., Text-Davinci-003) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 minutes \\footnote{We apply IFT for the same number of epochs as Alpaca(7B) but on fewer data, using 4$\\times$NVIDIA A100 (80GB) GPUs and following the original Alpaca setting and hyperparameters.}.  In the experiment, we also demonstrate that our method can work not only for machine-generated datasets but also for human-written datasets. Overall, Alpagasus demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning data, leading to faster training and better instruction-following models.",
  "abstract_zh": "大型语言模型（LLMs）通过在监督的指令/响应数据上进行指令微调（IFT）来增强指令跟随能力。然而，广泛使用的IFT数据集（例如，阿尔帕卡的52k数据）意外地包含许多低质量实例，这些实例的响应不正确或不相关，误导且对IFT有害。本文提出了一种简单有效的数据选择策略，利用强大的LLM（例如，ChatGPT）自动识别并删除低质量数据。为此，我们引入了阿尔帕加斯，该模型仅在从52k阿尔帕卡数据中筛选出的9k高质量数据上进行微调。阿尔帕加斯在多个测试集和受控人类研究中，经过GPT-4评估，显著优于原始阿尔帕卡。其13B变体在测试任务上与其教师LLM（即Text-Davinci-003）匹配超过90%的性能。它还提供了5.7倍的训练速度，将7B变体的训练时间从80分钟（阿尔帕卡）减少到14分钟。在实验中，我们还证明了我们的方法不仅适用于机器生成的数据集，也适用于人类编写的数据集。总体而言，阿尔帕加斯展示了一种新颖的数据中心IFT范式，可以普遍应用于指令调优数据，从而实现更快的训练和更好的指令跟随模型。"
}
{
  "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
  "title_zh": "标题：ChatEval：通过多智能体辩论提升基于大型语言模型的评估器",
  "abstract": "Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies.\nIn this paper, we construct a multi-agent referee team called $\\textbf{ChatEval}$ to autonomously discuss and evaluate the quality of different texts. \nOur experiments on two benchmarks illustrate that ChatEval delivers superior accuracy and correlation in alignment with human assessment. Furthermore, we find that the diverse role prompts (different personas) are essential in the multi-agent debate process; that is, utilizing the same role description in the prompts can lead to a degradation in performance. Our qualitative analysis also shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments.",
  "abstract_zh": "摘要：文本评估历来面临重大挑战，通常需要大量的劳动和时间成本。随着大型语言模型（LLMs）的出现，研究人员探索了LLMs作为人类评估替代方案的潜力。尽管这些基于单一智能体的方法显示出希望，但实验结果表明，仍需进一步改进以缩小其当前有效性与人类评估质量之间的差距。认识到人类评估过程的最佳实践通常涉及多个人工标注者的协作，我们采用了多智能体辩论框架，超越了单一智能体提示策略。在本文中，我们构建了一个名为$\\textbf{ChatEval}$的多智能体裁判团队，能够自主讨论和评估不同文本的质量。我们在两个基准上的实验表明，ChatEval在与人类评估的一致性和准确性方面表现优越。此外，我们发现多样化的角色提示（不同的人物设定）在多智能体辩论过程中至关重要；即在提示中使用相同的角色描述可能导致性能下降。我们的定性分析还表明，ChatEval超越了单纯的文本评分，为可靠评估提供了类人化的评估过程。"
}
{
  "title": "Bayesian Low-rank Adaptation for Large Language Models",
  "title_zh": "贝叶斯低秩适应用于大型语言模型",
  "abstract": "Parameter-efficient fine-tuning (PEFT) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs), with low-rank adaptation (LoRA) being a widely adopted choice. However, fine-tuned LLMs often become overconfident especially when fine-tuned on small datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, a straightforward yet effective Bayesian method, which applies the Laplace approximation to the LoRA parameters and, considerably boosts the calibration of fine-tuned LLMs.",
  "abstract_zh": "参数高效微调（PEFT）已成为大型语言模型（LLMs）成本高效微调的新范式，其中低秩适应（LoRA）是广泛采用的选择。然而，微调后的LLMs在小数据集上微调时往往变得过于自信。贝叶斯方法凭借其固有的估计不确定性的能力，成为缓解过度自信和增强校准的有力工具。在这项工作中，我们介绍了Laplace-LoRA，这是一种简单而有效的贝叶斯方法，它将拉普拉斯近似应用于LoRA参数，并显著提升微调后LLMs的校准效果。"
}
{
  "title": "QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models",
  "title_zh": "QLLM：针对大型语言模型的准确且高效的低位宽量化",
  "abstract": "Large Language Models (LLMs) have demonstrated unparalleled efficacy in natural language processing. However, their high computational demands and memory overheads hinder their broad deployment. To address this, two quantization strategies emerge, including Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). For LLMs, the billions of parameters make the QAT impractical due to the prohibitive training cost and thus PTQ becomes more prevalent. In existing studies, activation outliers in particular channels are identified as the biggest challenge to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a more balanced distribution of activation magnitudes. Then similar channels are merged to maintain the original channel number for efficiency. Additionally, an adaptive strategy is designed to autonomously determine the optimal number of sub-channels for channel disassembly. To further compensate for the performance loss caused by quantization, we propose an efficient tuning method that only learns a small number of low-rank weights while freezing the pre-trained quantized model. After training, these low-rank parameters can be fused into the frozen weights without affecting inference. Extensive experiments on LLaMA-1 and LLaMA-2 show that QLLM is able to obtain accurate quantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B within 10 hours on a single A100-80G GPU, outperforming the previous state-of-the-art method by 7.89% on the average accuracy across five zero-shot tasks. Code is available at [ZIP Lab](https://github.com/ziplab/QLLM) and [ModelTC](https://github.com/ModelTC/QLLM).",
  "abstract_zh": "大型语言模型（LLMs）在自然语言处理方面表现出无与伦比的有效性。然而，它们的高计算需求和内存开销阻碍了它们的广泛部署。为了解决这个问题，出现了两种量化策略，包括量化感知训练（QAT）和后训练量化（PTQ）。对于LLMs，数十亿的参数使得QAT因高昂的训练成本而不切实际，因此PTQ变得更加普遍。在现有研究中，特定通道中的激活异常值被认为是PTQ准确性的最大挑战。研究者们提出将激活的幅度转化为权重，但这只能有限缓解或遭受不稳定梯度，导致低位宽下性能严重下降。本文提出QLLM，一种针对LLMs设计的准确且高效的低位宽PTQ方法。QLLM引入了一种自适应通道重组技术，将异常值的幅度重新分配到其他通道，从而减轻其对量化范围的影响。这是通过通道拆解和通道组装实现的，首先将异常通道分解为多个子通道，以确保激活幅度的更平衡分布。然后，将相似通道合并以保持原始通道数量以提高效率。此外，设计了一种自适应策略，能够自主确定通道拆解的最佳子通道数量。为了进一步弥补量化造成的性能损失，我们提出了一种高效的调优方法，仅学习少量低秩权重，同时冻结预训练的量化模型。训练后，这些低秩参数可以在不影响推理的情况下融合到冻结权重中。在LLaMA-1和LLaMA-2上的大量实验表明，QLLM能够高效地获得准确的量化模型。例如，QLLM在单个A100-80G GPU上在10小时内对4位LLaMA-2-70B进行量化，在五个零样本任务的平均准确率上超越了之前的最先进方法7.89%。代码可在[ZIP Lab](https://github.com/ziplab/QLLM)和[ModelTC](https://github.com/ModelTC/QLLM)获取。"
}
{
  "title": "CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?",
  "title_zh": "标题：平衡数据在多模态学习中的实用性：CLIP的偏见研究",
  "abstract": "We study data-balancing for mitigating biases in contrastive language-image pretraining (CLIP), identifying areas of strength and limitation. First, we reaffirm prior conclusions that CLIP can inadvertently absorb stereotypes. To counter this, we present a novel algorithm, called Multi-Modal Moment Matching (M4), designed to reduce both representation and association biases in multimodal data. We use M4 to conduct an in-depth analysis taking into account various factors, such as the model, representation, and data size. Our study also explores the dynamic nature of how CLIP learns/unlearns biases. In particular, we find that fine-tuning is effective in countering representation biases, though its impact diminishes for association biases. Also, data balancing has a mixed impact on quality: it tends to improve classification but can hurt retrieval. Interestingly, data and architectural improvements seem to mitigate the negative impact of data balancing on performance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves COCO image-to-text retrieval @5 from 86% (without data balancing) to 87% and ImageNet 0-shot classification from 77% to 77.5%! Finally, we conclude with recommendations for improving the efficacy of data balancing in multimodal systems.",
  "abstract_zh": "摘要：我们研究了数据平衡在对比语言-图像预训练（CLIP）中缓解偏见的作用，识别出其优势和局限性。首先，我们重申了先前的结论，即CLIP可能会无意中吸收刻板印象。为此，我们提出了一种新算法，称为多模态时刻匹配（M4），旨在减少多模态数据中的表示偏见和关联偏见。我们使用M4进行深入分析，考虑模型、表示和数据大小等各种因素。我们的研究还探讨了CLIP学习/遗忘偏见的动态特性。特别是，我们发现微调在对抗表示偏见方面有效，但对关联偏见的影响减弱。此外，数据平衡对质量的影响是混合的：它往往改善分类，但可能会损害检索。有趣的是，数据和架构的改进似乎减轻了数据平衡对性能的负面影响；例如，将M4应用于带有数据质量过滤器的SigLIP-B/16，使COCO图像到文本检索的准确率从86%（未平衡数据）提高到87%，ImageNet零-shot分类从77%提高到77.5%！最后，我们总结了改善多模态系统中数据平衡有效性的建议。"
}
{
  "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
  "title_zh": "稀疏自编码器在语言模型中发现高度可解释的特征",
  "abstract": "One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.",
  "abstract_zh": "神经网络内部理解的一个障碍是“多义性”，即神经元在多个语义上不同的上下文中激活。多义性阻碍了我们识别神经网络内部行为的简洁、人类可理解的解释。多义性的一个假设原因是“叠加”，即神经网络通过将特征分配给激活空间中的过完备方向，而不是单个神经元，来表示比其拥有的神经元更多的特征。在这里，我们尝试使用稀疏自编码器重构语言模型的内部激活，以识别这些方向。这些自编码器学习到的稀疏激活特征集比其他方法识别的方向更具可解释性和单义性，其中可解释性通过自动化方法进行测量。此外，我们展示了通过我们学习的特征集，我们可以更精确地确定在间接宾语识别任务中导致反事实行为的特征。这项工作表明，使用可扩展的无监督方法解决语言模型中的叠加是可能的。我们的方法可以作为未来机制可解释性工作的基础，我们希望这将促进更大的模型透明度和可操控性。"
}
{
  "title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
  "title_zh": "ToRA：一种集成工具的数学问题解决推理代理",
  "abstract": "Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.",
  "abstract_zh": "大型语言模型在各种语言任务中取得了显著进展，但在复杂数学问题上仍然存在困难。本文提出了ToRA，一系列设计用于解决挑战性数学问题的集成工具推理代理，通过将自然语言推理与外部工具（如计算库和符号求解器）的使用无缝结合，从而融合了语言的分析能力和工具的计算效率。为了训练ToRA，我们在数学数据集上策划了交互式工具使用轨迹，对注释应用模仿学习，并提出输出空间塑形以进一步优化模型的推理行为。结果显示，ToRA模型在10个数学推理数据集上的表现显著优于开源模型，平均绝对提升为13%-19%。值得注意的是，ToRA-7B在竞争级数据集MATH上的准确率达到44.6%，比最佳开源模型WizardMath-70B高出22%。ToRA-34B也是第一个在MATH上实现超过50%准确率的开源模型，显著优于GPT-4的链式推理结果，并在使用程序解决问题时与GPT-4具有竞争力。此外，我们对工具交互在数学推理中的优势和剩余挑战进行了全面分析，为未来研究提供了宝贵的见解。"
}
{
  "title": "An Emulator for Fine-tuning Large Language Models using Small Language Models",
  "title_zh": "大型语言模型微调的仿真器：使用小型语言模型",
  "abstract": "Widely used language models (LMs) are typically built by scaling up a two-stage training pipeline: a pre-training stage that uses a very large, diverse dataset of text and a fine-tuning (sometimes, 'alignment') stage using more targeted examples of specific behaviors and/or human preferences. While it has been hypothesized that knowledge and skills come from pre-training, and fine-tuning mostly filters this knowledge and skillset, this intuition has not been rigorously tested. In this paper, we test this hypothesis with a novel methodology for scaling these two stages independently, essentially asking, *What would happen if we combined the knowledge learned by a large model during pre-training with the knowledge learned by a small model during fine-tuning (or vice versa)?* Using an RL-based framework derived from recent developments in learning from human preferences, we introduce *emulated fine-tuning (EFT)*, a principled and practical method for sampling from a distribution that approximates the result of pre-training and fine-tuning at different scales. Our experiments with EFT show that scaling up fine-tuning tends to improve helpfulness, while scaling up pre-training tends to improve factuality. Further, we show that EFT enables test-time adjustment of competing behavioral factors like helpfulness and harmlessness without additional training. Finally, we find that a special case of emulated fine-tuning, which we call LM *up-scaling*, avoids resource-intensive fine-tuning of large pre-trained models by ensembling small fine-tuned models with large pre-trained models, essentially 'emulating' the result of fine-tuning the large pre-trained model. Up-scaling consistently improves helpfulness and factuality of widely used pre-trained models like Llama, Llama-2, and Falcon, without additional hyperparameters or training.",
  "abstract_zh": "广泛使用的语言模型通常通过扩展两阶段训练流程构建：预训练阶段使用非常大且多样化的文本数据集，微调（有时称为“对齐”）阶段使用更具针对性的特定行为和/或人类偏好的示例。尽管有假设认为知识和技能来自预训练，而微调主要是过滤这些知识和技能集，但这种直觉尚未经过严格测试。本文通过一种新颖的方法论独立测试这两个阶段的扩展，基本上在问：“如果我们将大型模型在预训练期间学到的知识与小型模型在微调期间学到的知识（或反之）结合起来，会发生什么？”我们引入了基于人类偏好的学习最新进展的RL框架中的“仿真微调（EFT）”，这是一种原则性和实用的方法，用于从一个分布中采样，该分布近似于在不同规模下的预训练和微调结果。我们的EFT实验表明，扩大微调规模往往会提高有用性，而扩大预训练规模则倾向于提高事实性。此外，我们展示了EFT使得在测试时调整有用性和无害性等竞争行为因素成为可能，而无需额外训练。最后，我们发现一种特殊的仿真微调情况，我们称之为LM“上升”，通过将小型微调模型与大型预训练模型集成，避免了资源密集型的大型预训练模型微调，基本上“仿真”了微调大型预训练模型的结果。上升始终提高了广泛使用的预训练模型（如Llama、Llama-2和Falcon）的有用性和事实性，而无需额外的超参数或训练。"
}
{
  "title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
  "title_zh": "标题：基于大型语言模型模块化构建合作性具身代理",
  "abstract": "In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.",
  "abstract_zh": "摘要：在本研究中，我们解决了具有去中心化控制、原始感知观察、昂贵通信和多目标任务的多代理合作问题，这些任务在各种具身环境中得以实现。尽管之前的研究要么假设了无成本的通信通道，要么依赖于具有共享观察的中心控制器，我们利用大型语言模型（LLMs）的常识知识、推理能力、语言理解和文本生成能力，将其无缝整合到一个认知启发的模块化框架中，该框架与感知、记忆和执行相结合，从而构建了一个合作性具身语言代理CoELA，能够高效地规划、沟通和与他人合作以完成长期任务。我们在C-WAH和TDW-MAT上的实验表明，由GPT-4驱动的CoELA能够超越强大的基于规划的方法，并展现出突出的有效沟通能力。尽管当前的开放语言模型如LLAMA-2仍表现不佳，我们通过与我们的代理收集的数据对CoELA进行了微调，并展示了它们如何实现令人鼓舞的表现。我们还进行了用户研究以探讨人机交互，发现使用自然语言进行沟通的CoELA能够获得更多信任，并与人类更有效地合作。我们的研究强调了LLMs在未来多代理合作研究中的潜力。项目网站上可以找到视频 https://vis-www.cs.umass.edu/Co-LLM-Agents/。"
}
{
  "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation",
  "title_zh": "大型语言模型的自我矛盾幻觉：评估、检测与缓解",
  "abstract": "Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context. In this work, we present a comprehensive investigation into self-contradiction for various instruction-tuned LMs, covering evaluation, detection, and mitigation. Our primary evaluation task is open-domain text generation, but we also demonstrate the applicability of our approach to shorter question answering. Our analysis reveals the prevalence of self-contradictions, e.g., in 17.7% of all sentences produced by ChatGPT. We then propose a novel prompting-based framework designed to effectively detect and mitigate self-contradictions. Our detector achieves high accuracy, e.g., around 80% F1 score when prompting ChatGPT. The mitigation algorithm iteratively refines the generated text to remove contradictory information while preserving text fluency and informativeness. Importantly, our entire framework is applicable to black-box LMs and does not require retrieval of external knowledge. Rather, our method complements retrieval-based methods, as a large portion of self-contradictions (e.g., 35.2% for ChatGPT) cannot be verified using online text. Our approach is practically effective and has been released as a push-button tool to benefit the public at https://chatprotect.ai/.",
  "abstract_zh": "大型语言模型（大型LM）容易生成包含幻觉内容的文本。这个问题的一个重要实例是自我矛盾，即LM在同一上下文中生成两个相互矛盾的句子。在本研究中，我们对各种指令调优的LM进行了自我矛盾的全面调查，涵盖评估、检测和缓解。我们的主要评估任务是开放领域文本生成，但我们也展示了我们的方法在较短问答中的适用性。我们的分析揭示了自我矛盾的普遍性，例如，在ChatGPT生成的所有句子中有17.7%存在自我矛盾。然后，我们提出了一种基于提示的新框架，旨在有效检测和缓解自我矛盾。我们的检测器实现了高准确率，例如，在提示ChatGPT时F1分数约为80%。缓解算法迭代地精炼生成的文本，以去除矛盾信息，同时保持文本的流畅性和信息性。重要的是，我们的整个框架适用于黑箱LM，并不需要检索外部知识。相反，我们的方法补充了基于检索的方法，因为大量自我矛盾（例如，ChatGPT的35.2%）无法通过在线文本进行验证。我们的方法在实践中有效，并已作为一键工具发布，以造福公众，网址为https://chatprotect.ai/。"
}
{
  "title": "L2MAC: Large Language Model Automatic Computer for Extensive Code Generation",
  "title_zh": "L2MAC：用于广泛代码生成的大型语言模型自动计算机",
  "abstract": "Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and coherent outputs. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long output generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer (von Neumann architecture) framework, an LLM-based multi-agent system, for long and consistent output generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction in turn is executed by a separate LLM agent, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective interaction with the file store. These components enable L2MAC to generate extensive outputs, bypassing the constraints of the finite context window while producing outputs that fulfill a complex user-specified task. We empirically demonstrate that L2MAC achieves state-of-the-art performance in generating large codebases for system design tasks, significantly outperforming other coding methods in implementing the detailed user-specified task, and we provide valuable insights into the reasons for this performance gap.",
  "abstract_zh": "基于变换器的大型语言模型（LLMs）受限于底层变换器架构的固定上下文窗口，阻碍了它们生成长且连贯输出的能力。增强记忆的LLMs是一个有前景的解决方案，但当前的方法无法处理长输出生成任务，因为它们（1）仅关注读取记忆，并将其演变简化为新记忆的连接，或（2）使用非常专业化的记忆，无法适应其他领域。本文提出了L2MAC，这是第一个基于LLM的存储程序自动计算机（冯·诺依曼架构）框架，是一个基于LLM的多代理系统，用于长且一致的输出生成。其内存有两个组成部分：指令寄存器，其中填充了解决用户给定任务的提示程序，以及文件存储，其中将包含最终和中间输出。每条指令依次由一个独立的LLM代理执行，其上下文由一个控制单元管理，该单元能够精确地读取和写入记忆，以确保与文件存储的有效交互。这些组件使L2MAC能够生成广泛的输出，绕过有限上下文窗口的限制，同时产生满足复杂用户指定任务的输出。我们通过实证证明，L2MAC在生成系统设计任务的大型代码库方面达到了最先进的性能，显著优于其他编码方法在实现详细用户指定任务方面的表现，并提供了有关这一性能差距原因的宝贵见解。"
}
{
  "title": "HyperAttention: Long-context Attention in Near-Linear Time",
  "title_zh": "超注意力：近线性时间内的长上下文注意力",
  "abstract": "We present an approximate attention mechanism named `HyperAttention` to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). \nRecent work suggests that in the worst-case scenario, the quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. \nWe introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. \nDespite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small.\nHyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention. \nEmpirically, employing Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions like FlashAttention. \nThis development presents substantial implications for enabling LLMs to handle significantly larger contexts.",
  "abstract_zh": "我们提出了一种名为“超注意力”的近似注意力机制，以应对大型语言模型（LLMs）中长上下文带来的计算挑战。最近的研究表明，在最坏情况下，除非注意力矩阵的条目有界或矩阵具有低稳定秩，否则需要二次时间。我们引入两个参数来衡量：（1）归一化注意力矩阵中的最大列范数，以及（2）在检测并移除大条目后未归一化注意力矩阵中的行范数比率。我们使用这些细粒度参数来捕捉问题的难度。尽管之前有下界，我们仍然能够实现线性时间的采样算法，即使矩阵具有无界条目或较大稳定秩，只要上述参数较小。超注意力具有模块化设计，便于集成其他快速低级实现，特别是FlashAttention。通过使用局部敏感哈希（LSH）识别大条目，超注意力在速度上优于现有方法，与FlashAttention等最先进的解决方案相比，显著提高了速度。这一发展对使LLMs能够处理显著更大上下文具有重要意义。"
}
{
  "title": "Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy",
  "title_zh": "开放式视觉语言模型的视觉问答基准评估：利用分类数据集及其语义层次",
  "abstract": "The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models’ capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our benchmark to a suite of vision-language models and show a detailed comparison of their abilities on object, action, and attribute classification. Our contributions aim to lay the foundation for more precise and meaningful assessments, facilitating targeted progress in the exciting field of vision-language modeling.",
  "abstract_zh": "对文本生成的视觉语言模型进行评估是一项具有挑战性但至关重要的工作。通过解决现有视觉问答（VQA）基准的局限性并提出创新的评估方法，我们的研究旨在加深对这些模型能力的理解。我们提出了一种基于知名视觉分类数据集的新型VQA基准，允许对文本生成的视觉语言模型进行细致评估，并与判别性视觉语言模型进行比较。为了改善对细粒度分类任务中粗略答案的评估，我们建议利用标签空间的语义层次自动生成关于真实类别的后续问题。最后，我们比较了传统自然语言处理和基于大语言模型的指标，以评估给定真实答案的模型预测问题。我们进行了人类评估研究，以此为基础决定最终指标。我们将基准应用于一系列视觉语言模型，并详细比较它们在对象、动作和属性分类方面的能力。我们的贡献旨在为更精确和有意义的评估奠定基础，促进视觉语言建模这一激动人心领域的有针对性进展。"
}
{
  "title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors",
  "title_zh": "标题：AgentVerse：促进多智能体协作与探索涌现行为",
  "abstract": "Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework AgentVerse that can effectively orchestrate a collaborative group of expert agents as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that AgentVerse can proficiently deploy multi-agent groups that outperform a single agent. Extensive experiments on text understanding, reasoning, coding, tool utilization, and embodied AI confirm the effectiveness of AgentVerse. Moreover, our analysis of agent interactions within AgentVerse reveals the emergence of specific collaborative behaviors, contributing to heightened group efficiency. We will release our codebase, AgentVerse, to further facilitate multi-agent research.",
  "abstract_zh": "摘要：由大型语言模型（LLMs）赋能的自主智能体经历了显著的改进，使其能够在广泛的任务中进行泛化。然而，在现实场景中，个体之间的合作往往是提高任务完成效率和效果所必需的。因此，受到人类群体动态的启发，我们提出了一个多智能体框架AgentVerse，能够有效地协调一组专家智能体，形成一个大于部分之和的系统。我们的实验表明，AgentVerse能够熟练地部署多智能体组，表现优于单个智能体。在文本理解、推理、编码、工具利用和具身人工智能等方面的广泛实验验证了AgentVerse的有效性。此外，我们对AgentVerse中智能体交互的分析揭示了特定协作行为的涌现，促进了群体效率的提高。我们将发布我们的代码库AgentVerse，以进一步促进多智能体研究。"
}
{
  "title": "Vision-by-Language for Training-Free Compositional Image Retrieval",
  "title_zh": "基于语言的视觉训练无关组合图像检索",
  "abstract": "Given an image and a target modification (e.g an image of the Eiffel tower and the text “without people and at night-time”), Compositional Image Retrieval (CIR) aims to retrieve the relevant target image in a database. While supervised approaches rely on annotating triplets that is costly (i.e. query image, textual modification, and target image), recent research sidesteps this need by using large-scale vision-language models (VLMs), performing Zero-Shot CIR (ZS-CIR). However, state-of-the-art approaches in ZS-CIR still require training task-specific, customized models over large amounts of image-text pairs. In this work, we proposeto tackle CIR in a training-free manner via our Compositional Image Retrieval through Vision-by-Language (CIReVL), a simple, yet human-understandable and scalable pipeline that effectively recombines large-scale VLMs with large language models (LLMs). By captioning the reference image using a pre-trained generative VLM and asking a LLM to recompose the caption based on the textual target modification for subsequent retrieval via e.g. CLIP, we achieve modular language reasoning. In four ZS-CIR benchmarks, we find competitive, in-part state-of-the-art performance - improving over supervised methods Moreover, the modularity of CIReVL offers simple scalability without re-training, allowing us to both investigate scaling laws and bottlenecks for ZS-CIR while easily scaling up to in parts more than double of previously reported results. Finally, we show that CIReVL makes CIR human-understandable by composing image and text in a modular fashion in the language domain, thereby making it intervenable, allowing to post-hoc re-align failure cases. Code will be released upon acceptance.",
  "abstract_zh": "给定一幅图像和一个目标修改（例如埃菲尔铁塔的图像和文本“无人物且在夜间”），组合图像检索（CIR）旨在从数据库中检索相关的目标图像。尽管监督方法依赖于标注三元组，这种方法成本高昂（即查询图像、文本修改和目标图像），但最近的研究通过使用大规模视觉-语言模型（VLMs）避免了这种需求，实现了零样本CIR（ZS-CIR）。然而，ZS-CIR中的最先进方法仍然需要在大量图像-文本对上训练特定任务的定制模型。在本研究中，我们提出通过我们的基于语言的视觉组合图像检索（CIReVL）以无训练的方式解决CIR，这是一种简单但易于理解且可扩展的流程，有效地将大规模VLM与大型语言模型（LLM）重新组合。通过使用预训练的生成VLM对参考图像进行标注，并要求LLM根据文本目标修改重新组合标注，以便通过例如CLIP进行后续检索，我们实现了模块化语言推理。在四个ZS-CIR基准测试中，我们发现具有竞争力的、部分最先进的性能——优于监督方法。此外，CIReVL的模块化提供了简单的可扩展性，无需重新训练，使我们能够调查ZS-CIR的扩展规律和瓶颈，同时轻松扩展到之前报告结果的两倍以上。最后，我们展示了CIReVL通过在语言领域以模块化方式组合图像和文本，使CIR对人类可理解，从而使其可干预，允许事后重新对齐失败案例。代码将在接受后发布。"
}
{
  "title": "Group Preference Optimization: Few-Shot Alignment of Large Language Models",
  "title_zh": "群体偏好优化：大型语言模型的少量样本对齐",
  "abstract": "Many applications of large language models (LLMs), ranging from chatbots to\ncreative writing, require nuanced subjective judgments that can differ significantly\nacross different groups. Existing alignment algorithms can be expensive to align\nfor each group, requiring prohibitive amounts of group-specific preference data\nand computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base\nLLM with an independent transformer module trained to predict the preferences\nof a group for the LLM generations. For few-shot learning, we parameterize this\nmodule as an in-context autoregressive transformer and train it via meta-learning\non several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences of US demographic\ngroups, global countries, and individual users. Our results demonstrate that GPO\nnot only aligns models more accurately but also requires fewer group-specific\npreferences and less training and inference computing resources, outperforming\nexisting strategies such as in-context steering and fine-tuning methods.",
  "abstract_zh": "许多大型语言模型（LLMs）的应用，从聊天机器人到创意写作，都需要细致的主观判断，这些判断在不同群体之间可能存在显著差异。现有的对齐算法在为每个群体进行对齐时可能成本高昂，需大量特定于群体的偏好数据和计算资源以满足实际应用需求。我们提出了群体偏好优化（GPO），这是一种对齐框架，以少量样本的方式引导语言模型适应个别群体的偏好。在GPO中，我们通过一个独立的变换器模块增强基础LLM，该模块经过训练以预测群体对LLM生成内容的偏好。对于少量样本学习，我们将该模块参数化为上下文自回归变换器，并通过元学习在多个群体上进行训练。我们通过对不同规模的LLM在三个与人类意见适应任务相关的严格评估中，实证验证了GPO的有效性。这些任务涉及适应美国人口群体、全球国家和个别用户的偏好。我们的结果表明，GPO不仅能够更准确地对齐模型，还需要更少的特定于群体的偏好数据和更少的训练与推理计算资源，优于现有的策略，如上下文引导和微调方法。"
}
{
  "title": "BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction",
  "title_zh": "贝叶斯提示：通过去偏域抽象在少样本推理中提示大规模预训练语言模型",
  "abstract": "As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous guidance for PLMs. Guided by such an intuition, we propose a simple yet effective approach, namely BayesPrompt, to learn prompts that contain the domain discriminative information against the interference from domain-irrelevant knowledge. BayesPrompt primitively leverages known distributions to approximate the debiased factual distributions of target domains and further uniformly samples certain representative features from the approximated distributions to generate the ultimate prompts for PLMs. We provide theoretical insights with the connection to domain adaptation. Empirically, our method achieves state-of-the-art performance on benchmarks.",
  "abstract_zh": "作为一种基于大规模预训练语言模型（PLMs）的新颖有效的微调范式，提示微调旨在缩小下游任务与预训练目标之间的差距。尽管提示微调在各种任务中取得了持续进展，但这种方法仍然存在一个持续的缺陷：提示微调方法无法推广到特定的少样本模式。从分布分析的角度来看，我们揭示了这一现象背后的内在问题是PLMs中包含的过多概念知识和针对目标下游领域的简化知识，这共同导致PLMs在通用知识嵌入空间中错误定位与目标领域相对应的知识分布。为此，我们直观地探索以去偏的方式近似下游任务的未简化目标领域，然后抽象这些领域以生成区分性提示，从而为PLMs提供明确的指导。在这种直觉的指导下，我们提出了一种简单而有效的方法，即贝叶斯提示，旨在学习包含领域区分信息的提示，以抵御与领域无关知识的干扰。贝叶斯提示初步利用已知分布来近似目标领域的去偏事实分布，并进一步从近似分布中均匀采样某些代表性特征，以生成PLMs的最终提示。我们提供了与领域适应相关的理论见解。在经验上，我们的方法在基准测试中实现了最先进的性能。"
}
{
  "title": "The LLM Surgeon",
  "title_zh": "标题：大语言模型外科医生",
  "abstract": "State-of-the-art language models are becoming increasingly large in an effort to achieve the highest performance on large corpora of available textual data. However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or device-specific constraints. We explore data-driven compression of existing pretrained models as an alternative to training smaller models from scratch. To do so, we scale Kronecker-factored curvature approximations of the target loss landscape to large language models. In doing so, we can compute both the dynamic allocation of structures that can be removed as well as updates of remaining weights that account for the removal. We provide a general framework for unstructured, semi-structured and structured pruning and improve upon weight updates to capture more correlations between weights, while remaining computationally efficient. Experimentally, our method can prune rows and columns from a range of OPT models and Llamav2-7B by 20\\%-30\\%, with a negligible loss in performance, and achieve state-of-the-art results in unstructured and semi-structured pruning of large language models. We will open source our code on GitHub upon acceptance.",
  "abstract_zh": "摘要：最先进的语言模型正变得越来越庞大，以期在可用文本数据的大型语料库上实现最高性能。然而，Transformer架构的巨大规模使得在计算、环境或设备特定约束下部署模型变得困难。我们探索了现有预训练模型的数据驱动压缩，作为从头训练更小模型的替代方案。为此，我们将目标损失景观的Kronecker分解曲率近似扩展到大型语言模型。通过这样做，我们可以计算可以移除的结构的动态分配，以及考虑到移除的剩余权重的更新。我们提供了一个通用框架，用于无结构、半结构和结构化剪枝，并改进权重更新以捕捉权重之间更多的相关性，同时保持计算效率。在实验中，我们的方法可以将一系列OPT模型和Llamav2-7B的行和列剪枝20%-30%，且性能损失微乎其微，并在大型语言模型的无结构和半结构剪枝中取得了最先进的结果。我们将在接受后在GitHub上开源我们的代码。"
}
{
  "title": "On the Reliability of Watermarks for Large Language Models",
  "title_zh": "关于大型语言模型水印的可靠性",
  "abstract": "As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. _Watermarking_ is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed.  For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a $1\\mathrm{e}{-5}$ false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.",
  "abstract_zh": "随着大型语言模型（LLMs）变得普遍，机器生成的文本可能会充斥互联网，带来垃圾信息、社交媒体机器人和无价值内容。水印是一种简单有效的策略，可以通过检测和记录LLM生成的文本来减轻这些危害。然而，一个关键问题仍然存在：在现实环境中，水印的可靠性如何？在这种情况下，水印文本可能会被修改以满足用户需求，或完全重写以避免检测。我们研究了水印文本在被人类重写、被未加水印的LLM改写或混入更长的手写文档后的鲁棒性。我们发现，即使在经过人类和机器改写后，水印仍然可以被检测到。虽然这些攻击削弱了水印的强度，但改写在统计上可能会泄露n-grams或更长的原始文本片段，从而在观察到足够的标记时导致高置信度的检测。例如，在经过强烈的人类改写后，当设置为$1\\mathrm{e}{-5}$的假阳性率时，平均观察到800个标记后水印仍然可被检测到。我们还考虑了一系列新的检测方案，这些方案对嵌入在大型文档中的短水印文本段落敏感，并比较了水印与其他类型检测器的鲁棒性。"
}
{
  "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
  "title_zh": "标题：FLASK：基于对齐技能集的细粒度语言模型评估",
  "abstract": "Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlation between model-based and human-based evaluations.",
  "abstract_zh": "摘要：大语言模型（LLMs）的评估具有挑战性，因为遵循指令需要与人类价值观对齐，而所需的技能集因指令而异。然而，以往的研究主要集中在粗粒度评估（即基于整体偏好的评估），这限制了可解释性，因为它没有考虑用户指令的性质，这些指令需要逐实例的技能组合。在本文中，我们介绍了FLASK（基于对齐技能集的细粒度语言模型评估），这是一种用于人类和模型评估的细粒度评估协议，它将粗级评分分解为每个指令的技能集级评分。我们通过实验观察到，评估的细粒度性对于获得模型性能的整体视图和提高评估的可靠性至关重要。使用FLASK，我们比较了多个开源和专有的LLM，并观察到模型评估与人类评估之间存在高度相关性。"
}
{
  "title": "Test-Time Training on Nearest Neighbors for Large Language Models",
  "title_zh": "标题：针对大型语言模型的最近邻测试时训练",
  "abstract": "Many recent efforts augment language models with retrieval, by adding retrieved data to the input context. For this approach to succeed, the retrieved data must be added at both training and test time. Moreover, as input length grows linearly with the size of retrieved data, cost in computation and memory grows quadratically for modern Transformers. To avoid these complications, we simply fine-tune the model on retrieved data at test time, using its standard training setup. We build a large-scale distributed index based on text embeddings of the Pile dataset. For each test input, our system retrieves its neighbors and fine-tunes the model on their text. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than 20 language modeling tasks in the Pile. For example, test-time training with nearest neighbors significantly narrows the performance gap between a small GPT-2 and a GPT-Neo model more than 10 times larger. Sufficient index quality and size, however, are necessary. Our work establishes a first baseline of test-time training for language modeling.",
  "abstract_zh": "摘要：许多近期的努力通过将检索到的数据添加到输入上下文中来增强语言模型。为了使这种方法成功，检索到的数据必须在训练和测试时都添加。此外，随着输入长度与检索数据的大小线性增长，现代Transformer的计算和内存成本呈平方增长。为了避免这些复杂性，我们简单地在测试时对检索到的数据对模型进行微调，使用其标准训练设置。我们基于Pile数据集的文本嵌入构建了一个大规模分布式索引。对于每个测试输入，我们的系统检索其邻居并对它们的文本进行微调。令人惊讶的是，仅检索和训练20个邻居，每个邻居仅进行一次梯度迭代，便能在Pile的20多个语言建模任务中显著提高性能。例如，使用最近邻的测试时训练显著缩小了小型GPT-2与一个大于其10倍的GPT-Neo模型之间的性能差距。然而，足够的索引质量和规模是必要的。我们的工作为语言建模的测试时训练建立了第一个基准。"
}
{
  "title": "The Generative AI Paradox: “What It Can Create, It May Not Understand”",
  "title_zh": "生成性人工智能悖论：“它能创造的，可能并不理解”",
  "abstract": "The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, we propose and test the **Generative AI Paradox** hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon---and can therefore exceed---their ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to\ngenerate expert-level outputs. We test this hypothesis through controlled experiments analyzing generation vs.~understanding in generative models, across both language and image modalities. Our results show that although models can outperform humans in generation, they consistently fall short of human capabilities in measures of understanding, as well as weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. Our findings support the hypothesis that models' generative capability may not be contingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence.",
  "abstract_zh": "最近一波生成性人工智能引发了前所未有的全球关注，人们对潜在的超人类水平的人工智能既感到兴奋又担忧：模型现在只需几秒钟就能产生输出，这些输出甚至挑战或超越了专家人类的能力。同时，模型在理解上仍然表现出基本错误，这在非专家人类中也不应出现。这给我们带来了一个明显的悖论：我们如何调和看似超人类的能力与少数人类几乎不会犯的错误的持续存在？在这项工作中，我们假设这种紧张关系反映了当今生成模型的智能配置与人类智能之间的差异。具体而言，我们提出并测试**生成性人工智能悖论**假设：生成模型经过直接训练以复制专家级输出，获得的生成能力并不依赖于——因此可以超越——它们理解这些相同类型输出的能力。这与人类形成对比，人类的基本理解几乎总是先于生成专家级输出的能力。我们通过控制实验分析生成与理解在生成模型中的表现，涵盖语言和图像两种模态，来测试这一假设。我们的结果表明，尽管模型在生成方面可以超越人类，但在理解的测量上，它们始终落后于人类能力，生成与理解表现之间的相关性较弱，并且对对抗性输入更脆弱。我们的发现支持了模型的生成能力可能不依赖于理解能力的假设，并呼吁在将人工智能与人类智能进行类比时保持谨慎。"
}
{
  "title": "Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images",
  "title_zh": "标题：通过冗长图像诱导大型视觉-语言模型的高能耗-延迟",
  "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of VLMs necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources. In this paper, we explore this attack surface about availability of VLMs and aim to induce high energy-latency cost during inference of VLMs. We find that high energy-latency cost during inference of VLMs can be manipulated by maximizing the length of generated sequences. To this end, we propose verbose images, with the goal of crafting an imperceptible perturbation to induce VLMs to generate long sentences during inference. Concretely, we design three loss objectives. First, a loss is proposed to delay the occurrence of end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop generating further tokens. Moreover, an uncertainty loss and a token diversity loss are proposed to increase the uncertainty over each generated token and the diversity among all tokens of the whole generated sequence, respectively, which can break output dependency at token-level and sequence-level. Furthermore, a temporal weight adjustment algorithm is proposed, which can effectively balance these losses. Extensive experiments demonstrate that our verbose images can increase the length of generated sequences by 7.87× and 8.56× compared to original images on MS-COCO and ImageNet datasets, which presents potential challenges for various applications.",
  "abstract_zh": "摘要：大型视觉-语言模型（VLMs）如GPT-4在各种多模态任务中表现出色。然而，部署VLMs需要大量的能量消耗和计算资源。一旦攻击者恶意诱导VLMs在推理过程中产生高能耗和延迟时间（能量-延迟成本），将耗尽计算资源。本文探讨了VLMs可用性的攻击面，旨在诱导VLMs在推理过程中产生高能耗-延迟成本。我们发现，通过最大化生成序列的长度，可以操控VLMs在推理过程中的高能耗-延迟成本。为此，我们提出了冗长图像，旨在制作一种不可察觉的扰动，以诱导VLMs在推理过程中生成长句子。具体而言，我们设计了三个损失目标。首先，提出了一种损失，旨在延迟结束序列（EOS）标记的出现，EOS标记是VLMs停止生成更多标记的信号。此外，提出了不确定性损失和标记多样性损失，分别用于增加每个生成标记的不确定性和整个生成序列中所有标记的多样性，这可以打破标记级和序列级的输出依赖关系。此外，提出了一种时间权重调整算法，可以有效平衡这些损失。大量实验表明，与原始图像相比，我们的冗长图像可以在MS-COCO和ImageNet数据集上将生成序列的长度分别增加7.87倍和8.56倍，这为各种应用带来了潜在挑战。"
}
{
  "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
  "title_zh": "LLM-CXR：用于CXR图像理解和生成的指令微调大型语言模型",
  "abstract": "Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual input/output. This direction of research is particularly relevant to medical imaging because accurate medical image analysis and generation consist of a combination of reasoning based on visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing (encoding or generating) networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM’s existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our LLM-CXR trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks.",
  "abstract_zh": "随着大型语言模型（LLMs）的显著发展，LLMs中的视觉语言对齐正在积极研究，以实现多模态推理和视觉输入/输出。这一研究方向与医学成像特别相关，因为准确的医学图像分析和生成需要基于视觉特征和先验知识的推理结合。许多近期的工作集中在训练适配器网络，这些网络作为图像处理（编码或生成）网络与LLMs之间的信息桥梁；但可以推测，为了最大限度地发挥LLMs在视觉信息上的推理潜力，视觉和语言特征应允许更自由地相互作用。这在医学领域尤其重要，因为理解和生成医学图像（如胸部X光片）不仅需要准确的视觉和基于语言的推理，还需要两种模态之间更紧密的映射。因此，受到之前关于变换器和VQ-GAN组合用于双向图像和文本生成的工作的启发，我们在此基础上开发了一种方法，通过指令微调仅在文本上预训练的LLM，以获得医学图像的视觉语言能力。具体而言，我们利用预训练LLM现有的问答和遵循指令的能力，通过指示其回答关于图像输入的问题，教会它理解视觉输入，并对给定查询输出适当的文本和图像响应，通过多样化任务微调LLM，这些任务涵盖基于图像的文本生成和基于文本的图像生成。我们展示了以这种方法训练的LLM-CXR在CXR理解和生成任务中表现出更好的图像-文本对齐，同时与之前开发的执行较窄任务范围的模型相比，体积更小。"
}
{
  "title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature",
  "title_zh": "快速检测GPT：通过条件概率曲率高效零-shot检测机器生成文本",
  "abstract": "Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational  costs. In this paper, we introduce the concept of **conditional probability curvature** to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around 75\\% in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in Table 1.",
  "abstract_zh": "大型语言模型（LLMs）展现了生成流畅且有条理内容的能力，既带来了生产力的机会，也带来了社会风险。为了构建可信赖的人工智能系统，区分机器生成内容和人类创作内容至关重要。领先的零-shot检测器DetectGPT表现出色，但其计算成本高昂。在本文中，我们引入了**条件概率曲率**的概念，以阐明LLMs与人类在特定上下文中词汇选择的差异。利用这一曲率作为基础指标，我们提出了**快速检测GPT**，这是一种优化的零-shot检测器，它用更高效的采样步骤替代了DetectGPT的扰动步骤。我们在各种数据集、源模型和测试条件下的评估表明，快速检测GPT在白盒和黑盒设置中相较于DetectGPT的性能提升约75%，并且检测过程加速了340倍，具体见表1。"
}
{
  "title": "(InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild",
  "title_zh": "标题: (InThe)WildChat：570K 真实环境下的 ChatGPT 互动日志",
  "abstract": "Chatbots such as GPT-4 and ChatGPT are now serving millions of users. Despite their widespread use, there remains a lack of public datasets showcasing how these tools are used by a population of users in practice. To bridge this gap, we offered free access to ChatGPT for online users in exchange for their affirmative, consensual, opt-in for anonymous collection of their chat transcripts. From this, we compiled (InThe)WildChat, a corpus of 570K user-ChatGPT conversations, which consists of over 1.5 million interaction turns. We compare WildChat with other popular user-chatbot interaction datasets, and find that our dataset offers the most diverse user prompts, contains the largest number of languages, and presents the richest variety of potentially toxic use-cases for researchers to study. In particular, in WildChat we find that a majority of the potentially unsafe use is produced by users attempting to “jailbreak” the model using prompts posted on online platforms; these are successful more than 70% of the time for ChatGPT. Finally, because it captures a broad range of use cases, we demonstrate the dataset’s potential utility in fine-tuning state-of-the-art instruction following models. WildLlama, a chatbot fine-tuned on WildChat, outperforms the latest Vicuna model of the same size on MT-Bench, which shows that WildChat has a high utility in addition to being a source for toxicity study. We will release WildChat and WildLlama with a license that emphasizes on accountability, collaboration, and transparency. The clean portion of WildChat will be publicly available, and the portion that contains potentially unsafe content will be made available upon request with a justification for AI safety research.",
  "abstract_zh": "摘要: 像 GPT-4 和 ChatGPT 这样的聊天机器人现在为数百万用户提供服务。尽管它们被广泛使用，但仍然缺乏展示这些工具在实际中如何被用户群体使用的公共数据集。为填补这一空白，我们为在线用户提供了免费使用 ChatGPT 的机会，以换取他们对匿名收集聊天记录的积极、同意的选择。由此，我们编制了 (InThe)WildChat，一个包含 570K 用户与 ChatGPT 对话的语料库，包含超过 150 万个互动回合。我们将 WildChat 与其他流行的用户-聊天机器人互动数据集进行了比较，发现我们的数据集提供了最丰富的用户提示，包含最多的语言，并为研究人员呈现了最丰富的潜在有害使用案例。特别是在 WildChat 中，我们发现大多数潜在不安全的使用是由用户尝试使用在在线平台上发布的提示来“越狱”模型产生的；对于 ChatGPT，这种尝试成功率超过 70%。最后，由于它捕捉了广泛的使用案例，我们展示了该数据集在微调最先进的指令跟随模型中的潜在效用。基于 WildChat 微调的聊天机器人 WildLlama 在 MT-Bench 上的表现超过了同等规模的最新 Vicuna 模型，这表明 WildChat 除了作为有害性研究的来源外，还具有很高的实用价值。我们将以强调问责、协作和透明度的许可发布 WildChat 和 WildLlama。WildChat 的干净部分将公开可用，而包含潜在不安全内容的部分将在请求时提供，需附上 AI 安全研究的理由。"
}
{
  "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning",
  "title_zh": "什么样的数据适合对齐？关于指令调优中自动数据选择的综合研究",
  "abstract": "Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance. However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively. In this work, we delve deeply into automatic data selection strategies for alignment. We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement. Subsequently, we propose a simple strategy to select data samples based on the measurement. We present Deita (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA models using data samples automatically selected with our proposed approach.  When assessed through both automatic metrics and human evaluation, Deita performs better or on par with the state-of-the-art open-source alignment models such as Vicuna and WizardLM with only 6K training data samples -- 10x less than the data used in the baselines. We anticipate this work to provide clear guidelines and tools on automatic data selection, aiding researchers and practitioners in achieving data-efficient alignment.",
  "abstract_zh": "指令调优是一种标准技术，用于在初始预训练阶段后将大型语言模型与最终任务和用户偏好对齐。最近的研究表明，数据工程在指令调优中起着关键作用——当数据选择得当时，仅需有限的数据即可实现卓越性能。然而，我们仍然缺乏对什么构成良好的指令调优数据以实现对齐的原则性理解，以及如何自动有效地选择数据。在这项工作中，我们深入探讨了对齐的自动数据选择策略。我们首先进行控制研究，从复杂性、质量和多样性三个维度测量数据，并在此基础上审视现有方法并引入新技术以增强数据测量。随后，我们提出了一种基于测量结果选择数据样本的简单策略。我们展示了Deita（即数据高效指令调优以实现对齐），这是从LLaMA模型微调的一系列模型，使用我们提出的方法自动选择的数据样本。在通过自动指标和人工评估进行评估时，Deita的表现优于或与最先进的开源对齐模型（如Vicuna和WizardLM）相当，仅使用了6000个训练数据样本——比基准中使用的数据少10倍。我们期待这项工作为自动数据选择提供明确的指导和工具，帮助研究人员和从业者实现数据高效对齐。"
}
{
  "title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset",
  "title_zh": "标题：LMSYS-Chat-1M：大规模真实世界LLM对话数据集",
  "abstract": "Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.",
  "abstract_zh": "摘要：研究人们在真实世界场景中如何与大型语言模型（LLMs）互动越来越重要，因为它们在各种应用中的广泛使用。本文介绍了LMSYS-Chat-1M，这是一个包含100万个与25种最先进LLM的真实对话的大规模数据集。该数据集来自我们Vicuna演示和Chatbot Arena网站上的210K个独特IP地址。我们提供了数据集内容的概述，包括其策划过程、基本统计数据和主题分布，突显其多样性、原创性和规模。我们通过四个用例展示了其多功能性：开发与GPT-4性能相似的内容审核模型，建立安全基准，训练与Vicuna性能相似的指令跟随模型，以及创建具有挑战性的基准问题。我们相信该数据集将成为理解和推动LLM能力的重要资源。数据集可在https://huggingface.co/datasets/lmsys/lmsys-chat-1m公开获取。"
}
{
  "title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple",
  "title_zh": "压缩大型语言模型：真相很少是纯粹的，且从不简单",
  "abstract": "Despite their remarkable achievements, modern Large Language Models (LLMs) encounter exorbitant computational and memory footprints. Recently, several works have shown significant success in *training-free* and  *data-free* compression (pruning and quantization) of LLMs achieving 50-60\\% sparsity and reducing the bit-width down to 3 or 4 bits per weight, with negligible perplexity degradation over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back, and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce **K**nowledge-**I**ntensive **C**ompressed LLM Benchmar**K** **(LLM-KICK)**, a collection of carefully-curated tasks to re-define the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts, and perplexity fail to capture subtle change in their true capabilities. LLM-KICK unveils many favorable merits and unfortunate plights of current SoTA compression methods: all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (*e.g.*, 25-30\\%), and fail for N:M sparsity on knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned LLMs even at $\\geq 50$\\% sparsity are robust in-context retrieval and summarization systems; among others. LLM-KICK is designed to holistically access compressed LLMs' ability for language understanding, reasoning, generation, in-context retrieval, in-context summarization, *etc.* We hope our study can foster the development of better LLM compression methods. The reproduced codes are available at https://github.com/VITA-Group/llm-kick.",
  "abstract_zh": "尽管现代大型语言模型（LLMs）取得了显著成就，但它们面临着巨大的计算和内存开销。最近，一些研究工作在无训练和无数据的压缩（剪枝和量化）方面取得了显著成功，实现了50-60%的稀疏性，并将每个权重的位宽降低到3或4位，同时对未压缩基线的困惑度影响微乎其微。随着近期研究努力集中于开发越来越复杂的压缩方法，我们的工作则退后一步，重新评估现有最先进压缩方法的有效性，这些方法依赖于一个相对简单且受到广泛质疑的指标——困惑度（即使对于稠密的LLMs）。我们引入了**K**nowledge-**I**ntensive **C**ompressed LLM Benchmar**K**（LLM-KICK），这是一个精心策划的任务集合，旨在重新定义压缩LLMs的评估协议，与其稠密对应物有显著的一致性，而困惑度未能捕捉到其真实能力的微妙变化。LLM-KICK揭示了当前最先进压缩方法的许多有利优点和不幸困境：所有剪枝方法都遭受了显著的性能下降，有时在微不足道的稀疏比率（例如，25-30%）下失效，并且在知识密集型任务上对N:M稀疏性失效；当前的量化方法比剪枝更成功；然而，即使在≥50%的稀疏性下，剪枝的LLMs在上下文检索和摘要系统中仍然表现稳健；等等。LLM-KICK旨在全面评估压缩LLMs在语言理解、推理、生成、上下文检索、上下文摘要等方面的能力。我们希望我们的研究能够促进更好的LLM压缩方法的发展。重现的代码可在https://github.com/VITA-Group/llm-kick获取。"
}
{
  "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data",
  "title_zh": "开放聊天：利用混合质量数据推动开源语言模型的发展",
  "abstract": "Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling.\nThrough extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat and https://huggingface.co/openchat.",
  "abstract_zh": "如今，开源大型语言模型如LLaMA已逐渐出现。近期的发展结合了监督微调（SFT）和强化学习微调（RLFT），以使这些模型与人类目标对齐。然而，SFT方法将所有混合质量的训练数据视为同等，而RLFT方法则需要高质量的成对或基于排名的偏好数据。在本研究中，我们提出了一种新颖的框架，称为OpenChat，以利用混合质量数据推动开源语言模型的发展。具体而言，我们考虑一般的SFT训练数据，该数据由少量专家数据与大量次优数据混合而成，且没有任何偏好标签。我们提出了C（onditioned）-RLFT，将不同数据源视为粗粒度奖励标签，并学习一个类条件策略以利用互补的数据质量信息。有趣的是，C-RLFT中的最优策略可以通过单阶段、无RL的监督学习轻松解决，这种方法轻量且避免了昂贵的人类偏好标注。通过在三个标准基准上的广泛实验，我们的openchat-13b经过C-RLFT微调后，在所有13b开源语言模型中实现了最高的平均性能。此外，我们使用AGIEval验证模型的泛化性能，其中只有openchat-13b超越了基础模型。最后，我们进行了一系列分析，以阐明OpenChat的有效性和鲁棒性。我们的代码、数据和模型已在https://github.com/imoneoi/openchat和https://huggingface.co/openchat上公开。"
}
{
  "title": "Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks",
  "title_zh": "标题：机制性分析微调对程序性定义任务的影响",
  "abstract": "Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been little work that explains how fine-tuning alters the underlying capabilities learnt by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just inhibit existing ones? An answer to this question would improve our ability to trust fine-tuning protocols meant to improve the safety of pre-trained models and delete unsafe capabilities. \nWe aim to make progress on this question by answering it in controlled settings where we can use mechanistic interpretability tools (e.g.~ network pruning and probing) to understand how the model's underlying capabilities are changing. \nWe perform an exhaustive analysis of the effects of fine-tuning in these settings, and show: (i) the ubiquitous protocol of fine-tuning with a small learning rate rarely alters the underlying model capabilities; (ii) often a minimal transformation, which we call a wrapper, is learned on top of the underlying model capability, yielding the impression that a new capability has been learned or a prior capability has been deleted; and (iii) continuing the fine-tuning process on a task where the pretraining capabilities are relevant leads to sample-efficient ``revival'' of the capability, i.e., the model starts to accurately reuse that capability in just a few gradient steps. \\textit{This potentially indicates a practitioner could unintentionally render a safe model to be unsafe by merely fine-tuning on a downstream task.} We additionally perform analysis on language models trained on the TinyStories dataset to support our claims in a realistic setting.",
  "abstract_zh": "摘要：微调大型预训练模型已成为开发特定任务和通用机器学习系统（包括开发安全可部署模型）的事实标准。尽管其重要性显而易见，但关于微调如何改变模型在预训练期间学习的基本能力的研究却很少：微调是否会产生全新的能力，还是仅仅抑制现有能力？对此问题的回答将提高我们对旨在改善预训练模型安全性和删除不安全能力的微调协议的信任。我们旨在通过在受控环境中使用机制可解释性工具（例如网络剪枝和探测）来理解模型的基本能力如何变化，从而在这一问题上取得进展。我们对这些环境中微调的影响进行了详尽分析，并显示：（i）使用小学习率的微调协议通常不会改变基础模型能力；（ii）通常会在基础模型能力之上学习到一种最小变换，我们称之为封装器，这给人一种学习到了新能力或删除了先前能力的印象；（iii）在预训练能力相关的任务上继续微调过程会导致能力的样本高效“复苏”，即模型在仅仅几个梯度步骤内开始准确地重用该能力。 \\textit{这可能表明，实践者仅通过在下游任务上微调就可能无意中使安全模型变得不安全。} 我们还对在TinyStories数据集上训练的语言模型进行了分析，以支持我们在现实环境中的主张。"
}
{
  "title": "On the Learnability of Watermarks for Language Models",
  "title_zh": "标题：关于语言模型水印可学习性的研究",
  "abstract": "Language model watermarking enables reliable detection of model-generated text, which has many applications in the responsible deployment of language models. Existing watermarking strategies operate by altering the decoder of an existing language model, and the ability for a language model to directly learn to generate the watermark would have significant implications for the real-world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, allowing for open models to benefit from watermarking. Second, if watermarking is used to determine the provenance of generated text, an adversary can damage the reputation of a victim model by spoofing its watermark and generating harmful watermarked text. To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking. We test our approach on three distinct decoding-based watermarking strategies, finding that models can learn to generate watermarked text with high detectability. We also find limitations to learnability, including the loss of watermarking capabilities under fine-tuning on normal text and high sample complexity when learning low-distortion watermarks.",
  "abstract_zh": "摘要：语言模型水印技术可以可靠地检测模型生成的文本，这在语言模型的负责任部署中具有许多应用。现有的水印策略通过改变现有语言模型的解码器来操作，而语言模型直接学习生成水印的能力将对水印的实际部署产生重大影响。首先，学习到的水印可以用于构建自然生成水印文本的开放模型，使开放模型能够受益于水印。其次，如果水印用于确定生成文本的来源，攻击者可以通过伪造水印并生成有害的水印文本来损害受害模型的声誉。为了研究水印的可学习性，我们提出了水印蒸馏，训练一个学生模型使其表现得像一个使用基于解码的水印的教师模型。我们在三种不同的基于解码的水印策略上测试了我们的方法，发现模型能够学习生成具有高可检测性的水印文本。我们还发现可学习性的限制，包括在正常文本上微调时水印能力的丧失以及学习低失真水印时的高样本复杂性。"
}
{
  "title": "Towards Understanding Factual Knowledge of Large Language Models",
  "title_zh": "标题：理解大型语言模型的事实知识",
  "abstract": "Large language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language generation. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, LLMs implicitly store facts in their parameters. Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time. To this end, we aim to explore the extent and scope of factual knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether LLMs can compose multiple facts, update factual knowledge temporally, reason over multiple pieces of facts, identify subtle factual differences, and resist adversarial examples. Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing trustworthy artificial intelligence. The dataset Pinocchio and our codes are publicly available at: https://github.com/THU-BPM/Pinocchio.",
  "abstract_zh": "摘要：大型语言模型（LLMs）最近在多种自然语言处理任务中推动了显著的性能提升。在预训练和指令调优过程中获得的事实知识在诸如问答和语言生成等各种下游任务中是有用的。与显式存储事实知识的传统知识库（KBs）不同，LLMs在其参数中隐式存储事实。LLMs生成的内容往往会表现出不准确或偏离真实的情况，因为事实可能被错误诱导或随着时间的推移而变得过时。为此，我们旨在通过设计基准测试Pinocchio来探索LLMs中事实知识的程度和范围。Pinocchio包含20,000个来自不同来源、时间线、领域、地区和语言的多样化事实问题。此外，我们还研究了LLMs是否能够组合多个事实、在时间上更新事实知识、对多个事实进行推理、识别微妙的事实差异以及抵御对抗性示例。对不同规模和类型的LLMs进行的广泛实验表明，现有的LLMs仍然缺乏事实知识，并受到各种虚假相关性的影响。我们认为这是实现可信人工智能的一个关键瓶颈。数据集Pinocchio和我们的代码已公开发布在：https://github.com/THU-BPM/Pinocchio。"
}
{
  "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",
  "title_zh": "现实世界中的WebAgent：具备规划、长上下文理解和程序合成",
  "abstract": "Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.\nHowever, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.\nWe introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.\nWebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.\nWe design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.\nWe empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.",
  "abstract_zh": "预训练的大型语言模型（LLMs）最近在自主网页自动化方面实现了更好的泛化和样本效率。然而，在真实网站上的表现仍然受到（1）开放域性、（2）有限的上下文长度和（3）缺乏对HTML的归纳偏见的影响。我们介绍了WebAgent，一个由LLM驱动的代理，它通过自我经验学习，根据自然语言指令在真实网站上完成任务。WebAgent通过将指令分解为规范子指令来提前规划，将长HTML文档总结为与任务相关的片段，并通过从这些片段生成的Python程序在网站上执行操作。我们设计了WebAgent与Flan-U-PaLM结合，用于基础代码生成，以及HTML-T5，这是一种新的预训练LLM，适用于长HTML文档，采用局部和全局注意机制以及混合长跨度去噪目标，用于规划和总结。我们通过实证证明，我们的模块化方案使真实网站的成功率提高了50%以上，并且HTML-T5是解决各种HTML理解任务的最佳模型；在MiniWoB网页自动化基准上，其成功率比先前的方法高出18.7%，并在Mind2Web（一个离线任务规划评估）上实现了最先进的性能。"
}
{
  "title": "MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data",
  "title_zh": "标题：MUSTARD：掌握定理和证明数据的统一合成",
  "abstract": "Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-wise formal solutions. (3) Lastly, the framework utilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With the proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE with 5,866 valid data points. Each data point contains an informal statement, an informal proof, and a translated formal proof that passes the prover validation. We perform extensive analysis and demonstrate that MUSTARD generates validated high-quality step-by-step data. We further apply the MUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B achieves a 15.41% average relative performance gain in automated theorem proving, and 8.18% in math word problems. Codes and data are available at https://github.com/Eleanor-H/MUSTARD.",
  "abstract_zh": "摘要：近期的大型语言模型（LLMs）在数学推理和定理证明等多项任务中取得了显著进展。由于这两项任务需要严格和正式的多步骤推理，因此它们成为探索LLMs推理能力的吸引领域，但仍面临重要挑战。以往的研究如思维链（CoT）揭示了中间步骤指导的有效性。然而，这种逐步注释需要大量劳动，导致当前基准的训练步骤不足。为填补这一空白，本研究引入了MUSTARD，这是一个数据生成框架，能够掌握高质量和多样性的定理与证明数据的统一合成。MUSTARD在三个阶段合成数据：（1）它抽取一些数学概念种子作为问题类别；（2）然后，它用抽取的概念提示生成语言模型，以获得问题及其逐步的正式解决方案；（3）最后，该框架利用证明助手（例如，Lean Prover）过滤有效证明。通过提出的MUSTARD，我们展示了一个包含5,866个有效数据点的定理和证明基准MUSTARDSAUCE。每个数据点包含一个非正式陈述、一个非正式证明和一个通过证明者验证的翻译正式证明。我们进行了广泛的分析，证明MUSTARD生成了经过验证的高质量逐步数据。我们进一步将MUSTARDSAUCE应用于小型语言模型的微调。微调后的Llama 2-7B在自动定理证明中实现了15.41%的平均相对性能提升，在数学文字问题中实现了8.18%的提升。代码和数据可在https://github.com/Eleanor-H/MUSTARD获取。"
}
{
  "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models",
  "title_zh": "全向校准量化：用于大型语言模型的OmniQuant",
  "abstract": "Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, leading to low performance, especially in extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization ($\\textbf{OmniQuant}$) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activation outliers by shifting the challenge of quantization from activations to weights. Operating within a differentiable framework using block-wise error minimization, OmniQuant can optimize the quantization process efficiently for both weight-only and weight-activation quantization. For instance, the LLaMA-2 model family size 7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4 (4-bit weight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16. Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices. Codes are available at \n\\url{https://github.com/OpenGVLab/OmniQuant}.",
  "abstract_zh": "大型语言模型（LLMs）彻底改变了自然语言处理任务。然而，它们的实际部署受到巨大的内存和计算需求的限制。尽管最近的后训练量化（PTQ）方法在减少内存占用和提高LLM的计算效率方面有效，但它们手动调整量化参数，导致性能较低，特别是在极低位量化时。为了解决这个问题，我们引入了一种全向校准量化（$\\textbf{OmniQuant}$）技术，能够在多种量化设置中实现良好的性能，同时通过有效优化各种量化参数保持PTQ的计算效率。OmniQuant包括两个创新组件：可学习权重裁剪（LWC）和可学习等效变换（LET）。LWC通过优化裁剪阈值来调节权重的极端值。同时，LET通过将量化挑战从激活转移到权重来处理激活异常值。在使用块级误差最小化的可微分框架内，OmniQuant能够高效优化仅权重和权重-激活量化的量化过程。例如，LLaMA-2模型系列的7-70B可以在单个A100-40G GPU上使用128个样本在1-16小时内处理。大量实验验证了OmniQuant在W4A4（4位权重，4位激活）、W6A6、W4A16、W3A16和W2A16等多种量化配置中的优越性能。此外，OmniQuant在指令调优模型中表现出有效性，并在实际设备上显著提高了推理速度和内存减少。代码可在 \\url{https://github.com/OpenGVLab/OmniQuant} 获取。"
}
{
  "title": "CPPO: Continual Learning for Reinforcement Learning with Human Feedback",
  "title_zh": "CPPO：基于人类反馈的强化学习的持续学习",
  "abstract": "The approach of Reinforcement Learning from Human Feedback (RLHF) is widely used for enhancing pre-trained Language Models (LM), enabling them to better align with human preferences. Existing RLHF-based LMs however require complete retraining whenever new queries or feedback are introduced, as human preferences may differ across different domains or topics. LM retraining is of\u0002ten impracticable in most real-world scenarios, due to the substantial time and computational costs involved, as well as data privacy concerns. To address this limitation, we propose Continual Proximal Policy Optimization (CPPO), a novel method that is able to continually align LM with dynamic human preferences. Specifically, CPPO adopts a weighting strategy to decide which samples should be utilized for enhancing policy learning and which should be used for solidifying past experiences. This seeks a good trade-off between policy learning and knowledge retention. Our experimental results show that CPPO outperforms strong Contin\u0002uous learning (CL) baselines when it comes to consistently aligning with human preferences. Furthermore, compared to PPO, CPPO offers more efficient and stable learning in non-continual scenarios.",
  "abstract_zh": "基于人类反馈的强化学习（RLHF）方法广泛用于增强预训练语言模型（LM），使其更好地与人类偏好对齐。然而，现有的基于RLHF的语言模型在引入新查询或反馈时需要完全重新训练，因为人类偏好可能在不同领域或主题之间有所不同。在大多数现实场景中，语言模型的重新训练往往不可行，因为涉及大量的时间和计算成本，以及数据隐私问题。为了解决这一限制，我们提出了持续近端策略优化（CPPO），这是一种能够持续使语言模型与动态人类偏好对齐的新方法。具体而言，CPPO采用加权策略来决定哪些样本应被用于增强策略学习，哪些样本应被用于巩固过去的经验。这寻求在策略学习和知识保留之间取得良好的平衡。我们的实验结果表明，CPPO在持续与人类偏好一致性方面优于强大的持续学习（CL）基准。此外，与PPO相比，CPPO在非持续场景中提供了更高效和稳定的学习。"
}
{
  "title": "Detecting, Explaining, and Mitigating Memorization in Diffusion Models",
  "title_zh": "标题：检测、解释和减轻扩散模型中的记忆化",
  "abstract": "Recent breakthroughs in diffusion models have exhibited exceptional image-generation capabilities. However, studies show that some outputs are merely replications of training data. Such replications present potential legal challenges for model owners, especially when the generated content contains proprietary information. In this work, we introduce a straightforward yet effective method for detecting memorized prompts by inspecting the magnitude of text-conditional predictions. Our proposed method seamlessly integrates without disrupting sampling algorithms, and delivers high accuracy even at the first generation step, with a single generation per prompt. Building on our detection strategy, we unveil an explainable approach that shows the contribution of individual words or tokens to memorization. This offers an interactive medium for users to adjust their prompts. Moreover, we propose two strategies i.e., to mitigate memorization by leveraging the magnitude of text-conditional predictions, either through minimization during inference or filtering during training. These proposed strategies effectively counteract memorization while maintaining high-generation quality. Code is available at https://github.com/YuxinWenRick/diffusion_memorization.",
  "abstract_zh": "摘要：最近在扩散模型领域的突破展示了卓越的图像生成能力。然而，研究表明，一些输出仅仅是训练数据的复制。这种复制对模型所有者可能带来法律挑战，尤其是当生成的内容包含专有信息时。在本研究中，我们通过检查文本条件预测的幅度，提出了一种简单而有效的检测记忆化提示的方法。我们提出的方法无缝集成，不会干扰采样算法，并且在第一次生成步骤中即能提供高准确率，每个提示仅生成一次。基于我们的检测策略，我们揭示了一种可解释的方法，展示了单个词或标记对记忆化的贡献。这为用户调整提示提供了交互媒介。此外，我们提出了两种策略，即通过利用文本条件预测的幅度来减轻记忆化，分别通过推理过程中的最小化或训练过程中的过滤。这些提出的策略有效地对抗记忆化，同时保持高生成质量。代码可在 https://github.com/YuxinWenRick/diffusion_memorization 获取。"
}
{
  "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks",
  "title_zh": "标题：敏感信息能从大型语言模型中删除吗？抵御提取攻击的目标",
  "abstract": "Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text. To mitigate these safety and informational issues, we propose an attack-and-defense framework for studying the task of deleting sensitive information directly from model weights. We study direct edits to model weights because (1) this approach should guarantee that particular deleted information is never extracted by future prompt attacks, and (2) it should protect against whitebox attacks, which is necessary for making claims about safety/privacy in a setting where publicly available model weights could be used to elicit sensitive information. Our threat model assumes that an attack succeeds if the answer to a sensitive question is located among a set of B generated candidates, based on scenarios where the information would be insecure if the answer is among B candidates. Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and blackbox attacks can recover “deleted” information from an edited model 38% of the time. These attacks leverage two key observations: (1) that traces of deleted information can be found in intermediate model hidden states, and (2) that applying an editing method for one question may not delete information across rephrased versions of the question. Finally, we provide new defense methods that protect against some extraction attacks, but we do not find a single universally effective defense method. Our results suggest that truly deleting sensitive information is a tractable but difficult problem, since even relatively low attack success rates have potentially severe implications for the deployment of language models in a world where individuals enjoy ownership of their personal data, a right to privacy, and safety from harmful model outputs.",
  "abstract_zh": "摘要：预训练语言模型有时会拥有我们不希望它们拥有的知识，包括记忆的个人信息和可能被用来伤害他人的知识。它们还可能输出有毒或有害的文本。为了减轻这些安全和信息问题，我们提出了一个攻击与防御框架，用于研究直接从模型权重中删除敏感信息的任务。我们研究对模型权重的直接编辑，因为（1）这种方法应确保特定删除的信息不会被未来的提示攻击提取，以及（2）它应能防御白盒攻击，这对于在公开可用的模型权重可能被用来引出敏感信息的情况下，提出安全/隐私的主张是必要的。我们的威胁模型假设，如果敏感问题的答案位于一组生成的B个候选答案中，则攻击成功，基于信息在答案位于B个候选中时不安全的场景。实验表明，即使是最先进的模型编辑方法如ROME也难以真正从像GPT-J这样的模型中删除事实信息，因为我们的白盒和黑盒攻击可以在编辑后的模型中以38%的概率恢复“删除”的信息。这些攻击利用了两个关键观察结果：（1）可以在中间模型隐藏状态中找到已删除信息的痕迹，以及（2）对一个问题应用编辑方法可能无法删除问题的不同表述中的信息。最后，我们提供了新的防御方法，以保护免受某些提取攻击，但我们没有找到一种普遍有效的防御方法。我们的结果表明，真正删除敏感信息是一个可行但困难的问题，因为即使相对较低的攻击成功率也可能对在个人享有其个人数据所有权、隐私权和免受有害模型输出的安全的世界中部署语言模型产生严重影响。"
}
{
  "title": "Human Feedback is not Gold Standard",
  "title_zh": "人类反馈并非黄金标准",
  "abstract": "Human feedback has become the de facto standard for evaluating the performance of Large Language Models, and is increasingly being used as a training objective. However, it is not clear which properties of a generated output this single `preference' score captures. We hypothesise that preference scores are subjective and open to undesirable biases. We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. We further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are not a fully reliable evaluation metric or training objective. Finally, we offer preliminary evidence that using human feedback as a training objective disproportionately increases the assertiveness of model outputs. We encourage future work to carefully consider whether preference scores are well aligned with the desired objective.",
  "abstract_zh": "人类反馈已成为评估大型语言模型性能的事实标准，并越来越多地被用作训练目标。然而，目前尚不清楚这个单一的“偏好”评分捕捉了生成输出的哪些特性。我们假设偏好评分是主观的，并且容易受到不良偏见的影响。我们批判性地分析了人类反馈在训练和评估中的使用，以验证它是否充分捕捉了一系列关键的错误标准。我们发现，尽管偏好评分覆盖面相当不错，但在事实性等重要方面的表现不足。我们进一步假设，偏好评分和错误注释可能受到混杂因素的影响，并利用指令调优模型生成在两个可能的混杂维度（自信度和复杂性）上变化的输出。我们发现，输出的自信度扭曲了对事实性错误的感知率，表明人类注释并不是一个完全可靠的评估指标或训练目标。最后，我们提供初步证据表明，将人类反馈作为训练目标会不成比例地增加模型输出的自信度。我们鼓励未来的工作仔细考虑偏好评分是否与期望目标良好对齐。"
}
{
  "title": "BooookScore: A systematic exploration of book-length summarization in the era of LLMs",
  "title_zh": "BooookScore：在大型语言模型时代对书籍长度摘要的系统探索",
  "abstract": "Summarizing book-length documents ($>$100K tokens)  that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving \\$15K USD and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than those generated by open-source models. While LLaMA 2 falls behind other models, Mixtral achieves performance on par with GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by annotators. We release code and annotations to spur more principled research on book-length summarization.",
  "abstract_zh": "对超过大型语言模型（LLMs）上下文窗口大小的书籍长度文档（超过10万标记）进行摘要需要首先将输入文档分解为较小的块，然后提示LLM合并、更新和压缩块级摘要。尽管这一任务复杂且重要，但由于评估的挑战，尚未得到有意义的研究：现有的书籍长度摘要数据集（例如BookSum）存在于大多数公共LLM的预训练数据中，而现有的评估方法难以捕捉现代LLM摘要生成器的错误。在本文中，我们首次研究了通过两种提示工作流程实现的基于LLM的书籍长度摘要生成器的连贯性：（1）分层合并块级摘要，和（2）增量更新运行摘要。我们对100本最近出版的书籍的GPT-4生成摘要进行了1193个细粒度的人类注释，并识别出LLM常见的八种连贯性错误类型。由于人类评估成本高且耗时，我们开发了一种自动化指标BooookScore，衡量摘要中不包含任何已识别错误类型的句子的比例。BooookScore与人类注释高度一致，并使我们能够系统评估许多其他关键参数（例如，块大小、基础LLM）的影响，同时节省了1.5万美元和500小时的人类评估成本。我们发现，闭源LLM如GPT-4和Claude 2生成的摘要的BooookScore高于开源模型生成的摘要。尽管LLaMA 2落后于其他模型，Mixtral的表现与GPT-3.5-Turbo相当。增量更新产生的BooookScore较低，但细节水平更高，这是注释者有时更倾向的权衡。我们发布代码和注释，以促进对书籍长度摘要的更有原则的研究。"
}
{
  "title": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models",
  "title_zh": "自动DAN：在对齐的大型语言模型上生成隐秘的越狱提示",
  "abstract": "The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively. Code is available at https://github.com/SheltonLiu-N/AutoDAN.",
  "abstract_zh": "对齐的大型语言模型（LLMs）是通过与人类反馈的广泛对齐创建的强大语言理解和决策工具。然而，这些大型模型仍然容易受到越狱攻击，攻击者操纵提示以引发不应由对齐LLMs提供的恶意输出。研究越狱提示可以让我们深入了解LLMs的局限性，并进一步指导我们保护它们。不幸的是，现有的越狱技术要么面临（1）可扩展性问题，攻击严重依赖手动制作提示，要么面临（2）隐秘性问题，因为攻击依赖基于令牌的算法生成通常语义上无意义的提示，使其容易通过基本的困惑度测试被检测到。针对这些挑战，我们打算回答这个问题：我们能否开发一种能够自动生成隐秘越狱提示的方法？在本文中，我们介绍了AutoDAN，这是一种针对对齐LLMs的新型越狱攻击。AutoDAN可以通过精心设计的层次遗传算法自动生成隐秘的越狱提示。广泛的评估表明，AutoDAN不仅自动化了这一过程，同时保持了语义的意义，而且在跨模型可转移性和跨样本通用性方面表现出优越的攻击强度。此外，我们还将AutoDAN与基于困惑度的防御方法进行了比较，结果表明AutoDAN能够有效绕过这些防御。代码可在 https://github.com/SheltonLiu-N/AutoDAN 获取。"
}
{
  "title": "Chain of Hindsight aligns Language Models with Feedback",
  "title_zh": "标题：后见之链使语言模型与反馈对齐",
  "abstract": "Learning from human preferences is important for language models to match human needs and to align with human and social values. \nPrior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models.\nWe condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors.  Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.",
  "abstract_zh": "摘要：从人类偏好中学习对语言模型匹配人类需求以及与人类和社会价值观对齐至关重要。以往的研究通过从人类反馈中学习以理解和遵循指令取得了显著成功。然而，这些方法要么基于人类注释者偏好的手工挑选模型生成，导致数据利用效率低下且难以普遍应用，要么依赖于强化学习，这通常受到不完善奖励函数的影响，并依赖于极具挑战性的优化。在本研究中，我们提出了一种新颖的技术——后见之链，它易于优化，并且可以从任何形式的反馈中学习，无论其极性如何。我们的想法受到人类如何从以语言形式呈现的广泛反馈中学习的启发。我们将所有类型的反馈转换为句子序列，然后用于微调模型，使我们能够利用语言模型的语言理解能力。我们将模型条件化于一系列与反馈配对的模型生成。通过这样做，模型被训练生成基于反馈的输出，同时学习识别和纠正负面属性或错误。将我们的方法应用于大型语言模型时，我们观察到后见之链在使语言模型与人类偏好对齐方面显著超越了以往的方法。我们在摘要和对话基准上报告了显著的改进，我们的方法在人工评估中明显受到偏爱。"
}
{
  "title": "A Semantic Invariant Robust Watermark for Large Language Models",
  "title_zh": "语义不变的强健水印用于大型语言模型",
  "abstract": "Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model.\nSubsequent analyses and experiments demonstrated the attack robustness of our method in semantically invariant settings: synonym substitution and text paraphrasing settings. Finally, we also show that our watermark possesses adequate security robustness.",
  "abstract_zh": "针对大型语言模型（LLMs）的水印算法在检测LLMs生成的文本方面已实现极高的准确性。这些算法通常涉及在每个生成步骤中将额外的水印logits添加到LLM的logits中。然而，先前的算法在攻击强健性和安全强健性之间存在权衡。这是因为一个标记的水印logits由一定数量的前置标记决定；数量少会导致安全强健性低，而数量多则会导致攻击强健性不足。在这项工作中，我们提出了一种针对LLMs的语义不变水印方法，提供了攻击强健性和安全强健性。我们工作的水印logits由所有前置标记的语义决定。具体而言，我们利用另一个嵌入LLM为所有前置标记生成语义嵌入，然后通过我们训练的水印模型将这些语义嵌入转换为水印logits。后续分析和实验表明，我们的方法在语义不变的设置下具有攻击强健性：同义词替换和文本释义设置。最后，我们还展示了我们的水印具有足够的安全强健性。"
}
{
  "title": "Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models",
  "title_zh": "揭示和改善数据可信度：训练无害语言模型的数据集研究",
  "abstract": "Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we find and fix an average of **6.16\\%** label errors in **11** datasets constructed from the above benchmarks. The data credibility and downstream learning performance can be remarkably improved by directly fixing label errors, indicating the significance of cleaning existing real-world datasets. Code is available at [https://github.com/Docta-ai/docta](https://github.com/Docta-ai/docta).",
  "abstract_zh": "语言模型在各种任务中显示出潜力，但在训练、微调或对齐过程中可能受到不良数据的影响。例如，如果一些不安全的对话被错误地标注为安全，则在这些样本上微调的模型可能会造成危害。因此，标注的正确性，即数据集的可信度，是非常重要的。本研究关注现实世界数据集的可信度，包括可以用于训练无害语言模型的流行基准数据集Jigsaw Civil Comments、Anthropic Harmless & Red Team、PKU BeaverTails & SafeRLHF。考虑到人工清理这些数据集的成本和难度，我们提出了一个系统框架，用于评估数据集的可信度、识别标签错误，并评估噪声标签在整理语言数据中的影响，特别关注不安全评论和对话分类。通过该框架，我们在上述基准构建的11个数据集中发现并修复了平均**6.16\\%**的标签错误。通过直接修复标签错误，数据的可信度和下游学习性能可以显著提高，表明清理现有现实世界数据集的重要性。代码可在[https://github.com/Docta-ai/docta](https://github.com/Docta-ai/docta)获取。"
}
{
  "title": "In-Context Learning Dynamics with Random Binary Sequences",
  "title_zh": "上下文学习动态与随机二进制序列",
  "abstract": "Large language models (LLMs) trained on huge corpora of text datasets demonstrate complex, emergent capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often unclear, and different prompts can elicit different capabilities through in-context learning. We propose a Cognitive Interpretability framework that enables us to analyze in-context learning dynamics to understand latent concepts in LLMs underlying behavioral patterns. This provides a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would require. Inspired by the cognitive science of human randomness perception, we use random binary sequences as context and study dynamics of in-context learning by manipulating properties of context data, such as sequence length. In the latest GPT-3.5+ models, we find emergent abilities to generate pseudo-random numbers and learn basic formal languages, with striking in-context learning dynamics where model outputs transition sharply from pseudo-random behaviors to deterministic repetition.",
  "abstract_zh": "大型语言模型（LLMs）在庞大的文本数据集上训练，展现出复杂的涌现能力，在未经过明确训练的任务上实现了最先进的性能。LLM能力的具体性质往往不明确，不同的提示可以通过上下文学习引发不同的能力。我们提出了一种认知可解释性框架，使我们能够分析上下文学习动态，以理解LLM中潜在概念与行为模式。这提供了比成功或失败评估基准更细致的理解，但不需要像机械解释电路那样观察内部激活。受到人类随机感知的认知科学启发，我们使用随机二进制序列作为上下文，通过操控上下文数据的属性（如序列长度）来研究上下文学习的动态。在最新的GPT-3.5+模型中，我们发现涌现能力生成伪随机数并学习基本形式语言，具有显著的上下文学习动态，模型输出从伪随机行为急剧过渡到确定性重复。"
}
{
  "title": "Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models",
  "title_zh": "标题：提示风险控制：大型语言模型负责任部署的严格框架",
  "abstract": "With the explosion of the zero-shot capabilities of (and thus interest in) pre-trained large language models, there has come accompanying interest in how best to prompt a language model to perform a given task. While it may be tempting to choose a prompt based on empirical results on a validation set, this can lead to a deployment where an unexpectedly high loss occurs. To mitigate this prospect, we propose a lightweight framework, Prompt Risk Control, for selecting a prompt based on rigorous upper bounds on families of informative risk measures. We provide and compare different methods for producing bounds on a diverse set of risk metrics like mean, CVaR, and the Gini coefficient of the loss distribution. In addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. Extensive experiments on high-impact applications like chatbots, medical question answering, and news summarization highlight why such a framework is necessary to reduce exposure to the worst outcomes.",
  "abstract_zh": "摘要：随着预训练大型语言模型零-shot 能力（因此也引发了兴趣）的爆炸性增长，如何最佳地提示语言模型执行特定任务也引起了相应的关注。虽然根据验证集上的经验结果选择提示可能很有诱惑力，但这可能导致意外的高损失发生。为了减轻这种前景，我们提出了一种轻量级框架——提示风险控制，旨在基于信息风险度量的严格上界选择提示。我们提供并比较了生成多样化风险指标（如均值、条件价值风险和损失分布的基尼系数）上界的不同方法。此外，我们扩展了基础统计界定技术，以适应部署中分布变化的可能性。在聊天机器人、医疗问答和新闻摘要等高影响应用上的广泛实验突显了这样一个框架在减少最坏结果暴露方面的必要性。"
}
{
  "title": "Jointly Training Large Autoregressive Multimodal Models",
  "title_zh": "联合训练大型自回归多模态模型",
  "abstract": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
  "abstract_zh": "近年来，大规模预训练语言和图像生成模型的进展彻底改变了机器学习领域。然而，将这两种模态整合为一个能够生成无缝多模态输出的强大模型仍然是一个重大挑战。为了解决这一问题，我们提出了联合自回归混合（JAM）框架，这是一种系统性融合现有文本和图像生成模型的模块化方法。我们还引入了一种专门的、数据高效的指令调优策略，旨在混合模态生成任务。我们最终的指令调优模型在生成高质量多模态输出方面表现出无与伦比的性能，并代表了第一个明确为此目的设计的模型。"
}
{
  "title": "INViTE: INterpret and Control Vision-Language Models with Text Explanations",
  "title_zh": "INViTE：通过文本解释来解释和控制视觉-语言模型",
  "abstract": "Large-scale pre-trained vision foundation models, such as CLIP, have become de facto backbones for various vision tasks. However, due to their black-box nature, understanding the underlying rules behind these models’ predictions and controlling model behaviors have remained open challenges. We present INViTE: a framework for INterpreting Vision Transformer’s latent tokens with Text Explanations. Given a latent token, INViTE retains its semantic information to the final layer using transformer’s local operations and retrieves the closest text for explanation. INViTE enables understanding of model visual reasoning procedure without needing additional model training or data collection. Based on the obtained interpretations, INViTE allows for model editing that controls model reasoning behaviors and improves model robustness against biases and spurious correlations. Our code is available at https://github.com/tonychenxyz/vit-interpret.",
  "abstract_zh": "大规模预训练的视觉基础模型，如CLIP，已成为各种视觉任务的事实标准。然而，由于其黑箱特性，理解这些模型预测背后的基本规则以及控制模型行为仍然是开放的挑战。我们提出了INViTE：一个通过文本解释来解释视觉变换器潜在标记的框架。给定一个潜在标记，INViTE利用变换器的局部操作将其语义信息保留到最终层，并检索最接近的文本进行解释。INViTE使得理解模型的视觉推理过程成为可能，而无需额外的模型训练或数据收集。基于获得的解释，INViTE允许对模型进行编辑，从而控制模型的推理行为，并提高模型对偏见和虚假相关性的鲁棒性。我们的代码可在 https://github.com/tonychenxyz/vit-interpret 获取。"
}
{
  "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
  "title_zh": "PandaLM：用于大型语言模型指令调优优化的自动评估基准",
  "abstract": "Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our findings reveal that PandaLM-7B offers a performance comparable to both GPT-3.5 and GPT-4. Impressively, PandaLM-70B surpasses their performance. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage.",
  "abstract_zh": "指令调优大型语言模型（LLMs）仍然是一项具有挑战性的任务，主要由于超参数选择的复杂性和评估调优模型的困难。为了确定最佳超参数，建立一个自动、稳健且可靠的评估基准至关重要。然而，由于评估准确性和隐私保护相关的挑战，建立这样的基准并非易事。为应对这些挑战，我们引入了一种名为PandaLM的评判大型语言模型，该模型经过训练以区分给定多个LLM时的优越模型。PandaLM的关注点不仅限于响应的客观正确性，这是传统评估数据集的主要关注点。它还考虑了相对简洁性、清晰度、遵循指令、全面性和正式性等重要主观因素。为了确保PandaLM的可靠性，我们收集了一个多样化的人类标注测试数据集，所有上下文均由人类生成，标签与人类偏好对齐。我们的研究发现，PandaLM-7B的性能与GPT-3.5和GPT-4相当。令人印象深刻的是，PandaLM-70B的性能超过了它们。PandaLM使LLM的评估更加公平且成本更低，模型通过PandaLM调优所取得的显著改进与使用默认Alpaca超参数训练的模型相比得到了证明。此外，PandaLM不依赖于基于API的评估，从而避免了潜在的数据泄露。"
}
{
  "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions",
  "title_zh": "如何捕捉人工智能的谎言：通过提问无关问题在黑箱大型语言模型中进行谎言检测",
  "abstract": "Large language models (LLMs) can “lie”, which we define as outputting false statements when incentivised to, despite “knowing” the truth in a demonstrable sense. LLMs might “lie”, for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM’s activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM’s yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting—prompting GPT-3.5 to lie about factual questions—the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.",
  "abstract_zh": "大型语言模型（LLMs）可能会“说谎”，我们将其定义为在有激励的情况下输出虚假陈述，尽管在可证明的意义上“知道”真相。例如，当被指示输出错误信息时，LLMs可能会“说谎”。在这里，我们开发了一种简单的谎言检测器，该检测器既不需要访问LLM的激活（黑箱），也不需要对相关事实的真实知识。该检测器通过在怀疑谎言后询问一组预定义的无关后续问题，并将LLM的“是/否”回答输入逻辑回归分类器来工作。尽管其简单性，这种谎言检测器的准确性很高，并且出乎意料地具有广泛的适用性。当在单一场景的示例上训练时——促使GPT-3.5对事实问题说谎——该检测器能够在分布外进行泛化，适用于（1）其他LLM架构，（2）经过微调以说谎的LLM，（3）谄媚性谎言，以及（4）在现实场景中出现的谎言，例如销售。这些结果表明，LLMs具有独特的与谎言相关的行为模式，在不同架构和上下文中保持一致，这可能使得通用的谎言检测成为可能。"
}
{
  "title": "Language Model Detectors Are Easily Optimized Against",
  "title_zh": "语言模型检测器容易被优化",
  "abstract": "The fluency and general applicability of large language models (LLMs) has motivated significant interest in detecting whether a piece of text was written by a language model. While both academic and commercial detectors have been deployed in some settings, particularly education, other research has highlighted the fragility of these systems. In this paper, we demonstrate a data-efficient attack that fine-tunes language models to confuse existing detectors, leveraging recent developments in reinforcement learning of language models. We use the 'human-ness' score (often just a log probability) of various open-source and commercial detectors as a reward function for reinforcement learning, subject to a KL-divergence constraint that the resulting model does not differ significantly from the original. For a 7B parameter Llama-2 model, fine-tuning for under a day reduces the AUROC of the OpenAI RoBERTa-Large detector from 0.84 to 0.62, while perplexity on OpenWebText increases from 8.7 to only 9.0; with a larger perplexity budget, we reduce AUROC to 0.30 (worse than random), with a perplexity increase to 9.9. Similar to traditional adversarial attacks, we find that this increase in 'detector evasion' generalizes to other detectors not used during training. In light of our empirical results, we advise against continued reliance on LLM-generated text detectors.",
  "abstract_zh": "大型语言模型（LLMs）的流畅性和广泛适用性引发了对检测文本是否由语言模型撰写的显著兴趣。尽管在某些环境中，特别是教育领域，已经部署了学术和商业检测器，但其他研究突显了这些系统的脆弱性。在本文中，我们展示了一种数据高效的攻击方法，该方法微调语言模型以混淆现有检测器，利用了最近在语言模型强化学习方面的发展。我们使用各种开源和商业检测器的“人性化”评分（通常仅为对数概率）作为强化学习的奖励函数，并施加KL散度约束，以确保生成的模型与原始模型没有显著差异。对于一个7B参数的Llama-2模型，微调不到一天将OpenAI RoBERTa-Large检测器的AUROC从0.84降低到0.62，而在OpenWebText上的困惑度仅从8.7增加到9.0；在更大的困惑度预算下，我们将AUROC降低到0.30（比随机还差），困惑度增加到9.9。与传统的对抗攻击类似，我们发现这种“检测器规避”的增加在训练期间未使用的其他检测器上也具有普遍性。根据我们的实证结果，我们建议不要继续依赖LLM生成的文本检测器。"
}
{
  "title": "Curiosity-driven Red-teaming for Large Language Models",
  "title_zh": "好奇心驱动的大语言模型红队测试",
  "abstract": "Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a $\\textit{red team}$ of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. \nHowever, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM.\nTo overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty. \nOur method of curiosity-driven red teaming (CRT) achieves greater coverage of test cases while mantaining or increasing their effectiveness compared to existing methods.\nOur method, CRT successfully provokes toxic responses from LLaMA2 model that has been heavily fine-tuned using human preferences to avoid toxic outputs. Code is available at https://github.com/Improbable-AI/curiosity_redteam.",
  "abstract_zh": "大语言模型（LLMs）在许多自然语言应用中具有巨大潜力，但存在生成不正确或有害内容的风险。为了探测何时生成不良内容，目前的范式是招募一个“红队”的人类测试者，设计输入提示（即测试用例），以引发LLMs的不良响应。然而，仅依赖人类测试者既昂贵又耗时。最近的研究通过训练一个独立的红队LLM，利用强化学习（RL）自动化红队测试，以生成最大化引发目标LLM不良响应的测试用例。然而，当前的RL方法只能生成少量有效的测试用例，导致对引发目标LLM不良响应的提示范围覆盖不足。为克服这一限制，我们将增加生成测试用例覆盖范围的问题与优化新颖性的好奇心驱动探索方法建立联系。我们的好奇心驱动红队测试（CRT）方法在保持或提高有效性的同时，实现了更大的测试用例覆盖范围。我们的CRT方法成功引发了经过大量人类偏好微调以避免有害输出的LLaMA2模型的不良响应。代码可在 https://github.com/Improbable-AI/curiosity_redteam 获取。"
}
{
  "title": "Language Model Decoding as Direct Metrics Optimization",
  "title_zh": "语言模型解码作为直接指标优化",
  "abstract": "Despite the remarkable advances in language modeling, current mainstream decoding methods still struggle to generate texts that align with human texts across different aspects. In particular, sampling-based methods produce less-repetitive texts which are often disjunctive in discourse, while search-based methods maintain topic coherence at the cost of increased repetition. Overall, these methods fall short in achieving holistic alignment across a broad range of aspects. In this work, we frame decoding from a language model as an optimization problem with the goal of strictly matching the expected performance with human texts measured by multiple metrics of desired aspects simultaneously. The resulting decoding distribution enjoys an analytical solution that scales the input language model distribution via a sequence-level energy function defined by these metrics. And most importantly, we prove that this induced distribution is guaranteed to improve the perplexity on human texts, which suggests a better approximation to the underlying distribution of human texts. To facilitate tractable sampling from this globally normalized distribution, we adopt the Sampling-Importance-Resampling technique. Experiments on various domains and model scales demonstrate the superiority of our method in metrics alignment with human texts and human evaluation over strong baselines.",
  "abstract_zh": "尽管语言建模取得了显著进展，但当前主流的解码方法在生成与人类文本在不同方面一致的文本时仍然面临挑战。特别是，基于采样的方法生成的文本重复性较低，但在话语上往往不连贯，而基于搜索的方法则在增加重复性的代价下保持主题一致性。总体而言，这些方法在实现广泛方面的整体一致性方面表现不佳。在本研究中，我们将语言模型的解码框架视为一个优化问题，目标是严格匹配期望性能与通过多个期望方面的指标同时测量的人类文本。所得到的解码分布享有一个解析解，该解通过这些指标定义的序列级能量函数来扩展输入语言模型分布。最重要的是，我们证明这种诱导分布能够改善人类文本的困惑度，这表明它更好地逼近人类文本的潜在分布。为了便于从这个全局归一化分布中进行可处理的采样，我们采用了采样重要性重采样技术。在各种领域和模型规模上的实验表明，我们的方法在与人类文本的指标一致性和人类评估方面优于强基线。"
}
{
  "title": "On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes",
  "title_zh": "标题：基于策略的语言模型蒸馏：从自生成错误中学习",
  "abstract": "Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by the student during inference. To address this issue, we introduce Generalized Knowledge Distillation (GKD). Instead of solely relying on a fixed set of output sequences, GKD trains the student on its self-generated output sequences by leveraging feedback from the teacher on such sequences. Unlike supervised KD approaches, GKD also offers the flexibility to employ alternative loss functions between the student and teacher, which can be useful when the student lacks the expressivity to mimic the teacher's distribution. Furthermore, GKD facilitates the seamless integration of distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for distilling auto-regressive T5 language models on summarization, translation, and arithmetic reasoning tasks.",
  "abstract_zh": "摘要：知识蒸馏（KD）广泛用于压缩教师模型，以降低推理成本和内存占用，通过训练一个更小的学生模型。然而，当前针对自回归序列模型的KD方法在训练期间看到的输出序列与学生在推理期间生成的序列之间存在分布不匹配的问题。为了解决这个问题，我们引入了广义知识蒸馏（GKD）。GKD不仅依赖于固定的输出序列集，而是通过利用教师对这些序列的反馈来训练学生的自生成输出序列。与监督KD方法不同，GKD还提供了在学生和教师之间使用替代损失函数的灵活性，这在学生缺乏模仿教师分布的表现力时非常有用。此外，GKD促进了蒸馏与强化学习微调（RLHF）的无缝结合。我们展示了GKD在摘要、翻译和算术推理任务上蒸馏自回归T5语言模型的有效性。"
}
{
  "title": "Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy",
  "title_zh": "标题：通过多种群体感知优化最大均值差异检测机器生成文本",
  "abstract": "Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues and hallucination information. Therefore, it is very urgent and important to detect MGTs in many situations. Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of LLMS. In this paper, we seek to exploit \\textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies. However,  directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \\textit{multiple text populations} due to various LLMs. This will severely impair MMD's ability to measure the difference between two samples. To tackle this, we propose a novel \\textit{multi-population} aware optimization method for MMD called MMD-MP, which can \\textit{avoid variance increases} and thus improve the stability to measure the distributional discrepancy. Relying on MMD-MP, we develop two methods for paragraph-based and sentence-based detection, respectively. Extensive experiments on various LLMs, \\eg,  GPT2 and ChatGPT, show superior detection performance of our MMD-MP.",
  "abstract_zh": "摘要：大型语言模型（LLMs）如ChatGPT在生成类人文本方面表现出色。然而，机器生成文本（MGTs）可能带来严重风险，如抄袭问题和虚假信息。因此，在许多情况下，检测MGTs是非常紧迫和重要的。不幸的是，由于LLMs的卓越表现，区分MGTs和人类撰写的文本具有挑战性，因为它们之间的分布差异通常非常微妙。本文旨在利用最大均值差异（MMD）来解决这一问题，因为MMD能够很好地识别分布差异。然而，直接使用多样化的MGTs训练检测器会显著增加MMD的方差，因为MGTs可能包含由于不同LLMs而产生的多种文本群体。这将严重削弱MMD测量两个样本之间差异的能力。为此，我们提出了一种新颖的针对MMD的多种群体感知优化方法，称为MMD-MP，能够避免方差增加，从而提高测量分布差异的稳定性。依赖于MMD-MP，我们分别开发了基于段落和基于句子的检测方法。在各种LLMs（例如GPT2和ChatGPT）上的广泛实验表明，我们的MMD-MP具有优越的检测性能。"
}
{
  "title": "Privately Aligning Language Models with Reinforcement Learning",
  "title_zh": "私密对齐语言模型的强化学习方法",
  "abstract": "Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.",
  "abstract_zh": "在预训练和用户部署之间，通过强化学习（RL）对大型语言模型（LLMs）进行对齐已成为训练指令跟随模型（如ChatGPT）的主要策略。在本研究中，我们开始探讨通过差分隐私（DP）结合RL进行LLMs的隐私保护对齐。借鉴Ziegler等人（2020）的影响力工作，我们研究了两种主要范式：（i）无人工干预的RL对齐（例如，生成正面评价）和（ii）基于人类反馈的RL对齐（RLHF）（例如，以人类偏好的方式进行总结）。我们提出了一个新的DP框架以实现通过RL进行对齐，并证明了其正确性。我们的实验结果验证了我们方法的有效性，提供了竞争力的效用，同时确保了强有力的隐私保护。"
}
{
  "title": "Evaluating Language Model Agency Through Negotiations",
  "title_zh": "评估语言模型代理性通过谈判",
  "abstract": "We introduce an approach to evaluate language model (LM) agency using negotiation games. This approach better reflects real-world use cases and addresses some of the shortcomings of alternative LM benchmarks. Negotiation games enable us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental evaluation data leakage. We use our approach to test six widely used and publicly accessible LMs, evaluating performance and alignment in both self-play and cross-play settings. Noteworthy findings include: (i) only closed-source models tested here were able to complete these tasks; (ii) cooperative bargaining games proved to be most challenging to the models; and (iii) even the most powerful models sometimes \"lose\" to weaker opponents.",
  "abstract_zh": "我们提出了一种通过谈判游戏评估语言模型（LM）代理性的方法。该方法更好地反映了现实世界的使用案例，并解决了一些替代LM基准的不足之处。谈判游戏使我们能够研究多轮和跨模型的互动，调节复杂性，并避免意外的评估数据泄漏。我们使用该方法测试了六个广泛使用且公开可访问的LM，评估了在自我对弈和交叉对弈设置中的性能和一致性。值得注意的发现包括：（i）在这里测试的只有闭源模型能够完成这些任务；（ii）合作谈判游戏被证明对模型来说是最具挑战性的；以及（iii）即使是最强大的模型有时也会“输给”较弱的对手。"
}
{
  "title": "Language Model Self-improvement by Reinforcement Learning Contemplation",
  "title_zh": "语言模型自我提升的强化学习思考",
  "abstract": "Language model self-improvement (LMSI) techniques have recently gained significant attention as they improve language models without requiring external supervision. A common approach is reinforcement learning from AI feedback (RLAIF), which trains a reward model based on AI preference data and employs a reinforcement learning algorithm to train the language model. \nHowever, RLAIF relies on the heuristic assumption that an AI model can provide effective feedback and correct wrong answers, requiring a solid capability of the language model. This paper presents a novel LMSI method, Reinforcement Learning Contemplation (RLC). We disclose that it is simpler for language models to evaluate a sentence than to generate it, even for small language models. Leveraging the gap between the evaluation and generation, RLC evaluates generated answers and updates language model parameters using reinforcement learning to maximize evaluation scores. Through testing on various challenging reasoning tasks and text summarization task, our experiments show that RLC effectively improves language model performance without external supervision, resulting in an answering accuracy increase (from 31.23% to 37.09%) for BigBench-hard reasoning tasks, and a rise in BERTScore for CNN/Daily Mail summarization tasks. Furthermore, RLC can be applied to models of different sizes, showcasing its broad applicability.",
  "abstract_zh": "语言模型自我提升（LMSI）技术最近受到广泛关注，因为它们在不需要外部监督的情况下改善语言模型。一种常见的方法是基于AI反馈的强化学习（RLAIF），该方法基于AI偏好数据训练奖励模型，并使用强化学习算法训练语言模型。然而，RLAIF依赖于启发式假设，即AI模型能够提供有效反馈并纠正错误答案，这要求语言模型具备强大的能力。本文提出了一种新颖的LMSI方法——强化学习思考（RLC）。我们揭示，对于语言模型来说，评估一个句子比生成它更简单，即使是对于小型语言模型。利用评估与生成之间的差距，RLC评估生成的答案，并使用强化学习更新语言模型参数，以最大化评估分数。通过在各种具有挑战性的推理任务和文本摘要任务上进行测试，我们的实验表明，RLC有效提高了语言模型的性能，无需外部监督，使BigBench-hard推理任务的回答准确率从31.23%提高到37.09%，并在CNN/Daily Mail摘要任务中提高了BERTScore。此外，RLC可以应用于不同规模的模型，展示了其广泛的适用性。"
}
{
  "title": "Enabling Lanuguage Models to Implicitly Learn Self-Improvement",
  "title_zh": "使语言模型能够隐式学习自我改进",
  "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpfulness and less harmful). To this end, we propose an imPlicit self-ImprovemenT (PIT) framework that implicitly learns the improvement goal from human preference data. PIT only requires preference data that are used to train reward models with no extra human efforts. Specifically, we reformulate the training objective of reinforcement learning from human feedback (RLHF) -- instead of maximizing response quality for a given input, we maximize the quality gap of the response conditioned on a reference response. In this way, PIT is implicitly trained with the improvement goal of better aligning with human preferences. Experiments on two real-world datasets and one synthetic dataset show that our method significantly outperforms prompting-based methods.",
  "abstract_zh": "大型语言模型（LLMs）在开放式文本生成任务中展现了卓越的能力。然而，这些任务固有的开放性意味着模型响应的质量总有改进的空间。为了解决这一挑战，已经提出了多种方法来增强LLMs的性能。越来越多的研究集中在使LLMs自我提高响应质量，从而减少对大量人工注释工作的依赖，以收集多样化和高质量的训练数据。最近，基于提示的方法因其有效性、效率和便利性而在自我改进方法中得到了广泛探索。然而，这些方法通常需要明确且详尽的评分标准作为LLMs的输入。手动推导并提供所有必要的评分标准以实现现实世界复杂目标（例如，更有帮助且更少有害）既昂贵又具有挑战性。为此，我们提出了一种隐式自我改进（PIT）框架，该框架从人类偏好数据中隐式学习改进目标。PIT只需使用偏好数据来训练奖励模型，而无需额外的人力投入。具体而言，我们重新制定了基于人类反馈的强化学习（RLHF）的训练目标——不是最大化给定输入的响应质量，而是最大化基于参考响应的响应质量差距。通过这种方式，PIT隐式地以更好地与人类偏好对齐的改进目标进行训练。在两个真实世界数据集和一个合成数据集上的实验表明，我们的方法显著优于基于提示的方法。"
}
{
  "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
  "title_zh": "标题：Ferret：在任何粒度下随时随地引用和定位任何事物",
  "abstract": "We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions,  we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with an additional 130K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination.",
  "abstract_zh": "摘要：我们介绍了Ferret，一种新的多模态大型语言模型（MLLM），能够理解图像中任何形状或粒度的空间引用，并准确地定位开放词汇描述。为了在LLM范式中统一引用和定位，Ferret采用了一种新颖且强大的混合区域表示，结合离散坐标和连续特征共同表示图像中的区域。为了提取多样区域的连续特征，我们提出了一种空间感知视觉采样器，擅长处理不同形状的稀疏性变化。因此，Ferret可以接受多种区域输入，如点、边界框和自由形状。为了增强Ferret的所需能力，我们整理了GRIT，这是一个全面的引用和定位指令调优数据集，包括110万个样本，包含丰富的层次空间知识，并附加了13万个困难负样本以促进模型的鲁棒性。最终模型不仅在经典的引用和定位任务中表现优异，而且在基于区域和需要定位的多模态聊天中大大超越了现有的MLLM。我们的评估还显示出在描述图像细节方面显著提高的能力，并在物体幻觉方面显著缓解。"
}
{
  "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints",
  "title_zh": "超越反向KL：通过多样化的散度约束对直接偏好优化进行泛化",
  "abstract": "The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative; and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush–Kuhn–Tucker conditions. This eliminates the need for estimating the normalizing constant in the Bradley-Terry model and enables a tractable mapping between the reward function and the optimal policy. Our approach optimizes LLMs to align with human preferences in a more efficient and supervised manner under a broad set of divergence constraints. Empirically, adopting these divergences ensures a balance between alignment performance and generation diversity. Importantly, our $f$-DPO outperforms PPO-based methods in divergence efficiency, and divergence constraints directly influence expected calibration error (ECE).",
  "abstract_zh": "大型语言模型（LLMs）能力的提升为人工通用智能带来了机遇，但同时也加剧了安全隐患，如AI系统的潜在误用，因此需要有效的AI对齐。人类反馈强化学习（RLHF）已成为实现AI对齐的有前景的途径，但由于其复杂性和对单独奖励模型的依赖，带来了挑战。直接偏好优化（DPO）被提出作为一种替代方案；在反向KL正则化约束下，它与RLHF等价。本文提出了$f$-DPO，这是一种通过引入多样化散度约束对DPO进行泛化的方法。我们展示了在某些$f$-散度下，包括詹森-香农散度、正向KL散度和$\\alpha$-散度，奖励与最优策略之间的复杂关系也可以通过解决Karush-Kuhn-Tucker条件来简化。这消除了在Bradley-Terry模型中估计归一化常数的需要，并实现了奖励函数与最优策略之间的可处理映射。我们的方法在广泛的散度约束下，以更高效和监督的方式优化LLMs，使其与人类偏好对齐。实证结果表明，采用这些散度确保了对齐性能和生成多样性之间的平衡。重要的是，我们的$f$-DPO在散度效率上优于基于PPO的方法，且散度约束直接影响预期校准误差（ECE）。"
}
{
  "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models",
  "title_zh": "大型语言模型中的时间旅行：追踪数据污染",
  "abstract": "Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs' real effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ \"guided instruction:\" a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a \"general instruction\" that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts. Further, our findings indicate that GPT-4 is contaminated with AG News, WNLI, and XSum datasets.",
  "abstract_zh": "数据污染，即下游任务中的测试数据出现在大型语言模型（LLMs）的训练数据中，是衡量LLMs在其他任务上真实有效性的一大潜在问题。我们提出了一种简单而有效的方法来识别LLMs中的数据污染。我们的核心方法首先在实例级别识别潜在污染；利用这些信息，我们的方法随后在分区级别评估更广泛的污染。为了估计单个实例的污染，我们采用“引导指令”：一个包含数据集名称、分区类型和参考实例随机长度初始段的提示，要求LLM完成它。如果LLM的输出与参考的后段完全或几乎匹配，则该实例被标记为污染。为了了解整个分区是否被污染，我们提出了两个想法。第一个想法是，如果与参考实例的平均重叠分数（通过ROUGE-L或BLEURT测量）在引导指令生成的完成与不包含数据集和分区名称的“通用指令”相比具有统计显著性更好，则将数据集分区标记为污染。第二个想法是，如果基于GPT-4的分类器通过少量示例的上下文学习提示将多个生成的完成标记为与相应参考实例的完全/近似匹配，则将数据集分区标记为污染。我们的方法在检测LLM是否受到七个数据集污染（包含训练和测试/验证分区）时，与人类专家的人工评估相比，准确率在92%到100%之间。此外，我们的研究结果表明，GPT-4受到AG News、WNLI和XSum数据集的污染。"
}
{
  "title": "What does the Knowledge Neuron Thesis Have to do with Knowledge?",
  "title_zh": "知识神经元理论与知识有什么关系？",
  "abstract": "We reassess the Knowledge Neuron (KN) Thesis: an interpretation of the mechanism underlying the ability of large language models to recall facts from a training corpus. This nascent thesis proposes that facts are recalled from the training corpus through the MLP weights in a manner resembling key-value memory, implying in effect that \"knowledge\" is stored in the network. Furthermore, by modifying the MLP modules, one can control the language model's generation of factual information. The plausibility of the KN thesis has been demonstrated by the success of KN-inspired model editing methods (Dai et al., 2022; Meng et al., 2022).\n\nWe find that this thesis is, at best, an oversimplification. Not only have we found that we can edit the expression of certain linguistic phenomena using the same model editing methods but, through a more comprehensive evaluation, we have found that the KN thesis does not adequately explain the process of factual expression. While it is possible to argue that the MLP weights store complex patterns that are interpretable both syntactically and semantically, these patterns do not constitute \"knowledge.\" To gain a more comprehensive understanding of the knowledge representation process, we must look beyond the MLP weights and explore recent models' complex layer structures  and attention mechanisms.",
  "abstract_zh": "我们重新评估了知识神经元（KN）理论：一种解释大型语言模型从训练语料库中回忆事实能力的机制的观点。该新兴理论提出，事实是通过类似键值存储的方式从训练语料库中通过MLP权重回忆的，实际上意味着“知识”存储在网络中。此外，通过修改MLP模块，可以控制语言模型生成事实信息的能力。KN理论的合理性已通过KN启发的模型编辑方法的成功得到了验证（Dai et al., 2022; Meng et al., 2022）。我们发现，这一理论充其量是一种过于简化的观点。我们不仅发现可以使用相同的模型编辑方法编辑某些语言现象的表达，而且通过更全面的评估，我们发现KN理论并不能充分解释事实表达的过程。虽然可以争辩说，MLP权重存储了在句法和语义上都可解释的复杂模式，但这些模式并不构成“知识”。要更全面地理解知识表示过程，我们必须超越MLP权重，探索最近模型的复杂层结构和注意机制。"
}
{
  "title": "PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization",
  "title_zh": "标题：PromptAgent：利用语言模型进行战略规划实现专家级提示优化",
  "abstract": "Expert-level prompts, carefully engineered by human experts who have a deep understanding of both large language models (LLMs) and domain knowledge, are the future of prompting and pivotal to harnessing the full power of advanced LLMs. Discovering such prompts with an automated process remains a sought-after and unresolved challenge. Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently. To address this pressing need and achieve expert-level prompting, we introduce PromptAgent, which autonomously discovers prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm (rooted in Monte Carlo Tree Search) to strategically explore the vast expert-level prompt space. PromptAgent interacts with the LLM in a human-like trial-and-error manner during the planning, and injects expert-level knowledge by reflecting on model errors and generating insightful error feedback. This novel formulation allows it to iteratively evaluate intermediate prompts, refine them based on errors, simulate future rewards, and search for high-reward paths leading to expert-level prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), domain-expert, and general NLU tasks, showing PromptAgent consistently outperforms strong prompting and prompt optimization baselines by great margins. Our qualitative analysis further emphasizes PromptAgent's capability to distill insightful errors into expert-level prompts.",
  "abstract_zh": "摘要：专家级提示由深刻理解大型语言模型（LLMs）和领域知识的人类专家精心设计，是提示的未来，并对充分发挥先进LLMs的潜力至关重要。通过自动化过程发现这样的提示仍然是一个备受追求且未解决的挑战。现有的提示优化技术虽然通过迭代采样实现自动化，但在有效注入领域知识和探索复杂专家级提示的广阔提示空间方面常常显得不足。为了解决这一紧迫需求并实现专家级提示，我们引入了PromptAgent，它能够自主发现与专家手工制作的提示质量相当的提示。PromptAgent的核心将提示优化视为一个战略规划问题，并采用基于蒙特卡洛树搜索的原则性规划算法，战略性地探索广阔的专家级提示空间。在规划过程中，PromptAgent以类人试错的方式与LLM互动，并通过反思模型错误和生成有见地的错误反馈来注入专家级知识。这种新颖的表述使其能够迭代评估中间提示，根据错误进行优化，模拟未来奖励，并寻找通向专家级提示的高奖励路径。我们将PromptAgent应用于涵盖三个实际领域的12个任务：BIG-Bench Hard (BBH)、领域专家和一般自然语言理解（NLU）任务，结果表明PromptAgent在强提示和提示优化基准上始终表现出显著优势。我们的定性分析进一步强调了PromptAgent将有见地的错误提炼为专家级提示的能力。"
}
{
  "title": "Unified Human-Scene Interaction via Prompted Chain-of-Contacts",
  "title_zh": "统一的人类场景交互通过提示链接触",
  "abstract": "Human-Scene Interaction (HSI) is a vital component of fields like embodied AI and virtual reality. Despite advancements in motion quality and physical plausibility, two pivotal factors, versatile interaction control and the development of a user-friendly interface, require further exploration before the practical application of HSI. This paper presents a unified HSI framework, UniHSI, which supports unified control of diverse interactions through language commands. The framework defines interaction as ``Chain of Contacts (CoC)\", representing steps involving human joint-object part pairs. This concept is inspired by the strong correlation between interaction types and corresponding contact regions. Based on the definition, UniHSI constitutes a Large Language Model (LLM) Planner to translate language prompts into task plans in the form of CoC, and a Unified Controller that turns CoC into uniform task execution. To facilitate training and evaluation, we collect a new dataset named ScenePlan that encompasses thousands of task plans generated by LLMs based on diverse scenarios. Comprehensive experiments demonstrate the effectiveness of our framework in versatile task execution and generalizability to real scanned scenes.",
  "abstract_zh": "人类场景交互（HSI）是具身人工智能和虚拟现实等领域的重要组成部分。尽管在运动质量和物理可信度方面取得了进展，但多样化交互控制和用户友好界面的开发这两个关键因素仍需进一步探索，以便在HSI的实际应用中发挥作用。本文提出了一个统一的HSI框架UniHSI，通过语言命令支持多样化交互的统一控制。该框架将交互定义为“接触链（CoC）”，表示涉及人类关节-物体部分对的步骤。该概念受到交互类型与相应接触区域之间强相关性的启发。基于该定义，UniHSI构建了一个大型语言模型（LLM）规划器，将语言提示转换为以CoC形式呈现的任务计划，以及一个统一控制器，将CoC转化为统一的任务执行。为了便于训练和评估，我们收集了一个名为ScenePlan的新数据集，涵盖了基于多样场景生成的数千个任务计划。全面的实验表明，我们的框架在多样化任务执行和对真实扫描场景的泛化能力方面的有效性。"
}
{
  "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
  "title_zh": "MiniGPT-4：利用先进的大型语言模型增强视觉-语言理解",
  "abstract": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed.\nWe believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). \nTo examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. \nOur work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, \nsuch as detailed image description generation and website creation from hand-drawn drafts.\nFurthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. \nIn our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability.",
  "abstract_zh": "最近的GPT-4展示了非凡的多模态能力，例如从手写文本直接生成网站和识别图像中的幽默元素。这些特性在之前的视觉-语言模型中很少见。然而，GPT-4背后的技术细节仍未披露。我们认为，GPT-4增强的多模态生成能力源于复杂的大型语言模型（LLM）的利用。为了研究这一现象，我们提出了MiniGPT-4，它通过一个投影层将冻结的视觉编码器与冻结的先进LLM Vicuna对齐。我们的工作首次揭示，正确地将视觉特征与先进的大型语言模型对齐，可以具备GPT-4所展示的众多先进多模态能力，例如详细的图像描述生成和从手绘草图创建网站。此外，我们还观察到MiniGPT-4中出现的其他新兴能力，包括根据给定图像创作故事和诗歌、根据食物照片教用户如何烹饪等。在我们的实验中，我们发现训练于短图像标题对的模型可能会产生不自然的语言输出（例如，重复和碎片化）。为了解决这个问题，我们在第二阶段策划了一个详细的图像描述数据集以微调模型，从而提高了模型的生成可靠性和整体可用性。"
}
{
  "title": "Self-Alignment with Instruction Backtranslation",
  "title_zh": "自对齐与指令反向翻译",
  "abstract": "We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then  selecting high quality examples from among these candidates (self-curation).  This data is then used to finetune a stronger model.  Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.",
  "abstract_zh": "我们提出了一种可扩展的方法，通过自动标记人类撰写的文本及其对应的指令，构建高质量的指令跟随语言模型。我们的方法称为指令反向翻译，首先使用在少量种子数据和给定网络语料库上微调的语言模型。种子模型用于通过为网络文档生成指令提示（自我增强）来构建训练示例，然后从这些候选示例中选择高质量示例（自我策划）。这些数据随后用于微调更强大的模型。在我们的两次迭代方法上微调LLaMa，产生的模型在不依赖蒸馏数据的情况下，在Alpaca排行榜上超越了所有其他基于LLaMa的模型，展示了高度有效的自对齐。"
}
{
  "title": "Dynamic Sparse No Training:  Training-Free Fine-tuning for Sparse LLMs",
  "title_zh": "动态稀疏无训练：稀疏大语言模型的无训练微调",
  "abstract": "The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training ($\\texttt{DSNT}$), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, $\\texttt{DSNT}$ minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, $\\texttt{DSNT}$ particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of $\\texttt{DSNT}$ in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, $\\texttt{DSNT}$ is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs.  Codes are available at https://github.com/zyxxmu/DSnoT.",
  "abstract_zh": "随着大语言模型（LLMs）的不断增加，尽管为即将到来的人工通用智能开辟了潜在路径，但在设备上部署时却面临着巨大的障碍。作为减少模型复杂性的一种成熟的预训练方法，网络剪枝在LLMs时代显得滞后，主要是由于在庞大的模型参数和训练数据下，其昂贵的微调（或再训练）需求。为缩小这一行业与学术界的差距，我们提出了动态稀疏无训练（$\\texttt{DSNT}$），这是一种无需训练的微调方法，可以在不进行昂贵的反向传播和权重更新的情况下，轻微更新稀疏LLMs。受动态稀疏训练的启发，$\\texttt{DSNT}$通过在稀疏LLMs上执行迭代的权重剪枝和增长，最小化稠密和稀疏LLMs之间的重构误差。为实现这一目标，$\\texttt{DSNT}$特别考虑了剪枝和增长时重构误差的预期减少，以及针对不同输入数据增长每个权重的方差。由于省去了对LLMs进行微调所需的反向传播，这一实践可以在线性时间内高效执行。在LLaMA-V1/V2、Vicuna和OPT等各种基准上的广泛实验表明，$\\texttt{DSNT}$在提高稀疏LLMs性能方面的有效性，尤其是在高稀疏水平下。例如，$\\texttt{DSNT}$能够在70%稀疏情况下以LLaMA-7B超越最先进的Wanda，降低26.79的困惑度。我们的论文为如何以高效的无训练方式微调稀疏LLMs提供了新的见解，并为将稀疏性的大潜力扩展到LLMs开辟了新的途径。代码可在https://github.com/zyxxmu/DSnoT获取。"
}
{
  "title": "Understanding Hidden Context in Preference Learning: Consequences for RLHF",
  "title_zh": "理解偏好学习中的隐藏上下文：对RLHF的影响",
  "abstract": "In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as having human annotators with varied preferences, cognitive processes that result in seemingly irrational behavior, and combining data labeled according to different criteria. We prove that standard applications of preference learning, including reinforcement learning from human feedback (RLHF), implicitly aggregate over hidden contexts according to a well-known voting rule called *Borda count*. We show this can produce counter-intuitive results that are very different from other methods which implicitly aggregate via expected utility. Furthermore, our analysis formalizes the way that preference learning from users with diverse values tacitly implements a social choice function. A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF. As a step towards mitigating these problems, we introduce a class of methods called *distributional preference learning* (DPL). DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context. Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability.",
  "abstract_zh": "在实践中，基于人类反馈的偏好学习依赖于具有隐藏上下文的不完整数据。隐藏上下文指的是影响所接收反馈的数据，但这些数据在用于训练偏好模型的数据中并未体现。这捕捉了数据收集中的常见问题，例如具有不同偏好的人工标注者、导致看似不理性行为的认知过程，以及根据不同标准标记的数据的组合。我们证明了偏好学习的标准应用，包括基于人类反馈的强化学习（RLHF），根据一种称为*博尔达计数*的著名投票规则隐式地聚合隐藏上下文。我们展示了这可能产生与通过期望效用隐式聚合的其他方法截然不同的反直觉结果。此外，我们的分析形式化了来自具有多样化价值观的用户的偏好学习如何默默实现社会选择函数。该结果的一个关键含义是，标注者有动机错误报告他们的偏好，以影响学习到的模型，从而导致RLHF部署中的脆弱性。作为缓解这些问题的一步，我们引入了一类称为*分布式偏好学习*（DPL）的方法。DPL方法估计每个备选项的可能得分值的分布，以更好地考虑隐藏上下文。实验结果表明，将DPL应用于LLM聊天机器人的RLHF能够识别数据中的隐藏上下文，并显著降低后续的越狱脆弱性。"
}
{
  "title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning",
  "title_zh": "NEFTune：噪声嵌入改善指令微调",
  "abstract": "We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation. \nNEFTune adds noise to the embedding vectors during training.\nStandard finetuning of LLaMA-2-7B using Alpaca achieves $29.79$\\% on AlpacaEval, which rises to $64.69$\\% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets.\nModels trained with Evol-Instruct see a $10$\\% improvement, with ShareGPT an $8$\\% improvement, and with OpenPlatypus an $8$\\% improvement. \nEven powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune. Particularly, we see these improvements on the conversational abilities of the instruction model and not on traditional tasks like those on the OpenLLM Leaderboard, where performance is the same.",
  "abstract_zh": "我们展示了语言模型微调可以通过一种简单的增强方法得到改善，有时甚至是显著的。NEFTune在训练过程中向嵌入向量添加噪声。使用Alpaca对LLaMA-2-7B进行标准微调在AlpacaEval上取得了29.79%的成绩，而使用噪声嵌入则提高到64.69%。NEFTune在现代指令数据集上也超越了强基线。使用Evol-Instruct训练的模型提高了10%，使用ShareGPT提高了8%，使用OpenPlatypus提高了8%。即使是经过RLHF进一步优化的强大模型，如LLaMA-2-Chat，经过NEFTune的额外训练也受益匪浅。特别是，我们在指令模型的对话能力上看到了这些改进，而在OpenLLM排行榜等传统任务上表现相同。"
}
{
  "title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision",
  "title_zh": "Q-Bench：低级视觉通用基础模型的基准测试",
  "abstract": "The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on **low-level visual perception and understanding**. To address this gap, we present **Q-Bench**, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. **_a)_** To evaluate the low-level **_perception_** ability, we construct the **LLVisionQA** dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. **_b)_** To examine the **_description_** ability of MLLMs on low-level information, we propose the **LLDescribe** dataset consisting of long expert-labelled *golden* low-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and the *golden* descriptions. **_c)_** Besides these two tasks, we further measure their visual quality **_assessment_** ability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predict *quantifiable* quality scores, and evaluate them on various existing image quality assessment (IQA) datasets. Our evaluation across the three abilities confirms that MLLMs possess preliminary low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities. We hope that our benchmark can encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs.",
  "abstract_zh": "多模态大型语言模型（MLLMs）的快速发展催生了计算机视觉从专用模型向通用基础模型的转变。然而，评估MLLMs在低级视觉感知和理解方面的能力仍然不足。为了解决这一问题，我们提出了Q-Bench，这是一个全面的基准，旨在系统地评估MLLMs在三个领域的潜在能力：低级视觉感知、低级视觉描述和整体视觉质量评估。我们构建了LLVisionQA数据集，以评估低级感知能力，该数据集包含2990张多样化来源的图像，每张图像都配有一个关注其低级属性的人类提问。我们随后测量MLLMs回答这些问题的正确性。我们还提出LLDescribe数据集，包含499张图像的长专家标注的黄金低级文本描述，并通过涉及GPT的比较流程评估MLLMs输出与黄金描述之间的差异。此外，我们进一步测量它们的视觉质量评估能力，以与人类意见评分对齐。具体而言，我们设计了一种基于softmax的策略，使MLLMs能够预测可量化的质量评分，并在各种现有的图像质量评估（IQA）数据集上进行评估。我们在这三种能力上的评估确认了MLLMs具备初步的低级视觉技能。然而，这些技能仍然不稳定且相对不精确，表明需要对MLLMs在这些能力上进行特定的增强。我们希望我们的基准能够鼓励研究界深入挖掘和提升MLLMs的这些未开发潜力。"
}
{
  "title": "Making LLaMA SEE and Draw with SEED Tokenizer",
  "title_zh": "让LLaMA通过SEED分词器实现视觉和绘图能力",
  "abstract": "The great success of Large Language Models (LLMs) has expanded the potential of multimodality, contributing to the gradual evolution of General Artificial Intelligence (AGI). A true AGI agent should not only possess the capability to perform predefined multi-tasks but also exhibit emergent abilities in an open-world context. However, despite the considerable advancements made by recent multimodal LLMs, they still fall short in effectively unifying comprehension and generation tasks, let alone open-world emergent abilities. We contend that the key to overcoming the present impasse lies in enabling text and images to be represented and processed interchangeably within a unified autoregressive Transformer. To this end, we introduce $\\textbf{SEED}$, an elaborate image tokenizer that empowers LLMs with the ability to $\\textbf{SEE}$ and $\\textbf{D}$raw at the same time. We identify two crucial design principles: (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a $\\textit{1D causal dependency}$, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture $\\textit{high-level semantics}$ consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. With SEED tokens, LLM is able to perform scalable multimodal autoregression under its original training recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by large-scale pretraining and instruction tuning on the interleaved textual and visual data, demonstrating impressive performance on a broad range of multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has exhibited compositional emergent abilities such as multi-turn in-context multimodal generation, acting like your AI assistant. The code (training and inference) and models are released in https://github.com/AILab-CVC/SEED.",
  "abstract_zh": "大型语言模型（LLMs）的巨大成功拓展了多模态的潜力，促进了通用人工智能（AGI）的逐步演进。真正的AGI代理不仅应具备执行预定义多任务的能力，还应在开放世界背景下展现出突现能力。然而，尽管最近的多模态LLMs取得了相当大的进展，但它们在有效统一理解和生成任务方面仍显不足，更不用说开放世界的突现能力。我们认为，克服当前僵局的关键在于使文本和图像能够在统一的自回归Transformer中互换表示和处理。为此，我们引入了$\\textbf{SEED}$，一种精细的图像分词器，使LLMs能够同时“看见”和“绘制”。我们确定了两个关键设计原则：（1）图像令牌应独立于二维物理补丁位置，而应以$\\textit{1D因果依赖}$生成，展现与LLMs中从左到右的自回归预测机制一致的内在相互依赖性。（2）图像令牌应捕捉与单词语义抽象程度一致的$\\textit{高级语义}$，并在分词器训练阶段优化其区分性和重构能力。通过SEED令牌，LLM能够在其原始训练配方下执行可扩展的多模态自回归，即下一个单词预测。因此，SEED-LLaMA通过对交错的文本和视觉数据进行大规模预训练和指令调优而生成，在广泛的多模态理解和生成任务中表现出色。更重要的是，SEED-LLaMA展现了组合突现能力，如多轮上下文多模态生成，表现得像你的AI助手。代码（训练和推理）及模型已在https://github.com/AILab-CVC/SEED发布。"
}
