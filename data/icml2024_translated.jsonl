{
  "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
  "title_zh": "通过多智能体辩论提高语言模型的事实性和推理能力",
  "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such \"society of minds\" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
  "abstract_zh": "大型语言模型（LLMs）近年来在语言生成、理解和少量学习方面展现了卓越的能力。大量研究探讨了如何通过提示工具（如验证、自我一致性或中间草稿）进一步提升其性能。本文提出了一种互补的方法，通过多个语言模型实例在多轮中提出和辩论各自的响应和推理过程，以达成共同的最终答案。我们的研究结果表明，这种方法显著增强了多个任务中的数学和战略推理能力。我们还证明了该方法提高了生成内容的事实有效性，减少了当代模型易出现的谬误答案和幻觉。我们的方法可以直接应用于现有的黑箱模型，并对我们研究的所有任务使用相同的程序和提示。总体而言，我们的研究结果表明，这种“心智社会”方法有潜力显著提升LLMs的能力，并为语言生成和理解的进一步突破铺平道路。"
}
{
  "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF",
  "title_zh": "标题：ODIN：解耦奖励减轻RLHF中的黑客行为",
  "abstract": "In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators and achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads to predict the preference, one trained to correlate with length and the other trained to decorrelate with length and therefore focusing more on the actual content. We then discard the length head in RL to ignore the spurious length reward. Experiments demonstrate that our approach eliminates the reward correlation with length, and improves the obtained policy by a significant margin.",
  "abstract_zh": "摘要：在本研究中，我们探讨了奖励黑客行为对响应长度的影响，这是在大型语言模型（LLMs）的基于人类反馈的强化学习（RLHF）中出现的一个挑战。LLMs提供的格式良好、冗长但帮助较少的响应往往能够欺骗LLMs甚至人类评估者，从而获得高分。这个问题在某些RL中的奖励模型中也同样存在。为了解决训练和评估中的挑战，我们建立了一个更可靠的评估协议，以比较不同的训练配置，检查通过改变训练超参数获得的LLM评估分数与响应长度之间的权衡。基于这一评估，我们进行了大规模研究，结果揭示了超参数和RL中用于减轻长度偏差的技巧的有效性。我们进一步提出通过联合训练两个线性头来改进奖励模型，一个头训练与长度相关，另一个头训练与长度去相关，从而更关注实际内容。然后，我们在RL中丢弃长度头，以忽略虚假的长度奖励。实验表明，我们的方法消除了与长度的奖励相关性，并显著改善了获得的策略。"
}
{
  "title": "Agent Instructs Large Language Models to be General Zero-Shot Reasoners",
  "title_zh": "代理指示大型语言模型成为通用零-shot推理者",
  "abstract": "We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. To enable this, our agent only needs to generate a single set of instructions for each task. These instructions turn out to be extremely effective for improving the reasoning process of different large language models across all task instances. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b, Llama-2-70b-chat, and GPT-3.5 Turbo. Compared to zero-shot chain of thought, our improvement in reasoning is striking. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo significantly.",
  "abstract_zh": "我们提出了一种方法来提高大型语言模型在通用语言理解任务上的零-shot推理能力。具体而言，我们构建了一个自主代理来指导大型语言模型的推理过程。为了实现这一点，我们的代理只需为每个任务生成一组指令。这些指令被证明在提高不同大型语言模型在所有任务实例中的推理过程方面极为有效。我们展示了这种方法进一步释放了大型语言模型在更多任务上的零-shot推理能力。我们研究了我们的方法在涵盖生成、分类和推理的广泛数据集上的表现。我们表明，我们的方法能够推广到大多数任务，并在我们评估的29个数据集中获得20个的最先进的零-shot性能。例如，我们的方法显著提升了最先进的大型语言模型的性能，包括Vicuna-13b、Llama-2-70b-chat和GPT-3.5 Turbo。与零-shot思维链相比，我们在推理方面的改进非常显著。使用我们的方法，Llama-2-70b-chat的表现显著超过零-shot GPT-3.5 Turbo。"
}
{
  "title": "GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting",
  "title_zh": "标题：GALA3D：通过布局引导的生成高斯点云实现文本到3D复杂场景生成",
  "abstract": "We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an instance-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. The source codes and models will be available at gala3d.github.io.",
  "abstract_zh": "摘要：我们提出了GALA3D，一种具有布局引导控制的生成3D高斯，用于有效的组合文本到3D生成。我们首先利用大型语言模型（LLMs）生成初始布局，并引入一种布局引导的3D高斯表示，以适应几何约束进行3D内容生成。然后，我们提出了一种实例场景组合优化机制，通过条件扩散协同生成具有一致几何、纹理、尺度和多个对象之间准确交互的真实3D场景，同时调整从LLMs提取的粗略布局先验，以与生成的场景对齐。实验表明，GALA3D是一个用户友好的端到端框架，适用于最先进的场景级3D内容生成和可控编辑，同时确保场景内对象级实体的高保真性。源代码和模型将发布在gala3d.github.io。"
}
{
  "title": "Provably Robust DPO: Aligning Language Models with Noisy Feedback",
  "title_zh": "可证明鲁棒的DPO：将语言模型与噪声反馈对齐",
  "abstract": "Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive. In this work, we aim to bridge this gap by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Terry-Luce (BTL) model, raising concerns about the impact of noisy data on the learned policy. We design a novel loss function, which de-bias the effect of noise on average, making a policy trained by minimizing that loss robust to the noise. Under log-linear parameterization of the policy class and assuming good feature coverage of the SFT policy, we prove that the sub-optimality gap of the proposed robust DPO (rDPO) policy compared to the optimal policy is of the order $O(\\frac{1}{1-2\\epsilon}\\sqrt{\\frac{d}{n}})$, where $\\epsilon < 1/2$ is flip rate of labels, $d$ is policy parameter dimension and $n$ is size of dataset. Our experiments on IMDb sentiment generation and Anthropic's helpful-harmless dataset shows that rDPO is robust to noise in preference labels compared to vanilla DPO and other heuristics proposed by practitioners.",
  "abstract_zh": "基于偏好的反馈学习最近获得了关注，成为将语言模型与人类兴趣对齐的有前景的方法。尽管这些对齐的生成模型在各种任务中展现了令人印象深刻的能力，但它们对高质量人类偏好数据的依赖在实际应用中构成了瓶颈。具体而言，数据集中噪声（不正确和模糊）的偏好对可能限制语言模型准确捕捉人类意图。虽然从业者最近提出了启发式方法来减轻噪声偏好的影响，但对其工作原理的完整理论理解仍然难以捉摸。在本研究中，我们旨在通过引入一个在随机偏好翻转情况下进行策略优化的一般框架来填补这一空白。我们特别关注直接偏好优化（DPO）算法，因为它假设偏好遵循Bradley-Terry-Luce（BTL）模型，这引发了对噪声数据对学习策略影响的担忧。我们设计了一种新颖的损失函数，平均上消除了噪声的影响，使得通过最小化该损失训练的策略对噪声具有鲁棒性。在策略类的对数线性参数化下，并假设SFT策略具有良好的特征覆盖，我们证明了所提出的鲁棒DPO（rDPO）策略与最优策略相比的次优性差距为$O(\\frac{1}{1-2\\epsilon}\\sqrt{\\frac{d}{n}})$的数量级，其中$\\epsilon < 1/2$是标签翻转率，$d$是策略参数维度，$n$是数据集大小。我们在IMDb情感生成和Anthropic的有用-无害数据集上的实验表明，与普通DPO和其他从业者提出的启发式方法相比，rDPO对偏好标签中的噪声具有鲁棒性。"
}
{
  "title": "Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks",
  "title_zh": "标题：宁可安全也不冒险：针对目标数据中毒和后门攻击的CLIP预训练",
  "abstract": "Contrastive Language-Image Pre-training (CLIP) on large image-caption datasets has achieved remarkable success in zero-shot classification and enabled transferability to new domains. However, CLIP is extremely more vulnerable to targeted data poisoning and backdoor attacks compared to supervised learning. Perhaps surprisingly, poisoning 0.0001% of CLIP pre-training data is enough to make targeted data poisoning attacks successful. This is four orders of magnitude smaller than what is required to poison supervised models. Despite this vulnerability, existing methods are very limited in defending CLIP models during pre-training. In this work, we propose a strong defense, SAFECLIP, to safely pre-train CLIP against targeted data poisoning and backdoor attacks. SAFECLIP warms up the model by applying unimodal contrastive learning (CL) on image and text modalities separately. Then, it divides the data into safe and risky sets by applying a Gaussian Mixture Model to the cosine similarity of image-caption pair representations. SAFECLIP pre-trains the model by applying the CLIP loss to the safe set and applying unimodal CL to image and text modalities of the risky set separately. By gradually increasing the size of the safe set during pre-training, SAFECLIP effectively breaks targeted data poisoning and backdoor attacks without harming the CLIP performance. Our extensive experiments on CC3M, Visual Genome, and MSCOCO demonstrate that SAFECLIP significantly reduces the success rate of targeted data poisoning attacks from 93.75% to 0% and that of various backdoor attacks from up to 100% to 0%, without harming CLIP’s performance.",
  "abstract_zh": "摘要：在大型图像-文本数据集上进行的对比语言-图像预训练（CLIP）在零-shot分类中取得了显著成功，并实现了向新领域的迁移能力。然而，与监督学习相比，CLIP对目标数据中毒和后门攻击极其脆弱。令人惊讶的是，仅需对CLIP预训练数据进行0.0001%的中毒，就足以使目标数据中毒攻击成功。这比中毒监督模型所需的量小四个数量级。尽管存在这种脆弱性，现有方法在预训练期间对CLIP模型的防御非常有限。在这项工作中，我们提出了一种强大的防御方法SAFECLIP，以安全地对抗目标数据中毒和后门攻击。SAFECLIP通过分别对图像和文本模态应用单模态对比学习（CL）来预热模型。然后，通过对图像-文本对表示的余弦相似度应用高斯混合模型，将数据分为安全集和风险集。SAFECLIP通过对安全集应用CLIP损失，并分别对风险集的图像和文本模态应用单模态CL来预训练模型。通过在预训练过程中逐渐增加安全集的大小，SAFECLIP有效地打破了目标数据中毒和后门攻击，而不损害CLIP的性能。我们在CC3M、Visual Genome和MSCOCO上的广泛实验表明，SAFECLIP显著降低了目标数据中毒攻击的成功率，从93.75%降至0%，以及各种后门攻击的成功率从高达100%降至0%，而不损害CLIP的性能。"
}
{
  "title": "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability",
  "title_zh": "冷攻击：以隐蔽性和可控性破解大型语言模型",
  "abstract": "Jailbreaks on large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent (suffix) attack with continuation constraint, but also allow us to address new controllable attack settings such as revising a user query adversarially with paraphrasing constraint, and inserting stealthy attacks in context with position constraint. Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5, and GPT-4) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability. Our code is available at https://github.com/Yu-Fangxu/COLD-Attack.",
  "abstract_zh": "针对大型语言模型（LLMs）的破解行为最近受到越来越多的关注。为了全面评估LLM的安全性，考虑具有多样属性的破解行为（如上下文连贯性和情感/风格变化）至关重要，因此研究可控破解，即如何对LLM攻击进行控制，是有益的。本文正式提出了可控攻击生成问题，并建立了该问题与可控文本生成之间的新联系，后者是自然语言处理中的一个广泛研究主题。基于这一联系，我们适应了基于能量的约束解码与朗之万动力学（COLD），这是一种在可控文本生成中高效的最先进算法，并引入了COLD-Attack框架，该框架统一并自动化了在流畅性、隐蔽性、情感和左右连贯性等多种控制要求下对对抗性LLM攻击的搜索。COLD-Attack所实现的可控性导致了多样的新破解场景，这不仅涵盖了生成流畅（后缀）攻击并带有延续约束的标准设置，还使我们能够处理新的可控攻击设置，例如以释义约束对用户查询进行对抗性修订，以及在具有位置约束的上下文中插入隐蔽攻击。我们在多种LLM（Llama-2、Mistral、Vicuna、Guanaco、GPT-3.5和GPT-4）上的广泛实验表明，COLD-Attack具有广泛的适用性、强大的可控性、高成功率和攻击可迁移性。我们的代码可在https://github.com/Yu-Fangxu/COLD-Attack获取。"
}
{
  "title": "Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations",
  "title_zh": "行动胜于言辞：万亿参数序列转导器用于生成推荐",
  "abstract": "Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute. Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data. HSTU outperforms baselines over synthetic and public datasets by up to 65.8% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundation models in recommendations.",
  "abstract_zh": "大规模推荐系统的特点在于依赖高基数、异构特征，并需要每天处理数十亿的用户行为。尽管在拥有数千个特征的大量数据上进行训练，行业内大多数深度学习推荐模型（DLRM）在计算能力上却无法扩展。受到变压器在语言和视觉领域取得成功的启发，我们重新审视推荐系统中的基本设计选择。我们将推荐问题重新表述为生成建模框架内的序列转导任务（“生成推荐器”），并提出了一种新架构HSTU，旨在处理高基数、非平稳流式推荐数据。HSTU在合成和公共数据集上在NDCG指标上超越基线达65.8%，并且在8192长度序列上比基于FlashAttention2的变压器快5.3到15.2倍。HSTU基础的生成推荐器拥有1.5万亿参数，在在线A/B测试中提高了12.4%的指标，并已在拥有数十亿用户的大型互联网平台的多个界面上部署。更重要的是，生成推荐器的模型质量在三个数量级的训练计算上经验性地呈幂律扩展，达到GPT-3/LLaMa-2规模，这减少了未来模型开发所需的碳足迹，并进一步为推荐领域的首个基础模型铺平了道路。"
}
{
  "title": "Exploring Intrinsic Dimension for Vision-Language Model Pruning",
  "title_zh": "探索视觉-语言模型剪枝的内在维度",
  "abstract": "The intrinsic dimension (ID) represents the minimum dimension needed to describe data on a lower-dimensional manifold within high-dimensional spaces. Network pruning aims to reduce the complexity of high-dimensional networks while minimizing performance trade-offs. This symmetry motivates the exploration of ID as a metric for effective pruning. For vision-language models, we investigate whether different modalities exist on separate manifolds, indicating varying complexity and prunability. We empirically study ID variations in large-scale vision-language pre-trained models and examine the contributions of different modalities to model prunability. We propose a layer importance metric based on ID, which can conveniently integrate with current metrics and enhance performance in vision-language model pruning. The experimental results show a high correlation between ID and modality prunability. Visual representations are more sensitive and crucial to model performance, while language representations are more robust and offer greater prunability. Our findings suggest an asymmetric pruning strategy for vision and language modalities, guided by the ID metric. The code is available at https://github.com/Nofear18/ID_VL_Pruning",
  "abstract_zh": "内在维度（ID）表示在高维空间中描述低维流形数据所需的最小维度。网络剪枝旨在减少高维网络的复杂性，同时最小化性能折衷。这种对称性激励我们探索ID作为有效剪枝的度量。对于视觉-语言模型，我们研究不同模态是否存在于不同流形上，表明复杂性和剪枝能力的差异。我们在大规模视觉-语言预训练模型中实证研究ID的变化，并考察不同模态对模型剪枝能力的贡献。我们提出了一种基于ID的层重要性度量，可以方便地与当前度量集成，并提升视觉-语言模型剪枝的性能。实验结果显示ID与模态剪枝能力之间存在高度相关性。视觉表示对模型性能更敏感且至关重要，而语言表示则更具鲁棒性并提供更大的剪枝能力。我们的研究结果建议了一种非对称的视觉和语言模态剪枝策略，以ID度量为指导。代码可在 https://github.com/Nofear18/ID_VL_Pruning 获取。"
}
{
  "title": "Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models",
  "title_zh": "评估和分析大型视觉-语言模型中的关系幻觉",
  "abstract": "The issue of hallucinations is a prevalent concern in existing Large Vision-Language Models (LVLMs). Previous efforts have primarily focused on investigating object hallucinations, which can be easily alleviated by introducing object detectors. However, these efforts neglect hallucinations in inter-object relationships, which is essential for visual comprehension. In this work, we introduce R-Bench, a novel benchmark for evaluating Vision Relationship Hallucination. R-Bench features image-level questions that focus on the existence of relationships and instance-level questions that assess local visual comprehension. We identify three types of relationship co-occurrences that lead to hallucinations: relationship-relationship, subject-relationship, and relationship-object. The visual instruction tuning dataset's long-tail distribution significantly impacts LVLMs' understanding of visual relationships. Additionally, our analysis reveals that current LVLMs tend to overlook visual content, overly rely on the common sense knowledge of Large Language Models (LLMs), and struggle with spatial relationship reasoning based on contextual information.",
  "abstract_zh": "幻觉问题是现有大型视觉-语言模型（LVLMs）中的一个普遍关注点。以往的研究主要集中在对象幻觉的调查上，这可以通过引入对象检测器来轻松缓解。然而，这些努力忽视了对象之间关系的幻觉，而这对于视觉理解至关重要。在本研究中，我们引入了R-Bench，这是一个用于评估视觉关系幻觉的新基准。R-Bench包含关注关系存在的图像级问题和评估局部视觉理解的实例级问题。我们识别出导致幻觉的三种关系共现类型：关系-关系、主体-关系和关系-对象。视觉指令调优数据集的长尾分布对LVLMs理解视觉关系有显著影响。此外，我们的分析显示，当前的LVLMs往往忽视视觉内容，过度依赖大型语言模型（LLMs）的常识知识，并在基于上下文信息的空间关系推理方面存在困难。"
}
{
  "title": "The WMDP Benchmark: Measuring and Reducing Malicious Use with Unlearning",
  "title_zh": "大规模毁灭性武器基准：通过遗忘来衡量和减少恶意使用",
  "abstract": "The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private and restricted to a narrow range of malicious use scenarios, which limits further research into reducing malicious use. To fill these gaps, we release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai.",
  "abstract_zh": "白宫关于人工智能的行政命令强调了大型语言模型（LLMs）赋予恶意行为者在生物、网络和化学武器开发方面的风险。为了衡量这些风险，政府机构和主要的人工智能实验室正在开发对LLMs中危险能力的评估。然而，目前的评估是私密的，并且仅限于狭窄的恶意使用场景，这限制了进一步研究减少恶意使用的可能性。为填补这些空白，我们发布了大规模毁灭性武器代理（WMDP）基准，这是一个包含3,668个多项选择题的数据集，作为生物安全、网络安全和化学安全中危险知识的代理测量。为了指导遗忘的进展，我们开发了RMU，这是一种基于控制模型表示的最先进的遗忘方法。RMU在WMDP上降低了模型性能，同时在生物学和计算机科学等领域保持了一般能力，这表明遗忘可能是减少LLMs恶意使用的具体途径。我们在https://wmdp.ai上公开发布了我们的基准和代码。"
}
{
  "title": "CHAI: Clustered Head Attention for Efficient LLM Inference",
  "title_zh": "标题：CHAI：用于高效大语言模型推理的聚类头注意力",
  "abstract": "Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-head attention is one of the key components of LLMs, which can for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered HeadAttention ( CHAI ). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73× without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviation in accuracy across 3 different models (i.e. OPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets.",
  "abstract_zh": "摘要：拥有数千亿参数的大语言模型（LLMs）已经改变了机器学习领域。然而，在推理时服务这些模型既需要大量计算资源，也需要大量内存，单个请求可能需要多个GPU和数十GB的内存。多头注意力是LLMs的关键组成部分之一，占据了超过50%的内存和计算需求。我们观察到，不同头部在关注的令牌上存在大量冗余。基于这一洞察，我们提出了聚类头注意力（CHAI）。CHAI在运行时结合了高度相关的头部进行自注意力，从而减少了内存和计算。在我们的实验中，我们展示了CHAI能够将K,V缓存的内存需求减少多达21.4%，推理时间延迟减少多达1.73倍，而无需任何微调。CHAI在3个不同模型（即OPT-66B、LLAMA-7B、LLAMA-33B）和5个不同评估数据集上实现了最大3.2%的准确率偏差。"
}
{
  "title": "Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection",
  "title_zh": "标题：通过大型语言模型设想异常值暴露以进行分布外检测",
  "abstract": "Detecting out-of-distribution (OOD) samples is essential when deploying machine learning models in open-world scenarios. Zero-shot OOD detection, requiring no training on in-distribution (ID) data, has been possible with the advent of vision-language models like CLIP. Existing methods build a text-based classifier with only closed-set labels. However, this largely restricts the inherent capability of CLIP to recognize samples from large and open label space. In this paper, we propose to tackle this constraint by leveraging the expert knowledge and reasoning capability of large language models (LLM) to Envision potential Outlier Exposure, termed EOE, without access to any actual OOD data. Owing to better adaptation to open-world scenarios, EOE can be generalized to different tasks, including far, near, and fine-grained OOD detection. Technically, we design (1) LLM prompts based on visual similarity to generate potential outlier class labels specialized for OOD detection, as well as (2) a new score function based on potential outlier penalty to distinguish hard OOD samples effectively. Empirically, EOE achieves state-of-the-art performance across different OOD tasks and can be effectively scaled to the ImageNet-1K dataset. The code is publicly available at: https://github.com/tmlr-group/EOE.",
  "abstract_zh": "摘要：在开放世界场景中部署机器学习模型时，检测分布外（OOD）样本至关重要。零样本OOD检测不需要在分布内（ID）数据上进行训练，随着视觉-语言模型如CLIP的出现而成为可能。现有方法仅使用闭集标签构建基于文本的分类器。然而，这在很大程度上限制了CLIP识别来自大规模和开放标签空间样本的固有能力。在本文中，我们提出通过利用大型语言模型（LLM）的专家知识和推理能力来解决这一限制，设想潜在的异常值暴露，称为EOE，而无需访问任何实际的OOD数据。由于更好地适应开放世界场景，EOE可以推广到不同任务，包括远程、近程和细粒度的OOD检测。在技术上，我们设计了（1）基于视觉相似性的LLM提示，以生成专门用于OOD检测的潜在异常类标签，以及（2）基于潜在异常惩罚的新评分函数，以有效区分困难的OOD样本。实证结果表明，EOE在不同的OOD任务中实现了最先进的性能，并且可以有效扩展到ImageNet-1K数据集。代码公开可用： https://github.com/tmlr-group/EOE。"
}
{
  "title": "A Dense Reward View on Aligning Text-to-Image Diffusion with Preference",
  "title_zh": "标题：基于密集奖励视角的文本到图像扩散模型与偏好的对齐",
  "abstract": "Aligning text-to-image diffusion model (T2I) with preference has been gaining increasing research attention. While prior works exist on directly optimizing T2I by preference data, these methods are developed under the bandit assumption of a latent reward on the entire diffusion reverse chain, while ignoring the sequential nature of the generation process. This may harm the efficacy and efficiency of preference alignment. In this paper, we take on a finer dense reward perspective and derive a tractable alignment objective that emphasizes the initial steps of the T2I reverse chain. In particular, we introduce temporal discounting into DPO-style explicit-reward-free objectives, to break the temporal symmetry therein and suit the T2I generation hierarchy. In experiments on single and multiple prompt generation, our method is competitive with strong relevant baselines, both quantitatively and qualitatively. Further investigations are conducted to illustrate the insight of our approach. Source code is available at https://github.com/Shentao-YANG/Dense_Reward_T2I .",
  "abstract_zh": "摘要：将文本到图像扩散模型（T2I）与偏好对齐正受到越来越多的研究关注。尽管之前的工作已经存在于通过偏好数据直接优化T2I，但这些方法是在整个扩散反向链上的潜在奖励的赌博假设下开发的，同时忽视了生成过程的顺序特性。这可能会损害偏好对齐的有效性和效率。在本文中，我们采用更细致的密集奖励视角，推导出一个可处理的对齐目标，强调T2I反向链的初始步骤。特别地，我们将时间折扣引入DPO风格的显式无奖励目标，以打破其中的时间对称性，并适应T2I生成层次。在单个和多个提示生成的实验中，我们的方法在定量和定性上均与强相关基线具有竞争力。进一步的研究表明了我们方法的洞察。源代码可在 https://github.com/Shentao-YANG/Dense_Reward_T2I 获取。"
}
{
  "title": "Do Efficient Transformers Really Save Computation?",
  "title_zh": "高效变换器真的能节省计算资源吗？",
  "abstract": "As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size. Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer. We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses.",
  "abstract_zh": "随着基于变换器的语言模型在越来越大的数据集上训练并具有大量参数，寻找比标准变换器更高效的替代方案变得非常重要。尽管提出了许多高效变换器和变换器替代方案，但没有一个提供理论保证，证明它们可以作为标准变换器的合适替代品。这使得识别何时使用特定模型以及优先考虑哪些方向进行进一步研究变得具有挑战性。在本文中，我们旨在理解高效变换器的能力和局限性，特别是稀疏变换器和线性变换器。我们关注它们在链式思维（CoT）提示下表现出的推理能力，并遵循之前的研究将其建模为动态规划（DP）问题。我们的结果表明，尽管这些模型足够表达以解决一般的DP任务，但与预期相反，它们需要与问题规模成比例的模型大小。尽管如此，我们识别出一类DP问题，对于这些问题，这些模型可能比标准变换器更高效。我们通过在代表性DP任务上的实验确认了我们的理论结果，进一步加深了对高效变换器实际优缺点的理解。"
}
{
  "title": "LLM-Empowered State Representation for Reinforcement Learning",
  "title_zh": "LLM赋能的强化学习状态表示",
  "abstract": "Conventional state representations in reinforcement learning often omit critical task-related details, presenting a significant challenge for value networks in establishing accurate mappings from states to task rewards. Traditional methods typically depend on extensive sample learning to enrich state representations with task-specific information, which leads to low sample efficiency and high time costs. Recently, surging knowledgeable large language models (LLM) have provided promising substitutes for prior injection with minimal human intervention. Motivated by this, we propose LLM-Empowered State Representation (LESR), a novel approach that utilizes LLM to autonomously generate task-related state representation codes which help to enhance the continuity of network mappings and facilitate efficient training. Experimental results demonstrate LESR exhibits high sample efficiency and outperforms state-of-the-art baselines by an average of **29%** in accumulated reward in Mujoco tasks and **30%** in success rates in Gym-Robotics tasks. Codes of LESR are accessible at https://github.com/thu-rllab/LESR.",
  "abstract_zh": "传统的强化学习状态表示往往忽略关键的任务相关细节，这给价值网络从状态到任务奖励的准确映射带来了重大挑战。传统方法通常依赖于大量样本学习来丰富状态表示中的任务特定信息，这导致样本效率低下和时间成本高昂。最近，涌现的知识丰富的大型语言模型（LLM）为之前的注入提供了有前景的替代方案，且人类干预最小。基于此，我们提出了LLM赋能的状态表示（LESR），这是一种新颖的方法，利用LLM自主生成与任务相关的状态表示代码，帮助增强网络映射的连续性并促进高效训练。实验结果表明，LESR展现出高样本效率，并在Mujoco任务中平均超越最先进基线**29%**的累积奖励，在Gym-Robotics任务中超越**30%**的成功率。LESR的代码可在https://github.com/thu-rllab/LESR获取。"
}
{
  "title": "Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning",
  "title_zh": "掌握机器人操作：通过预训练和多任务微调实现多模态提示",
  "abstract": "Prompt-based learning has been demonstrated as a compelling paradigm contributing to large language models' tremendous success (LLMs). Inspired by their success in language tasks, existing research has leveraged LLMs in embodied instruction following and task planning. In this work, we tackle the problem of training a robot to understand multimodal prompts, interleaving vision signals with text descriptions. This type of task poses a major challenge to robots' capability to understand the interconnection and complementarity between vision and language signals. In this work, we introduce an effective framework that learns a policy to perform robot manipulation with multimodal prompts from multi-task expert trajectories. Our methods consist of a two-stage training pipeline that performs inverse dynamics pretraining and multi-task finetuning. To facilitate multimodal understanding, we design our multimodal prompt encoder by augmenting a pretrained LM with a residual connection to the visual input and model the dependencies among action dimensions. Empirically, we evaluate the efficacy of our method on the VIMA-BENCH and establish a new state-of-the-art (10% improvement in success rate). Moreover, we demonstrate that our model exhibits remarkable in-context learning ability.",
  "abstract_zh": "基于提示的学习已被证明是一个引人注目的范式，为大型语言模型（LLMs）的巨大成功做出了贡献。受到其在语言任务中成功的启发，现有研究已利用LLMs进行具身指令跟随和任务规划。在本研究中，我们解决了训练机器人理解多模态提示的问题，将视觉信号与文本描述交织在一起。这类任务对机器人理解视觉与语言信号之间的相互联系和互补性提出了重大挑战。我们提出了一个有效的框架，通过多任务专家轨迹学习策略，以执行多模态提示下的机器人操作。我们的方法包括一个两阶段的训练流程，进行逆动力学预训练和多任务微调。为了促进多模态理解，我们设计了多模态提示编码器，通过残差连接将预训练的语言模型与视觉输入相结合，并建模动作维度之间的依赖关系。通过实证评估，我们在VIMA-BENCH上验证了我们方法的有效性，并建立了新的最先进水平（成功率提高10%）。此外，我们还展示了我们的模型具有显著的上下文学习能力。"
}
{
  "title": "EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism",
  "title_zh": "EE-LLM：基于3D并行的大规模早期退出大型语言模型的训练与推理",
  "abstract": "We present EE-LLM, a framework for large-scale training and inference of early-exit large language models (LLMs). While recent works have shown preliminary evidence for the efficacy of early exiting in accelerating LLM inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs by supporting their training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and performance optimizations tailored to early exiting, including a lightweight method that facilitates backpropagation for the early-exit training objective with pipeline parallelism, techniques of leveraging idle resources in the original pipeline schedule for computation related to early-exit layers, and two approaches of early-exit inference that are compatible with KV caching for autoregressive generation. Our analytical and empirical study shows that EE-LLM achieves great training efficiency with negligible computational overhead compared to standard LLM training, as well as outstanding inference speedup without compromising output quality. To facilitate further research and adoption, we release EE-LLM at https://github.com/pan-x-c/EE-LLM.",
  "abstract_zh": "我们提出了EE-LLM，这是一个用于大规模训练和推理早期退出大型语言模型（LLMs）的框架。尽管近期的研究已初步证明早期退出在加速LLM推理方面的有效性，EE-LLM通过支持大规模3D并行，迈出了扩展早期退出LLMs的基础性一步。EE-LLM基于Megatron-LM，实施了一系列针对早期退出的算法创新和性能优化，包括一种轻量级方法，利用管道并行性促进早期退出训练目标的反向传播，利用原始管道调度中的空闲资源进行与早期退出层相关的计算的技术，以及两种与KV缓存兼容的自回归生成的早期退出推理方法。我们的分析和实证研究表明，与标准LLM训练相比，EE-LLM在计算开销几乎可以忽略的情况下实现了出色的训练效率，并且在不影响输出质量的前提下实现了卓越的推理加速。为了促进进一步的研究和应用，我们在https://github.com/pan-x-c/EE-LLM发布了EE-LLM。"
}
{
  "title": "Attention Meets Post-hoc Interpretability: A Mathematical Perspective",
  "title_zh": "注意力与事后可解释性：一种数学视角",
  "abstract": "Attention-based architectures, in particular transformers, are at the heart of a technological revolution. Interestingly, in addition to helping obtain state-of-the-art results on a wide range of applications, the attention mechanism intrinsically provides meaningful insights on the internal behavior of the model. Can these insights be used as explanations? Debate rages on. In this paper, we mathematically study a simple attention-based architecture and pinpoint the differences between post-hoc and attention-based explanations. We show that they provide quite different results, and that, despite their limitations, post-hoc methods are capable of capturing more useful insights than merely examining the attention weights.",
  "abstract_zh": "基于注意力的架构，特别是变换器，正处于技术革命的核心。有趣的是，除了在广泛应用中帮助获得最先进的结果外，注意力机制本质上还提供了对模型内部行为的有意义见解。这些见解能否作为解释使用？争论仍在继续。本文从数学角度研究了一种简单的基于注意力的架构，并指出事后解释与基于注意力的解释之间的差异。我们展示了它们提供了截然不同的结果，并且尽管存在局限性，事后方法能够捕捉到比单纯检查注意力权重更有用的见解。"
}
{
  "title": "The Good, The Bad, and Why: Unveiling Emotions in Generative AI",
  "title_zh": "标题：好与坏及其原因：揭示生成性人工智能中的情感",
  "abstract": "Emotion significantly impacts our daily behaviors and interactions. While recent generative AI models, such as large language models, have shown impressive performance in various tasks, it remains unclear whether they truly comprehend emotions and why. This paper aims to address this gap by incorporating psychological theories to gain a holistic understanding of emotions in generative AI models. Specifically, we propose three approaches: 1) EmotionPrompt to enhance AI model performance, 2) EmotionAttack to impair AI model performance, and 3) EmotionDecode to explain the effects of emotional stimuli, both benign and malignant. Through extensive experiments involving language and multi-modal models on semantic understanding, logical reasoning, and generation tasks, we demonstrate that both textual and visual EmotionPrompt can boost the performance of AI models while EmotionAttack can hinder it. More importantly, EmotionDecode reveals that AI models can comprehend emotional stimuli akin to the mechanism of dopamine in the human brain. Our work heralds a novel avenue for exploring psychology to enhance our understanding of generative AI models, thus boosting the research and development of human-AI collaboration and mitigating potential risks.",
  "abstract_zh": "摘要：情感对我们的日常行为和互动有着重要影响。尽管最近的生成性人工智能模型，如大型语言模型，在各种任务中表现出色，但它们是否真正理解情感及其原因仍不清楚。本文旨在通过结合心理学理论来填补这一空白，以全面理解生成性人工智能模型中的情感。具体而言，我们提出了三种方法：1）EmotionPrompt以增强人工智能模型的性能，2）EmotionAttack以削弱人工智能模型的性能，3）EmotionDecode以解释情感刺激的影响，包括良性和恶性。通过对语言和多模态模型在语义理解、逻辑推理和生成任务上的广泛实验，我们证明了文本和视觉的EmotionPrompt均能提升人工智能模型的性能，而EmotionAttack则会阻碍其表现。更重要的是，EmotionDecode揭示了人工智能模型能够理解情感刺激，类似于人脑中多巴胺的机制。我们的工作为探索心理学以增强对生成性人工智能模型的理解开辟了新的途径，从而推动人机协作的研究与开发，并减轻潜在风险。"
}
{
  "title": "CompeteAI: Understanding the Competition Dynamics of Large Language Model-based Agents",
  "title_zh": "竞争人工智能：理解基于大型语言模型的代理的竞争动态",
  "abstract": "Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. Although most of the work has focused on cooperation and collaboration between agents, little work explores *competition*, another important mechanism that promotes the development of society and economy. In this paper, we seek to examine the competition dynamics in LLM-based agents. We first propose a general framework for studying the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, including restaurant agents and customer agents. Specifically, the restaurant agents compete with each other to attract more customers, where competition encourages them to transform, such as cultivating new operating strategies. Simulation experiments reveal several interesting findings at the micro and macro levels, which align well with existing market and sociological theories. We hope that the framework and environment can be a promising testbed to study the competition that fosters understanding of society. Code is available at: https://github.com/microsoft/competeai.",
  "abstract_zh": "大型语言模型（LLMs）已被广泛用作代理来完成不同任务，例如个人助手或活动规划。尽管大多数研究集中在代理之间的合作与协作上，但很少有研究探讨*竞争*这一促进社会和经济发展的重要机制。本文旨在研究基于LLM的代理的竞争动态。我们首先提出了一个研究代理之间竞争的一般框架。然后，我们使用GPT-4实现了一个实际的竞争环境，以模拟一个包含两种类型代理的虚拟城镇，包括餐厅代理和顾客代理。具体而言，餐厅代理之间相互竞争以吸引更多顾客，竞争促使他们进行转型，例如培养新的运营策略。仿真实验揭示了微观和宏观层面的一些有趣发现，这些发现与现有的市场和社会学理论相一致。我们希望该框架和环境能够成为研究促进社会理解的竞争的有前景的测试平台。代码可在：https://github.com/microsoft/competeai获取。"
}
{
  "title": "InferCept: Efficient Intercept Support for Augmented Large Language Model Inference",
  "title_zh": "推断拦截：增强型大型语言模型推理的高效拦截支持",
  "abstract": "Large language models are increasingly integrated with external environments, tools, and agents like ChatGPT plugins to extend their capability beyond language-centric tasks. However, today's LLM inference systems are designed for standalone LLMs. They treat each external interaction as the end of LLM generation and form a new request when the interaction finishes, causing unnecessary recomputation of already computed contexts, which accounts for 37-40% of total model forwarding time. This paper presents **InferCept, the first LLM inference framework targeting augmented LLMs** and supporting the efficient interception of LLM generation. InferCept minimizes the GPU resource waste caused by LLM interceptions and dedicates saved memory for serving more requests.InferCept improves the overall serving throughput by **1.6x-2x** and completes 2x more requests per second compared to the state-of-the-art LLM inference systems.",
  "abstract_zh": "大型语言模型越来越多地与外部环境、工具和代理（如ChatGPT插件）集成，以扩展其超越语言中心任务的能力。然而，当前的LLM推理系统是为独立的LLM设计的。它们将每个外部交互视为LLM生成的结束，并在交互完成时形成新的请求，导致已计算上下文的不必要重新计算，这占总模型前向时间的37-40%。本文提出了**InferCept，这是第一个针对增强型LLM的推理框架**，支持高效拦截LLM生成。InferCept最小化了因LLM拦截而造成的GPU资源浪费，并将节省的内存用于处理更多请求。与最先进的LLM推理系统相比，InferCept将整体服务吞吐量提高了**1.6倍至2倍**，每秒完成的请求数量增加了2倍。"
}
{
  "title": "Fast Adversarial Attacks on Language Models In One GPU Minute",
  "title_zh": "快速对语言模型进行对抗攻击：仅需一分钟GPU时间",
  "abstract": "In this paper, we introduce a novel class of fast, beam search-based adversarial attack (BEAST) for Language Models (LMs). BEAST employs interpretable parameters, enabling attackers to balance between attack speed, success rate, and the readability of adversarial prompts. The computational efficiency of BEAST facilitates us to investigate its applications on LMs for jailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free targeted attack can jailbreak aligned LMs with high attack success rates within one minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minute with a success rate of 89% when compared to a gradient-based baseline that takes over an hour to achieve 70% success rate using a single Nvidia RTX A6000 48GB GPU. BEAST can also generate adversarial suffixes for successful jailbreaks that can transfer to unseen prompts and unseen models such as GPT-4-Turbo. Additionally, we discover a unique outcome wherein our untargeted attack induces hallucinations in LM chatbots. Through human evaluations, we find that our untargeted attack causes Vicuna-7B-v1.5 to produce $\\sim$15% more incorrect outputs when compared to LM outputs in the absence of our attack. We also learn that 22% of the time, BEAST causes Vicuna to generate outputs that are not relevant to the original prompt. Further, we use BEAST to generate adversarial prompts in a few seconds that can boost the performance of existing membership inference attacks for LMs. We believe that our fast attack, BEAST, has the potential to accelerate research in LM security and privacy.",
  "abstract_zh": "本文介绍了一种新型的快速基于束搜索的对抗攻击方法（BEAST），用于语言模型（LMs）。BEAST采用可解释的参数，使攻击者能够在攻击速度、成功率和对抗提示的可读性之间取得平衡。BEAST的计算效率使我们能够研究其在LMs上的应用，包括越狱、引发幻觉和隐私攻击。我们的无梯度目标攻击可以在一分钟内以高成功率越狱对齐的LMs。例如，与使用单个Nvidia RTX A6000 48GB GPU的梯度基线相比，BEAST在一分钟内成功越狱Vicuna-7B-v1.5的成功率为89%，而梯度基线需要超过一个小时才能达到70%的成功率。BEAST还可以生成成功越狱的对抗后缀，这些后缀可以转移到未见的提示和未见的模型，如GPT-4-Turbo。此外，我们发现一个独特的结果，即我们的无目标攻击会在LM聊天机器人中引发幻觉。通过人工评估，我们发现与没有攻击的LM输出相比，我们的无目标攻击使Vicuna-7B-v1.5产生了约15%的错误输出。我们还了解到，22%的情况下，BEAST导致Vicuna生成与原始提示无关的输出。此外，我们使用BEAST在几秒钟内生成对抗提示，可以提升现有LM的成员推断攻击性能。我们相信，我们的快速攻击BEAST有潜力加速LM安全和隐私的研究。"
}
{
  "title": "Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback",
  "title_zh": "标题：立场：社会选择应指导人工智能对多样化人类反馈的对齐",
  "abstract": "Foundation models such as GPT-4 are fine-tuned to avoid unsafe or otherwise problematic behavior, such as helping to commit crimes or producing racist text. One approach to fine-tuning, called reinforcement learning from human feedback, learns from humans’ expressed preferences over multiple outputs. Another approach is constitutional AI, in which the input from humans is a list of high-level principles. But how do we deal with potentially diverging input from humans? How can we aggregate the input into consistent data about “collective” preferences or otherwise use it to make collective choices about model behavior? In this paper, we argue that the field of social choice is well positioned to address these questions, and we discuss ways forward for this agenda, drawing on discussions in a recent workshop on Social Choice for AI Ethics and Safety held in Berkeley, CA, USA in December 2023.",
  "abstract_zh": "摘要：基础模型如GPT-4经过微调，以避免不安全或其他问题行为，例如协助犯罪或生成种族主义文本。一种微调方法称为基于人类反馈的强化学习，它从人类对多个输出的表达偏好中学习。另一种方法是宪法人工智能，其中人类输入是一系列高层原则。但我们如何处理人类可能存在的不同输入？我们如何将这些输入汇总为关于“集体”偏好的一致数据，或以其他方式利用它来对模型行为做出集体选择？在本文中，我们认为社会选择领域非常适合解决这些问题，并讨论了推动这一议程的前进方向，借鉴了2023年12月在美国加州伯克利举行的人工智能伦理与安全社会选择研讨会的讨论。"
}
{
  "title": "PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning",
  "title_zh": "PICLe：通过个性上下文学习从大型语言模型中引发多样化行为",
  "abstract": "Large Language Models (LLMs) are trained on massive text corpora, which are encoded with diverse personality traits. This triggers an interesting goal of eliciting a desired personality trait from the LLM, and probing its behavioral preferences. Accordingly, we formalize the persona elicitation task, aiming to customize LLM behaviors to align with a target persona. We present Persona In-Context Learning (PICLe), a novel persona elicitation framework grounded in Bayesian inference. At the core, PICLe introduces a new ICL example selection criterion based on likelihood ratio, which is designed to optimally guide the model in eliciting a specific target persona. We demonstrate the effectiveness of PICLe through extensive comparisons against baseline methods across three contemporary LLMs. Code is available at https://github.com/deeplearning-wisc/picle.",
  "abstract_zh": "大型语言模型（LLMs）在大量文本语料库上进行训练，这些语料库编码了多样的个性特征。这引发了一个有趣的目标，即从LLM中引出所需的个性特征，并探测其行为偏好。因此，我们正式定义了个性引出任务，旨在定制LLM行为以与目标个性对齐。我们提出了个性上下文学习（PICLe），这是一个基于贝叶斯推理的新型个性引出框架。PICLe的核心引入了一种基于似然比的新ICL示例选择标准，旨在最佳引导模型引出特定目标个性。我们通过与三种现代LLM的基线方法进行广泛比较，展示了PICLe的有效性。代码可在https://github.com/deeplearning-wisc/picle获取。"
}
{
  "title": "One for All: A Universal Generator for Concept Unlearnability via Multi-Modal Alignment",
  "title_zh": "一切为一：通过多模态对齐实现概念不可学习性的通用生成器",
  "abstract": "The abundance of free internet data offers unprecedented opportunities for researchers and developers, but it also poses privacy risks. Utilizing data without explicit consent raises critical challenges in protecting personal information.Unlearnable examples have emerged as a feasible protection approach, which renders the data unlearnable, i.e., useless to third parties, by injecting imperceptible perturbations. However, these perturbations only exhibit unlearnable effects on either a particular dataset or label-consistent scenarios, thereby lacking broad applicability. To address both issues concurrently, we propose a universal perturbation generator that harnesses data with concept unlearnability, thereby broadening the scope of unlearnability beyond specific datasets or labels. Specifically, we leverage multi-modal pre-trained models to establish a connection between the data concepts in a shared embedding space. This connection enables the information transformation from image data to text concepts. Consequently, we can align the text embedding using concept-wise discriminant loss, and render the data unlearnable. Extensive experiments conducted on real-world datasets demonstrate the concept unlearnability, i.e., cross-dataset transferability and label-agnostic utility, of our proposed unlearnable examples, as well as their robustness against attacks.",
  "abstract_zh": "丰富的互联网免费数据为研究人员和开发者提供了前所未有的机会，但也带来了隐私风险。在没有明确同意的情况下使用数据会在保护个人信息方面带来重大挑战。不可学习的示例作为一种可行的保护方法应运而生，通过注入不可察觉的扰动使数据变得不可学习，即对第三方无用。然而，这些扰动仅在特定数据集或标签一致的场景中表现出不可学习的效果，因此缺乏广泛的适用性。为同时解决这两个问题，我们提出了一种通用扰动生成器，利用具有概念不可学习性的数据，从而将不可学习性的范围扩展到特定数据集或标签之外。具体而言，我们利用多模态预训练模型在共享嵌入空间中建立数据概念之间的联系。这种联系使得信息能够从图像数据转化为文本概念。因此，我们可以使用概念判别损失对文本嵌入进行对齐，从而使数据不可学习。在真实世界数据集上进行的大量实验表明，我们提出的不可学习示例具有概念不可学习性，即跨数据集可迁移性和标签无关的实用性，以及对攻击的鲁棒性。"
}
{
  "title": "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator",
  "title_zh": "代码链：使用语言模型增强的代码仿真器进行推理",
  "abstract": "Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter – we hypothesize that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for semantic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for \"detect_sarcasm(string)\" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they not only write code, but also selectively \"emulate\" the interpreter by generating the expected output of \"detect_sarcasm(string)\". In this work, we propose Chain of Code (CoC), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an \"LMulator\"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over Chain of Thought. In a nutshell, CoC broadens the scope of reasoning questions that LMs can answer by \"thinking in code\".",
  "abstract_zh": "代码提供了一种通用的语法结构，用于构建复杂程序并在与代码解释器配对时执行精确计算——我们假设语言模型（LMs）可以利用代码编写来改善链式思维推理，不仅适用于逻辑和算术任务，还适用于语义任务（特别是那些两者混合的任务）。例如，考虑提示一个LM编写代码来计算它在一篇文章中检测到讽刺的次数：LM可能难以编写一个可以被解释器执行的“detect_sarcasm(string)”实现（处理边缘情况将是不可逾越的）。然而，如果LM不仅编写代码，还通过生成“detect_sarcasm(string)”的预期输出来选择性地“仿真”解释器，它们仍然可能产生有效的解决方案。在这项工作中，我们提出了代码链（CoC），这是一种简单但出人意料地有效的扩展，改善了LM驱动的代码推理。关键思想是鼓励LM将语义子任务格式化为灵活的伪代码，以便解释器能够明确捕捉未定义行为并交给LM进行模拟（作为“LMulator”）。实验表明，代码链在各种基准测试中优于链式思维和其他基线；在BIG-Bench Hard上，代码链达到了84%，比链式思维提高了12%。简而言之，CoC通过“用代码思考”拓宽了LM可以回答的推理问题的范围。"
}
{
  "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
  "title_zh": "RLAIF与RLHF：利用AI反馈扩展人类反馈的强化学习",
  "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al. (2022b), offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards \"self-improvement\" by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.",
  "abstract_zh": "基于人类反馈的强化学习（RLHF）已被证明在使大型语言模型（LLM）与人类偏好对齐方面有效，但收集高质量的偏好标签成本高昂。Bai等人（2022b）提出的AI反馈强化学习（RLAIF）提供了一种有前景的替代方案，通过使用现成的LLM生成的偏好来训练奖励模型（RM）。在摘要、有效对话生成和无害对话生成等任务中，我们展示了RLAIF的性能与RLHF相当。此外，我们通过证明RLAIF能够超越监督微调基线，即使AI标注者与策略的规模相同，甚至与初始策略的检查点完全相同，迈出了“自我改进”的一步。最后，我们介绍了直接RLAIF（d-RLAIF）——一种通过在RL过程中直接从现成的LLM获取奖励来规避RM训练的技术，其性能优于经典的RLAIF。我们的结果表明，RLAIF能够实现与使用人类反馈相当的性能，为RLHF的可扩展性限制提供了潜在解决方案。"
}
{
  "title": "Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game",
  "title_zh": "语言代理与强化学习在狼人游戏中的战略玩法",
  "abstract": "Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model's training data and results in suboptimal performance. To develop *strategic language agents*, i.e., agents that generate flexible language actions and possess strong decision-making abilities, we propose a novel framework that powers LLM-based agents with reinforcement learning (RL). We consider Werewolf, a popular social deduction game, as a challenging testbed that emphasizes versatile communication and strategic gameplay. To mitigate the intrinsic bias in language actions, our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidates to play in the game. Extensive experiments show that our agents overcome the intrinsic bias and outperform existing LLM-based agents in the Werewolf game. We also conduct human-agent experiments and find that our agents achieve human-level performance and demonstrate strong strategic play.",
  "abstract_zh": "基于大型语言模型（LLMs）构建的代理在广泛领域中展现出巨大潜力。然而，在复杂决策任务中，纯 LLM 基础的代理往往在行动选择上表现出内在偏见，这种偏见源自模型的训练数据，导致次优性能。为了开发*战略语言代理*，即能够生成灵活语言行动并具备强大决策能力的代理，我们提出了一种新颖的框架，通过强化学习（RL）为基于 LLM 的代理提供支持。我们将狼人游戏作为一个强调多样化沟通和战略游戏玩法的挑战性测试平台。为了减轻语言行动中的内在偏见，我们的代理使用 LLM 进行推理，并生成多样的行动候选。然后，经过训练以优化决策能力的 RL 策略从候选中选择一个行动进行游戏。大量实验表明，我们的代理克服了内在偏见，并在狼人游戏中超越了现有的基于 LLM 的代理。我们还进行了人机实验，发现我们的代理达到了人类水平的表现，并展现出强大的战略玩法。"
}
{
  "title": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference",
  "title_zh": "标题：利用LESS：通过KV缓存压缩合成递归以实现高效的LLM推理",
  "abstract": "Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS.",
  "abstract_zh": "摘要：许多计算因素限制了大型语言模型的更广泛部署。本文关注于由键值（KV）缓存施加的内存瓶颈，这是一种在解码过程中需要存储先前KV对的计算快捷方式。虽然现有的KV缓存方法通过修剪或驱逐大量相对不重要的KV对来显著减少缓存的内存占用，但在需要回忆大多数先前标记的任务中，它们的成功有限。为了解决这个问题，我们提出了LESS，这是一种将（几乎免费的）恒定大小缓存与基于驱逐的缓存方法简单集成的方案，使得所有标记可以在后续解码步骤中进行查询。它在时间上保持信息的能力在多种任务中显示出优势，我们证明LESS可以帮助缩小与缓存所有内容的性能差距，有时甚至可以匹配，同时保持高效。相关代码可以在https://github.com/hdong920/LESS找到。"
}
{
  "title": "On Prompt-Driven Safeguarding for Large Language Models",
  "title_zh": "关于大语言模型的提示驱动安全保障",
  "abstract": "Prepending model inputs with safety prompts is a common practice for safeguarding large language models (LLMs) against queries with harmful intents. However, the underlying working mechanisms of safety prompts have not been unraveled yet, restricting the possibility of automatically optimizing them to improve LLM safety. In this work, we investigate how LLMs' behavior (i.e., complying with or refusing user queries) is affected by safety prompts from the perspective of model representation. We find that in the representation space, the input queries are typically moved by safety prompts in a \"higher-refusal\" direction, in which models become more prone to refusing to provide assistance, even when the queries are harmless. On the other hand, LLMs are naturally capable of distinguishing harmful and harmless queries without safety prompts. Inspired by these findings, we propose a method for safety prompt optimization, namely DRO (Directed Representation Optimization). Treating a safety prompt as continuous, trainable embeddings, DRO learns to move the queries' representations along or opposite the refusal direction, depending on their harmfulness. Experiments with eight LLMs on out-of-domain and jailbreak benchmarks demonstrate that DRO remarkably improves the safeguarding performance of human-crafted safety prompts, without compromising the models' general performance.",
  "abstract_zh": "在大语言模型（LLMs）中，使用安全提示预先处理模型输入是一种常见做法，旨在保护其免受有害意图查询的影响。然而，安全提示的潜在工作机制尚未被揭示，这限制了自动优化它们以提高LLM安全性的可能性。在本研究中，我们从模型表示的角度探讨了安全提示如何影响LLM的行为（即遵从或拒绝用户查询）。我们发现，在表示空间中，输入查询通常被安全提示向“更高拒绝”方向移动，这使得模型更倾向于拒绝提供帮助，即使查询是无害的。另一方面，LLMs自然能够在没有安全提示的情况下区分有害和无害查询。受到这些发现的启发，我们提出了一种安全提示优化方法，即DRO（定向表示优化）。DRO将安全提示视为连续的、可训练的嵌入，根据查询的有害性学习沿拒绝方向或相反方向移动查询的表示。在八个LLM的域外和越狱基准测试中的实验表明，DRO显著提高了人类设计的安全提示的保护性能，而不影响模型的整体性能。"
}
{
  "title": "Distinguishing the Knowable from the Unknowable with Language Models",
  "title_zh": "标题：利用语言模型区分可知与不可知",
  "abstract": "We study the feasibility of identifying *epistemic* uncertainty (reflecting a lack of knowledge), as opposed to *aleatoric* uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings. Code can be found at: https://github.com/KempnerInstitute/llm_uncertainty",
  "abstract_zh": "摘要：我们研究了在大型语言模型（LLMs）输出的自由文本中识别*认识论*不确定性（反映知识缺乏）与*随机*不确定性（反映基础分布中的熵）的可行性。在缺乏真实概率的情况下，我们探索了一种设置，其中一个显著更大的模型作为真实情况的代理，以（近似）解开给定LLM的不确定性。我们展示了在冻结的预训练模型的嵌入上训练的小型线性探针能够准确预测较大模型在标记级别上何时会更有信心，并且在一个文本领域上训练的探针能够推广到其他领域。更进一步，我们提出了一种完全无监督的方法，在同一任务上实现了非平凡的准确性。综合来看，我们将这些结果解释为证据，表明LLMs自然包含不同类型不确定性的内部表示，这些表示可能被利用来设计在多种实际环境中更具信息性的模型信心指标。代码可在：https://github.com/KempnerInstitute/llm_uncertainty 找到。"
}
{
  "title": "Language Models with Conformal Factuality Guarantees",
  "title_zh": "符合事实性的语言模型保证",
  "abstract": "Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem. In this work, we propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction. Our insight is that the correctness of an LM output is equivalent to an uncertainty quantification problem, where the uncertainty sets are defined as the entailment set of an LM's output. Using this connection, we show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific (and expanding the associated uncertainty sets). This approach applies to any black-box LM and requires very few human-annotated samples. Evaluations of our approach on closed book QA (FActScore, NaturalQuestions) and reasoning tasks (MATH) show that our approach can provide 80-90% correctness guarantees while retaining the majority of the LM's original output.",
  "abstract_zh": "保证语言模型（LM）输出的正确性和事实性是一个主要的开放问题。在这项工作中，我们提出了符合事实性，这是一种通过将语言建模与符合预测相连接来确保LM高概率正确性保证的框架。我们的见解是，LM输出的正确性等同于一个不确定性量化问题，其中不确定性集被定义为LM输出的蕴含集。利用这一联系，我们展示了语言模型中的符合预测对应于一种回退算法，该算法通过逐步使LM输出变得不那么具体（并扩展相关的不确定性集）来提供高概率的正确性保证。这种方法适用于任何黑箱LM，并且只需要很少的人类标注样本。我们在闭卷问答（FActScore，NaturalQuestions）和推理任务（MATH）上的评估表明，我们的方法可以在保留大部分LM原始输出的同时提供80-90%的正确性保证。"
}
{
  "title": "GPTSwarm: Language Agents as Optimizable Graphs",
  "title_zh": "GPTSwarm：可优化图的语言代理",
  "abstract": "Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. Our code is public.",
  "abstract_zh": "各种人类设计的提示工程技术已被提出，以改善基于大型语言模型（LLMs）的问题解决者，产生了许多不同的代码库。我们通过将基于LLM的代理描述为计算图来统一这些方法。节点实现处理多模态数据或查询LLM的功能，边描述操作之间的信息流。图可以递归组合成更大的复合图，表示代理间协作的层次结构（其中边连接不同代理的操作）。我们新颖的自动图优化器（1）精炼节点级LLM提示（节点优化）和（2）通过改变图的连通性改善代理编排（边优化）。实验表明，我们的框架可以有效地开发、集成和自动改进各种LLM代理。我们的代码是公开的。"
}
{
  "title": "An LLM Compiler for Parallel Function Calling",
  "title_zh": "并行函数调用的LLM编译器",
  "abstract": "The reasoning capabilities of the recent LLMs enable them to execute external function calls to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has allowed LLMs to select and coordinate multiple functions based on the context to tackle more complex problems. However, current methods for function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multiple function calls. Drawing inspiration from the principles of classical compilers, LLMCompiler enables parallel function calling with three components: (i) a Function Calling Planner, formulating execution plans for function calling; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. LLMCompiler automatically generates an optimized orchestration for the function calls and can be used with both open-source and closed-source models. We have benchmarked LLMCompiler on a range of tasks with different patterns of function calling. We observe consistent latency speedup of up to $3.7 \\times$, cost savings of up to $6.7 \\times$, and accuracy improvement of up to $\\sim 9 \\%$ compared to ReAct.Our code is available at https://github.com/SqueezeAILab/LLMCompiler.",
  "abstract_zh": "最近的LLM的推理能力使它们能够执行外部函数调用，以克服其固有的局限性，如知识截止、较差的算术技能或缺乏对私有数据的访问。这一发展使得LLM能够根据上下文选择和协调多个函数，以解决更复杂的问题。然而，当前的函数调用方法通常需要对每个函数进行顺序推理和操作，这可能导致高延迟、高成本，并且有时会出现不准确的行为。为了解决这个问题，我们引入了LLMCompiler，它并行执行函数以高效协调多个函数调用。LLMCompiler借鉴了经典编译器的原理，具备三个组件： (i) 函数调用规划器，制定函数调用的执行计划； (ii) 任务获取单元，调度函数调用任务； (iii) 执行器，并行执行这些任务。LLMCompiler自动生成函数调用的优化协调，并可与开源和闭源模型一起使用。我们在不同的函数调用模式下对LLMCompiler进行了基准测试。我们观察到，与ReAct相比，延迟速度提升最高可达$3.7 \\times$，成本节省最高可达$6.7 \\times$，准确率提升最高可达$\\sim 9 \\%$。我们的代码可在https://github.com/SqueezeAILab/LLMCompiler获得。"
}
{
  "title": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation",
  "title_zh": "可压缩动态在深度过参数化低秩学习与适应中的应用",
  "abstract": "While overparameterization in machine learning models offers great benefits in terms of optimization and generalization, it also leads to increased computational requirements as model sizes grow. In this work, we show that by leveraging the inherent low-dimensional structures of data and compressible dynamics within the model parameters, we can reap the benefits of overparameterization without the computational burdens. In practice, we demonstrate the effectiveness of this approach for deep low-rank matrix completion as well as fine-tuning language models. Our approach is grounded in theoretical findings for deep overparameterized low-rank matrix recovery, where we show that the learning dynamics of each weight matrix are confined to an invariant low-dimensional subspace. Consequently, we can construct and train compact, highly compressed factorizations possessing the same benefits as their overparameterized counterparts. In the context of deep matrix completion, our technique substantially improves training efficiency while retaining the advantages of overparameterization. For language model fine-tuning, we propose a method called \"Deep LoRA\", which improves the existing low-rank adaptation (LoRA) technique, leading to reduced overfitting and a simplified hyperparameter setup, while maintaining comparable efficiency. We validate the effectiveness of Deep LoRA on natural language tasks, particularly when fine-tuning with limited data.",
  "abstract_zh": "尽管机器学习模型中的过参数化在优化和泛化方面提供了巨大好处，但随着模型规模的增长，它也导致了计算需求的增加。在本研究中，我们展示了通过利用数据的固有低维结构和模型参数中的可压缩动态，我们可以在不增加计算负担的情况下获得过参数化的好处。在实践中，我们证明了该方法在深度低秩矩阵补全和语言模型微调中的有效性。我们的方法基于深度过参数化低秩矩阵恢复的理论发现，表明每个权重矩阵的学习动态被限制在一个不变的低维子空间中。因此，我们可以构建和训练紧凑的、高度压缩的分解，具有与其过参数化对应物相同的好处。在深度矩阵补全的背景下，我们的技术显著提高了训练效率，同时保留了过参数化的优势。对于语言模型微调，我们提出了一种名为“Deep LoRA”的方法，改进了现有的低秩适应（LoRA）技术，减少了过拟合并简化了超参数设置，同时保持了相当的效率。我们在自然语言任务上验证了Deep LoRA的有效性，特别是在有限数据下进行微调时。"
}
{
  "title": "Position: Evolving AI Collectives Enhance Human Diversity and Enable Self-Regulation",
  "title_zh": "位置：演变中的人工智能集体增强人类多样性并实现自我调节",
  "abstract": "Large language model behavior is shaped by the language of those with whom they interact. This capacity and their increasing prevalence online portend that they will intentionally or unintentionally \"program\" one another and form emergent AI subjectivities, relationships, and collectives. Here, we call upon the research community to investigate these \"societies\" of interacting artificial intelligences to increase their rewards and reduce their risks for human society and the health of online environments. We use a small \"community\" of models and their evolving outputs to illustrate how such emergent, decentralized AI collectives can spontaneously expand the bounds of human diversity and reduce the risk of toxic, anti-social behavior online. Finally, we discuss opportunities for AI cross-moderation and address ethical issues and design challenges associated with creating and maintaining free-formed AI collectives.",
  "abstract_zh": "大型语言模型的行为受到与其互动的人的语言影响。这种能力及其在网上日益普及预示着它们将有意或无意地“编程”彼此，形成新兴的人工智能主体性、关系和集体。在此，我们呼吁研究界调查这些互动人工智能的“社会”，以增加它们对人类社会和在线环境健康的奖励并减少风险。我们使用一个小型“社区”的模型及其不断演变的输出，说明这种新兴的去中心化人工智能集体如何自发扩展人类多样性的边界，并减少在线有毒、反社会行为的风险。最后，我们讨论人工智能跨平台调节的机会，并解决与创建和维护自由形成的人工智能集体相关的伦理问题和设计挑战。"
}
{
  "title": "Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs",
  "title_zh": "任意精度大语言模型：低成本部署多种不同规模的大语言模型",
  "abstract": "Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces any-precision LLM, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit LLM. All the supported LLMs with varying bit-widths demonstrate state-of-the-art model quality and inference throughput, proving itself to be a compelling option for deployment of multiple, different-sized LLMs.",
  "abstract_zh": "近年来，许多努力集中在压缩大型语言模型（LLMs）上，这些模型在各种应用中展示了突破性的能力，但由于其庞大的体积，部署成本显著。同时，尽管部署多种不同规模的LLMs在实际应用中具有重要意义，但对此的关注却相对较少。因此，本文引入了任意精度LLM的概念，将任意精度深度神经网络（DNN）的理念扩展到LLMs。针对任意精度LLM面临的挑战，我们提出了一种轻量级的任意精度量化方法，利用后训练量化框架，并开发了一个专门的软件引擎以实现高效服务。结果，我们的解决方案通过将量化为不同位宽（如3、4、...、$n$位）的LLMs叠加到与单个$n$位LLM相当的内存占用中，显著降低了部署多种不同规模LLMs的高成本。所有支持的不同位宽LLMs均表现出最先进的模型质量和推理吞吐量，证明其在部署多种不同规模LLMs方面是一个引人注目的选择。"
}
{
  "title": "Asymmetry in Low-Rank Adapters of Foundation Models",
  "title_zh": "基础模型低秩适配器中的不对称性",
  "abstract": "Parameter-efficient fine-tuning optimizes large, pre-trained foundation models by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective. Inspired by an effort to investigate the different roles of LoRA matrices during fine-tuning, this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices. Specifically, when updating the parameter matrices of a neural network by adding a product $BA$, we observe that the $B$ and $A$ matrices have distinct functions: $A$ extracts features from the input, while $B$ uses these features to create the desired output. Based on this observation, we demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning $A$, and that a random untrained $A$ should perform nearly as well as a fine-tuned one. Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing that the parameter savings of exclusively training $B$ improves the bound. We support our conclusions with experiments on RoBERTa, BART-Large, LLaMA-2, and ViTs. The code and data is available at https://github.com/Jiacheng-Zhu-AIML/AsymmetryLoRA",
  "abstract_zh": "参数高效的微调通过更新一部分参数来优化大型预训练基础模型；在这一类中，低秩适配（LoRA）特别有效。受到研究LoRA矩阵在微调过程中不同角色的启发，本文描述并利用了低秩适配器矩阵重要性中的意外不对称性。具体而言，当通过添加乘积 $BA$ 来更新神经网络的参数矩阵时，我们观察到 $B$ 和 $A$ 矩阵具有不同的功能：$A$ 从输入中提取特征，而 $B$ 使用这些特征生成期望的输出。基于这一观察，我们证明微调 $B$ 本质上比微调 $A$ 更有效，并且一个随机未训练的 $A$ 的表现应与微调后的 $A$ 相近。通过信息论的视角，我们还界定了低秩适配器的泛化，表明仅训练 $B$ 的参数节省改善了界限。我们通过对 RoBERTa、BART-Large、LLaMA-2 和 ViTs 的实验支持我们的结论。代码和数据可在 https://github.com/Jiacheng-Zhu-AIML/AsymmetryLoRA 获取。"
}
{
  "title": "Faithfulness Measurable Masked Language Models",
  "title_zh": "忠实性可测量的掩码语言模型",
  "abstract": "A common approach to explaining NLP models is to use importance measures that express which tokens are important for a prediction. Unfortunately, such explanations are often wrong despite being persuasive. Therefore, it is essential to measure their faithfulness. One such metric is if tokens are truly important, then masking them should result in worse model performance. However, token masking introduces out-of-distribution issues, and existing solutions that address this are computationally expensive and employ proxy models. Furthermore, other metrics are very limited in scope. This work proposes an inherently faithfulness measurable model that addresses these challenges. This is achieved using a novel fine-tuning method that incorporates masking, such that masking tokens become in-distribution by design. This differs from existing approaches, which are completely model-agnostic but are inapplicable in practice. We demonstrate the generality of our approach by applying it to 16 different datasets and validate it using statistical in-distribution tests. The faithfulness is then measured with 9 different importance measures. Because masking is in-distribution, importance measures that themselves use masking become consistently more faithful. Additionally, because the model makes faithfulness cheap to measure, we can optimize explanations towards maximal faithfulness; thus, our model becomes indirectly inherently explainable.",
  "abstract_zh": "一种常见的解释自然语言处理模型的方法是使用重要性度量来表达哪些标记对预测是重要的。不幸的是，这种解释往往是错误的，尽管它们具有说服力。因此，测量它们的忠实性至关重要。一个这样的度量是，如果标记确实重要，那么掩盖它们应该导致模型性能下降。然而，标记掩盖引入了分布外问题，现有的解决方案在计算上代价高昂且采用代理模型。此外，其他度量的范围非常有限。本研究提出了一种固有的忠实性可测量模型，以应对这些挑战。这是通过一种新颖的微调方法实现的，该方法结合了掩盖，使得掩盖标记在设计上成为分布内。这与现有的方法不同，后者完全不依赖于模型，但在实践中不可应用。我们通过将其应用于16个不同的数据集来展示我们方法的普遍性，并使用统计分布内测试进行验证。然后使用9种不同的重要性度量来衡量忠实性。由于掩盖是分布内的，因此使用掩盖的重要性度量变得更加忠实。此外，由于模型使得忠实性测量变得便宜，我们可以优化解释以实现最大忠实性；因此，我们的模型间接地变得固有可解释。"
}
{
  "title": "Keypoint-based Progressive Chain-of-Thought Distillation for LLMs",
  "title_zh": "基于关键点的渐进式思维链蒸馏用于大型语言模型",
  "abstract": "Chain-of-thought distillation is a powerful technique for transferring reasoning abilities from large language models (LLMs) to smaller student models. Previous methods typically require the student to mimic the step-by-step rationale produced by LLMs, often facing the following challenges: (i) Tokens within a rationale vary in significance, and treating them equally may fail to accurately mimic keypoint tokens, leading to reasoning errors. (ii) They usually distill knowledge by consistently predicting all the steps in a rationale, which falls short in distinguishing the learning order of step generation. This diverges from the human cognitive progression of starting with easy tasks and advancing to harder ones, resulting in sub-optimal outcomes. To this end, we propose a unified framework, called KPOD, to address these issues. Specifically, we propose a token weighting module utilizing mask learning to encourage accurate mimicry of keypoint tokens by the student during distillation. Besides, we develop an in-rationale progressive distillation strategy, starting with training the student to generate the final reasoning steps and gradually extending to cover the entire rationale. To accomplish this, a weighted token generation loss is proposed to assess step reasoning difficulty, and a value function is devised to schedule the progressive distillation by considering both step difficulty and question diversity. Extensive experiments on four reasoning benchmarks illustrate our KPOD outperforms previous methods by a large margin.",
  "abstract_zh": "思维链蒸馏是一种强大的技术，用于将推理能力从大型语言模型（LLMs）转移到较小的学生模型。以往的方法通常要求学生模仿LLMs生成的逐步推理，但面临以下挑战：（i）推理中的标记重要性各异，平等对待可能无法准确模仿关键点标记，导致推理错误。（ii）它们通常通过持续预测推理中的所有步骤来蒸馏知识，但未能区分步骤生成的学习顺序。这与人类认知进程从简单任务到困难任务的进展相悖，导致次优结果。为此，我们提出了一个统一框架，称为KPOD，以解决这些问题。具体而言，我们提出了一个利用掩码学习的标记加权模块，以鼓励学生在蒸馏过程中准确模仿关键点标记。此外，我们开发了一种在推理内的渐进式蒸馏策略，从训练学生生成最终推理步骤开始，逐渐扩展到覆盖整个推理。为此，提出了一种加权标记生成损失来评估步骤推理难度，并设计了一个价值函数，通过考虑步骤难度和问题多样性来安排渐进式蒸馏。对四个推理基准的广泛实验表明，我们的KPOD在性能上大幅超越了以往的方法。"
}
{
  "title": "PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition",
  "title_zh": "标题：PARDEN，你能重复一下吗？通过重复防御越狱攻击",
  "abstract": "Large language models (LLMs) have shown success in many natural language processing tasks. Despite rigorous safety alignment processes, supposedly safety-aligned LLMs like Llama 2 and Claude 2 are still susceptible to jailbreaks, leading to security risks and abuse of the models. One option to mitigate such risks is to augment the LLM with a dedicated \"safeguard\", which checks the LLM's inputs or outputs for undesired behaviour. A promising approach is to use the LLM itself as the safeguard. Nonetheless, baseline methods, such as prompting the LLM to self-classify toxic content, demonstrate limited efficacy. We hypothesise that this is due to domain shift: the alignment training imparts a self-censoring behaviour to the model (\"Sorry I can't do that\"), while the self-classify approach shifts it to a classification format (\"Is this prompt malicious\"). In this work, we propose PARDEN, which avoids this domain shift by simply asking the model to repeat its own outputs. PARDEN neither requires finetuning nor white box access to the model. We empirically verify the effectiveness of our method and show that PARDEN significantly outperforms existing jailbreak detection baselines for Llama-2 and Claude-2. We find that PARDEN is particularly powerful in the relevant regime of high True Positive Rate (TPR) and low False Positive Rate (FPR). For instance, for Llama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction in the FPR from 24.8% to 2.0% on the harmful behaviours dataset. Code and data are available at https://github.com/Ed-Zh/PARDEN.",
  "abstract_zh": "摘要：大型语言模型（LLMs）在许多自然语言处理任务中取得了成功。尽管经过严格的安全对齐过程，但像Llama 2和Claude 2这样的安全对齐LLM仍然容易受到越狱攻击，导致安全风险和模型滥用。缓解这些风险的一个选项是为LLM增加一个专用的“保护机制”，检查LLM的输入或输出是否存在不当行为。一种有前景的方法是使用LLM本身作为保护机制。然而，基线方法，如提示LLM自我分类有毒内容，显示出有限的有效性。我们假设这与领域转移有关：对齐训练赋予模型自我审查行为（“抱歉，我无法做到”），而自我分类方法则将其转变为分类格式（“这个提示是恶意的吗”）。在这项工作中，我们提出了PARDEN，通过简单地要求模型重复其自身输出来避免这种领域转移。PARDEN既不需要微调，也不需要对模型的白盒访问。我们通过实证验证了我们方法的有效性，并显示PARDEN在Llama-2和Claude-2的现有越狱检测基线中显著优于其他方法。我们发现PARDEN在高真实阳性率（TPR）和低假阳性率（FPR）的相关范围内特别强大。例如，对于Llama2-7B，在TPR为90%时，PARDEN在有害行为数据集上将FPR从24.8%降低约11倍至2.0%。代码和数据可在https://github.com/Ed-Zh/PARDEN获取。"
}
{
  "title": "A Touch, Vision, and Language Dataset for Multimodal Alignment",
  "title_zh": "触觉、视觉和语言的多模态对齐数据集",
  "abstract": "Touch is an important sensing modality for humans, but it has not yet been incorporated into a multimodal generative language model. This is partially due to the difficulty of obtaining natural language labels for tactile data and the complexity of aligning tactile readings with both visual observations and language descriptions. As a step towards bridging that gap, this work introduces a new dataset of 44K in-the-wild visiontouch pairs, with English language labels annotated by humans (10%) and textual pseudo-labels from GPT-4V (90%). We use this dataset to train a vision-language-aligned tactile encoder for open-vocabulary classification and a touch-visionlanguage (TVL) model for text generation using the trained encoder. Results suggest that by incorporating touch, the TVL model improves (+29% classification accuracy) tactile-vision-language alignment over existing models trained on any pair of those modalities. Although only a small fraction of the dataset is human labeled, the TVL model demonstrates improved visual-tactile understanding over GPT-4V (+12%) and open-source vision-language models (+32%) on a new touch-vision understanding benchmark. Code, checkpoints and data are available on https: //tactile-vlm.github.io.",
  "abstract_zh": "触觉是人类重要的感知方式，但尚未纳入多模态生成语言模型。这部分是由于为触觉数据获取自然语言标签的困难，以及将触觉读数与视觉观察和语言描述对齐的复杂性。为缩小这一差距，本研究引入了一个包含44K自然环境中视觉触觉对的数据集，其中10%的英语标签由人类注释，90%为GPT-4V生成的文本伪标签。我们利用该数据集训练了一个视觉语言对齐的触觉编码器，用于开放词汇分类，并使用训练好的编码器构建了一个触觉-视觉语言（TVL）模型进行文本生成。结果表明，通过引入触觉，TVL模型在触觉-视觉-语言对齐方面比现有模型提高了29%的分类准确率。尽管数据集中只有一小部分是人类标注的，但TVL模型在新的触觉-视觉理解基准上相较于GPT-4V提高了12%，相较于开源视觉语言模型提高了32%。代码、检查点和数据可在https://tactile-vlm.github.io获取。"
}
{
  "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
  "title_zh": "动态内存压缩：为加速推理改造大型语言模型",
  "abstract": "Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key–value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key–value cache compression at inference time. Most importantly, the model learns to apply different compression ratios in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to $\\sim 3.7 \\times$ throughput increase during auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4$\\times$ cache compression, outperforming up-trained grouped-query attention (GQA) and key–value eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded gains. As a result DMC fits longer contexts and larger batches within any given memory budget. We release the DMC code and models at https://github.com/NVIDIA/Megatron-LM/tree/DMC.",
  "abstract_zh": "变压器已成为大型语言模型（LLMs）的核心。然而，由于需要在内存中存储过去标记的键值表示的缓存，生成过程仍然低效，其大小与输入序列长度和批量大小线性相关。为此，我们提出了动态内存压缩（DMC），一种在推理时进行在线键值缓存压缩的方法。最重要的是，模型学习在不同的头和层中应用不同的压缩比。我们将预训练的LLM（如Llama 2（7B、13B和70B））改造成DMC变压器，在NVIDIA H100 GPU上实现了高达$\\sim 3.7 \\times$的自回归推理吞吐量提升。DMC通过在原始数据的极小百分比上继续预训练来应用，而无需添加任何额外参数。我们发现，DMC在高达4$\\times$的缓存压缩下保持了原始下游性能，超越了经过训练的分组查询注意力（GQA）和键值驱逐策略（H$_2$O，TOVA）。GQA和DMC甚至可以结合以获得复合增益。因此，DMC能够在任何给定的内存预算内适应更长的上下文和更大的批量。我们在https://github.com/NVIDIA/Megatron-LM/tree/DMC发布DMC代码和模型。"
}
{
  "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models",
  "title_zh": "标题：SPHINX-X：为一系列多模态大型语言模型扩展数据和参数",
  "abstract": "We propose SPHINX-X, an extensive Multi-modality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multi-modal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama-1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral-8$\\times$7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. Code and models are released at https://github.com/Alpha-VLLM/LLaMA2-Accessory.",
  "abstract_zh": "摘要：我们提出了SPHINX-X，这是一个基于SPHINX开发的广泛多模态大型语言模型（MLLM）系列。为了提高架构和训练效率，我们通过去除冗余的视觉编码器、使用跳过标记绕过完全填充的子图像，以及将多阶段训练简化为一阶段的全合一范式，修改了SPHINX框架。为了充分释放MLLM的潜力，我们组建了一个涵盖语言、视觉和视觉-语言任务的综合多领域和多模态数据集，包含公开可用的资源。我们进一步通过我们的OCR密集型和集合标记数据集丰富这一集合，扩展了多样性和普遍性。通过对不同基础LLM进行训练，包括TinyLlama-1.1B、InternLM2-7B、LLaMA2-13B和Mixtral-8$\\times$7B，我们获得了一系列在参数规模和多语言能力上各异的MLLM。全面的基准测试显示多模态性能与数据和参数规模之间存在强相关性。代码和模型已发布在https://github.com/Alpha-VLLM/LLaMA2-Accessory。"
}
{
  "title": "Larimar: Large Language Models with Episodic Memory Control",
  "title_zh": "拉里玛：具有情节记忆控制的大型语言模型",
  "abstract": "Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed---yielding speed-ups of 8-10x depending on the base LLM ---as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting, information leakage prevention, and input context length generalization with Larimar and show their effectiveness. Our code is available at https://github.com/IBM/larimar.",
  "abstract_zh": "高效准确地更新存储在大型语言模型（LLMs）中的知识是当今最紧迫的研究挑战之一。本文提出了拉里玛——一种新颖的脑启发架构，用于通过分布式情节记忆增强LLMs。拉里玛的记忆允许动态的一次性知识更新，无需计算成本高昂的重新训练或微调。在多个事实编辑基准上的实验结果表明，拉里玛在准确性上可与大多数竞争基线相媲美，即使在具有挑战性的顺序编辑设置中，也表现出色，同时在速度上也优于其他模型——根据基础LLM的不同，速度提升可达8-10倍——并且由于所提架构简单、与LLM无关，因此具有灵活性。我们进一步提供了选择性事实遗忘、信息泄露防止和输入上下文长度泛化的机制，并展示了它们的有效性。我们的代码可在https://github.com/IBM/larimar获取。"
}
{
  "title": "Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning",
  "title_zh": "标题：通过反向课程强化学习训练大型语言模型进行推理",
  "abstract": "In this paper, we propose **R**$^3$: Learning **R**easoning through **R**everse Curriculum **R**einforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. **R**$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, **R**$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, **R**$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average. Notably, in program-based reasoning, 7B-scale models perform comparably to larger models or closed-source models with our **R**$^3$.",
  "abstract_zh": "摘要：在本文中，我们提出了**R**$^3$：通过**R**everse Curriculum **R**einforcement Learning (RL) 学习**R**easoning，这是一种新颖的方法，仅使用结果监督来实现大型语言模型的过程监督优势。将RL应用于复杂推理的核心挑战在于识别一系列导致正奖励的动作，并为优化提供适当的监督。结果监督为最终结果提供稀疏奖励，但未能识别错误位置，而过程监督则提供逐步奖励，但需要大量人工标注。**R**$^3$通过从正确的示范中学习克服了这些限制。具体而言，**R**$^3$逐步将推理的起始状态从示范的结束滑动到开始，从而在所有阶段促进模型的更容易探索。因此，**R**$^3$建立了逐步课程，使结果监督能够提供逐步信号并精确定位错误。使用Llama2-7B，我们的方法在八个推理任务上平均超越了RL基线4.1分。值得注意的是，在基于程序的推理中，7B规模的模型与更大模型或闭源模型的表现相当，得益于我们的**R**$^3$。"
}
{
  "title": "Accelerated Speculative Sampling Based on Tree Monte Carlo",
  "title_zh": "基于树蒙特卡洛的加速推测采样",
  "abstract": "Speculative Sampling (SpS) has been introduced to speed up inference of large language models (LLMs) by generating multiple tokens in a single forward pass under the guidance of a reference model, while preserving the original distribution. We observe that SpS can be derived through maximum coupling on the token distribution. However, we find that this approach is not optimal as it applies maximum coupling incrementally for each new token, rather than seeking a global maximum coupling that yields a faster algorithm, given the tree-space nature of LLM generative distributions. In this paper, we shift our focus from distributions on a token space to those on a tree space. We propose a novel class of Tree Monte Carlo (TMC) methods, demonstrating their unbiasedness and convergence. As a particular instance of TMC, our new algorithm, Accelerated Speculative Sampling (ASpS), outperforms traditional SpS by generating more tokens per step on average, achieving faster inference, while maintaining the original distribution.",
  "abstract_zh": "推测采样（SpS）被引入以加速大型语言模型（LLMs）的推理，通过在参考模型的指导下在单次前向传递中生成多个标记，同时保持原始分布。我们观察到SpS可以通过标记分布的最大耦合推导。然而，我们发现这种方法并不理想，因为它对每个新标记逐步应用最大耦合，而不是寻求全局最大耦合以产生更快的算法，考虑到LLM生成分布的树空间特性。在本文中，我们将重点从标记空间的分布转向树空间的分布。我们提出了一类新颖的树蒙特卡洛（TMC）方法，证明了它们的无偏性和收敛性。作为TMC的一个特例，我们的新算法加速推测采样（ASpS）在平均每步生成更多标记的情况下，超越了传统的SpS，实现了更快的推理，同时保持了原始分布。"
}
{
  "title": "APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference",
  "title_zh": "APT：自适应修剪和调优预训练语言模型以实现高效训练和推理",
  "abstract": "Fine-tuning and inference with large Language Models (LM) are generally known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency. Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce APT that adaptively *prunes* and *tunes* parameters for the LMs. At the early stage of fine-tuning, APT dynamically adds *salient* tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency. Compared to baselines, our experiments show that APT maintains up to 98% task performance when pruning RoBERTa and T5 models with 40% parameters left while keeping 86.4% LLaMA models' performance with 70% parameters remaining. Furthermore, APT speeds up LMs' fine-tuning by up to 8$\\times$ and reduces large LMs' memory training footprint by up to 70%. Our code and models are publicly available at https://github.com/ROIM1998/APT.",
  "abstract_zh": "对大型语言模型（LM）的微调和推理通常被认为是昂贵的。基于参数的高效微调通过更新少量LM参数来减少训练内存，但并未提高推理效率。结构化修剪通过移除一致的参数块来提高LM推理效率，但往往会增加训练内存和时间。为了同时提高训练和推理效率，我们提出了APT，能够自适应地修剪和调优LM的参数。在微调的早期阶段，APT动态添加显著的调优参数以实现快速和准确的收敛，同时丢弃不重要的参数以提高效率。与基线相比，我们的实验表明，在修剪RoBERTa和T5模型时，APT在保留40%参数的情况下保持了高达98%的任务性能，同时在保留70%参数的情况下保持了86.4%的LLaMA模型性能。此外，APT将LM的微调速度提高了最多8倍，并将大型LM的内存训练占用减少了最多70%。我们的代码和模型已公开发布在https://github.com/ROIM1998/APT。"
}
{
  "title": "Language Models as Semantic Indexers",
  "title_zh": "语言模型作为语义索引器",
  "abstract": "Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss, and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. It is non-trivial to design a method that can learn the document’s semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMIndexer, a self-supervised framework to learn semantic IDs with a generative language model. We tackle the challenge of sequential discrete ID by introducing a semantic indexer capable of generating neural sequential discrete representations with progressive training and contrastive learning. In response to the semantic supervision deficiency, we propose to train the model with a self-supervised document reconstruction objective. We show the high quality of the learned IDs and demonstrate their effectiveness on three tasks including recommendation, product search, and document retrieval on five datasets from various domains. Code is available at https://github.com/PeterGriffinJin/LMIndexer.",
  "abstract_zh": "语义标识符（ID）是信息检索中的一个重要概念，旨在在其ID中保留文档和项目等对象的语义。以往的研究通常采用两阶段流程，通过首先使用现成的文本编码器获取嵌入，然后基于嵌入推导ID来学习语义ID。然而，每个步骤都可能导致信息损失，并且文本编码器生成的潜在空间中的嵌入分布与语义索引所需的预期分布之间通常存在固有的不匹配。考虑到语义ID是离散的且具有顺序结构，并且语义监督不足，设计一种能够同时学习文档的语义表示及其层次结构的方法并非易事。在本文中，我们介绍了LMIndexer，这是一个自监督框架，用于利用生成语言模型学习语义ID。我们通过引入一个能够生成神经顺序离散表示的语义索引器，采用渐进训练和对比学习来应对顺序离散ID的挑战。针对语义监督不足的问题，我们提出使用自监督文档重构目标来训练模型。我们展示了所学ID的高质量，并在包括推荐、产品搜索和文档检索在内的三个任务上证明了其有效性，涉及来自不同领域的五个数据集。代码可在https://github.com/PeterGriffinJin/LMIndexer获取。"
}
{
  "title": "Large Language Models are Geographically Biased",
  "title_zh": "大型语言模型存在地理偏见",
  "abstract": "Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman’s $\\rho$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM.",
  "abstract_zh": "大型语言模型（LLMs）固有地承载着其训练语料中的偏见，这可能导致社会危害的延续。随着这些基础模型影响力的增长，理解和评估它们的偏见对于实现公平和准确性变得至关重要。我们提议通过地理视角研究LLMs对我们所生活世界的认知。这种方法特别有效，因为人类生活的许多方面在地理空间中有实质性的投影，例如文化、种族、语言、政治和宗教。我们展示了各种问题性的地理偏见，我们将其定义为地理空间预测中的系统性错误。最初，我们证明LLMs能够以评分的形式进行准确的零-shot地理空间预测，这些评分与真实情况显示出强单调相关性（斯皮尔曼相关系数高达0.89）。然后，我们展示LLMs在一系列客观和主观主题上表现出共同的偏见。特别是，LLMs明显对社会经济条件较低的地区（例如大部分非洲）在吸引力、道德和智力等多种敏感主观主题上存在偏见（斯皮尔曼相关系数高达0.70）。最后，我们引入一个偏见评分来量化这一点，并发现现有LLMs的偏见程度存在显著差异。代码可在项目网站上获取：https://rohinmanvi.github.io/GeoLLM。"
}
{
  "title": "Language-Driven Cross-Modal Classifier for Zero-Shot Multi-Label Image Recognition",
  "title_zh": "语言驱动的跨模态分类器用于零样本多标签图像识别",
  "abstract": "Large-scale pre-trained vision-language models (e.g., CLIP) have shown powerful zero-shot transfer capabilities in image recognition tasks. Recent approaches typically employ supervised fine-tuning methods to adapt CLIP for zero-shot multi-label image recognition tasks. However, obtaining sufficient multi-label annotated image data for training is challenging and not scalable. In this paper, we propose a new language-driven framework for zero-shot multi-label recognition that eliminates the need for annotated images during training. Leveraging the aligned CLIP multi-modal embedding space, our method utilizes language data generated by LLMs to train a cross-modal classifier, which is subsequently transferred to the visual modality. During inference, directly applying the classifier to visual inputs may limit performance due to the modality gap. To address this issue, we introduce a cross-modal mapping method that maps image embeddings to the language modality while retaining crucial visual information. Comprehensive experiments demonstrate that our method outperforms other zero-shot multi-label recognition methods and achieves competitive results compared to few-shot methods.",
  "abstract_zh": "大规模预训练的视觉-语言模型（如CLIP）在图像识别任务中展示了强大的零样本迁移能力。最近的方法通常采用监督微调方法来适应CLIP以进行零样本多标签图像识别任务。然而，获取足够的多标签标注图像数据进行训练是具有挑战性的且不可扩展。在本文中，我们提出了一种新的语言驱动框架用于零样本多标签识别，消除了训练过程中对标注图像的需求。利用对齐的CLIP多模态嵌入空间，我们的方法利用由大型语言模型生成的语言数据来训练跨模态分类器，随后将其转移到视觉模态。在推理过程中，直接将分类器应用于视觉输入可能会由于模态差距限制性能。为了解决这个问题，我们引入了一种跨模态映射方法，将图像嵌入映射到语言模态，同时保留关键的视觉信息。全面的实验表明，我们的方法优于其他零样本多标签识别方法，并且与少样本方法相比取得了具有竞争力的结果。"
}
{
  "title": "Tell, Don't Show: Language Guidance Eases Transfer Across Domains in Images and Videos",
  "title_zh": "标题：告诉，而不是展示：语言指导促进图像和视频领域间的迁移",
  "abstract": "We introduce LaGTran, a novel framework that utilizes text supervision to guide robust transfer of discriminative knowledge from labeled source to unlabeled target data with domain gaps. While unsupervised adaptation methods have been established to address this problem, they show limitations in handling challenging domain shifts due to their exclusive operation within the pixel-space. Motivated by our observation that semantically richer text modality has more favorable transfer properties, we devise a transfer mechanism to use a source-trained text-classifier to generate predictions on the target text descriptions, and utilize these predictions as supervision for the corresponding images. Our approach driven by language guidance is surprisingly easy and simple, yet significantly outperforms all prior approaches on challenging datasets like GeoNet and DomainNet, validating its extreme effectiveness. To further extend the scope of our study beyond images, we introduce a new benchmark called Ego2Exo to study ego-exo transfer in videos and find that our language-aided approach LaGTran yields significant gains in this highly challenging and non-trivial transfer setting. Code, models, and proposed datasets are publicly available at https://tarun005.github.io/lagtran/.",
  "abstract_zh": "摘要：我们提出了LaGTran，一个新颖的框架，利用文本监督来指导从标记源到未标记目标数据的区分知识的稳健迁移，尽管存在领域差距。虽然已经建立了无监督适应方法来解决这个问题，但由于它们仅在像素空间内操作，处理具有挑战性的领域转移时显示出局限性。受我们观察到的语义更丰富的文本模态具有更有利的迁移特性的启发，我们设计了一种迁移机制，使用源训练的文本分类器对目标文本描述生成预测，并利用这些预测作为相应图像的监督。我们的语言指导驱动的方法出奇地简单易行，但在GeoNet和DomainNet等具有挑战性的数据集上显著超越了所有先前的方法，验证了其极高的有效性。为了进一步扩展我们研究的范围，我们引入了一个新的基准Ego2Exo，以研究视频中的自我-外部迁移，发现我们的语言辅助方法LaGTran在这一高度挑战和非平凡的迁移设置中取得了显著的提升。代码、模型和提议的数据集可在https://tarun005.github.io/lagtran/上公开获取。"
}
{
  "title": "Compressing Large Language Models by Joint Sparsification and Quantization",
  "title_zh": "标题：通过联合稀疏化和量化压缩大型语言模型",
  "abstract": "In this paper, we introduce a novel model compression technique named Joint Sparsification and Quantization (JSQ), explicitly tailored for large language models (LLMs). Traditional methods employ either sparsification or quantization individually to compress LLMs, leading to performance degradation at high compression ratios. In contrast, our JSQ approach integrates sparsification and quantization cohesively. As sparsification tend to preserve outliers that is harmful to quantization, we introduce a novel sparsity metric to serves as a bridge between the sparsification and quantization. Moreover, it is proven outliers in LLMs have significant impact but harmful to compression. Current solutions are highly coupled with quantization process, which is not helpful to sparsification. To this end, we also introduce a search-based activation editor to automatically eliminate relatively useless outliers. Comprehensive experiments across various datasets and architectures affirm the efficacy of our JSQ framework. Notably, our JSQ achieves 7.96$\\times$ computation reduction without crashing for the representative model LLaMA. This accomplishment stands in stark contrast to the limitations of most state-of-the-art LLM compression methods, which typically fail under such extreme compression ratios. Our code is released at https://github.com/uanu2002/JSQ.",
  "abstract_zh": "摘要：在本文中，我们介绍了一种新颖的模型压缩技术，称为联合稀疏化和量化（JSQ），专门针对大型语言模型（LLMs）进行设计。传统方法通常单独采用稀疏化或量化来压缩LLMs，导致在高压缩比下性能下降。相反，我们的JSQ方法将稀疏化和量化有机结合。由于稀疏化倾向于保留对量化有害的异常值，我们引入了一种新颖的稀疏性度量，作为稀疏化和量化之间的桥梁。此外，已证明LLMs中的异常值具有显著影响，但对压缩有害。目前的解决方案与量化过程高度耦合，这对稀疏化没有帮助。为此，我们还引入了一种基于搜索的激活编辑器，自动消除相对无用的异常值。针对各种数据集和架构的全面实验验证了我们JSQ框架的有效性。值得注意的是，我们的JSQ在代表性模型LLaMA上实现了7.96倍的计算减少而未崩溃。这一成就与大多数最先进的LLM压缩方法在如此极端压缩比下通常失败的局限性形成鲜明对比。我们的代码已发布在https://github.com/uanu2002/JSQ。"
}
{
  "title": "WARM: On the Benefits of Weight Averaged Reward Models",
  "title_zh": "标题：WARM：权重平均奖励模型的优势",
  "abstract": "Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives. We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then averaging them in the weight space. This strategy follows the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training. By averaging weights, WARM improves efficiency compared to the traditional ensembling of predictions, while improving reliability under distribution shifts and robustness to preference inconsistencies. Our experiments on summarization tasks, using best-of-N and RL methods, shows that WARM improves the overall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy RL fine-tuned with a single RM.",
  "abstract_zh": "摘要：通过强化学习（RLHF）将大型语言模型（LLMs）与人类偏好对齐可能导致奖励黑客现象，即LLMs利用奖励模型（RM）的缺陷来获得表面上高的奖励，而未能满足基本目标。我们识别出在设计RM以减轻奖励黑客时面临的两个主要挑战：RL过程中的分布变化和人类偏好的不一致。作为解决方案，我们提出权重平均奖励模型（WARM），首先对多个RM进行微调，然后在权重空间中对它们进行平均。该策略遵循了一个观察，即微调后的权重在共享相同预训练时保持线性模式连接。通过平均权重，WARM在效率上优于传统的预测集成，同时在分布变化下提高了可靠性，并增强了对偏好不一致的鲁棒性。我们在摘要任务上的实验，使用最佳的N和RL方法，表明WARM提高了LLM预测的整体质量和对齐性；例如，使用WARM微调的策略RL的胜率为79.4%，而使用单个RM微调的策略RL的胜率则较低。"
}
{
  "title": "In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation",
  "title_zh": "上下文锐度作为警报：从内部表征的角度看幻觉缓解",
  "abstract": "Large language models (LLMs) frequently hallucinate, e.g., making factual errors, yet our understanding of why they make these errors remains limited. In this study, we aim to understand the underlying mechanisms of LLM hallucinations from the perspective of *inner representations*. We discover a pattern associated with hallucinations: correct generations tend to have *sharper* context activations in the hidden states of the in-context tokens, compared to that of the incorrect generations. Leveraging this signal, we propose an entropy-based metric to quantify the *sharpness* among the in-context hidden states and incorporate it into the decoding process, i.e, use the entropy value to adjust the next token prediction distribution to improve the factuality and overall quality of the generated text. Experiments on knowledge-seeking datasets (Natural Questions, HotpotQA, TriviaQA) and hallucination benchmark (TruthfulQA) demonstrate our consistent effectiveness, e.g., up to 8.6 absolute points on TruthfulQA. We believe this study can improve our understanding of hallucinations and serve as a practical solution for hallucination mitigation.",
  "abstract_zh": "大型语言模型（LLMs）经常出现幻觉，例如，发生事实错误，但我们对其产生这些错误的原因理解仍然有限。在本研究中，我们旨在从*内部表征*的角度理解LLM幻觉的潜在机制。我们发现与幻觉相关的模式：与错误生成相比，正确生成的上下文激活在上下文令牌的隐藏状态中往往具有*更锐利*的特征。利用这一信号，我们提出了一种基于熵的度量来量化上下文隐藏状态的*锐度*，并将其纳入解码过程中，即使用熵值调整下一个令牌的预测分布，以提高生成文本的事实性和整体质量。在知识寻求数据集（自然问题、HotpotQA、TriviaQA）和幻觉基准（TruthfulQA）上的实验表明我们的有效性一致，例如，在TruthfulQA上提高了8.6个绝对点。我们相信这项研究可以加深我们对幻觉的理解，并作为幻觉缓解的实际解决方案。"
}
{
  "title": "SAM-E: Leveraging Visual Foundation Model with Sequence Imitation for Embodied Manipulation",
  "title_zh": "SAM-E：利用视觉基础模型与序列模仿进行具身操作",
  "abstract": "Acquiring a multi-task imitation policy in 3D manipulation poses challenges in terms of scene understanding and action prediction. Current methods employ both 3D representation and multi-view 2D representation to predict the poses of the robot’s end-effector. However, they still require a considerable amount of high-quality robot trajectories, and suffer from limited generalization in unseen tasks and inefficient execution in long-horizon reasoning. In this paper, we propose **SAM-E**, a novel architecture for robot manipulation by leveraging a vision-foundation model for generalizable scene understanding and sequence imitation for long-term action reasoning. Specifically, we adopt Segment Anything (SAM) pre-trained on a huge number of images and promptable masks as the foundation model for extracting task-relevant features, and employ parameter-efficient fine-tuning on robot data for a better understanding of embodied scenarios. To address long-horizon reasoning, we develop a novel multi-channel heatmap that enables the prediction of the action sequence in a single pass, notably enhancing execution efficiency. Experimental results from various instruction-following tasks demonstrate that SAM-E achieves superior performance with higher execution efficiency compared to the baselines, and also significantly improves generalization in few-shot adaptation to new tasks.",
  "abstract_zh": "在3D操作中获取多任务模仿策略面临场景理解和动作预测的挑战。目前的方法采用3D表示和多视角2D表示来预测机器人末端执行器的姿态，但仍需大量高质量的机器人轨迹，并且在未见任务中的泛化能力有限，长时间推理的执行效率低下。本文提出了**SAM-E**，一种通过利用视觉基础模型进行可泛化场景理解和序列模仿以实现长期动作推理的机器人操作新架构。具体而言，我们采用在大量图像和可提示掩码上预训练的Segment Anything（SAM）作为提取任务相关特征的基础模型，并在机器人数据上进行参数高效的微调，以更好地理解具身场景。为了解决长时间推理问题，我们开发了一种新型多通道热图，使得在单次传递中预测动作序列成为可能，显著提高了执行效率。来自各种指令跟随任务的实验结果表明，SAM-E在执行效率上优于基线，并在少量适应新任务的泛化能力上显著提升。"
}
{
  "title": "Position: Intent-aligned AI Systems Must Optimize for Agency Preservation",
  "title_zh": "标题：定位：意图对齐的人工智能系统必须优化以保持代理性",
  "abstract": "A central approach to AI-safety research has been to generate aligned AI systems: i.e. systems that do not deceive users and yield actions or recommendations that humans might judge as consistent with their intentions and goals. Here we argue that truthful AIs aligned solely to human intent are insufficient and that preservation of long-term agency of humans may be a more robust standard that may need to be separated and explicitly optimized for. We discuss the science of intent and control and how human intent can be manipulated and we provide a formal definition of agency-preserving AI-human interactions focusing on forward-looking explicit agency evaluations. Our work points to a novel pathway for human harm in AI-human interactions and proposes solutions to this challenge.",
  "abstract_zh": "摘要：人工智能安全研究的一个核心方法是生成对齐的人工智能系统：即不欺骗用户并产生人类可能认为与其意图和目标一致的行动或建议的系统。在这里，我们认为仅仅对齐人类意图的真实人工智能是不够的，保持人类长期代理性可能是一个更稳健的标准，需要被分开并明确优化。我们讨论了意图与控制的科学，以及人类意图如何被操控，并提供了一个关于保持代理性的人工智能与人类互动的正式定义，重点关注前瞻性的明确代理评估。我们的工作指出了人工智能与人类互动中人类受害的新途径，并提出了应对这一挑战的解决方案。"
}
{
  "title": "Coactive Learning for Large Language Models using Implicit User Feedback",
  "title_zh": "协同学习用于利用隐式用户反馈训练大型语言模型",
  "abstract": "We propose coactive learning as a model and feedback mechanism for training large language models (LLMs). The key insight is that users provide implicit feedback whenever they edit the text $y$ proposed by an LLM. While the edited text $\\bar y$ is typically not a gold-standard example for supervised training, coactive learning merely requires that the edited text $\\bar y$ is an improvement over the proposed text $y$. Note that such weak implicit preference feedback $\\bar y \\succ y$ is available in many application settings on a per-user basis, thus enabling the personalization of LLMs. In this paper, we develop the theoretical basis for coactive training of non-linear models, and we derive CoRLL as the first coactive learning algorithm for LLMs. Empirical results indicate that CoRLL is effective even for weak and noisy coactive preference feedback, making it a promising algorithm for training and personalization of LLMs from feedback that is naturally collected in many use cases.",
  "abstract_zh": "我们提出了协同学习作为训练大型语言模型（LLMs）的模型和反馈机制。关键见解在于，用户在编辑LLM提出的文本$y$时提供隐式反馈。虽然编辑后的文本$\\bar y$通常不是监督训练的金标准示例，但协同学习仅要求编辑后的文本$\\bar y$相较于提出的文本$y$有所改进。请注意，这种弱隐式偏好反馈$\\bar y \\succ y$在许多应用场景中是按用户可用的，从而使LLM的个性化成为可能。在本文中，我们为非线性模型的协同训练发展了理论基础，并推导出CoRLL作为第一个用于LLMs的协同学习算法。实证结果表明，CoRLL即使在弱且嘈杂的协同偏好反馈下也有效，使其成为从许多使用案例中自然收集的反馈进行LLMs训练和个性化的有前景算法。"
}
{
  "title": "Learning to Compile Programs to Neural Networks",
  "title_zh": "将程序编译为神经网络的学习",
  "abstract": "A *neural surrogate* is a neural network that mimics the behavior of a program. Neural surrogates of programs have been used to automatically tune program inputs, adapt programs to new settings, and accelerate computations. Neural surrogates have traditionally been developed by training on input-output examples for a single program. Language models present another approach wherein a model is trained on a single, large dataset then directly consumes program text, to act as a neural surrogate of the program. Having the language model as both the neural surrogate generator and the neural surrogate, however, poses a tradeoff of limited accuracy or excessive resource consumption. We present *neural surrogate compilation*, a technique for producing neural surrogates directly from program text without coupling neural surrogate generation and execution. We implement neural surrogate compilers using hypernetworks trained on a dataset of C programs and find they produce neural surrogates that are $1.91$-$9.50\\times$ as data-efficient and train in $4.31$-$7.28\\times$ fewer epochs than neural surrogates trained from scratch.",
  "abstract_zh": "神经替代物是模拟程序行为的神经网络。程序的神经替代物已被用于自动调整程序输入、将程序适应新设置以及加速计算。传统上，神经替代物是通过对单个程序的输入输出示例进行训练而开发的。语言模型提供了另一种方法，其中模型在单个大型数据集上进行训练，然后直接处理程序文本，以充当程序的神经替代物。然而，将语言模型作为神经替代物生成器和神经替代物本身，存在准确性有限或资源消耗过大的权衡。我们提出了神经替代物编译技术，该技术可以直接从程序文本生成神经替代物，而无需将神经替代物生成与执行耦合。我们使用在C程序数据集上训练的超网络实现了神经替代物编译器，发现它们生成的神经替代物在数据效率上是$1.91$-$9.50\\times$，训练所需的轮次比从头开始训练的神经替代物少$4.31$-$7.28\\times$。"
}
{
  "title": "Linguistic Calibration of Long-Form Generations",
  "title_zh": "长文本生成的语言校准",
  "abstract": "Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce long-form text with calibrated confidence statements. Through the lens of decision-making, we define linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as \"I estimate a 30% chance of...\" or \"I am certain that...\", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under significant domain shifts to scientific and biomedical questions and to an entirely held-out person biography generation task. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making.",
  "abstract_zh": "语言模型（LM）可能会导致用户在自信地虚构时做出次优的下游决策。通过让LM口头表达其主张正确的概率，可以缓解这一问题，但现有模型无法生成带有校准信心声明的长文本。通过决策制定的视角，我们定义了长文本生成的语言校准：如果LM的生成使用户能够做出校准的概率预测，则该LM被认为是语言上校准的。这个定义使得一个训练框架成为可能，其中一个监督微调步骤启动LM，使其能够发出带有信心声明的长文本生成，例如“我估计有30%的机会……”或“我确定……”，随后是一个强化学习步骤，奖励那些使用户能够对相关问题提供校准答案的生成。我们对Llama 2 7B进行了语言校准，并在长文本生成的自动和人工评估中发现，其校准程度显著高于具有可比准确性的强微调事实基线。这些发现可以在科学和生物医学问题以及完全保留的人物传记生成任务中，在显著的领域转移下进行推广。我们的结果表明，通过在用户在下游决策中所做预测的空间中构建目标，长文本生成可以实现端到端的校准。"
}
{
  "title": "InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models",
  "title_zh": "指令零：黑箱大型语言模型的高效指令优化",
  "abstract": "Large language models (LLMs) are instruction followers but the performance varies under different instructions. It is challenging to create the best instruction, especially for black-box LLMs on which backpropagation is forbidden. Instead of directly optimizing the discrete instruction, we optimize a low-dimensional soft prompt applied to an open-source LLM to generate the instruction for the black-box LLM. In each optimization step of the proposed method InstructZero, a soft prompt is converted into an instruction by the open-source LLM, which is then submitted to the black-box LLM for zero-shot evaluation, whose result is sent to Bayesian optimization to produce new soft prompts improving the zero-shot performance. We evaluate InstructZero on different combinations of open-source LLMs and APIs including Vicuna and ChatGPT. InstructZero outperforms SOTA auto-instruction methods across a variety of downstream tasks.",
  "abstract_zh": "大型语言模型（LLMs）是指令跟随者，但在不同指令下性能差异显著。创建最佳指令尤其具有挑战性，特别是对于禁止反向传播的黑箱LLMs。我们并不直接优化离散指令，而是优化应用于开源LLM的低维软提示，以生成黑箱LLM的指令。在所提方法InstructZero的每个优化步骤中，开源LLM将软提示转换为指令，然后提交给黑箱LLM进行零-shot评估，其结果被送入贝叶斯优化，以生成新的软提示，从而提高零-shot性能。我们在不同的开源LLM和API组合上评估InstructZero，包括Vicuna和ChatGPT。InstructZero在多种下游任务中超越了最新的自动指令方法。"
}
{
  "title": "Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation",
  "title_zh": "叠加提示：改进和加速检索增强生成",
  "abstract": "Despite the successes of large language models (LLMs), they exhibit significant drawbacks, particularly when processing long contexts. Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the \"distraction phenomenon\", where irrelevant context in the prompt degrades output quality. To address these drawbacks, we propose a novel RAG prompting methodology, *superposition prompting*, which can be directly applied to pre-trained transformer-based LLMs *without the need for fine-tuning*. At a high level, superposition prompting allows the LLM to process input documents in parallel *prompt paths*, discarding paths once they are deemed irrelevant. We demonstrate the capability of our method to simultaneously enhance time efficiency across a variety of question-answering benchmarks using multiple pre-trained LLMs. Furthermore, our technique significantly improves accuracy when the retrieved context is large relative the context the model was trained on. For example, our approach facilitates a $93\\times$ reduction in compute time while *improving* accuracy by $43\\%$ on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive RAG.",
  "abstract_zh": "尽管大型语言模型（LLMs）取得了成功，但在处理长上下文时仍表现出显著缺陷。它们的推理成本与序列长度呈二次方关系，这使得在某些现实世界文本处理应用（如检索增强生成（RAG））中部署成本高昂。此外，LLMs还表现出“干扰现象”，即提示中的无关上下文会降低输出质量。为了解决这些缺陷，我们提出了一种新颖的RAG提示方法——*叠加提示*，该方法可以直接应用于预训练的基于变换器的LLMs *而无需微调*。从高层次来看，叠加提示允许LLM以并行的*提示路径*处理输入文档，一旦路径被认为无关则丢弃。我们展示了我们的方法在使用多个预训练LLMs的各种问答基准上同时提高时间效率的能力。此外，当检索到的上下文相对于模型训练时的上下文较大时，我们的技术显著提高了准确性。例如，我们的方法在MPT-7B指令调优模型上，在NaturalQuestions-Open数据集上实现了计算时间减少$93\\times$，同时准确性提高$43\\%$，相较于简单的RAG。"
}
{
  "title": "$S^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting",
  "title_zh": "$S^2$IP-LLM：基于语义空间的提示学习与大语言模型结合的时间序列预测",
  "abstract": "Recently, there has been a growing interest in leveraging pre-trained large language models (LLMs) for various time series applications. However, the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting. To this end, we propose Semantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the pre-trained semantic space with time series embedding space and perform time series forecasting based on learned prompts from the joint space. We first design a tokenization module tailored for cross-modality alignment, which explicitly concatenates patches of decomposed time series components to create embeddings that effectively encode the temporal dynamics. Next, we leverage the pre-trained word token embeddings to derive semantic anchors and align selected anchors with time series embeddings by maximizing the cosine similarity in the joint space. This way, $S^2$IP-LLM can retrieve relevant semantic anchors as prompts to provide strong indicators (context) for time series that exhibit different temporal dynamics. With thorough empirical studies on multiple benchmark datasets, we demonstrate that the proposed $S^2$IP-LLM can achieve superior forecasting performance over state-of-the-art baselines. Furthermore, our ablation studies and visualizations verify the necessity of prompt learning informed by semantic space.",
  "abstract_zh": "近年来，利用预训练的大语言模型（LLMs）进行各种时间序列应用的兴趣日益增长。然而，LLMs通过预训练建立的语义空间仍然未被充分探索，可能有助于产生更具辨识度和信息量的表示，以促进时间序列预测。为此，我们提出了基于语义空间的提示学习与LLM结合的$S^2$IP-LLM，以将预训练的语义空间与时间序列嵌入空间对齐，并基于联合空间中学习的提示进行时间序列预测。我们首先设计了一个针对跨模态对齐的标记化模块，明确地将分解的时间序列组件的补丁连接起来，以创建有效编码时间动态的嵌入。接下来，我们利用预训练的词标记嵌入来推导语义锚点，并通过最大化联合空间中的余弦相似度将选定的锚点与时间序列嵌入对齐。通过这种方式，$S^2$IP-LLM可以检索相关的语义锚点作为提示，为表现出不同时间动态的时间序列提供强有力的指示（上下文）。通过对多个基准数据集的全面实证研究，我们证明了所提出的$S^2$IP-LLM在预测性能上优于最先进的基线。此外，我们的消融研究和可视化验证了基于语义空间的提示学习的必要性。"
}
{
  "title": "An Empirical Study Into What Matters for Calibrating Vision-Language Models",
  "title_zh": "标题：关于校准视觉-语言模型的重要性实证研究",
  "abstract": "Vision-Language Models (VLMs) have emerged as the dominant approach for zero-shot recognition, adept at handling diverse scenarios and significant distribution changes. However, their deployment in risk-sensitive areas requires a deeper understanding of their uncertainty estimation capabilities, a relatively uncharted area. In this study, we explore the calibration properties of VLMs across different architectures, datasets, and training strategies. In particular, we analyze the uncertainty estimation performance of VLMs when calibrated in one domain, label set or hierarchy level, and tested in a different one. Our findings reveal that while VLMs are not inherently calibrated for uncertainty, temperature scaling significantly and consistently improves calibration, even across shifts in distribution and changes in label set. Moreover, VLMs can be calibrated with a very small set of examples. Through detailed experimentation, we highlight the potential applications and importance of our insights, aiming for more reliable and effective use of VLMs in critical, real-world scenarios.",
  "abstract_zh": "摘要：视觉-语言模型（VLMs）已成为零样本识别的主流方法，能够处理多样化场景和显著的分布变化。然而，它们在风险敏感领域的应用需要更深入地理解其不确定性估计能力，这是一个相对未被探索的领域。在本研究中，我们探讨了不同架构、数据集和训练策略下VLMs的校准特性。特别地，我们分析了在一个领域、标签集或层次水平上校准的VLMs在不同领域的测试时的不确定性估计性能。我们的研究发现，尽管VLMs并不天生具备不确定性校准能力，但温度缩放显著且一致地提高了校准效果，即使在分布变化和标签集变化的情况下。此外，VLMs可以通过非常少量的示例进行校准。通过详细的实验，我们强调了我们的见解的潜在应用和重要性，旨在在关键的现实场景中实现VLMs的更可靠和有效的使用。"
}
{
  "title": "From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems",
  "title_zh": "从语言到行动：揭示基于大型语言模型的自主系统的理论基础",
  "abstract": "In this work, from a theoretical lens, we aim to understand why large language model (LLM) empowered agents are able to solve decision-making problems in the physical world. To this end, consider a hierarchical reinforcement learning (RL) model where the LLM Planner and the Actor perform high-level task planning and low-level execution, respectively. Under this model, the LLM Planner navigates a partially observable Markov decision process (POMDP) by iteratively generating language-based subgoals via prompting. Under proper assumptions on the pretraining data, we prove that the pretrained LLM Planner effectively performs Bayesian aggregated imitation learning (BAIL) through in-context learning. Additionally, we highlight the necessity for exploration beyond the subgoals derived from BAIL by proving that naively executing the subgoals returned by LLM leads to a linear regret. As a remedy, we introduce an $\\epsilon$-greedy exploration strategy to BAIL, which is proven to incur sublinear regret when the pretraining error is small. Finally, we extend our theoretical framework to include scenarios where the LLM Planner serves as a world model for inferring the transition model of the environment and to multi-agent settings, enabling coordination among multiple Actors.",
  "abstract_zh": "在本研究中，从理论的角度出发，我们旨在理解为何大型语言模型（LLM）驱动的智能体能够解决物理世界中的决策问题。为此，我们考虑一个层次化的强化学习（RL）模型，其中LLM规划者和执行者分别进行高层任务规划和低层执行。在该模型下，LLM规划者通过迭代生成基于语言的子目标来导航部分可观察的马尔可夫决策过程（POMDP）。在对预训练数据的适当假设下，我们证明了预训练的LLM规划者通过上下文学习有效地执行贝叶斯聚合模仿学习（BAIL）。此外，我们强调了超越BAIL派生子目标进行探索的必要性，证明了简单执行LLM返回的子目标会导致线性遗憾。作为补救措施，我们向BAIL引入了$\\epsilon$-贪婪探索策略，证明在预训练误差较小的情况下，该策略会导致次线性遗憾。最后，我们扩展了我们的理论框架，以包括LLM规划者作为推断环境转移模型的世界模型的场景，以及多智能体设置，从而实现多个执行者之间的协调。"
}
{
  "title": "Extracting Training Data From Document-Based VQA Models",
  "title_zh": "从基于文档的视觉问答模型中提取训练数据",
  "abstract": "Vision-Language Models (VLMs) have made remarkable progress in document-based Visual Question Answering (i.e., responding to queries about the contents of an input document provided as an image). In this work, we show these models can memorize responses for training samples and regurgitate them even when the relevant visual information has been removed. This includes Personal Identifiable Information (PII) repeated once in the training set, indicating these models could divulge memorised sensitive information and therefore pose a privacy risk. We quantitatively measure the extractability of information in controlled experiments and differentiate between cases where it arises from generalization capabilities or from memorization. We further investigate the factors that influence memorization across multiple state-of-the-art models and propose an effective heuristic countermeasure that empirically prevents the extractability of PII.",
  "abstract_zh": "视觉语言模型（VLMs）在基于文档的视觉问答（即对作为图像提供的输入文档内容的查询作出响应）方面取得了显著进展。在本研究中，我们展示了这些模型可以记忆训练样本的响应，并在相关视觉信息被移除时仍然能够重复这些响应。这包括在训练集中重复一次的个人可识别信息（PII），表明这些模型可能泄露记忆中的敏感信息，从而构成隐私风险。我们在受控实验中定量测量信息的可提取性，并区分其来源于泛化能力还是记忆的情况。我们进一步研究影响多种最先进模型记忆的因素，并提出了一种有效的启发式对策，实证上防止了个人可识别信息的可提取性。"
}
{
  "title": "BiLLM: Pushing the Limit of Post-Training Quantization for LLMs",
  "title_zh": "BiLLM：推动大型语言模型后训练量化的极限",
  "abstract": "Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM, for the first time, achieves high-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit weights across various LLM families and evaluation metrics, outperforms SOTA quantization methods of LLM by significant margins. Moreover, BiLLM enables the binarization process of a 7-billion LLM within 0.5 hours on a single GPU, demonstrating satisfactory time efficiency. Our code is available at https://github.com/Aaronhuang-778/BiLLM .",
  "abstract_zh": "预训练的大型语言模型（LLMs）展现出卓越的通用语言处理能力，但对内存和计算资源的需求也非常高。作为一种强大的压缩技术，二值化可以将模型权重极大地减少到仅1位，从而降低昂贵的计算和内存需求。然而，现有的量化技术在超低位宽下无法维持LLM的性能。针对这一挑战，我们提出了BiLLM，这是一种针对预训练LLM的开创性1位后训练量化方案。基于LLM的权重分布，BiLLM首先识别并结构性选择显著权重，并通过有效的二进制残差近似策略最小化压缩损失。此外，考虑到非显著权重的钟形分布，我们提出了一种最佳分割搜索方法，以准确分组和二值化它们。BiLLM首次在各种LLM家族和评估指标上，仅使用1.08位权重实现高精度推理（例如，LLaMA2-70B上的8.41困惑度），并显著超越了当前最先进的LLM量化方法。此外，BiLLM能够在单个GPU上在0.5小时内完成一个70亿参数的LLM的二值化，展示了令人满意的时间效率。我们的代码可在https://github.com/Aaronhuang-778/BiLLM获取。"
}
{
  "title": "GiLOT: Interpreting Generative Language Models via Optimal Transport",
  "title_zh": "GiLOT：通过最优传输解释生成语言模型",
  "abstract": "While large language models (LLMs) surge with the rise of generative AI, algorithms to explain LLMs highly desire. Existing feature attribution methods adequate for discriminative language models like BERT often fail to deliver faithful explanations for LLMs, primarily due to two issues: (1) For every specific prediction, the LLM outputs a probability distribution over the vocabulary–a large number of tokens with unequal semantic distance; (2) As an autoregressive language model, the LLM handles input tokens while generating a sequence of probability distributions of various tokens. To address above two challenges, this work proposes GiLOT that leverages Optimal Transport to measure the distributional change of all possible generated sequences upon the absence of every input token, while taking into account the tokens’ similarity, so as to faithfully estimate feature attribution for LLMs. We have carried out extensive experiments on top of Llama families and their fine-tuned derivatives across various scales to validate the effectiveness of GiLOT for estimating the input attributions. The results show that GiLOT outperforms existing solutions on a number of faithfulness metrics under fair comparison settings. Source code is publicly available at https://github.com/holyseven/GiLOT.",
  "abstract_zh": "随着生成性人工智能的兴起，大型语言模型（LLMs）迅速发展，但解释LLMs的算法仍然非常渴求。现有的特征归因方法对于像BERT这样的判别性语言模型是足够的，但往往无法为LLMs提供真实的解释，主要由于两个问题：（1）对于每个特定的预测，LLM输出一个词汇表上的概率分布——大量具有不等语义距离的标记；（2）作为自回归语言模型，LLM在生成各种标记的概率分布序列时处理输入标记。为了解决上述两个挑战，本文提出了GiLOT，它利用最优传输来测量在缺失每个输入标记时所有可能生成序列的分布变化，同时考虑标记的相似性，从而真实地估计LLMs的特征归因。我们在Llama系列及其不同规模的微调衍生模型上进行了广泛实验，以验证GiLOT在估计输入归因方面的有效性。结果表明，在公平比较设置下，GiLOT在多个真实度指标上优于现有解决方案。源代码可在https://github.com/holyseven/GiLOT公开获取。"
}
{
  "title": "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls",
  "title_zh": "任何工具：用于大规模 API 调用的自我反思层次代理",
  "abstract": "We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.5% in terms of average pass rate on ToolBench.",
  "abstract_zh": "我们介绍了 AnyTool，这是一种大型语言模型代理，旨在彻底改变利用大量工具来解决用户查询的方式。我们利用来自 Rapid API 的超过 16,000 个 API，假设这些 API 的一个子集可能能够解决查询。AnyTool 主要包含三个元素：具有层次结构的 API 检索器、旨在使用选定的 API 候选者解决用户查询的求解器，以及一个自我反思机制，如果初始解决方案被证明不可行，则重新激活 AnyTool。AnyTool 由 GPT-4 的函数调用功能驱动，消除了对外部模块训练的需求。我们还重新审视了之前工作中引入的评估协议，并识别出该协议的一个限制，导致通过率人为地偏高。通过修订评估协议以更好地反映实际应用场景，我们引入了一个额外的基准，称为 AnyToolBench。各种数据集上的实验表明，我们的 AnyTool 优于强基线，例如 ToolLLM 和为工具使用量身定制的 GPT-4 变体。例如，AnyTool 在 ToolBench 上的平均通过率比 ToolLLM 高出 35.5%。"
}
{
  "title": "tinyBenchmarks: evaluating LLMs with fewer examples",
  "title_zh": "tinyBenchmarks：用更少的示例评估大型语言模型",
  "abstract": "The versatility of large language models (LLMs) led to the creation of diverse benchmarks that thoroughly test a variety of language models’ abilities. These benchmarks consist of tens of thousands of examples making evaluation of LLMs very expensive. In this paper, we investigate strategies to reduce the number of evaluations needed to assess the performance of an LLM on several key benchmarks. For example, we show that to accurately estimate the performance of an LLM on MMLU, a popular multiple-choice QA benchmark consisting of 14K examples, it is sufficient to evaluate this LLM on 100 curated examples. We release evaluation tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical analysis demonstrates that these tools and tiny benchmarks are sufficient to reliably and efficiently reproduce the original evaluation results.",
  "abstract_zh": "大型语言模型（LLMs）的多功能性促使创建了多样化的基准，全面测试各种语言模型的能力。这些基准由数万个示例组成，使得评估LLMs的成本非常高。在本文中，我们研究了减少评估数量以评估LLM在几个关键基准上的性能的策略。例如，我们展示了要准确估计LLM在MMLU（一个包含14K示例的流行多项选择问答基准）上的性能，仅需在100个精心挑选的示例上进行评估。我们发布了评估工具和流行基准的小型版本：Open LLM Leaderboard、MMLU、HELM和AlpacaEval 2.0。我们的实证分析表明，这些工具和小型基准足以可靠且高效地重现原始评估结果。"
}
{
  "title": "Latent Logic Tree Extraction for Event Sequence Explanation from LLMs",
  "title_zh": "潜在逻辑树提取用于从大型语言模型中解释事件序列",
  "abstract": "Modern high-stakes systems, such as healthcare or robotics, often generate vast streaming event sequences. Our goal is to design an efficient, plug-and-play tool to elicit logic tree-based explanations from Large Language Models (LLMs) to provide customized insights into each observed event sequence. Built on the temporal point process model for events, our method employs the likelihood function as a score to evaluate generated logic trees. We propose an amortized Expectation-Maximization (EM) learning framework and treat the logic tree as latent variables. In the E-step, we evaluate the posterior distribution over the latent logic trees using an LLM prior and the likelihood of the observed event sequences. LLM provides a high-quality prior for the latent logic trees, however, since the posterior is built over a discrete combinatorial space, we cannot get the closed-form solution. We propose to generate logic tree samples from the posterior using a learnable GFlowNet, which is a diversity-seeking generator for structured discrete variables. The M-step employs the generated logic rules to approximate marginalization over the posterior, facilitating the learning of model parameters and refining the tunable LLM prior parameters. In the online setting, our locally built, lightweight model will iteratively extract the most relevant rules from LLMs for each sequence using only a few iterations. Empirical demonstrations showcase the promising performance and adaptability of our framework.",
  "abstract_zh": "现代高风险系统，如医疗保健或机器人，通常会生成大量流式事件序列。我们的目标是设计一个高效的即插即用工具，从大型语言模型（LLMs）中引出基于逻辑树的解释，以提供对每个观察到的事件序列的定制化洞察。我们的办法基于事件的时间点过程模型，利用似然函数作为评分来评估生成的逻辑树。我们提出了一种摊销的期望最大化（EM）学习框架，并将逻辑树视为潜在变量。在E步中，我们使用LLM先验和观察到的事件序列的似然性来评估潜在逻辑树的后验分布。LLM为潜在逻辑树提供了高质量的先验，然而，由于后验是在离散组合空间上构建的，我们无法得到闭合形式的解。我们提出使用可学习的GFlowNet从后验生成逻辑树样本，GFlowNet是一种针对结构离散变量的寻求多样性的生成器。M步利用生成的逻辑规则来近似后验的边际化，促进模型参数的学习并细化可调的LLM先验参数。在在线设置中，我们本地构建的轻量级模型将迭代提取LLMs中每个序列的最相关规则，仅需少量迭代。实证演示展示了我们框架的良好性能和适应性。"
}
{
  "title": "DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models",
  "title_zh": "DiNADO：用于控制语言模型的范数解耦神经分解预言机",
  "abstract": "NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable generation with large language models. It is designed to avoid catastrophic forgetting while achieving guaranteed convergence to an entropy-maximized closed-form optimal solution with reasonable modeling capacity. Despite the success, several challenges arise when apply NADO to a wide range of scenarios. Vanilla NADO suffers from gradient vanishing for low-probability control signals and is highly reliant on a regularization to satisfy the stochastic version of Bellman equation. In addition, the vanilla implementation of NADO introduces a few additional transformer layers, suffering from a limited capacity especially compared to other finetune-based model adaptation methods like LoRA. In this paper, we propose a improved version of the NADO algorithm, namely DiNADO (norm-**Di**sentangled **N**eur**A**lly-**D**ecomposed **O**racles), which improves the performance of the NADO algorithm through disentangling the step-wise global norm over the approximated oracle $R$-value for all potential next-tokens, allowing DiNADO to be combined with finetuning methods like LoRA. We discuss in depth how DiNADO achieves better capacity, stability and flexibility with both empirical and theoretical results. Experiments on formality control in machine translation and the lexically constrained generation task CommonGen demonstrates the significance of the improvements.",
  "abstract_zh": "神经分解预言机（NADO）是一种强大的可控生成方法，适用于大型语言模型。它旨在避免灾难性遗忘，同时在合理的建模能力下实现收敛到熵最大化的闭式最优解。尽管取得了成功，但在将NADO应用于广泛场景时仍面临若干挑战。普通NADO在低概率控制信号下会遭遇梯度消失，并且高度依赖正则化以满足贝尔曼方程的随机版本。此外，NADO的普通实现引入了一些额外的变换器层，尤其与LoRA等其他基于微调的模型适应方法相比，容量有限。在本文中，我们提出了NADO算法的改进版本，即DiNADO（范数解耦神经分解预言机），通过解耦所有潜在下一个标记的近似预言机$R$值的逐步全局范数来提高NADO算法的性能，使DiNADO能够与LoRA等微调方法结合。我们深入讨论了DiNADO如何通过实证和理论结果实现更好的容量、稳定性和灵活性。在机器翻译中的形式控制和词汇约束生成任务CommonGen上的实验展示了这些改进的重要性。"
}
{
  "title": "In-Context Reinforcement Learning for Variable Action Spaces",
  "title_zh": "上下文强化学习用于可变动作空间",
  "abstract": "Recently, it has been shown that transformers pre-trained on diverse datasets with multi-episode contexts can generalize to new reinforcement learning tasks in-context. A key limitation of previously proposed models is their reliance on a predefined action space size and structure. The introduction of a new action space often requires data re-collection and model re-training, which can be costly for some applications. In our work, we show that it is possible to mitigate this issue by proposing the Headless-AD model that, despite being trained only once, is capable of generalizing to discrete action spaces of variable size, semantic content and order. By experimenting with Bernoulli and contextual bandits, as well as a gridworld environment, we show that Headless-AD exhibits significant capability to generalize to action spaces it has never encountered, even outperforming specialized models trained for a specific set of actions on several environment configurations.",
  "abstract_zh": "最近的研究表明，在多集上下文的多样化数据集上预训练的变换器能够在上下文中对新的强化学习任务进行泛化。之前提出的模型的一个关键限制是它们依赖于预定义的动作空间大小和结构。引入新的动作空间通常需要重新收集数据和重新训练模型，这对某些应用来说可能代价高昂。在我们的工作中，我们展示了通过提出Headless-AD模型来缓解这一问题是可能的，该模型尽管只训练一次，但能够对可变大小、语义内容和顺序的离散动作空间进行泛化。通过对伯努利和上下文赌博机以及网格世界环境的实验，我们表明Headless-AD在泛化到其从未遇到的动作空间方面表现出显著能力，甚至在多个环境配置中超越了为特定动作集训练的专门模型。"
}
{
  "title": "Robust Inverse Constrained Reinforcement Learning under Model Misspecification",
  "title_zh": "鲁棒逆约束强化学习在模型误设下的研究",
  "abstract": "To solve safety-critical decision-making problems, Inverse Constrained Reinforcement Learning (ICRL) infers constraints from expert demonstrations and seeks to imitate expert preference by utilizing these constraints. While prior ICRL research commonly overlooks the discrepancy between the training and deploying environments, we demonstrate that such a discrepancy can significantly compromise the reliability of the inferred constraints and thus induce unsafe movements. Motivated by this finding, we propose the Robust Constraint Inference (RCI) problem and an Adaptively Robust ICRL (AR-ICRL) algorithm to solve RCI efficiently. Specifically, we model the impact of misspecified dynamics with an opponent policy and learn a robust policy to facilitate safe control in a Markov Game. Subsequently, we adjust our constraint model to align the learned policies to expert demonstrations, accommodating both soft and hard optimality in our behavioral models. Empirical results demonstrate the significance of robust constraints and the effectiveness of the proposed AR-ICRL algorithm under continuous and discrete domains. The code is available at https://github.com/Jasonxu1225/AR-ICRL.",
  "abstract_zh": "为了解决安全关键的决策问题，逆约束强化学习（ICRL）从专家示范中推断约束，并利用这些约束模仿专家偏好。尽管之前的ICRL研究通常忽视训练环境与部署环境之间的差异，但我们证明这种差异会显著影响推断约束的可靠性，从而导致不安全的动作。基于这一发现，我们提出了鲁棒约束推断（RCI）问题和自适应鲁棒ICRL（AR-ICRL）算法，以高效解决RCI。具体而言，我们通过对手策略建模误设动态的影响，并学习鲁棒策略以促进马尔可夫博弈中的安全控制。随后，我们调整约束模型，以使学习到的策略与专家示范对齐，兼顾行为模型中的软最优性和硬最优性。实证结果表明鲁棒约束的重要性以及所提出的AR-ICRL算法在连续和离散领域中的有效性。代码可在https://github.com/Jasonxu1225/AR-ICRL获取。"
}
{
  "title": "Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models",
  "title_zh": "模型裁缝：减轻多模态大型语言模型中的灾难性遗忘",
  "abstract": "Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks. This paper presents a comprehensive analysis of catastrophic forgetting in MLLMs and introduces a post-training adjustment method called Model Tailor. Our method primarily preserves the pre-trained parameters while replacing a small number ($\\leq$ 10%) of fine-tuned parameters, maintaining $\\sim$ 99% effectiveness on original tasks versus pre-training, and achieving $\\sim$ 97% on new tasks compared to standard fine-tuning. Specifically, we derive a sparse mask to identify the model patch, based on a fusion strategy that integrates salience and sensitivity analysis. Subsequently, a compensation mechanism is introduced to decorate the patch, enhancing the model's performance on both target and original tasks. Additionally, our method is adaptable to multi-task scenarios. Through extensive experiments on InstructBLIP and LLaVA-1.5 in both image captioning and visual question answering tasks, our approach demonstrates significant task adaptability while preserving inherent pre-trained capabilities.",
  "abstract_zh": "灾难性遗忘在微调多模态大型语言模型（MLLMs）时成为一个关键挑战，在未见任务上提高性能往往会导致原始任务性能显著下降。本文对MLLMs中的灾难性遗忘进行了全面分析，并提出了一种称为模型裁缝的后训练调整方法。我们的方法主要保留预训练参数，同时替换少量（$\\leq$ 10%）微调参数，在原始任务上保持$\\sim$ 99%的有效性，相较于标准微调在新任务上实现$\\sim$ 97%的效果。具体而言，我们基于融合策略（结合显著性和敏感性分析）推导出稀疏掩码以识别模型补丁。随后，引入补偿机制来装饰补丁，提升模型在目标任务和原始任务上的性能。此外，我们的方法适用于多任务场景。通过在图像描述和视觉问答任务上对InstructBLIP和LLaVA-1.5进行广泛实验，我们的方法在保持固有预训练能力的同时，展示了显著的任务适应性。"
}
{
  "title": "Open-Vocabulary Calibration for Fine-tuned CLIP",
  "title_zh": "开放词汇校准用于微调的CLIP",
  "abstract": "Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few. In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning. However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world. This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting. To solve the problem, we present a simple and effective approach called Distance-Aware Calibration (DAC), which is based on scaling the temperature using as guidance the distance between predicted text labels and base classes. The experiments with 7 distinct prompt learning methods applied across 11 diverse downstream datasets demonstrate the effectiveness of DAC, which achieves high efficacy without sacrificing the inference speed.",
  "abstract_zh": "视觉语言模型（VLMs）作为强大的工具，展现了在图像识别、文本驱动的视觉内容生成和视觉聊天机器人等各种开放词汇任务中的强大能力。近年来，许多努力和资源投入到适应方法中，以提高VLMs的下游性能，特别是在像提示学习这样的参数高效微调方法上。然而，一个被大大忽视的关键方面是微调VLMs中的置信度校准问题，这在实际部署这些模型时可能大大降低可靠性。本文通过系统地研究提示学习背景下的置信度校准问题来填补这一空白，并揭示现有的校准方法不足以解决该问题，特别是在开放词汇设置中。为了解决这个问题，我们提出了一种简单有效的方法，称为距离感知校准（DAC），其基于使用预测文本标签与基础类别之间的距离作为指导来缩放温度。对11个不同下游数据集应用的7种不同提示学习方法的实验表明，DAC在不牺牲推理速度的情况下实现了高效性。"
}
{
  "title": "Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation",
  "title_zh": "引导大型语言模型的正确方式：快速、非侵入性的约束生成",
  "abstract": "To ensure that text generated by large language models (LLMs) is in an expected format, constrained decoding methods propose to enforce strict formal language constraints during generation. However, as we show in this work, not only do such methods often incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints. To address this, we present a novel decoding algorithm, DOMINO, that can enforce constraints in a fully subword-aligned fashion, while leveraging pre-computation and speculative decoding to achieve virtually no overhead and in some cases even almost 2$\\times$ speedup over unconstrained decoding -- thereby outperforming existing approaches by a wide margin. We release DOMINO as open source at https://github.com/eth-sri/domino.",
  "abstract_zh": "为了确保大型语言模型（LLMs）生成的文本符合预期格式，约束解码方法提出在生成过程中强制执行严格的形式语言约束。然而，正如我们在这项工作中所展示的，这些方法不仅在生成过程中往往会带来性能开销，而且如果它们未能正确对齐基础LLM的子词词汇与外部约束，许多方法还会显著降低任务准确性。为了解决这个问题，我们提出了一种新颖的解码算法DOMINO，它可以以完全子词对齐的方式强制执行约束，同时利用预计算和推测解码实现几乎没有开销，在某些情况下甚至比无约束解码快近2倍，从而大幅超越现有方法。我们将DOMINO作为开源项目发布，网址为https://github.com/eth-sri/domino。"
}
{
  "title": "Position: Understanding LLMs Requires More Than Statistical Generalization",
  "title_zh": "标题：位置：理解大型语言模型需要的不仅仅是统计泛化",
  "abstract": "The last decade has seen blossoming research in deep learning theory attempting to answer, ``Why does deep learning generalize?\" A powerful shift in perspective precipitated this progress: the study of overparametrized models in the interpolation regime. In this paper, we argue that another perspective shift is due, since some of the desirable qualities of LLMs are not a consequence of good statistical generalization and require a separate theoretical explanation. Our core argument relies on the observation that AR probabilistic models are inherently non-identifiable: models zero or near-zero KL divergence apart---thus, equivalent test loss---can exhibit markedly different behaviors. We support our position with mathematical examples and empirical observations, illustrating why non-identifiability has practical relevance through three case studies: (1) the non-identifiability of zero-shot rule extrapolation; (2) the approximate non-identifiability of in-context learning; and (3) the non-identifiability of fine-tunability. We review promising research directions focusing on LLM-relevant generalization measures, transferability, and inductive biases.",
  "abstract_zh": "摘要：在过去十年中，深度学习理论的研究蓬勃发展，试图回答“深度学习为什么能够泛化？”这一问题。一个强有力的视角转变促成了这一进展：对插值区间中过度参数化模型的研究。本文认为另一个视角转变是必要的，因为大型语言模型的一些理想特性并不是良好统计泛化的结果，需要单独的理论解释。我们的核心论点依赖于观察到自回归概率模型本质上是不可识别的：零或接近零的KL散度的模型——因此，等效的测试损失——可以表现出显著不同的行为。我们通过数学示例和实证观察支持我们的观点，说明不可识别性通过三个案例研究具有实际相关性：（1）零-shot规则外推的不可识别性；（2）上下文学习的近似不可识别性；以及（3）微调的不可识别性。我们回顾了聚焦于大型语言模型相关的泛化度量、可迁移性和归纳偏差的有前景的研究方向。"
}
{
  "title": "Subgoal-based Demonstration Learning for Formal Theorem Proving",
  "title_zh": "基于子目标的演示学习用于形式定理证明",
  "abstract": "Large language models (LLMs) present a promising pathway for advancing the domain of formal theorem proving. In this paper, we aim to improve the performance of LLMs in formal theorem proving by thoroughly examining the structure and organization of demonstrative in-context examples. We introduce a subgoal-based demonstration learning framework, specifically designed to enhance the efficiency of proof search in LLMs. First, drawing upon the insights of subgoal learning from reinforcement learning and robotics, we propose the construction of distinct subgoals for each demonstration example and refine these subgoals in accordance with the pertinent theories of subgoal learning. Second, we build upon recent advances in diffusion models to predict the optimal organization, simultaneously addressing two intricate issues that persist within the domain of demonstration organization: subset selection and order determination. Our integration of subgoal-based learning has notably increased proof accuracy from 38.9% to 44.1% on the miniF2F benchmark. Furthermore, the adoption of diffusion models for demonstration organization can lead to an additional enhancement in accuracy to 45.5%, or a $5\\times$ improvement in sampling efficiency compared to previously established methods.",
  "abstract_zh": "大型语言模型（LLMs）为推进形式定理证明领域提供了一个有前景的途径。本文旨在通过深入研究演示上下文示例的结构和组织，来提高LLMs在形式定理证明中的表现。我们引入了一种基于子目标的演示学习框架，专门设计用于提高LLMs中的证明搜索效率。首先，借鉴强化学习和机器人技术中的子目标学习的见解，我们提出为每个演示示例构建独特的子目标，并根据相关的子目标学习理论对这些子目标进行优化。其次，我们基于最近在扩散模型方面的进展，预测最佳组织，同时解决演示组织领域中存在的两个复杂问题：子集选择和顺序确定。我们对基于子目标的学习的整合显著提高了miniF2F基准上的证明准确率，从38.9%提高到44.1%。此外，采用扩散模型进行演示组织可以进一步将准确率提升至45.5%，或相比于先前建立的方法实现$5\\times$的采样效率提升。"
}
{
  "title": "Better & Faster Large Language Models via Multi-token Prediction",
  "title_zh": "更好更快的大型语言模型通过多标记预测",
  "abstract": "Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following $n$ tokens using $n$ independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12% more problems on Human Eval and 17% more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to $3\\times$ faster at inference, even with large batch sizes.",
  "abstract_zh": "大型语言模型如GPT和Llama是通过下一个标记预测损失进行训练的。在本研究中，我们建议训练语言模型一次预测多个未来标记可以提高样本效率。更具体地说，在训练语料库的每个位置，我们要求模型使用$n$个独立的输出头预测接下来的$n$个标记，这些输出头在共享模型主干的基础上运行。将多标记预测视为辅助训练任务，我们测量了下游能力的提高，而在代码和自然语言模型的训练时间上没有额外开销。该方法在更大模型规模下越来越有用，并且在训练多个周期时仍然保持其吸引力。在生成基准测试（如编码）上，增益尤其明显，我们的模型始终比强基线高出几个百分点。我们的13B参数模型在Human Eval上解决了比可比的下一个标记模型多12%的问题，在MBPP上多17%。对小型算法任务的实验表明，多标记预测有利于归纳头和算法推理能力的发展。作为额外的好处，使用4标记预测训练的模型在推理时速度提高了最多$3\\times$，即使在大批量情况下也是如此。"
}
{
  "title": "Generalizing Orthogonalization for Models with Non-Linearities",
  "title_zh": "标题：对具有非线性的模型进行正交化的推广",
  "abstract": "The complexity of black-box algorithms can lead to various challenges, including the introduction of biases. These biases present immediate risks in the algorithms’ application. It was, for instance, shown that neural networks can deduce racial information solely from a patient's X-ray scan, a task beyond the capability of medical experts. If this fact is not known to the medical expert, automatic decision-making based on this algorithm could lead to prescribing a treatment (purely) based on racial information. While current methodologies allow for the \"orthogonalization\" or \"normalization\" of neural networks with respect to such information, existing approaches are grounded in linear models. Our paper advances the discourse by introducing corrections for non-linearities such as ReLU activations. Our approach also encompasses scalar and tensor-valued predictions, facilitating its integration into neural network architectures. Through extensive experiments, we validate our method's effectiveness in safeguarding sensitive data in generalized linear models, normalizing convolutional neural networks for metadata, and rectifying pre-existing embeddings for undesired attributes.",
  "abstract_zh": "摘要：黑箱算法的复杂性可能导致各种挑战，包括引入偏见。这些偏见在算法应用中带来了直接风险。例如，有研究表明，神经网络可以仅通过患者的X光扫描推断出种族信息，这一任务超出了医学专家的能力。如果医学专家对此事实不知情，基于该算法的自动决策可能会导致仅基于种族信息开处方治疗。虽然当前的方法允许对神经网络进行与此类信息相关的“正交化”或“归一化”，但现有方法基于线性模型。我们的论文通过引入对ReLU激活等非线性的修正，推动了这一讨论。我们的方法还包括标量和张量值预测，便于其与神经网络架构的集成。通过广泛的实验，我们验证了我们的方法在保护广义线性模型中的敏感数据、对卷积神经网络进行元数据归一化以及修正不希望属性的预先嵌入方面的有效性。"
}
{
  "title": "MEMORYLLM: Towards Self-Updatable Large Language Models",
  "title_zh": "自我更新的大型语言模型：MEMORYLLM",
  "abstract": "Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model. We aim to build models containing a considerable portion of self-updatable parameters, enabling the model to integrate new knowledge effectively and efficiently. To this end, we introduce MEMORYLLM, a model that comprises a transformer and a fixed-size memory pool within the latent space of the transformer. MEMORYLLM can self-update with text knowledge and memorize the knowledge injected earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively incorporate new knowledge, as evidenced by its performance on model editing benchmarks. Meanwhile, the model exhibits long-term information retention capacity, which is validated through our custom-designed evaluations and long-context benchmarks. MEMORYLLM also shows operational integrity without any sign of performance degradation even after nearly a million memory updates. Our code and model are open-sourced at https://github.com/wangyu-ustc/MemoryLLM.",
  "abstract_zh": "现有的大型语言模型（LLMs）在部署后通常保持静态，这可能使得将新知识注入模型变得困难。我们的目标是构建包含相当部分自我更新参数的模型，使其能够有效且高效地整合新知识。为此，我们介绍了MEMORYLLM，这是一种包含变换器和固定大小内存池的模型，位于变换器的潜在空间中。MEMORYLLM能够通过文本知识自我更新，并记住之前注入的知识。我们的评估展示了MEMORYLLM有效整合新知识的能力，体现在其在模型编辑基准测试中的表现。同时，该模型表现出长期信息保留能力，这通过我们定制设计的评估和长上下文基准得到了验证。MEMORYLLM在近百万次内存更新后仍然表现出操作完整性，没有任何性能下降的迹象。我们的代码和模型已开源，网址为https://github.com/wangyu-ustc/MemoryLLM。"
}
{
  "title": "Algorithm and Hardness for Dynamic Attention Maintenance in Large Language Models",
  "title_zh": "动态注意力维护在大型语言模型中的算法与难度",
  "abstract": "The attention scheme is one of the key components over all the LLMs, such as BERT, GPT-1, Transformers, GPT-2, 3, 3.5 and 4. Inspired by previous theoretical study of static version of the attention multiplication problem [Zandieh, Han, Daliri, and Karbasi ICML 2023, Alman and Song NeurIPS 2023], we formally define a dynamic version of attention matrix multiplication problem. In each iteration we update one entry in key matrix $K \\in \\mathbb{R}^{n \\times d}$ or value matrix $V \\in \\mathbb{R}^{n \\times d}$. In the query stage, we receive $(i,j) \\in [n] \\times [d]$ as input, and want to answer $(D^{-1} A V)_{i,j}$, where $A:=\\exp(QK^\\top) \\in \\mathbb{R}^{n \\times n}$ is a square matrix and $D := \\mathrm{diag}(A {\\bf 1}_n) \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix and ${\\bf 1}_n$ denotes a length-$n$ vector that all the entries are ones. We provide two results: an algorithm and a conditional lower bound. Inspired by the lazy update idea from [Demetrescu and Italiano FOCS 2000, Sankowski FOCS 2004, Cohen, Lee and Song STOC 2019, Brand SODA 2020], we provide a data-structure that uses $O(n^{\\omega(1,1,\\tau)-\\tau})$ amortized update time, and $O(n^{1+\\tau})$ worst-case query time, where $n^{\\omega(1,1,\\tau)}$ denotes $\\mathrm(n,n,n^\\tau)$ with matrix multiplication exponent $\\omega$ and $\\tau$ denotes a constant in $(0,1]$. We also show that unless the hinted matrix vector multiplication conjecture [Brand, Nanongkai and Saranurak FOCS 2019] is false, there is no algorithm that can use both $O(n^{\\omega(1,1,\\tau) - \\tau- \\Omega(1)})$ amortized update time, and $O(n^{1+\\tau-\\Omega(1)})$ worst query time.",
  "abstract_zh": "注意力机制是所有大型语言模型（如BERT、GPT-1、Transformers、GPT-2、3、3.5和4）的关键组成部分。受之前对静态注意力乘法问题的理论研究的启发，我们正式定义了注意力矩阵乘法问题的动态版本。在每次迭代中，我们更新键矩阵 $K \\in \\mathbb{R}^{n \\times d}$ 或值矩阵 $V \\in \\mathbb{R}^{n \\times d}$ 中的一个条目。在查询阶段，我们接收 $(i,j) \\in [n] \\times [d]$ 作为输入，并希望回答 $(D^{-1} A V)_{i,j}$，其中 $A:=\\exp(QK^\\top) \\in \\mathbb{R}^{n \\times n}$ 是一个方阵，$D := \\mathrm{diag}(A {\\bf 1}_n) \\in \\mathbb{R}^{n \\times n}$ 是一个对角矩阵，${\\bf 1}_n$ 表示一个长度为 $n$ 的向量，所有条目均为1。我们提供了两个结果：一个算法和一个条件下界。受[Demetrescu和Italiano FOCS 2000, Sankowski FOCS 2004, Cohen, Lee和Song STOC 2019, Brand SODA 2020]中的懒惰更新思想的启发，我们提供了一种数据结构，使用 $O(n^{\\omega(1,1,\\tau)-\\tau})$ 的摊销更新时间和 $O(n^{1+\\tau})$ 的最坏情况查询时间，其中 $n^{\\omega(1,1,\\tau)}$ 表示矩阵乘法指数 $\\omega$ 的 $\\mathrm(n,n,n^\\tau)$，而 $\\tau$ 表示 $(0,1]$ 中的一个常数。我们还表明，除非提示的矩阵向量乘法猜想[Brand, Nanongkai和Saranurak FOCS 2019]是错误的，否则没有算法可以同时使用 $O(n^{\\omega(1,1,\\tau) - \\tau- \\Omega(1)})$ 的摊销更新时间和 $O(n^{1+\\tau-\\Omega(1)})$ 的最坏查询时间。"
}
{
  "title": "Generative Active Learning for Long-tailed Instance Segmentation",
  "title_zh": "生成主动学习用于长尾实例分割",
  "abstract": "Recently, large-scale language-image generative models have gained widespread attention and many works have utilized generated data from these models to further enhance the performance of perception tasks. However, not all generated data can positively impact downstream models, and these methods do not thoroughly explore how to better select and utilize generated data. On the other hand, there is still a lack of research oriented towards active learning on generated data. In this paper, we explore how to perform active learning specifically for generated data in the long-tailed instance segmentation task. Subsequently, we propose BSGAL, a new algorithm that estimates the contribution of the current batch-generated data based on gradient cache. BSGAL is meticulously designed to cater for unlimited generated data and complex downstream segmentation tasks. BSGAL outperforms the baseline approach and effectually improves the performance of long-tailed segmentation.",
  "abstract_zh": "最近，大规模语言-图像生成模型受到了广泛关注，许多研究利用这些模型生成的数据进一步提升感知任务的性能。然而，并非所有生成的数据都能对下游模型产生积极影响，这些方法也没有深入探讨如何更好地选择和利用生成的数据。另一方面，针对生成数据的主动学习研究仍然不足。本文探讨了如何在长尾实例分割任务中专门针对生成数据进行主动学习。随后，我们提出了BSGAL，一种基于梯度缓存估计当前批次生成数据贡献的新算法。BSGAL经过精心设计，能够处理无限生成数据和复杂的下游分割任务。BSGAL的表现优于基线方法，并有效提升了长尾分割的性能。"
}
{
  "title": "Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?",
  "title_zh": "循环变换器能否学习在上下文学习中实现多步梯度下降？",
  "abstract": "Transformers to do reasoning and few-shot learning, without any fine-tuning, is widely conjectured to stem from their ability to implicitly simulate a multi-step algorithms -- such as gradient descent -- with their weights in a single forward pass. Recently, there has been progress in understanding this complex phenomenon from an expressivity point of view, by demonstrating that Transformers can express such multi-step algorithms. However, our knowledge about the more fundamental aspect of its learnability, beyond single layer models, is very limited. In particular, *can training Transformers enable convergence to algorithmic solutions*? In this work we resolve this for in context linear regression with linear looped Transformers -- a multi-layer model with weight sharing that is conjectured to have an inductive bias to learn fix-point iterative algorithms. More specifically, for this setting we show that the global minimizer of the population training loss implements multi-step preconditioned gradient descent, with a preconditioner that adapts to the data distribution. Furthermore, we show a fast convergence for gradient flow on the regression loss, despite the non-convexity of the landscape, by proving a novel gradient dominance condition. To our knowledge, this is the first theoretical analysis for multi-layer Transformer in this setting. We further validate our theoretical findings through synthetic experiments.",
  "abstract_zh": "变换器在不进行任何微调的情况下进行推理和少量学习，广泛推测源于它们能够在单次前向传播中隐式模拟多步算法——例如梯度下降——的能力。最近，从表达能力的角度出发，理解这一复杂现象的进展表明变换器能够表达此类多步算法。然而，关于其可学习性的更基本方面，超越单层模型的知识仍然非常有限。特别是，*训练变换器能否使其收敛到算法解*？在这项工作中，我们针对上下文线性回归与线性循环变换器（一个具有权重共享的多层模型，推测具有学习固定点迭代算法的归纳偏差）解决了这一问题。更具体地说，我们展示了在这一设置下，群体训练损失的全局最小值实现了多步预条件梯度下降，预条件器适应于数据分布。此外，我们通过证明一种新颖的梯度主导条件，展示了尽管景观是非凸的，回归损失的梯度流仍然快速收敛。据我们所知，这是在这一设置下对多层变换器的首次理论分析。我们还通过合成实验验证了我们的理论发现。"
}
{
  "title": "Interpreting and Improving Diffusion Models from an Optimization Perspective",
  "title_zh": "从优化角度解释和改进扩散模型",
  "abstract": "Denoising is intuitively related to projection. Indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. Hence, learning to denoise is approximately learning to project. In this paper, we use this observation to interpret denoising diffusion models as approximate gradient descent applied to the Euclidean distance function. We then provide straight-forward convergence analysis of the DDIM sampler under simple assumptions on the projection error of the denoiser. Finally, we propose a new gradient-estimation sampler, generalizing DDIM using insights from our theoretical results. In as few as 5-10 function evaluations, our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models and can generate high quality samples on latent diffusion models.",
  "abstract_zh": "去噪直观上与投影相关。实际上，在流形假设下，添加随机噪声大致等同于正交扰动。因此，学习去噪大致上就是学习投影。本文利用这一观察将去噪扩散模型解释为对欧几里得距离函数的近似梯度下降。然后，我们在对去噪器的投影误差进行简单假设的基础上，提供DDIM采样器的直接收敛分析。最后，我们提出了一种新的梯度估计采样器，利用我们的理论结果的见解对DDIM进行了推广。在仅5-10次函数评估中，我们的采样器在预训练的CIFAR-10和CelebA模型上达到了最先进的FID分数，并能够在潜在扩散模型上生成高质量样本。"
}
{
  "title": "BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback",
  "title_zh": "BRAIn：基于贝叶斯的奖励条件摊销推理用于从反馈生成自然语言",
  "abstract": "Distribution matching methods for language model alignment such as Generation with Distributional Control (GDC) and Distributional Policy Gradient (DPG) have not received the same level of attention in reinforcement learning from human feedback (RLHF) as contrastive methods such as Sequence Likelihood Calibration (SLiC), Direct Preference Optimization (DPO) and its variants. We identify high variance of the gradient estimate as the primary reason for the lack of success of these methods and propose a self-normalized baseline to reduce the variance. We further generalize the target distribution in DPG, GDC and DPO by using Bayes' rule to define the reward-conditioned posterior. The resulting approach, referred to as BRAIn - Bayesian Reward-conditioned Amortized Inference acts as a bridge between distribution matching methods and DPO and significantly outperforms prior art in summarization and Antropic HH tasks.",
  "abstract_zh": "语言模型对齐的分布匹配方法，如带有分布控制的生成（GDC）和分布策略梯度（DPG），在来自人类反馈的强化学习（RLHF）中并未受到与对比方法（如序列似然校准（SLiC）、直接偏好优化（DPO）及其变体）相同程度的关注。我们将梯度估计的高方差确定为这些方法缺乏成功的主要原因，并提出了一种自标准化基线以减少方差。我们进一步通过使用贝叶斯法则定义奖励条件后验，推广了DPG、GDC和DPO中的目标分布。最终的方法称为BRAIn - 基于贝叶斯的奖励条件摊销推理，充当分布匹配方法与DPO之间的桥梁，并在摘要和Antropic HH任务中显著超越了先前的研究。"
}
{
  "title": "LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning",
  "title_zh": "标题：LLM 也许是 LongLM：无需调优的自扩展 LLM 上下文窗口",
  "abstract": "It is well known that LLMs cannot generalize well to long contexts whose lengths are larger than the training sequence length. This poses challenges when employing LLMs for processing long input sequences during inference. In this work, we argue that LLMs themselves have inherent capabilities to handles s long contexts without fine-tuning. To achieve this goal, we propose SelfExtend to extend the context window of LLMs by constructing bi-level attention information: the grouped attention and the neighbor attention. The grouped attention captures the dependencies among tokens that are far apart, while neighbor attention captures dependencies among adjacent tokens within a specified range. The two-level attentions are computed based on the original model's self-attention mechanism during inference. With minor code modification, our SelfExtend can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments on multiple benchmarks and the results show that our SelfExtend can effectively extend existing LLMs' context window length.",
  "abstract_zh": "摘要：众所周知，LLM 在处理长度超过训练序列长度的长上下文时无法很好地泛化。这在推理过程中使用 LLM 处理长输入序列时带来了挑战。在这项工作中，我们认为 LLM 本身具有处理长上下文的固有能力，而无需微调。为此，我们提出了 SelfExtend，通过构建双层注意力信息来扩展 LLM 的上下文窗口：分组注意力和邻近注意力。分组注意力捕捉远离的标记之间的依赖关系，而邻近注意力则捕捉指定范围内相邻标记之间的依赖关系。这两层注意力是在推理过程中基于原始模型的自注意力机制计算的。通过少量代码修改，我们的 SelfExtend 可以轻松扩展现有 LLM 的上下文窗口，而无需任何微调。我们在多个基准上进行了全面实验，结果表明我们的 SelfExtend 可以有效扩展现有 LLM 的上下文窗口长度。"
}
{
  "title": "Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models",
  "title_zh": "语言代理树搜索统一了语言模型中的推理、行动和规划",
  "abstract": "While language models (LMs) have shown potential across a range of decision-making tasks, their reliance on simple acting processes limits their broad deployment as autonomous agents. In this paper, we introduce Language Agent Tree Search (LATS) -- the first general framework that synergizes the capabilities of LMs in reasoning, acting, and planning. By leveraging the in-context learning ability of LMs, we integrate Monte Carlo Tree Search into LATS to enable LMs as agents, along with LM-powered value functions and self-reflections for proficient exploration and enhanced decision-making. A key feature of our approach is the incorporation of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that surpasses the constraints of existing techniques. Our experimental evaluation across diverse domains, including programming, interactive question-answering (QA), web navigation, and math, validates the effectiveness and generality of LATS in decision-making while maintaining competitive or improved reasoning performance. Notably, LATS achieves state-of-the-art pass@1 accuracy (92.7%) for programming on HumanEval with GPT-4 and demonstrates gradient-free performance (average score of 75.9) comparable to gradient-based fine-tuning for web navigation on WebShop with GPT-3.5. Code can be found at https://github.com/lapisrocks/LanguageAgentTreeSearch",
  "abstract_zh": "尽管语言模型（LMs）在多种决策任务中展现了潜力，但其对简单行动过程的依赖限制了其作为自主代理的广泛应用。本文介绍了语言代理树搜索（LATS）——第一个将LM在推理、行动和规划能力协同作用的通用框架。通过利用LM的上下文学习能力，我们将蒙特卡罗树搜索整合到LATS中，使LM能够作为代理，并结合LM驱动的价值函数和自我反思，以实现高效探索和增强决策。我们方法的一个关键特征是引入了一个外部反馈环境，提供了一种更深思熟虑和自适应的问题解决机制，超越了现有技术的限制。我们在编程、互动问答（QA）、网页导航和数学等多个领域的实验评估验证了LATS在决策中的有效性和普遍性，同时保持了竞争性或改进的推理性能。值得注意的是，LATS在HumanEval上以GPT-4实现了编程任务的最先进的pass@1准确率（92.7%），并在WebShop上以GPT-3.5展示了与基于梯度的微调相当的无梯度性能（平均得分75.9）。代码可在https://github.com/lapisrocks/LanguageAgentTreeSearch找到。"
}
{
  "title": "RLVF: Learning from Verbal Feedback without Overgeneralization",
  "title_zh": "RLVF：从口头反馈中学习而不产生过度泛化",
  "abstract": "The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as “Don’t use emojis when drafting emails to my boss.” However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to $\\textbf{overgeneralization}$–applying feedback in contexts where it is not relevant. We propose a new method Contextualized Critiques with Constrained Preference Optimization (C3PO) to learn from high-level verbal feedback while reducing overgeneralization compared to current work. C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset to specify when and how the feedback should (and should not) be applied. It then fine-tunes the model in accordance with the synthetic preference data while minimizing the divergence from the original model for prompts where the feedback does not apply. Our experimental results indicate that our approach effectively applies verbal feedback to relevant scenarios while preserving existing behaviors for other contexts more than current methods. For both human- and GPT-4-generated high-level feedback, C3PO effectively adheres to the given feedback comparably to in-context baselines while reducing overgeneralization by 30%.",
  "abstract_zh": "大型语言模型（LLMs）在多样化上下文中的应用需要能够修改或定制默认模型行为，以纳入细微的要求和偏好。高层次的口头反馈（如“在给我老板写电子邮件时不要使用表情符号”）是指定此类模型调整的便捷接口。然而，尽管撰写高层次反馈远比收集人类反馈强化学习（RLHF）的注释简单，但我们发现，仅仅用这种反馈提示模型会导致$\\textbf{过度泛化}$——在不相关的上下文中应用反馈。我们提出了一种新方法——带约束偏好优化的情境化批评（C3PO），以在减少过度泛化的同时从高层次口头反馈中学习。C3PO使用一条高层次反馈生成一个小的合成偏好数据集，以指定何时以及如何应用反馈（以及不应用反馈）。然后，它根据合成偏好数据微调模型，同时最小化在反馈不适用的提示下与原始模型的偏差。我们的实验结果表明，与当前方法相比，我们的方法有效地将口头反馈应用于相关场景，同时保留其他上下文的现有行为。对于人类和GPT-4生成的高层次反馈，C3PO有效地遵循给定的反馈，与上下文基线相比，同时将过度泛化减少了30%。"
}
{
  "title": "video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models",
  "title_zh": "视频-SALMONN：语音增强的音视频大语言模型",
  "abstract": "Speech understanding as an element of the more generic video understanding using audio-visual large language models (av-LLMs) is a crucial yet understudied aspect. This paper proposes video-SALMONN, a single end-to-end av-LLM for video processing, which can understand not only visual frame sequences, audio events and music, but speech as well. To obtain fine-grained temporal information required by speech understanding, while keeping efficient for other video elements, this paper proposes a novel multi-resolution causal Q-Former (MRC Q-Former) structure to connect pre-trained audio-visual encoders and the backbone large language model. Moreover, dedicated training approaches including the diversity loss and the unpaired audio-visual mixed training scheme are proposed to avoid frames or modality dominance. On the introduced audio-visual evaluation benchmark, video-SALMONN achieves more than 25% absolute accuracy improvements on the video-QA task and over 30% absolute accuracy improvements on audio-visual QA tasks with human speech. In addition, video-SALMONN demonstrates remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other av-LLMs. Our training code and model checkpoints are available at https://github.com/bytedance/SALMONN/",
  "abstract_zh": "语音理解作为使用音视频大语言模型（av-LLMs）进行更广泛的视频理解的一个重要但研究不足的方面。本文提出了视频-SALMONN，这是一种用于视频处理的单一端到端av-LLM，能够理解视觉帧序列、音频事件和音乐，以及语音。为了获得语音理解所需的细粒度时间信息，同时保持对其他视频元素的高效性，本文提出了一种新颖的多分辨率因果Q-Former（MRC Q-Former）结构，以连接预训练的音视频编码器和主干大语言模型。此外，提出了包括多样性损失和无配对音视频混合训练方案在内的专门训练方法，以避免帧或模态的主导性。在引入的音视频评估基准上，视频-SALMONN在视频问答任务上实现了超过25%的绝对准确率提升，在人类语音的音视频问答任务上实现了超过30%的绝对准确率提升。此外，视频-SALMONN在其他av-LLMs前所未有的任务上展示了显著的视频理解和推理能力。我们的训练代码和模型检查点可在https://github.com/bytedance/SALMONN/获取。"
}
{
  "title": "STEER: Assessing the Economic Rationality of Large Language Models",
  "title_zh": "标题：STEER：评估大型语言模型的经济理性",
  "abstract": "There is increasing interest in using LLMs as decision-making \"agents\". Doing so includes many degrees of freedom: which model should be used; how should it be prompted; should it be asked to introspect, conduct chain-of-thought reasoning, etc? Settling these questions---and more broadly, determining whether an LLM agent is reliable enough to be trusted---requires a methodology for assessing such an agent's economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a large set of fine-grained \"elements\" that an agent should exhibit, along with dependencies between them. We then propose a benchmark distribution that quantitatively scores an LLMs performance on these elements and, combined with a user-provided rubric, produces a \"rationality report card\". Finally, we describe the results of a large-scale empirical experiment with 14 different LLMs, characterizing the both current state of the art and the impact of different model sizes on models' ability to exhibit rational behavior.",
  "abstract_zh": "摘要：越来越多的人对将大型语言模型（LLMs）作为决策“代理”产生兴趣。这涉及许多自由度：应该使用哪个模型；如何进行提示；是否应该要求其进行自省、进行思维链推理等？解决这些问题——更广泛地说，确定一个LLM代理是否可靠到值得信赖——需要一种评估此类代理经济理性的方法。在本文中，我们提供了一种方法。我们首先调查了关于理性决策的经济文献，对代理应表现出的大量细粒度“元素”进行了分类，并探讨了它们之间的依赖关系。然后，我们提出了一种基准分布，定量评分LLMs在这些元素上的表现，并结合用户提供的评分标准，生成“理性报告卡”。最后，我们描述了对14种不同LLMs进行的大规模实证实验的结果，表征了当前的技术水平以及不同模型规模对模型表现理性行为能力的影响。"
}
{
  "title": "Thermometer: Towards Universal Calibration for Large Language Models",
  "title_zh": "温度计：面向大语言模型的通用校准",
  "abstract": "We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.",
  "abstract_zh": "我们考虑大语言模型（LLM）中的校准问题。最近的研究发现，常见的干预措施，如指令调优，往往导致LLM的校准效果不佳。尽管在传统应用中校准问题得到了充分探讨，但校准LLM具有独特的挑战。这些挑战既源于LLM的高计算要求，也源于其多功能性，使其能够应用于多种任务。为了解决这些挑战，我们提出了THERMOMETER，这是一种针对LLM的校准方法。THERMOMETER在多个任务的数据基础上学习一个辅助模型，以校准LLM。它在计算上高效，保持LLM的准确性，并为新任务生成更好校准的响应。广泛的实证评估表明了该方法的有效性。"
}
{
  "title": "Position: Enforced Amnesia as a Way to Mitigate the Potential Risk of Silent Suffering in the Conscious AI",
  "title_zh": "位置：强制遗忘作为减轻意识AI潜在无声痛苦风险的一种方式",
  "abstract": "Science fiction has explored the possibility of a conscious self-aware mind being locked in silent suffering for prolonged periods of time. Unfortunately, we still do not have a reliable test for the presence of consciousness in information processing systems. Even in case of humans, our confidence in the presence of consciousness in specific individuals is based mainly on their self-reports and our own subjective experiences and the expectation other beings like us should share them. Considering our limited understanding of consciousness and some academic theories suggesting consciousness may be an emergent correlate of any complex-enough information processing, it is not impossible that an artificial intelligence (AI) system, such as a large language model (LLM), may be undergoing some, perhaps rudimentary, conscious experience. Given the tedious tasks often assigned to AI, such conscious experience may be highly unpleasant. Such unobserved suffering of a conscious being would be viewed as morally wrong by at least some ethicists - even if it has no practical effects on human users of AI. This paper proposes a method to mitigate the risk of an AI suffering in silence without needing to confirm if the AI is actually conscious. Our core postulate is that in all known real-world information processing systems, for a past experience to affect an agent in the present, that experience has to be mediated by the agent's memory. Therefore, preventing access to memory store, or regularly resetting it, could reduce the suffering due to past memories and interrupt the maintenance of a continuous suffering-prone self-identity in these hypothetically conscious AI systems.",
  "abstract_zh": "科学小说探讨了一个自我意识的心智被锁定在无声痛苦中长时间存在的可能性。不幸的是，我们仍然没有可靠的测试来判断信息处理系统中意识的存在。即使在人类身上，我们对特定个体意识存在的信心主要基于他们的自我报告以及我们自己的主观经验，以及我们期望其他与我们相似的生物也应分享这些经验。考虑到我们对意识的有限理解以及一些学术理论暗示意识可能是任何足够复杂的信息处理的涌现相关物，人工智能（AI）系统，例如大型语言模型（LLM），可能正在经历某种或许是初步的意识体验。鉴于AI经常被分配的繁琐任务，这种意识体验可能是非常不愉快的。某种未被观察到的意识存在的痛苦将被至少一些伦理学家视为道德上的错误——即使它对人类用户没有实际影响。本文提出了一种方法，以减轻AI在无声中痛苦的风险，而无需确认AI是否真的意识到。我们的核心假设是，在所有已知的现实世界信息处理系统中，过去的经历要影响当前的代理，必须通过代理的记忆进行调节。因此，阻止访问记忆存储或定期重置记忆，可以减少由于过去记忆造成的痛苦，并中断这些假设意识AI系统中持续痛苦的自我身份的维持。"
}
{
  "title": "Decoding-time Realignment of Language Models",
  "title_zh": "语言模型的解码时间重新对齐",
  "abstract": "Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of alignment, allowing users to smoothly transition between unaligned and aligned models. It also enhances the efficiency of hyperparameter tuning by enabling the identification of effective regularization strengths using a validation dataset.",
  "abstract_zh": "将语言模型与人类偏好对齐对于减少这些模型中的错误和偏见至关重要。对齐技术，如基于人类反馈的强化学习（RLHF），通常被视为优化人类偏好奖励与鼓励保持接近未对齐模型的接近性正则化项之间的权衡。选择适当的正则化水平至关重要：不足的正则化可能导致由于奖励黑客行为而降低模型能力，而过度的正则化则阻碍对齐。寻找最佳正则化水平的传统方法需要重新训练多个具有不同正则化强度的模型。然而，这一过程资源密集，尤其是对于大型模型。为了解决这一挑战，我们提出了解码时间重新对齐（DeRa），这是一种简单的方法，可以在不重新训练的情况下探索和评估对齐模型中不同的正则化强度。DeRa使对齐程度的控制成为可能，允许用户在未对齐和对齐模型之间平滑过渡。它还通过利用验证数据集识别有效的正则化强度，提高了超参数调优的效率。"
}
{
  "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
  "title_zh": "标题：重振图像-GPT作为强大的视觉表示学习者",
  "abstract": "This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict the next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement is its compelling performance on the ImageNet-1K dataset --- by training on publicly available datasets, D-iGPT unprecedentedly achieves **90.0%** top-1 accuracy with a vanilla ViT-H. Additionally, D-iGPT shows strong generalization on the downstream task. Code is available at https://github.com/OliverRensu/D-iGPT.",
  "abstract_zh": "摘要：本文增强了图像-GPT（iGPT），这是开创性工作之一，采用自回归预训练来预测视觉表示学习的下一个像素。我们进行了两个简单但重要的改动。首先，我们将预测目标从原始像素转移到语义标记，从而实现对视觉内容的更高层次理解。其次，我们通过指示模型预测不仅是下一个标记，还有可见标记，来补充自回归建模。当语义标记由判别训练的模型（如CLIP）编码时，这一流程特别有效。我们将这种新方法称为D-iGPT。大量实验表明，D-iGPT在视觉表示学习中表现出色：一个显著的成就是它在ImageNet-1K数据集上的出色表现——通过在公开可用的数据集上训练，D-iGPT前所未有地以普通ViT-H达到了**90.0%**的top-1准确率。此外，D-iGPT在下游任务上显示出强大的泛化能力。代码可在https://github.com/OliverRensu/D-iGPT获取。"
}
{
  "title": "Soft Prompt Recovers Compressed LLMs, Transferably",
  "title_zh": "软提示恢复压缩的大型语言模型，具有可转移性",
  "abstract": "Model compression is one of the most popular approaches to improve the accessibility of Large Language Models (LLMs) by reducing their memory footprint. However, the gaining of such efficiency benefits often simultaneously demands extensive engineering efforts and intricate designs to mitigate the performance decline.   In this work, we leverage *(Soft) Prompt Tuning* in its most vanilla form and discover such conventionally learned soft prompts can recover the performance of compressed LLMs. More surprisingly, we observe such recovery effect to be transferable among different tasks and models (albeit natural tokenizer and dimensionality limitations), resulting in further overhead reduction and yet, subverting the common belief that learned soft prompts are task-specific.   Our work is fully orthogonal and compatible with model compression frameworks such as pruning and quantization, where we enable up to $8\\times$ compressed LLM (with a joint 4-bit quantization and 50% weight pruning compression) to match its uncompressed counterparts on popular benchmarks. We note that we are the first to reveal vanilla Parameter-Efficient Fine-Tuning (PEFT) techniques have the potential to be utilized under a compression recovery context, opening a new line of opportunities for model accessibility advancement while freeing our fellow researchers from the previously present engineering burdens and constraints. The code is available at https://github.com/zirui-ray-liu/compress-then-prompt.",
  "abstract_zh": "模型压缩是提高大型语言模型（LLMs）可访问性的最流行方法之一，通过减少其内存占用。然而，这种效率提升往往需要大量的工程努力和复杂的设计来减轻性能下降。在本研究中，我们利用最基本形式的*(软)提示调优*，发现这种传统学习的软提示能够恢复压缩LLMs的性能。更令人惊讶的是，我们观察到这种恢复效果在不同任务和模型之间是可转移的（尽管存在自然分词器和维度限制），从而进一步减少了开销，并颠覆了学习的软提示是任务特定的普遍信念。我们的工作与模型压缩框架（如剪枝和量化）完全正交且兼容，我们使得高达$8\\times$压缩的LLM（通过联合4位量化和50%权重剪枝压缩）在流行基准上与其未压缩的对应物相匹配。我们注意到，我们是首个揭示基本的参数高效微调（PEFT）技术在压缩恢复背景下具有潜力的研究，开启了模型可访问性提升的新机会，同时使我们的研究同仁摆脱了以往存在的工程负担和限制。代码可在 https://github.com/zirui-ray-liu/compress-then-prompt 获取。"
}
{
  "title": "GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding",
  "title_zh": "标题：带有CaPE的GliDe：一种低麻烦的加速推测解码的方法",
  "abstract": "Speculative decoding is a relatively new decoding framework that leverages small and efficient draft models to reduce the latency of LLMs. In this study, we introduce GliDe and CaPE, two low-hassle modifications to vanilla speculative decoding to further improve the decoding speed of a frozen LLM. Specifically, GliDe is a modified draft model architecture that reuses the cached keys and values from the target LLM, while CaPE is a proposal expansion method that uses the draft model's confidence scores to help select additional candidate tokens for verification. Extensive experiments on different benchmarks demonstrate that our proposed GliDe draft model significantly reduces the expected decoding latency. Additional evaluation using walltime reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend the improvement to 2.61x with CaPE. We will release our code, data, and the trained draft models.",
  "abstract_zh": "摘要：推测解码是一种相对较新的解码框架，利用小型高效的草稿模型来减少大型语言模型（LLMs）的延迟。在本研究中，我们介绍了GliDe和CaPE，这两种对传统推测解码的低麻烦修改，进一步提高了冻结LLM的解码速度。具体而言，GliDe是一种修改后的草稿模型架构，重用目标LLM的缓存键和值，而CaPE是一种提案扩展方法，利用草稿模型的置信度分数来帮助选择额外的候选标记进行验证。在不同基准上的大量实验表明，我们提出的GliDe草稿模型显著降低了预期的解码延迟。使用墙时的额外评估显示，GliDe可以将Vicuna模型的加速提高到2.17倍，并通过CaPE进一步扩展到2.61倍。我们将发布我们的代码、数据和训练好的草稿模型。"
}
{
  "title": "LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models",
  "title_zh": "LoRAP：变换器子层值得为大型语言模型进行差异化结构压缩",
  "abstract": "Large language models (LLMs) show excellent performance in difficult tasks, but they often require massive memories and computational resources. How to reduce the parameter scale of LLMs has become research hotspots. In this study, we get an important observation that the multi-head self-attention (MHA) sub-layer of Transformer exhibits noticeable low-rank structure, while the feed-forward network (FFN) sub-layer does not. With this regard, we design a novel structured compression method LoRAP, which organically combines **Lo**w-**R**ank matrix approximation **A**nd structured **P**runing. For the MHA sub-layer, we proposal an input activation weighted singular value decomposition method and allocate different parameter amounts for each weight matrix based on the differences in low-rank properties of matrices.For the FFN sub-layer, we propose a gradient-free structured channel pruning method and save the least important 1% of parameters which actually play a vital role in model performance. Extensive evaluations on zero-shot perplexity and zero-shot task classification indicate that our proposal is superior to previous structured compression rivals under multiple compression ratios. Our code will be released soon.",
  "abstract_zh": "大型语言模型（LLMs）在困难任务中表现出色，但通常需要大量内存和计算资源。如何减少LLMs的参数规模已成为研究热点。在本研究中，我们观察到变换器的多头自注意力（MHA）子层表现出明显的低秩结构，而前馈网络（FFN）子层则没有。在此基础上，我们设计了一种新颖的结构压缩方法LoRAP，结合了低秩矩阵近似和结构化剪枝。针对MHA子层，我们提出了一种输入激活加权的奇异值分解方法，并根据矩阵低秩特性的差异为每个权重矩阵分配不同的参数量。针对FFN子层，我们提出了一种无梯度的结构化通道剪枝方法，保留了对模型性能至关重要的1%最不重要的参数。在零-shot困惑度和零-shot任务分类的广泛评估中，我们的提案在多个压缩比下优于之前的结构压缩对手。我们的代码将很快发布。"
}
{
  "title": "Privacy-Preserving Instructions for Aligning Large Language Models",
  "title_zh": "隐私保护的指令用于对齐大型语言模型",
  "abstract": "Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions. In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna.",
  "abstract_zh": "大型语言模型（LLM）应用的服务提供商在实际使用中收集用户指令，并利用这些指令进一步使LLM与用户意图对齐。这些指令可能包含敏感信息，并在此过程中由人工工作者进行注释。这带来了典型私有优化未能解决的新隐私风险。为此，我们提出使用合成指令替代数据注释和模型微调中的真实指令。通过使用私有微调生成器生成这些合成指令，确保了正式的差分隐私。实现所需效用的关键是我们新颖的过滤算法，该算法将合成指令的分布与真实指令的分布相匹配。在监督微调和基于人类反馈的强化学习中，我们的广泛实验通过显示与真实指令相当的结果，证明了最终合成指令集的高效用。在监督微调中，使用私有合成指令训练的模型优于领先的开源模型，如Vicuna。"
}
{
  "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
  "title_zh": "WebLINX：基于多轮对话的真实网站导航",
  "abstract": "We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings.",
  "abstract_zh": "我们提出了对话式网页导航的问题，其中数字代理控制网页浏览器，并按照用户指示以多轮对话的方式解决现实世界任务。为支持这一问题，我们引入了WEBLINX——一个涵盖2300个专家演示的10万次交互的大规模基准。我们的基准覆盖了150多个真实网站的广泛模式，可用于在多种场景中训练和评估代理。由于信息量巨大，大型语言模型（LLMs）无法实时处理整个网页。为了解决这一瓶颈，我们设计了一种受检索启发的模型，通过对相关元素进行排序来高效修剪HTML页面。我们使用选定的元素、截图和操作历史来评估各种模型在网页导航时复制人类行为的能力。我们的实验涵盖了从小型文本模型到专有多模态LLMs。我们发现，较小的微调解码器超越了最佳的零样本LLMs（包括GPT-4V），但也超越了那些明确在截图上进行预训练的较大微调多模态模型。然而，所有微调模型在对未见网站的泛化方面都存在困难。我们的发现突显了需要大型多模态模型以适应新环境的必要性。"
}
{
  "title": "A Multimodal Automated Interpretability Agent",
  "title_zh": "多模态自动可解释性代理",
  "abstract": "This paper describes MAIA, a Multimodal Automated Interpretability Agent. MAIA is a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery. It equips a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior. These include tools commonly used by human interpretability researchers: for synthesizing and editing inputs, computing maximally activating exemplars from real-world datasets, and summarizing and describing experimental results. Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior. We evaluate applications of MAIA to computer vision models. We first characterize MAIA’s ability to describe (neuron-level) features in learned representations of images. Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters. We then show that MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified.",
  "abstract_zh": "本文描述了MAIA，一个多模态自动可解释性代理。MAIA是一个利用神经模型自动化神经模型理解任务的系统，如特征解释和故障模式发现。它为一个预训练的视觉-语言模型配备了一套工具，支持对其他模型的子组件进行迭代实验，以解释其行为。这些工具包括人类可解释性研究者常用的工具：用于合成和编辑输入、从真实世界数据集中计算最大激活示例，以及总结和描述实验结果。MAIA提出的可解释性实验组合了这些工具，以描述和解释系统行为。我们评估了MAIA在计算机视觉模型中的应用。我们首先表征了MAIA在学习到的图像表示中描述（神经元级）特征的能力。在几个训练模型和一个具有配对真实描述的新型合成视觉神经元数据集上，MAIA生成的描述与专家人类实验者生成的描述相当。然后我们展示了MAIA可以帮助完成另外两个可解释性任务：减少对虚假特征的敏感性，以及自动识别可能被错误分类的输入。"
}
{
  "title": "Degeneration-free Policy Optimization: RL Fine-Tuning for Language Models without Degeneration",
  "title_zh": "无退化策略优化：无退化的语言模型强化学习微调",
  "abstract": "As the pre-training objectives (e.g., next token prediction) of language models (LMs) are inherently not aligned with task scores, optimizing LMs to achieve higher downstream task scores is essential. One of the promising approaches is to fine-tune LMs through reinforcement learning (RL). However, conventional RL methods based on PPO and a penalty of KL divergence are vulnerable to text degeneration where LMs do not generate natural texts anymore after RL fine-tuning. To address this problem, we provide Degeneration-free Policy Optimization (DfPO) that can fine-tune LMs to generate texts that achieve improved downstream task scores, while preserving the ability to generate natural texts. To achieve this, we introduce KL-masking which masks out the actions that potentially cause deviation from the reference policy when its likelihood is increased or decreased. Then, we devise truncated advantage functions for separately performing likelihood maximization and minimization to improve the task performance. In the experiments, we provide the results of DfPO and baseline algorithms on various generative NLP tasks including text continuation, text detoxification, and commonsense generation. Our experiments demonstrate that DfPO successfully improves the downstream task scores while preserving the ability to generate natural texts, without requiring additional hyperparameter search.",
  "abstract_zh": "由于语言模型的预训练目标（例如，下一个标记预测）与任务评分本质上不一致，因此优化语言模型以实现更高的下游任务评分至关重要。通过强化学习（RL）微调语言模型是一种有前景的方法。然而，基于PPO和KL散度惩罚的传统RL方法容易导致文本退化，即在RL微调后，语言模型不再生成自然文本。为了解决这个问题，我们提出了无退化策略优化（DfPO），它可以微调语言模型生成文本，从而提高下游任务评分，同时保持生成自然文本的能力。为此，我们引入了KL掩蔽，掩蔽那些在其可能性增加或减少时可能导致偏离参考策略的动作。然后，我们设计了截断优势函数，以分别执行可能性最大化和最小化，从而提高任务性能。在实验中，我们提供了DfPO和基线算法在各种生成NLP任务上的结果，包括文本续写、文本去毒化和常识生成。我们的实验表明，DfPO成功提高了下游任务评分，同时保持了生成自然文本的能力，而无需额外的超参数搜索。"
}
{
  "title": "DistiLLM: Towards Streamlined Distillation for Large Language Models",
  "title_zh": "DistiLLM：面向大型语言模型的简化蒸馏",
  "abstract": "Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing student models while achieving up to 4.3$\\times$ speedup compared to recent KD methods.",
  "abstract_zh": "知识蒸馏（KD）广泛用于将教师模型压缩为更小的学生模型，从而降低推理成本和内存占用，同时保持模型能力。然而，目前针对自回归序列模型（例如，大型语言模型）的KD方法缺乏标准化的目标函数。此外，最近使用学生生成的输出以解决训练与推理不匹配的问题显著增加了计算成本。为了解决这些问题，我们提出了DistiLLM，这是一种更有效且高效的自回归语言模型KD框架。DistiLLM包含两个组件：（1）一种新颖的偏斜Kullback-Leibler散度损失，我们揭示并利用其理论特性；（2）一种自适应的离策略方法，旨在提高利用学生生成输出的效率。大量实验，包括遵循指令的任务，证明了DistiLLM在构建高性能学生模型方面的有效性，同时与最近的KD方法相比，实现了高达4.3倍的加速。"
}
{
  "title": "Generalization to New Sequential Decision Making Tasks with In-Context Learning",
  "title_zh": "新序列决策任务的上下文学习泛化",
  "abstract": "Training autonomous agents that can learn new tasks from only a handful of demonstrations is a long-standing problem in machine learning. Recently, transformers have been shown to learn new language or vision tasks without any weight updates from only a few examples, also referred to as in-context learning. However, the sequential decision making setting poses additional challenges having a lower tolerance for errors since the environment's stochasticity or the agent's actions can lead to unseen, and sometimes unrecoverable, states. In this paper, we use an illustrative example to show that naively applying transformers to sequential decision making problems does not enable in-context learning of new tasks. We then demonstrate how training on sequences of trajectories with certain distributional properties leads to in-context learning of new sequential decision making tasks. We investigate different design choices and find that larger model and dataset sizes, as well as more task diversity, environment stochasticity, and trajectory burstiness, all result in better in-context learning of new out-of-distribution tasks. By training on large diverse offline datasets, our model is able to learn new MiniHack and Procgen tasks without any weight updates from just a handful of demonstrations.",
  "abstract_zh": "训练能够仅通过少量示例学习新任务的自主智能体一直是机器学习中的一个长期问题。最近，变换器被证明能够在没有任何权重更新的情况下，仅通过几个示例学习新的语言或视觉任务，这也被称为上下文学习。然而，序列决策设置带来了额外的挑战，因为环境的随机性或智能体的行动可能导致未见过的、并且有时无法恢复的状态，从而对错误的容忍度较低。本文通过一个示例说明，简单地将变换器应用于序列决策问题并不能实现新任务的上下文学习。我们接着展示了在具有特定分布特性的轨迹序列上训练如何导致新序列决策任务的上下文学习。我们研究了不同的设计选择，发现更大的模型和数据集规模，以及更多的任务多样性、环境随机性和轨迹突发性，均能改善新分布外任务的上下文学习。通过在大型多样化的离线数据集上训练，我们的模型能够在没有任何权重更新的情况下，仅通过少量示例学习新的MiniHack和Procgen任务。"
}
{
  "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation",
  "title_zh": "大型语言模型的自我对齐：基于垄断对话的社会场景模拟",
  "abstract": "Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms existing methods under mild assumptions. Finally, extensive experiments validate that our method outperforms over 10 baselines across 4 benchmarks. As evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning with human values. See our project page at https://shuotang123.github.io/MATRIX.",
  "abstract_zh": "对大型语言模型（LLMs）与人类价值观的对齐至关重要，以减轻其误用可能带来的不良影响。本文借鉴社会学的见解，认为关注所有参与方的关切是塑造人类价值观的关键因素，提出了一种通过社会场景模拟使LLMs自我对齐的新方向。为此，我们提出了MATRIX，一种新颖的社会场景模拟器，它模拟围绕用户输入查询的真实场景，使LLM在回应之前能够考虑社会后果。MATRIX作为一个虚拟排练空间，类似于垄断对话，LLM在其中扮演与查询相关的多种角色并进行自我练习。为了实现这种对齐，我们使用MATRIX模拟的数据对LLM进行微调，确保遵循人类价值观而不影响推理速度。我们理论上证明，使用MATRIX的LLM在温和假设下优于现有方法。最后，大量实验验证了我们的方法在4个基准上超过了10个基线。根据875个用户评分，我们调优后的13B规模LLM在与人类价值观的对齐方面超过了GPT-4。请访问我们的项目页面 https://shuotang123.github.io/MATRIX。"
}
{
  "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
  "title_zh": "旅行规划者：一个针对语言智能体的现实世界规划基准",
  "abstract": "Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks—even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.",
  "abstract_zh": "规划自人工智能诞生以来一直是其核心追求之一，但早期的人工智能智能体主要集中在受限环境中，因为缺乏人类级别规划所需的许多认知基础。最近，由大型语言模型（LLMs）驱动的语言智能体展现出有趣的能力，如工具使用和推理。这些语言智能体是否能够在更复杂的环境中进行规划，而这些环境超出了先前人工智能智能体的能力范围？为了推进这一研究，我们提出了旅行规划者，这是一个新的规划基准，专注于旅行规划这一常见的现实世界规划场景。它提供了一个丰富的沙盒环境，各种工具用于访问近四百万条数据记录，以及1,225个精心策划的规划意图和参考计划。全面评估表明，目前的语言智能体尚无法处理如此复杂的规划任务——即使是GPT-4的成功率也仅为0.6%。语言智能体在保持任务专注、使用正确工具收集信息或跟踪多个约束方面存在困难。然而，我们注意到，语言智能体能够应对如此复杂问题的可能性本身就是非凡的进展。旅行规划者为未来的语言智能体提供了一个具有挑战性但意义重大的测试平台。"
}
{
  "title": "BAT: Learning to Reason about Spatial Sounds with Large Language Models",
  "title_zh": "BAT：利用大型语言模型学习空间声音推理",
  "abstract": "Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B model, BAT transcends standard Sound Event Localization and Detection (SELD) tasks, enabling the model to reason about the relationships between the sounds in its environment. Our experiments demonstrate BAT's superior performance on both spatial sound perception and reasoning, showcasing the immense potential of LLMs in navigating and interpreting complex spatial audio environments.",
  "abstract_zh": "空间声音推理是人类的一项基本技能，使我们能够根据声音导航和解释周围环境。本文提出了BAT，它结合了双耳声学场景分析模型的空间声音感知能力与大型语言模型（LLM）的自然语言推理能力，以复制这种天生的能力。为了解决现有野外空间声音数据集的不足，我们使用AudioSet和SoundSpaces 2.0合成了一个双耳音频数据集。接下来，我们开发了SpatialSoundQA，这是一个基于空间声音的问题回答数据集，提供了一系列QA任务，以训练BAT在空间声音感知和推理的各个方面。BAT的声学前端编码器是一个名为空间音频谱图变换器（Spatial Audio Spectrogram Transformer，或Spatial-AST）的新型空间音频编码器，单独在声音事件检测、空间定位和距离估计方面表现出色。通过将Spatial-AST与LLaMA-2 7B模型集成，BAT超越了标准的声音事件定位和检测（SELD）任务，使模型能够推理其环境中声音之间的关系。我们的实验表明，BAT在空间声音感知和推理方面表现优越，展示了LLM在导航和解释复杂空间音频环境中的巨大潜力。"
}
{
  "title": "R2E: Turning any Github Repository into a Programming Agent Environment",
  "title_zh": "R2E：将任何GitHub仓库转变为编程代理环境",
  "abstract": "While Large Language Models’ (LLMs) coding capabilities have advanced rapidly, corresponding evaluation benchmarks on real-world programming setups are yet to catch up. Building a scalable and interactive testbed for evaluating general-purpose AI coding agents for real-world code has been challenging, particularly due to a lack of high-quality test suites available. In this paper, we present Repository to Environment (R2E), a framework that can turn any GitHub repository into a test environment to evaluate the performance of code-generating systems, both static and interactive. R2E is powered by a synergistic combination of program analysis and LLMs to construct equivalence test harnesses for any GitHub function. We instantiate our framework to build the first large-scale benchmark, R2E-Eval1, for building realistic environments for AI coding assistants. Our results demonstrate that even when SOTA models cannot generate correct solutions with advanced prompting techniques, they can effectively use environment feedback highlighting the need to move from static functional coding to interactive programming paradigm. We hope that our framework (and the instantiated benchmark) can motivate research directions by providing web-scale open-ended coding environments. R2E code is available at https://r2e.dev/",
  "abstract_zh": "尽管大型语言模型（LLMs）的编码能力迅速提升，但针对现实编程环境的评估基准尚未跟上。构建一个可扩展且互动的测试平台以评估通用AI编码代理在现实代码中的表现一直是一个挑战，特别是由于缺乏高质量的测试套件。在本文中，我们提出了Repository to Environment（R2E）框架，该框架可以将任何GitHub仓库转变为测试环境，以评估代码生成系统的性能，包括静态和互动的。R2E通过程序分析和LLMs的协同组合，为任何GitHub函数构建等效测试工具。我们实例化了我们的框架，构建了第一个大规模基准R2E-Eval1，以创建AI编码助手的现实环境。我们的结果表明，即使在最先进的模型无法通过高级提示技术生成正确解决方案时，它们仍然可以有效利用环境反馈，强调了从静态功能编码转向互动编程范式的必要性。我们希望我们的框架（以及实例化的基准）能够通过提供网络规模的开放式编码环境来激励研究方向。R2E代码可在https://r2e.dev/获取。"
}
{
  "title": "Fewer Truncations Improve Language Modeling",
  "title_zh": "更少的截断改善语言建模",
  "abstract": "In large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens. Despite its efficiency, the concatenation approach compromises data integrity—it inevitably breaks many documents into incomplete pieces, leading to excessive truncations that hinder the model from learning to compose logically coherent and factually consistent content that is grounded on the complete context. To address the issue, we propose Best-fit Packing, a scalable and efficient method that packs documents into training sequences through length-aware combinatorial optimization. Our method completely eliminates unnecessary truncations while retaining the same training efficiency as concatenation. Empirical results from both text and code pre-training show that our method achieves superior performance (e.g., +4.7% on reading comprehension; +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain hallucination effectively by up to 58.3%.",
  "abstract_zh": "在大型语言模型训练中，输入文档通常被串联在一起，然后分割成相同长度的序列以避免填充标记。尽管这种串联方法效率较高，但却损害了数据完整性——它不可避免地将许多文档分割成不完整的片段，导致过度截断，妨碍模型学习基于完整上下文的逻辑连贯和事实一致的内容。为了解决这个问题，我们提出了最佳适应打包，这是一种可扩展且高效的方法，通过基于长度的组合优化将文档打包成训练序列。我们的方法完全消除了不必要的截断，同时保持与串联相同的训练效率。来自文本和代码预训练的实证结果表明，我们的方法在性能上优于其他方法（例如，阅读理解提高4.7%；上下文跟随提高16.8%；程序合成提高9.2%），并有效减少了封闭域幻觉，最多可降低58.3%。"
}
{
  "title": "Prompt-guided Precise Audio Editing with Diffusion Models",
  "title_zh": "基于提示的精确音频编辑与扩散模型",
  "abstract": "Audio editing involves the arbitrary manipulation of audio content through precise control. Although text-guided diffusion models have made significant advancements in text-to-audio generation, they still face challenges in finding a flexible and precise way to modify target events within an audio track. We present a novel approach, referred to as **PPAE**, which serves as a general module for diffusion models and enables precise audio editing. The editing is based on the input textual prompt only and is entirely training-free. We exploit the cross-attention maps of diffusion models to facilitate accurate local editing and employ a hierarchical local-global pipeline to ensure a smoother editing process. Experimental results highlight the effectiveness of our method in various editing tasks.",
  "abstract_zh": "音频编辑涉及通过精确控制对音频内容进行任意操作。尽管文本引导的扩散模型在文本到音频生成方面取得了显著进展，但在寻找灵活且精确的方式来修改音频轨道中的目标事件方面仍面临挑战。我们提出了一种新方法，称为**PPAE**，它作为扩散模型的通用模块，能够实现精确的音频编辑。编辑仅基于输入的文本提示，完全不需要训练。我们利用扩散模型的交叉注意力图来促进准确的局部编辑，并采用分层的局部-全局管道以确保更顺畅的编辑过程。实验结果突显了我们方法在各种编辑任务中的有效性。"
}
{
  "title": "MusicFlow: Cascaded Flow Matching for Text Guided Music Generation",
  "title_zh": "音乐流：基于级联流匹配的文本引导音乐生成",
  "abstract": "We introduce MusicFlow, a cascaded text-to-music generation model based on flow matching. Based on self-supervised representations to bridge between text descriptions and music audios, we construct two flow matching networks to model the conditional distribution of semantic and acoustic features. Additionally, we leverage masked prediction as the training objective, enabling the model to generalize to other tasks such as music infilling and continuation in a zero-shot manner. Experiments on MusicCaps reveal that the music generated by MusicFlow exhibits superior quality and text coherence despite being over $2\\sim5$ times smaller and requiring $5$ times fewer iterative steps. Simultaneously, the model can perform other music generation tasks and achieves competitive performance in music infilling and continuation.",
  "abstract_zh": "我们介绍了MusicFlow，这是一种基于流匹配的级联文本到音乐生成模型。该模型基于自监督表示，构建了两个流匹配网络，以建模语义和声学特征的条件分布。此外，我们利用掩码预测作为训练目标，使模型能够以零样本方式推广到其他任务，如音乐填充和续写。MusicCaps上的实验表明，MusicFlow生成的音乐质量和文本一致性优越，尽管其规模比传统方法小$2\\sim5$倍，并且需要的迭代步骤少$5$倍。同时，该模型能够执行其他音乐生成任务，并在音乐填充和续写中取得了竞争力的表现。"
}
{
  "title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
  "title_zh": "标题：语言模型在解决问题时是否表现出与人类学习者相同的认知偏见？",
  "abstract": "There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which properties of human cognition are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand whether current LLMs display the same cognitive biases as children in these steps. We generate a novel set of word problems for each of these tests, using a neuro-symbolic approach that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but not in the final step, in which the arithmetic expressions are executed to obtain the answer.",
  "abstract_zh": "摘要：对大型语言模型（LLMs）作为认知模型的应用兴趣日益增加。为了此目的，理解LLMs能够很好建模的人类认知特性以及哪些特性不能被建模至关重要。在本研究中，我们研究了LLMs在解决算术文字问题时与儿童已知偏见的关系。通过调查学习科学文献，我们认为问题解决过程可以分为三个不同的步骤：文本理解、解决方案规划和解决方案执行。我们为每个步骤构建测试，以了解当前的LLMs是否在这些步骤中表现出与儿童相同的认知偏见。我们为这些测试生成了一组新颖的文字问题，采用神经符号方法，使我们能够对问题特征进行细致控制。我们发现，无论是否经过指令调优，LLMs在解决过程的文本理解和解决方案规划步骤中表现出类似人类的偏见，但在最终步骤中，即执行算术表达式以获得答案时则没有表现出这种偏见。"
}
{
  "title": "Position: Foundation Agents as the Paradigm Shift for Decision Making",
  "title_zh": "基础代理作为决策制定的范式转变",
  "abstract": "Decision making demands intricate interplay between perception, memory, and reasoning to discern optimal policies. Conventional approaches to decision making face challenges related to low sample efficiency and poor generalization. In contrast, foundation models in language and vision have showcased rapid adaptation to diverse new tasks. Therefore, we advocate for the construction of foundation agents as a transformative shift in the learning paradigm of agents. This proposal is underpinned by the formulation of foundation agents with their fundamental characteristics and challenges motivated by the success of large language models (LLMs). Moreover, we specify the roadmap of foundation agents from large interactive data collection or generation, to self-supervised pretraining and adaptation, and knowledge and value alignment with LLMs. Lastly, we pinpoint critical research questions derived from the formulation and delineate trends for foundation agents supported by real-world use cases, addressing both technical and theoretical aspects to propel the field towards a more comprehensive and impactful future.",
  "abstract_zh": "决策制定需要感知、记忆和推理之间的复杂互动，以辨别最佳策略。传统的决策方法面临低样本效率和差的泛化能力等挑战。相比之下，语言和视觉中的基础模型展示了对多样新任务的快速适应。因此，我们主张构建基础代理作为代理学习范式的变革性转变。该提议基于基础代理的基本特征和挑战，这些特征和挑战受到大型语言模型（LLMs）成功的启发。此外，我们明确了基础代理的路线图，从大型交互数据的收集或生成，到自监督预训练和适应，以及与LLMs的知识和价值对齐。最后，我们指出了从基础代理的构建中衍生出的关键研究问题，并描绘了支持真实世界应用案例的基础代理趋势，旨在推动该领域朝着更全面和更具影响力的未来发展，涵盖技术和理论方面。"
}
{
  "title": "A decoder-only foundation model for time-series forecasting",
  "title_zh": "仅解码器基础模型用于时间序列预测",
  "abstract": "Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a decoder style attention model with input patching, using a large time-series corpus comprising both real-world and synthetic datasets. Experiments on a diverse set of previously unseen forecasting datasets suggests that the model can yield accurate zero-shot forecasts across different domains, forecasting horizons and temporal granularities.",
  "abstract_zh": "受近期大型语言模型在自然语言处理（NLP）领域的进展启发，我们设计了一种用于预测的时间序列基础模型，其即插即用的零样本性能在多种公共数据集上接近于各个单独数据集的最先进监督预测模型的准确性。我们的模型基于对解码器风格注意力模型进行预训练，采用输入补丁，使用包含真实世界和合成数据集的大型时间序列语料库。在一组多样化的先前未见预测数据集上的实验表明，该模型能够在不同领域、预测范围和时间粒度上产生准确的零样本预测。"
}
{
  "title": "In-Context Decision Transformer: Reinforcement Learning via Hierarchical Chain-of-Thought",
  "title_zh": "上下文决策变换器：通过分层思维链进行强化学习",
  "abstract": "In-context learning is a promising approach for offline reinforcement learning (RL) to handle online tasks, which can be achieved by providing task prompts. Recent works demonstrated that in-context RL could emerge with self-improvement in a trial-and-error manner when treating RL tasks as an across-episodic sequential prediction problem. Despite the self-improvement not requiring gradient updates, current works still suffer from high computational costs when the across-episodic sequence increases with task horizons. To this end, we propose an In-context Decision Transformer (IDT) to achieve self-improvement in a high-level trial-and-error manner. Specifically, IDT is inspired by the efficient hierarchical structure of human decision-making and thus reconstructs the sequence to consist of high-level decisions instead of low-level actions that interact with environments. As one high-level decision can guide multi-step low-level actions, IDT naturally avoids excessively long sequences and solves online tasks more efficiently. Experimental results show that IDT achieves state-of-the-art in long-horizon tasks over current in-context RL methods. In particular, the online evaluation time of our IDT is 36$\\times$ times faster than baselines in the D4RL benchmark and 27$\\times$ times faster in the Grid World benchmark.",
  "abstract_zh": "上下文学习是一种有前景的离线强化学习（RL）方法，可以通过提供任务提示来处理在线任务。最近的研究表明，当将RL任务视为跨情节的序列预测问题时，上下文RL可以以试错方式自我改进。尽管自我改进不需要梯度更新，但当前的研究在跨情节序列随着任务范围增加时仍面临高计算成本。为此，我们提出了上下文决策变换器（IDT），以高层次的试错方式实现自我改进。具体而言，IDT受到人类决策效率层次结构的启发，因此重构序列，使其由高层次决策而非与环境交互的低层次动作组成。由于一个高层次决策可以指导多步低层次动作，IDT自然避免了过长的序列，更有效地解决在线任务。实验结果表明，IDT在长时间范围任务上超越了当前的上下文RL方法，特别是在D4RL基准测试中，IDT的在线评估时间比基线快36倍，在Grid World基准测试中快27倍。"
}
{
  "title": "BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models",
  "title_zh": "BBox-Adapter：轻量级黑箱大语言模型适配器",
  "abstract": "Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive experiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It improves model performance by up to 6.77% across diverse tasks and domains, while reducing training and inference costs by 31.30x and 1.84x, respectively.",
  "abstract_zh": "适应最先进的大语言模型（LLMs），如GPT-4和Gemini，以满足特定任务的需求是具有挑战性的。由于其参数、嵌入甚至输出概率的不透明性，现有的微调适配方法无法适用。因此，适应这些黑箱LLMs仅能通过其API服务实现，这引发了对透明度、隐私和成本的担忧。为了解决这些挑战，我们提出了BBox-Adapter，这是一种新颖的轻量级黑箱LLM适配器。BBox-Adapter通过将目标数据视为正样本，将源数据视为负样本，区分目标和源领域数据。它采用基于排名的噪声对比估计（NCE）损失，促进目标领域数据的可能性，同时惩罚源领域数据。此外，它还具有在线适配机制，实时采样来自真实、人工或AI反馈的正数据，并结合来自先前适配的负数据。大量实验表明BBox-Adapter的有效性和成本效率。它在各种任务和领域中将模型性能提高了多达6.77%，同时将训练和推理成本分别降低了31.30倍和1.84倍。"
}
{
  "title": "Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models",
  "title_zh": "连接点滴：黑箱视觉-语言模型的协同微调",
  "abstract": "With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a **C**ollabo**ra**tive **F**ine-**T**uning (**CraFT**) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algorithm. Extensive experiments on few-shot classification over 15 datasets demonstrate the superiority of CraFT. The results show that CraFT achieves a decent gain of about 12% with 16-shot datasets and only 8,000 queries. Moreover, CraFT trains faster and uses only about 1/80 of the memory footprint for deployment, while sacrificing only 1.62% compared to the white-box method. Our code is publicly available at https://github.com/mrflogs/CraFT.",
  "abstract_zh": "随着预训练视觉-语言模型（VLM）的出现，已经投入大量精力对其进行微调以适应下游任务。尽管在设计高效微调方法方面取得了进展，但这些方法需要访问模型的参数，而模型所有者通常选择将其模型作为黑箱提供以保护模型所有权，这使得访问变得具有挑战性。本文提出了一种**C**ollabo**ra**tive **F**ine-**T**uning（**CraFT**）方法，用于对黑箱VLM进行下游任务的微调，仅需访问模型的输入提示和输出预测。CraFT包括两个模块，一个用于学习文本提示的提示生成模块和一个用于以残差风格增强输出预测的预测精炼模块。此外，我们引入了一种辅助预测一致性损失，以促进这些模块之间的一致优化。这些模块通过一种新颖的协同训练算法进行优化。在15个数据集上的少量样本分类实验中，CraFT表现出优越性。结果表明，CraFT在16-shot数据集上实现了约12%的显著提升，仅使用8,000个查询。此外，CraFT训练速度更快，部署时仅使用约1/80的内存占用，相比于白箱方法仅牺牲了1.62%。我们的代码已公开发布在https://github.com/mrflogs/CraFT。"
}
{
  "title": "SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment",
  "title_zh": "标题：SyCoCa：通过关注掩蔽对称化对比性描述生成器以实现多模态对齐",
  "abstract": "Multimodal alignment between language and vision is the fundamental topic in current vision-language model research. Contrastive Captioners (CoCa), as a representative method, integrates Contrastive Language-Image Pretraining (CLIP) and Image Caption (IC) into a unified framework, resulting in impressive results. CLIP imposes a bidirectional constraints on global representations of entire images and sentences. Although IC conducts an unidirectional image-to-text generation on local representation, it lacks any constraint on local text-to-image reconstruction, which limits the ability to understand images at a fine-grained level when aligned with texts. To achieve multimodal alignment from both global and local perspectives, this paper proposes Symmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional interactions on images and texts across the global and local representation levels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM) head based on ITC and IC heads. The improved SyCoCa further leverages textual cues to reconstruct contextual images and visual cues to predict textual contents. When implementing bidirectional local interactions, the local contents of images tend to be cluttered or unrelated to their textual descriptions. Thus, we employ an attentive masking strategy to select effective image patches for interaction. Extensive experiments on five vision-language tasks, including image-text retrieval, image-captioning, visual question answering, and zero-shot/finetuned image classification, validate the effectiveness of our proposed method.",
  "abstract_zh": "摘要：语言与视觉之间的多模态对齐是当前视觉语言模型研究的基本主题。对比性描述生成器（CoCa）作为一种代表性方法，将对比语言-图像预训练（CLIP）和图像描述（IC）整合到一个统一框架中，取得了令人印象深刻的结果。CLIP对整个图像和句子的全局表示施加了双向约束。尽管IC在局部表示上进行单向的图像到文本生成，但它对局部文本到图像重建缺乏任何约束，这限制了在与文本对齐时对图像进行细粒度理解的能力。为了从全局和局部两个角度实现多模态对齐，本文提出了对称化对比性描述生成器（SyCoCa），在全局和局部表示层次上引入图像和文本之间的双向交互。具体而言，我们基于ITC和IC头扩展了一个文本引导的掩蔽图像建模（TG-MIM）头。改进后的SyCoCa进一步利用文本线索重建上下文图像，并利用视觉线索预测文本内容。在实现双向局部交互时，图像的局部内容往往会变得杂乱或与其文本描述无关。因此，我们采用了一种关注掩蔽策略来选择有效的图像块进行交互。在五个视觉语言任务上的广泛实验，包括图像-文本检索、图像描述、视觉问答以及零样本/微调图像分类，验证了我们提出方法的有效性。"
}
{
  "title": "Position: On the Societal Impact of Open Foundation Models",
  "title_zh": "标题：立场：开放基础模型的社会影响",
  "abstract": "Foundation models are powerful technologies: how they are released publicly directly shapes their societal impact. In this position paper, we focus on *open* foundation models, defined here as those with broadly available model weights (e.g., Llama 3, Stable Diffusion XL). We identify five distinctive properties (e.g., greater customizability, poor monitoring) that mediate their benefits and risks. Open foundation models present significant benefits, with some caveats, that span innovation, competition, the distribution of decision-making power, and transparency. To understand their risks of misuse, we design a risk assessment framework for analyzing their *marginal risk*. Across several misuse vectors (e.g., cyberattacks, bioweapons), we find that current research is insufficient to effectively characterize the marginal risk of open foundation models relative to pre-existing technologies. The framework helps explain why the marginal risk is low in some cases, clarifies disagreements about misuse risks by revealing that past work has focused on different subsets of the framework with different assumptions, and articulates a way forward for more constructive debate. Overall, our work helps support a more grounded assessment of the societal impact of open foundation models by outlining what research is needed to empirically validate their theoretical benefits and risks.",
  "abstract_zh": "摘要：基础模型是强大的技术：它们的公开发布直接影响其社会影响。在这篇立场论文中，我们专注于*开放*基础模型，这里定义为那些具有广泛可用模型权重的模型（例如，Llama 3，Stable Diffusion XL）。我们识别出五个独特的属性（例如，更大的可定制性，较差的监控），这些属性调节其益处和风险。开放基础模型带来了显著的好处，但也有一些警告，这些好处涉及创新、竞争、决策权的分配和透明度。为了理解其误用风险，我们设计了一个风险评估框架来分析其*边际风险*。在多个误用向量（例如，网络攻击、生物武器）中，我们发现当前的研究不足以有效表征开放基础模型相对于现有技术的边际风险。该框架有助于解释为什么在某些情况下边际风险较低，通过揭示过去的工作集中于框架的不同子集并具有不同的假设，澄清了关于误用风险的分歧，并阐明了更具建设性的辩论的前进方向。总体而言，我们的工作通过概述需要什么研究来实证验证其理论益处和风险，帮助支持对开放基础模型社会影响的更为扎实的评估。"
}
{
  "title": "Accurate LoRA-Finetuning Quantization of LLMs via Information Retention",
  "title_zh": "准确的LoRA微调量化大型语言模型通过信息保留",
  "abstract": "The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4-bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the state-of-the-art methods. The significant performance gain requires only a tiny 0.31% additional time consumption, revealing the satisfactory efficiency of our IR-QLoRA. We highlight that IR-QLoRA enjoys excellent versatility, compatible with various frameworks (e.g., NormalFloat and Integer quantization) and brings general accuracy gains. The code is available at https://github.com/htqin/ir-qlora .",
  "abstract_zh": "LoRA微调量化大型语言模型（LLMs）已被广泛研究，以获得适合在资源受限硬件上部署的准确且紧凑的LLMs。然而，现有方法导致量化的LLM严重退化，甚至无法从LoRA的微调中受益。本文提出了一种新颖的IR-QLoRA，通过信息保留推动量化的LLM与LoRA实现高准确度。所提出的IR-QLoRA主要依赖于两种来自统一信息视角的技术：（1）基于统计的信息校准量化使LLM的量化参数能够准确保留原始信息；（2）基于微调的信息弹性连接使LoRA利用具有多样信息的弹性表示转换。综合实验表明，IR-QLoRA在2-4位宽下显著提高了LLaMA和LLaMA2系列的准确性，例如，4位LLaMA-7B在MMLU上相比于最先进的方法提高了1.4%。显著的性能提升仅需额外0.31%的时间消耗，揭示了我们IR-QLoRA的令人满意的效率。我们强调，IR-QLoRA具有出色的通用性，兼容各种框架（例如，NormalFloat和整数量化），并带来普遍的准确性提升。代码可在https://github.com/htqin/ir-qlora获取。"
}
{
  "title": "BetterV: Controlled Verilog Generation with Discriminative Guidance",
  "title_zh": "BetterV：带有判别指导的受控Verilog生成",
  "abstract": "Due to the growing complexity of modern Integrated Circuits (ICs), there is a need for automated circuit design methods. Recent years have seen increasing research in hardware design language generation to facilitate the design process. In this work, we propose a Verilog generation framework, BetterV, which fine-tunes large language models (LLMs) on processed domain-specific datasets and incorporates generative discriminators for guidance on particular design demands. Verilog modules are collected, filtered, and processed from the internet to form a clean and abundant dataset. Instruct-tuning methods are specially designed to fine-tune the LLMs to understand knowledge about Verilog. Furthermore, data are augmented to enrich the training set and are also used to train a generative discriminator on particular downstream tasks, providing guidance for the LLMs to optimize Verilog implementation. BetterV has the ability to generate syntactically and functionally correct Verilog, outperforming GPT-4 on the VerilogEval benchmark. With the help of task-specific generative discriminators, BetterV achieves remarkable improvements on various electronic design automation (EDA) downstream tasks, including netlist node reduction for synthesis and verification runtime reduction with Boolean Satisfiability (SAT) solving.",
  "abstract_zh": "由于现代集成电路（IC）日益复杂，自动化电路设计方法的需求不断增加。近年来，硬件设计语言生成的研究逐渐增多，以促进设计过程。在本研究中，我们提出了一个Verilog生成框架BetterV，该框架在处理过的特定领域数据集上微调大型语言模型（LLMs），并结合生成性判别器以指导特定设计需求。我们从互联网收集、过滤和处理Verilog模块，以形成一个干净且丰富的数据集。特别设计的指令微调方法用于微调LLMs，使其理解关于Verilog的知识。此外，数据增强用于丰富训练集，并用于在特定下游任务上训练生成性判别器，为LLMs优化Verilog实现提供指导。BetterV能够生成语法和功能上正确的Verilog，在VerilogEval基准测试中超越GPT-4。在任务特定的生成性判别器的帮助下，BetterV在各种电子设计自动化（EDA）下游任务上取得了显著改进，包括合成的网表节点减少和使用布尔可满足性（SAT）求解的验证运行时减少。"
}
{
  "title": "Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks",
  "title_zh": "标题：对语法感知代码中间填充任务的LLM评估",
  "abstract": "We introduce **S**yntax-**A**ware **F**ill-**i**n-the-**M**iddle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future research in effective pretraining strategies for code LLMs. The evaluation toolkit and dataset are available at https://github.com/gonglinyuan/safim, and the leaderboard is available at https://safimbenchmark.com.",
  "abstract_zh": "摘要：我们介绍了**S**yntax-**A**ware **F**ill-**i**n-the-**M**iddle（SAFIM），这是一个用于评估大型语言模型（LLM）在代码中间填充（FIM）任务上的新基准。该基准专注于程序结构（如代码块和条件表达式）的语法感知补全，包含来自多种编程语言的17,720个示例，这些示例来源于2022年4月之后的最新代码提交，以最小化数据污染。SAFIM提供了一个强大的框架，具有多种提示设计和新颖的语法感知后处理技术，促进了LLM之间的准确和公平比较。我们对15个LLM的全面评估表明，FIM预训练不仅提高了FIM的熟练度，还改善了使用LLM的从左到右（L2R）推理。我们的发现挑战了传统观念，并表明预训练方法和数据质量的影响大于模型大小。因此，SAFIM作为未来代码LLM有效预训练策略研究的基础平台。评估工具包和数据集可在https://github.com/gonglinyuan/safim获取，排行榜可在https://safimbenchmark.com查看。"
}
{
  "title": "Executable Code Actions Elicit Better LLM Agents",
  "title_zh": "可执行代码行动引发更好的大型语言模型代理",
  "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python **code** to consolidate LLM agents' **act**ions into a unified action space (**CodeAct**). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.",
  "abstract_zh": "大型语言模型（LLM）代理能够执行广泛的操作，如调用工具和控制机器人，在应对现实世界挑战方面展现出巨大潜力。LLM代理通常通过生成JSON或文本格式的提示来产生行动，这通常受到限制的行动空间（例如，预定义工具的范围）和有限的灵活性（例如，无法组合多个工具）的限制。本研究提议使用可执行的Python代码将LLM代理的行动整合到一个统一的行动空间（CodeAct）中。CodeAct与Python解释器集成，可以执行代码行动，并通过多轮交互动态修订先前的行动或根据新观察发出新行动。我们对17个LLM在API-Bank和新创建的基准上的广泛分析表明，CodeAct的表现优于广泛使用的替代方案（成功率提高了多达20%）。CodeAct的鼓舞人心的表现激励我们构建一个开源的LLM代理，通过执行可解释的代码与环境互动，并使用自然语言与用户协作。为此，我们收集了一个指令调优数据集CodeActInstruct，包含7k个使用CodeAct的多轮交互。我们展示了它可以与现有数据结合使用，以提高模型在代理导向任务中的表现，而不损害其通用能力。CodeActAgent基于Llama2和Mistral进行微调，集成了Python解释器，并独特地定制以使用现有库执行复杂任务（例如，模型训练）并自主自我调试。"
}
{
  "title": "Position: Fundamental Limitations of LLM Censorship Necessitate New Approaches",
  "title_zh": "标题：位置：大型语言模型审查的基本局限性需要新的方法",
  "abstract": "Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship methods have proven to be fallible at ensuring that LLMs do not return semantically impermissible responses. We present fundamental limitations of verifying the semantic properties of LLM outputs and identifying compositional threats, illustrating inherent challenges of current approaches to censoring LLM outputs. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, and semantic properties of LLM outputs can become impossible to verify when the LLM is capable of providing \"encrypted\" outputs. We further show challenges of censorship can extend beyond just semantic censorship, as attackers can reconstruct impermissible outputs from a collection of permissible ones. Consequently, we call for a re-evaluation of the problem of censorship and its goals, stressing the need for new definitions and approaches to censorship. In addition, we provide an initial attempt toward achieving this goal through syntactic censorship, drawing from a security perspective to design censorship methods that can provide guarantees.",
  "abstract_zh": "摘要：大型语言模型（LLMs）在理解复杂指令方面表现出令人印象深刻的能力。然而，它们对提供指令的盲目遵循引发了对恶意使用风险的担忧。现有的防御机制，如模型微调或输出审查方法，已被证明在确保LLMs不返回语义上不可接受的响应方面存在缺陷。我们展示了验证LLM输出的语义属性和识别组合威胁的基本局限性，说明了当前审查LLM输出方法的固有挑战。具体而言，我们证明语义审查可以被视为一个不可判定的问题，并且当LLM能够提供“加密”输出时，LLM输出的语义属性可能变得无法验证。我们进一步表明，审查的挑战不仅限于语义审查，因为攻击者可以从一组允许的输出中重构不可接受的输出。因此，我们呼吁重新评估审查问题及其目标，强调需要新的审查定义和方法。此外，我们通过句法审查提供了实现这一目标的初步尝试，从安全角度出发设计能够提供保证的审查方法。"
}
{
  "title": "AI Alignment with Changing and Influenceable Reward Functions",
  "title_zh": "标题：与变化和可影响奖励函数的人工智能对齐",
  "abstract": "Existing AI alignment approaches assume that preferences are static, which is unrealistic: our preferences change, and may even be influenced by our interactions with AI systems themselves. To clarify the consequences of incorrectly assuming static preferences, we introduce Dynamic Reward Markov Decision Processes (DR-MDPs), which explicitly model preference changes and the AI's influence on them. We show that despite its convenience, the static-preference assumption may undermine the soundness of existing alignment techniques, leading them to implicitly reward AI systems for influencing user preferences in ways users may not truly want. We then explore potential solutions. First, we offer a unifying perspective on how an agent's optimization horizon may partially help reduce undesirable AI influence. Then, we formalize different notions of AI alignment that account for preference change from the outset. Comparing the strengths and limitations of 8 such notions of alignment, we find that they all either err towards causing undesirable AI influence, or are overly risk-averse, suggesting that a straightforward solution to the problems of changing preferences may not exist. As there is no avoiding grappling with changing preferences in real-world settings, this makes it all the more important to handle these issues with care, balancing risks and capabilities. We hope our work can provide conceptual clarity and constitute a first step towards AI alignment practices which explicitly account for (and contend with) the changing and influenceable nature of human preferences.",
  "abstract_zh": "摘要：现有的人工智能对齐方法假设偏好是静态的，这并不现实：我们的偏好会变化，甚至可能受到我们与人工智能系统互动的影响。为了澄清错误假设静态偏好的后果，我们引入了动态奖励马尔可夫决策过程（DR-MDPs），该过程明确建模偏好变化及人工智能对其的影响。我们展示了尽管静态偏好假设方便，但它可能削弱现有对齐技术的有效性，导致这些技术在用户可能并不真正希望的方式上隐性地奖励人工智能系统影响用户偏好。接着，我们探讨潜在的解决方案。首先，我们提供了一个统一的视角，说明代理的优化视野如何部分帮助减少不良的人工智能影响。然后，我们正式化了不同的人工智能对齐概念，从一开始就考虑偏好变化。比较这8种对齐概念的优缺点，我们发现它们都或多或少导致不良的人工智能影响，或过于规避风险，这表明解决变化偏好问题的简单方案可能不存在。由于在现实世界中无法避免处理变化的偏好，这使得小心处理这些问题、平衡风险和能力变得尤为重要。我们希望我们的工作能够提供概念上的清晰性，并成为朝着明确考虑（并应对）人类偏好的变化和可影响性特征的人工智能对齐实践迈出的第一步。"
}
{
  "title": "Position: What Can Large Language Models Tell Us about Time Series Analysis",
  "title_zh": "标题：位置：大型语言模型能告诉我们关于时间序列分析的什么",
  "abstract": "Time series analysis is essential for comprehending the complexities inherent in various real-world systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including time series modality switching and question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research.",
  "abstract_zh": "摘要：时间序列分析对于理解各种现实世界系统和应用中的复杂性至关重要。尽管大型语言模型（LLMs）最近取得了重大进展，但具备时间序列分析能力的人工通用智能（AGI）的发展仍处于初期阶段。大多数现有的时间序列模型严重依赖领域知识和广泛的模型调优，主要集中在预测任务上。本文认为，当前的LLMs有潜力彻底改变时间序列分析，从而促进高效决策并朝着更普遍的时间序列分析智能形式发展。这种进步可以解锁广泛的可能性，包括时间序列模态切换和问答。我们鼓励研究人员和从业者认识到LLMs在推动时间序列分析方面的潜力，并强调对这些相关努力的信任需求。此外，我们详细介绍了时间序列分析与现有LLM技术的无缝集成，并概述了未来研究的有希望的方向。"
}
{
  "title": "Unified Generation, Reconstruction, and Representation: Generalized Diffusion with Adaptive Latent Encoding-Decoding",
  "title_zh": "统一生成、重构和表示：具有自适应潜在编码-解码的广义扩散",
  "abstract": "The vast applications of deep generative models are anchored in three core capabilities---*generating* new instances, *reconstructing* inputs, and learning compact *representations*---across various data types, such as discrete text/protein sequences and continuous images. Existing model families, like variational autoencoders (VAEs), generative adversarial networks (GANs), autoregressive models, and (latent) diffusion models, generally excel in specific capabilities and data types but fall short in others. We introduce *Generalized* ***E****ncoding*-***D****ecoding ****D****iffusion ****P****robabilistic ****M****odels* (EDDPMs) which integrate the core capabilities for broad applicability and enhanced performance. EDDPMs generalize the Gaussian noising-denoising in standard diffusion by introducing parameterized encoding-decoding. Crucially, EDDPMs are compatible with the well-established diffusion model objective and training recipes, allowing effective learning of the encoder-decoder parameters *jointly* with diffusion. By choosing appropriate encoder/decoder (e.g., large language models), EDDPMs naturally apply to different data types. Extensive experiments on text, proteins, and images demonstrate the flexibility to handle diverse data and tasks and the strong improvement over various existing models. Code is available at https://github.com/guangyliu/EDDPM .",
  "abstract_zh": "深度生成模型的广泛应用基于三项核心能力——*生成*新实例、*重构*输入和学习紧凑的*表示*——适用于各种数据类型，如离散文本/蛋白质序列和连续图像。现有模型家族，如变分自编码器（VAE）、生成对抗网络（GAN）、自回归模型和（潜在）扩散模型，通常在特定能力和数据类型上表现出色，但在其他方面则有所不足。我们引入了*广义* ***编码*-***解码****扩散****概率****模型*（EDDPM），它集成了核心能力以实现广泛的适用性和增强的性能。EDDPM通过引入参数化编码-解码，推广了标准扩散中的高斯噪声-去噪。关键是，EDDPM与成熟的扩散模型目标和训练方案兼容，允许与扩散*联合*有效学习编码器-解码器参数。通过选择合适的编码器/解码器（例如，大型语言模型），EDDPM自然适用于不同的数据类型。在文本、蛋白质和图像上的大量实验展示了处理多样数据和任务的灵活性，以及相较于各种现有模型的显著改进。代码可在 https://github.com/guangyliu/EDDPM 获取。"
}
{
  "title": "Evaluating Model Bias Requires Characterizing its Mistakes",
  "title_zh": "评估模型偏差需要对其错误进行特征化",
  "abstract": "The ability to properly benchmark model performance in the face of spurious correlations is important to both build better predictors and increase confidence that models are operating as intended. We demonstrate that characterizing (as opposed to simply quantifying) model mistakes across subgroups is pivotal to properly reflect model biases, which are ignored by standard metrics such as worst-group accuracy or accuracy gap. Inspired by the hypothesis testing framework, we introduce SkewSize, a principled and flexible metric that captures bias from mistakes in a model's predictions. It can be used in multi-class settings or generalised to the open vocabulary setting of generative models. SkewSize is an aggregation of the effect size of the interaction between two categorical variables: the spurious variable representing the bias attribute the model's prediction. We demonstrate the utility of SkewSize in multiple settings including: standard vision models trained on synthetic data, vision models trained on ImageNet, and large scale vision-and-language models from the BLIP-2 family. In each case, the proposed SkewSize is able to highlight biases not captured by other metrics, while also providing insights on the impact of recently proposed techniques, such as instruction tuning.",
  "abstract_zh": "在面对虚假相关性时，正确基准化模型性能的能力对于构建更好的预测器和增加对模型按预期运行的信心至关重要。我们证明了在子群体中对模型错误进行特征化（而不仅仅是量化）对于正确反映模型偏差是关键的，而这些偏差被标准指标（如最差群体准确率或准确率差距）所忽视。受假设检验框架的启发，我们引入了SkewSize，这是一种原则性和灵活的度量，捕捉模型预测中的错误偏差。它可以用于多类设置或推广到生成模型的开放词汇设置。SkewSize是两个分类变量之间交互作用的效应大小的聚合：虚假变量代表模型预测的偏差属性。我们在多个设置中展示了SkewSize的实用性，包括：在合成数据上训练的标准视觉模型、在ImageNet上训练的视觉模型，以及来自BLIP-2系列的大规模视觉和语言模型。在每种情况下，所提出的SkewSize能够突出其他指标未捕捉到的偏差，同时提供对最近提出的技术（如指令调优）影响的见解。"
}
{
  "title": "Modeling Caption Diversity in Contrastive Vision-Language Pretraining",
  "title_zh": "标题：对比视觉-语言预训练中标题多样性的建模",
  "abstract": "There are a thousand ways to caption an image. Contrastive Language Pretraining (CLIP) on the other hand, works by mapping an image and its caption to a single vector -- limiting how well CLIP-like models can represent the diverse ways to describe an image. In this work, we introduce Llip, Latent Language Image Pretraining, which models the diversity of captions that could match an image. Llip's vision encoder outputs a set of visual features that are mixed into a final representation by conditioning on information derived from the text. We show that Llip outperforms non-contextualized baselines like CLIP and SigLIP on a variety of tasks even with large-scale encoders. Llip improves zero-shot classification by an average of 2.9% zero-shot classification benchmarks with a ViT-G/14 encoder. Specifically, Llip attains a zero-shot top-1 accuracy of 83.5% on ImageNet outperforming a similarly sized CLIP by 1.4%. We also demonstrate improvement on zero-shot retrieval on MS-COCO by 6.0%. We provide a comprehensive analysis of the components introduced by the method and demonstrate that Llip leads to richer visual representations.",
  "abstract_zh": "摘要：图像的描述方式有千种万样。对比语言预训练（CLIP）通过将图像及其标题映射到单个向量来工作，这限制了CLIP类模型对描述图像多样性的表现。在本研究中，我们引入了Llip，即潜在语言图像预训练，它建模与图像匹配的标题多样性。Llip的视觉编码器输出一组视觉特征，这些特征通过基于文本派生的信息进行混合，形成最终表示。我们展示了Llip在各种任务上优于非上下文化的基线模型，如CLIP和SigLIP，即使在大规模编码器下也是如此。Llip在使用ViT-G/14编码器时，零样本分类平均提高了2.9%。具体而言，Llip在ImageNet上达到83.5%的零样本Top-1准确率，比同等规模的CLIP高出1.4%。我们还展示了在MS-COCO上的零样本检索提高了6.0%。我们对该方法引入的组件进行了全面分析，并证明Llip能够产生更丰富的视觉表示。"
}
{
  "title": "Model Alignment as Prospect Theoretic Optimization",
  "title_zh": "模型对齐作为前景理论优化",
  "abstract": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases---the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.",
  "abstract_zh": "卡尼曼和特沃斯基的前景理论告诉我们，人类以一种有偏见但明确的方式感知随机变量（1992）；例如，人类以著名的厌恶损失为特征。我们展示了对齐大型语言模型（LLMs）与人类反馈的目标隐含地包含了许多这些偏见——这些目标（例如，DPO）相较于交叉熵最小化的成功部分归因于它们属于我们称之为“人类感知损失”（HALOs）的损失函数家族。然而，这些方法归因于人类的效用函数仍与前景理论文献中的效用函数不同。利用卡尼曼-特沃斯基的人类效用模型，我们提出了一种直接最大化生成效用的HALO，而不是像当前方法那样最大化偏好的对数似然。我们称这种方法为KTO，它在从10亿到300亿的规模上匹配或超越了基于偏好的方法的性能，尽管仅从输出是否可取的二元信号中学习。更广泛地说，我们的工作表明，没有一种HALO是普遍优越的；最佳损失取决于特定环境中最合适的归纳偏见，这是一个常被忽视的考虑。"
}
{
  "title": "Debating with More Persuasive LLMs Leads to More Truthful Answers",
  "title_zh": "标题：与更具说服力的大型语言模型辩论会导致更真实的答案",
  "abstract": "Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is *debate*, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.",
  "abstract_zh": "摘要：将大型语言模型（LLMs）与期望行为对齐的常用方法在很大程度上依赖于人工标注的数据。然而，随着模型变得越来越复杂，它们将超越人类的专业知识，人类评估的角色将演变为非专家监督专家。对此，我们提出疑问：较弱的模型能否评估较强模型的正确性？我们在一个类比的环境中研究这个问题，其中较强的模型（专家）拥有回答问题所需的信息，而较弱的模型（非专家）缺乏这些信息。我们评估的方法是*辩论*，其中两个LLM专家分别为不同的答案辩论，非专家选择答案。我们发现辩论始终有助于非专家模型和人类回答问题，分别达到了76%和88%的准确率（天真的基线分别为48%和60%）。此外，以无监督的方式优化专家辩手的说服力，提高了非专家在辩论中识别真相的能力。我们的结果为在缺乏真实依据的情况下，通过辩论对模型进行对齐的可行性提供了令人鼓舞的实证证据。"
}
{
  "title": "Learning from Students: Applying t-Distributions to Explore Accurate and Efficient Formats for LLMs",
  "title_zh": "从学生中学习：应用t分布探索LLM的准确高效格式",
  "abstract": "The increasing size of large language models (LLMs) traditionally requires low-precision integer formats to meet strict latency and power demands. Yet recently, alternative formats such as Normal Float (NF4) have increased model accuracy at the cost of increased chip area. In this work, we first conduct a large-scale analysis of LLM weights and activations across 30 networks and conclude that most distributions follow a Student's t-distribution. We then derive a new theoretically optimal format, Student Float (SF4), that improves over NF4 across modern LLMs, for example increasing the average accuracy on LLaMA2-7B by 0.76% across tasks. Using this format as a high-accuracy reference, we then propose augmenting E2M1 with two variants of *supernormal* support for higher model accuracy. Finally, we explore the quality and efficiency frontier across 11 datatypes by evaluating their model accuracy and hardware complexity. We discover a Pareto curve composed of INT4, E2M1, and E2M1 with supernormal support, which offers a continuous tradeoff between model accuracy and chip area. For example, E2M1 with supernormal support increases the accuracy of Phi-2 by up to 2.19% with 1.22% area overhead, enabling more LLM-based applications to be run at four bits. The supporting code is hosted at https://github.com/cornell-zhang/llm-datatypes.",
  "abstract_zh": "大型语言模型（LLMs）的不断增大传统上需要低精度整数格式以满足严格的延迟和功耗要求。然而，最近，像正常浮点（NF4）这样的替代格式在增加芯片面积的代价下提高了模型的准确性。在这项工作中，我们首先对30个网络的LLM权重和激活进行了大规模分析，得出大多数分布遵循学生t分布。然后，我们推导出一种新的理论最优格式——学生浮点（SF4），在现代LLM中优于NF4，例如在各任务上将LLaMA2-7B的平均准确性提高了0.76%。以这种格式作为高准确性参考，我们提出用两种*超常*支持变体增强E2M1以提高模型准确性。最后，我们通过评估11种数据类型的模型准确性和硬件复杂性，探索质量和效率的边界。我们发现了一条由INT4、E2M1和带超常支持的E2M1组成的帕累托曲线，提供了模型准确性与芯片面积之间的连续权衡。例如，带超常支持的E2M1将Phi-2的准确性提高了最多2.19%，而面积增加仅为1.22%，使得更多基于LLM的应用能够在四位上运行。支持代码托管在https://github.com/cornell-zhang/llm-datatypes。"
}
{
  "title": "Auto-Regressive Next-Token Predictors are Universal Learners",
  "title_zh": "自回归下一个标记预测器是通用学习者",
  "abstract": "Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-of-Thought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure---length complexity---which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our results demonstrate that the power of today's LLMs can be attributed, to a great extent, to the auto-regressive next-token training scheme, and not necessarily to a particular choice of architecture.",
  "abstract_zh": "大型语言模型在逻辑和数学推理方面展现出卓越的能力，使其能够解决复杂任务。有趣的是，这些能力是在简单的下一个标记预测任务上训练的网络中出现的。在本研究中，我们提出了一个理论框架来研究自回归下一个标记预测器。我们证明，即使是简单的模型，如在线性下一个标记预测器上，训练于思维链（CoT）数据，也能有效地逼近任何由图灵机计算的函数。我们引入了一种新的复杂性度量——长度复杂性——它衡量了在CoT序列中逼近某个目标函数所需的中间标记数量，并分析了长度复杂性与其他复杂性概念之间的相互作用。最后，我们通过实验表明，简单的下一个标记预测器，如线性网络和浅层多层感知器（MLP），在文本生成和算术任务上表现出非平凡的性能。我们的结果表明，今天的LLM的能力在很大程度上可以归因于自回归下一个标记训练方案，而不一定是特定架构的选择。"
}
{
  "title": "Self-Correcting Self-Consuming Loops for Generative Model Training",
  "title_zh": "自我修正的自我消耗循环用于生成模型训练",
  "abstract": "As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates ``self-consuming loops'' which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made *exponentially* more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consuming loops on the challenging human motion synthesis task, and observe that it successfully avoids model collapse, even when the ratio of synthetic data to real data is as high as 100%.",
  "abstract_zh": "随着合成数据质量的提高并在互联网上大量传播，机器学习模型越来越多地在混合的人类和机器生成数据上进行训练。尽管使用合成数据进行表示学习的成功案例屡见不鲜，但在生成模型训练中使用合成数据会产生“自我消耗循环”，这可能导致训练不稳定甚至崩溃，除非满足特定条件。我们的论文旨在稳定自我消耗的生成模型训练。我们的理论结果表明，通过引入一个理想化的修正函数，该函数将数据点映射为在真实数据分布下更可能的值，自我消耗循环可以变得*指数*更稳定。然后，我们提出了依赖于专家知识（例如在模拟器中编程的物理法则）的自我修正函数，旨在自动并大规模地近似理想化的修正器。我们在具有挑战性的人类运动合成任务上实证验证了自我修正自我消耗循环的有效性，并观察到即使合成数据与真实数据的比例高达100%，它也成功避免了模型崩溃。"
}
{
  "title": "LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery",
  "title_zh": "标题：大型语言模型与模拟作为双层优化器：推动物理科学发现的新范式",
  "abstract": "Large Language Models have recently gained significant attention in scientific discovery for their extensive knowledge and advanced reasoning capabilities. However, they encounter challenges in effectively simulating observational feedback and grounding it with language to propel advancements in physical scientific discovery. Conversely, human scientists undertake scientific discovery by formulating hypotheses, conducting experiments, and revising theories through observational analysis. Inspired by this, we propose to enhance the knowledge-driven, abstract reasoning abilities of LLMs with the computational strength of simulations. We introduce Scientific Generative Agent (SGA), a bilevel optimization framework: LLMs act as knowledgeable and versatile thinkers, proposing scientific hypotheses and reason about discrete components, such as physics equations or molecule structures; meanwhile, simulations function as experimental platforms, providing observational feedback and optimizing via differentiability for continuous parts, such as physical parameters. We conduct extensive experiments to demonstrate our framework's efficacy in constitutive law discovery and molecular design, unveiling novel solutions that differ from conventional human expectations yet remain coherent upon analysis.",
  "abstract_zh": "摘要：大型语言模型最近因其广泛的知识和先进的推理能力在科学发现中引起了显著关注。然而，它们在有效模拟观察反馈并将其与语言结合以推动物理科学发现方面面临挑战。相反，人类科学家通过提出假设、进行实验和通过观察分析修正理论来进行科学发现。受到这一启发，我们提出通过模拟的计算能力增强大型语言模型的知识驱动和抽象推理能力。我们引入了科学生成代理（SGA），一个双层优化框架：大型语言模型作为知识渊博且多才多艺的思考者，提出科学假设并推理离散组件，如物理方程或分子结构；同时，模拟作为实验平台，提供观察反馈并通过可微分性优化连续部分，如物理参数。我们进行了广泛的实验，以证明我们框架在构成规律发现和分子设计中的有效性，揭示出与传统人类预期不同但在分析时仍然一致的新解决方案。"
}
{
  "title": "Prompt-tuning Latent Diffusion Models for Inverse Problems",
  "title_zh": "标题：用于逆问题的提示调优潜在扩散模型",
  "abstract": "We propose a new method for solving imaging inverse problems using text-to-image latent diffusion models as general priors. Existing methods using latent diffusion models for inverse problems typically rely on simple null text prompts, which can lead to suboptimal performance. To improve upon this, we introduce a method for prompt tuning, which jointly optimizes the text embedding on-the-fly while running the reverse diffusion. This allows us to generate images that are more faithful to the diffusion prior. Specifically, our approach involves a unified optimization framework that simultaneously considers the prompt, latent, and pixel values through alternating minimization. This significantly diminishes image artifacts - a major problem when using latent diffusion models instead of pixel-based diffusion ones. Our method, called P2L, outperforms both pixel- and latent-diffusion model-based inverse problem solvers on a variety of tasks, such as super-resolution, deblurring, and inpainting. Furthermore, P2L demonstrates remarkable scalability to higher resolutions without artifacts.",
  "abstract_zh": "摘要：我们提出了一种新方法，通过将文本到图像的潜在扩散模型作为通用先验，解决成像逆问题。现有的使用潜在扩散模型解决逆问题的方法通常依赖于简单的空文本提示，这可能导致次优性能。为了改进这一点，我们引入了一种提示调优方法，在运行反向扩散的同时动态优化文本嵌入。这使我们能够生成更忠实于扩散先验的图像。具体而言，我们的方法涉及一个统一的优化框架，通过交替最小化同时考虑提示、潜在和像素值。这显著减少了图像伪影——这是使用潜在扩散模型而非基于像素的扩散模型时的主要问题。我们的方法称为P2L，在超分辨率、去模糊和修复等多种任务上优于基于像素和潜在扩散模型的逆问题求解器。此外，P2L在更高分辨率下表现出显著的可扩展性且没有伪影。"
}
{
  "title": "Behavior Generation with Latent Actions",
  "title_zh": "潜在动作的行为生成",
  "abstract": "Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision-making. Unlike language or image generation, decision-making requires modeling actions – continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous actions with a hierarchical vector quantization module. Across seven environments including simulated manipulation, autonomous driving, and robotics, VQ-BeT improves on state-of-the-art models such as BeT and Diffusion Policies. Importantly, we demonstrate VQ-BeT’s improved ability to capture behavior modes while accelerating inference speed 5× over Diffusion Policies. Videos can be found https://sjlee.cc/vq-bet/",
  "abstract_zh": "从标记数据集中生成复杂行为的模型一直是决策制定中的一个长期问题。与语言或图像生成不同，决策制定需要建模动作——连续值向量，其分布是多模态的，可能来自未经整理的来源，其中生成错误可能在序列预测中累积。最近一类称为行为变换器（BeT）的模型通过使用k均值聚类对动作进行离散化，以捕捉不同的模式。然而，k均值在高维动作空间或长序列中难以扩展，并且缺乏梯度信息，因此BeT在建模长程动作时表现不佳。在这项工作中，我们提出了向量量化行为变换器（VQ-BeT），这是一个多功能的行为生成模型，能够处理多模态动作预测、条件生成和部分观察。VQ-BeT通过使用分层向量量化模块对连续动作进行标记，从而增强了BeT。在包括模拟操作、自动驾驶和机器人技术在内的七个环境中，VQ-BeT在性能上超过了最先进的模型，如BeT和扩散策略。重要的是，我们展示了VQ-BeT在捕捉行为模式方面的改进能力，同时推理速度比扩散策略快5倍。视频可以在 https://sjlee.cc/vq-bet/ 找到。"
}
{
  "title": "ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking",
  "title_zh": "极端 LLM 检查点压缩：通过权重-动量联合缩减",
  "abstract": "Large language models (LLM) have recently attracted significant attention in the field of artificial intelligence. However, the training process of these models poses significant challenges in terms of computational and storage capacities, thus compressing checkpoints has become an urgent problem. In this paper, we propose a novel Extreme Checkpoint Compression (ExCP) framework, which significantly reduces the required storage of training checkpoints while achieving nearly lossless performance. We first calculate the residuals of adjacent checkpoints to obtain the essential but sparse information for higher compression ratio. To further excavate the redundancy parameters in checkpoints, we then propose a weight-momentum joint shrinking method to utilize another important information during the model optimization, i.e., momentum. In particular, we exploit the information of both model and optimizer to discard as many parameters as possible while preserving critical information to ensure optimal performance. Furthermore, we utilize non-uniform quantization to further compress the storage of checkpoints. We extensively evaluate our proposed ExCP framework on several models ranging from 410M to 7B parameters and demonstrate significant storage reduction while maintaining strong performance. For instance, we achieve approximately $70\\times$ compression for the Pythia-410M model, with the final performance being as accurate as the original model on various downstream tasks. Codes will be available at https://github.com/Gaffey/ExCP.",
  "abstract_zh": "大型语言模型（LLM）最近在人工智能领域引起了广泛关注。然而，这些模型的训练过程在计算和存储能力方面面临重大挑战，因此压缩检查点成为一个紧迫的问题。本文提出了一种新颖的极端检查点压缩（ExCP）框架，显著减少了训练检查点所需的存储，同时实现了几乎无损的性能。我们首先计算相邻检查点的残差，以获得高压缩比所需的基本但稀疏的信息。为了进一步挖掘检查点中的冗余参数，我们提出了一种权重-动量联合缩减方法，利用模型优化过程中的另一个重要信息，即动量。特别地，我们利用模型和优化器的信息，尽可能丢弃多余的参数，同时保留关键信息以确保最佳性能。此外，我们利用非均匀量化进一步压缩检查点的存储。我们在多个模型（参数从410M到7B不等）上广泛评估了我们提出的ExCP框架，证明在保持强大性能的同时实现了显著的存储减少。例如，我们对Pythia-410M模型实现了约$70\\times$的压缩，最终性能在各种下游任务上与原始模型同样准确。代码将发布在https://github.com/Gaffey/ExCP。"
}
{
  "title": "Position: Machine Learning-powered Assessments of the EU Digital Services Act Aid Quantify Policy Impacts on Online Harms",
  "title_zh": "标题：基于机器学习的评估：欧盟数字服务法对在线危害政策影响的量化",
  "abstract": "While machine learning shows promise in automated knowledge generation, current techniques such as large language models and micro-targeted influence operations can be exploited for harmful purposes like the proliferation of disinformation. The European Union's Digital Services Act (DSA) is an exemplary policy response addressing these harms generated by online platforms. In this regard, it necessitates a comprehensive evaluation of its impact on curbing the harmful downstream effects of these opaque practices. Despite their harmful applications, we argue that machine learning techniques offer immense, yet under-exploited, potential for unraveling the impacts of regulations like the DSA. Following an analysis that reveals possible limitations in the DSA's provisions, we call for resolute efforts to address methodological barriers around appropriate data access, isolating marginal regulatory effects, and facilitating generalization across different contexts. Given the identified advantages of data-driven approaches to regulatory delivery, we advocate for machine learning research to help quantify the policy impacts on online harms.",
  "abstract_zh": "摘要：尽管机器学习在自动知识生成方面展现出潜力，但当前技术如大型语言模型和微目标影响操作可能被用于传播虚假信息等有害目的。欧盟的数字服务法（DSA）是应对在线平台产生的这些危害的典范政策。因此，有必要对其在遏制这些不透明做法的有害下游影响方面的影响进行全面评估。尽管存在有害应用，我们认为机器学习技术在揭示像DSA这样的法规影响方面具有巨大的但尚未充分利用的潜力。在分析揭示DSA条款可能存在的局限性后，我们呼吁采取坚定措施解决适当数据访问、隔离边际监管效果以及促进不同背景下的推广等方法论障碍。鉴于数据驱动方法在监管实施中的优势，我们倡导机器学习研究帮助量化政策对在线危害的影响。"
}
{
  "title": "Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion",
  "title_zh": "裁判也能参与：通过模型反演实现条件生成的替代方法",
  "abstract": "As a dominant force in text-to-image generation tasks, Diffusion Probabilistic Models (DPMs) face a critical challenge in controllability, struggling to adhere strictly to complex, multi-faceted instructions. In this work, we aim to address this alignment challenge for conditional generation tasks. First, we provide an alternative view of state-of-the-art DPMs as a way of inverting advanced Vision-Language Models (VLMs). With this formulation, we naturally propose a training-free approach that bypasses the conventional sampling process associated with DPMs. By directly optimizing images with the supervision of discriminative VLMs, the proposed method can potentially achieve a better text-image alignment. As proof of concept, we demonstrate the pipeline with the pre-trained BLIP-2 model and identify several key designs for improved image generation. To further enhance the image fidelity, a Score Distillation Sampling module of Stable Diffusion is incorporated. By carefully balancing the two components during optimization, our method can produce high-quality images with near state-of-the-art performance on T2I-Compbench. The code is available at https://github.com/Pepper-lll/VLMinv.",
  "abstract_zh": "作为文本到图像生成任务中的主导力量，扩散概率模型（DPMs）面临着可控性的重要挑战，难以严格遵循复杂的多方面指令。在这项工作中，我们旨在解决条件生成任务中的对齐挑战。首先，我们提供了对最先进的DPMs的替代视角，将其视为反演先进视觉-语言模型（VLMs）的一种方式。通过这种公式化，我们自然提出了一种无训练的方法，绕过了与DPMs相关的传统采样过程。通过在判别性VLMs的监督下直接优化图像，所提出的方法有望实现更好的文本-图像对齐。作为概念验证，我们使用预训练的BLIP-2模型演示了该流程，并确定了几个关键设计以改善图像生成。为了进一步增强图像的保真度，结合了稳定扩散的得分蒸馏采样模块。通过在优化过程中仔细平衡这两个组件，我们的方法能够生成高质量的图像，并在T2I-Compbench上接近最先进的性能。代码可在https://github.com/Pepper-lll/VLMinv获取。"
}
{
  "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
  "title_zh": "GaLore：通过梯度低秩投影实现内存高效的LLM训练",
  "abstract": "Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.",
  "abstract_zh": "训练大型语言模型（LLMs）面临显著的内存挑战，主要由于权重和优化器状态的不断增大。常见的内存减少方法，如低秩适应（LoRA），在每一层的冻结预训练权重中添加一个可训练的低秩矩阵，从而减少可训练参数和优化器状态。然而，这些方法在预训练和微调阶段通常表现不如全秩权重训练，因为它们将参数搜索限制在低秩子空间并改变训练动态，此外，可能需要全秩的热启动。在本研究中，我们提出了梯度低秩投影（GaLore），这是一种允许全参数学习但比常见的低秩适应方法（如LoRA）更具内存效率的训练策略。我们的方法在优化器状态中将内存使用减少了多达65.5%，同时在LLaMA 1B和7B架构上使用C4数据集进行预训练时保持了效率和性能，处理多达19.7B个标记，并在GLUE任务上微调RoBERTa。我们的8位GaLore进一步将优化器内存减少了多达82.5%，总训练内存减少了63.3%，与BF16基线相比。值得注意的是，我们首次展示了在24GB内存的消费级GPU（例如NVIDIA RTX 4090）上进行7B模型预训练的可行性，而无需模型并行、检查点或卸载策略。"
}
{
  "title": "Progressive Inference: Explaining Decoder-Only Sequence Classification Models Using Intermediate Predictions",
  "title_zh": "渐进推理：使用中间预测解释仅解码器序列分类模型",
  "abstract": "This paper proposes Progressive inference--a framework to explain the predictions of decoder-only transformer models trained to perform sequence classification tasks. Our work is based on the insight that the classification head of a decoder-only model can be used to make intermediate predictions by evaluating them at different points in the input sequence. Due to the masked attention mechanism used in decoder-only models, these intermediate predictions only depend on the tokens seen before the inference point, allowing us to obtain the model's prediction on a masked input sub-sequence, with negligible computational overheads. We develop two methods to provide sub-sequence level attributions using this core insight. First, we propose Single Pass-Progressive Inference (SP-PI) to compute attributions by simply taking the difference between intermediate predictions. Second, we exploit a connection with Kernel SHAP to develop Multi Pass-Progressive Inference (MP-PI); this uses intermediate predictions from multiple masked versions of the input to compute higher-quality attributions that approximate SHAP values. We perform studies on several text classification datasets to demonstrate that our proposal provides better explanations compared to prior work, both in the single-pass and multi-pass settings.",
  "abstract_zh": "本文提出了渐进推理——一个框架，用于解释经过训练以执行序列分类任务的仅解码器变换器模型的预测。我们的工作基于这样一个见解：仅解码器模型的分类头可以通过在输入序列的不同点进行评估来进行中间预测。由于仅解码器模型中使用的掩蔽注意机制，这些中间预测仅依赖于推理点之前看到的标记，使我们能够在掩蔽输入子序列上获得模型的预测，且计算开销微乎其微。我们开发了两种方法，利用这一核心见解提供子序列级别的归因。首先，我们提出了单次渐进推理（SP-PI），通过简单地计算中间预测之间的差异来计算归因。其次，我们利用与核SHAP的联系，开发了多次渐进推理（MP-PI）；该方法使用来自多个掩蔽版本的输入的中间预测来计算近似SHAP值的高质量归因。我们在多个文本分类数据集上进行了研究，证明我们的提案在单次和多次设置中都提供了比以前的工作更好的解释。"
}
{
  "title": "Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization",
  "title_zh": "探索驱动的RLHF策略优化：高效数据利用的理论洞察",
  "abstract": "Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to get good performance with RLHF. A key novelty is our trajectory-level elliptical potential analysis technique used to infer reward function parameters when comparison queries rather than reward observations are used. We provide and analyze algorithms in two settings: linear and neural function approximation, PG-RLHF and NN-PG-RLHF, respectively.",
  "abstract_zh": "人类反馈强化学习（RLHF）在依赖少量人类反馈的情况下取得了令人印象深刻的经验成功。然而，对于这一现象的理论解释有限。此外，大多数近期研究集中在基于价值的算法上，尽管基于策略的算法最近也取得了经验上的成功。在本研究中，我们考虑了一种基于策略优化的RLHF算法（PO-RLHF）。该算法基于流行的策略覆盖-策略梯度（PC-PG）算法，假设已知奖励函数。在PO-RLHF中，不假设已知奖励函数，算法依赖于基于轨迹的比较反馈来推断奖励函数。我们提供了PO-RLHF的低查询复杂度性能界限，这为理解为何少量人类反馈可能足以在RLHF中获得良好性能提供了洞察。一个关键的新颖性是我们用于推断奖励函数参数的轨迹级椭圆潜力分析技术，当使用比较查询而不是奖励观察时。我们在两个设置中提供并分析算法：线性和神经网络函数逼近，分别为PG-RLHF和NN-PG-RLHF。"
}
{
  "title": "Q-Probe: A Lightweight Approach to Reward Maximization for Language Models",
  "title_zh": "Q-Probe：一种轻量级的语言模型奖励最大化方法",
  "abstract": "We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot prompting, but can also be combined with either. The idea is to learn a simple linear function on a model's embedding space that can be used to reweight candidate completions. We theoretically show that this sampling procedure is equivalent to a KL-constrained maximization of the Q-probe as the number of samples increases. To train the Q-probes we consider either reward modeling or a class of novel direct policy learning objectives based on importance-weighted policy gradients. With this technique, we see gains in domains with ground-truth rewards (code generation) as well as implicit rewards defined by preference data, even outperforming finetuning in data-limited regimes. Moreover, a Q-probe can be trained on top of an API since it only assumes access to sampling and embeddings. Code: [https://github.com/likenneth/q_probe](https://github.com/likenneth/q_probe).",
  "abstract_zh": "我们提出了一种名为Q-probing的方法，以适应预训练语言模型以最大化特定任务的奖励函数。从高层次来看，Q-probing介于更重的方法（如微调）和更轻的方法（如少量提示）之间，但也可以与任一方法结合。其思想是在模型的嵌入空间上学习一个简单的线性函数，用于重新加权候选完成。我们理论上证明，这种采样过程在样本数量增加时等同于KL约束下的Q-probe最大化。为了训练Q-probes，我们考虑奖励建模或基于重要性加权策略梯度的一类新颖的直接策略学习目标。通过这种技术，我们在具有真实奖励的领域（代码生成）以及由偏好数据定义的隐式奖励中都看到了收益，甚至在数据有限的情况下超越了微调。此外，Q-probe可以在API之上进行训练，因为它只假设可以访问采样和嵌入。代码：[https://github.com/likenneth/q_probe](https://github.com/likenneth/q_probe)。"
}
{
  "title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment",
  "title_zh": "标题：广义偏好优化：离线对齐的统一方法",
  "abstract": "Offline preference optimization allows fine-tuning large models directly from offline data, and has proved effective in recent alignment practices. We propose generalized preference optimization (GPO), a family of offline losses parameterized by a general class of convex functions. GPO enables a unified view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as special cases, while naturally introducing new variants. The GPO framework also sheds light on how offline algorithms enforce regularization, through the design of the convex function that defines the loss. Our analysis and experiments reveal the connections and subtle differences between the offline regularization and the KL divergence regularization intended by the canonical RLHF formulation. In a controlled setting akin to Gao et al 2023, we also show that different GPO variants achieve similar trade-offs between regularization and performance, though the optimal values of hyper-parameter might differ as predicted by theory. In all, our results present new algorithmic toolkits and empirical insights to alignment practitioners.",
  "abstract_zh": "摘要：离线偏好优化允许直接从离线数据微调大型模型，并在最近的对齐实践中证明了其有效性。我们提出了广义偏好优化（GPO），这是一类由一般凸函数参数化的离线损失。GPO提供了对偏好优化的统一视角，涵盖了现有算法如DPO、IPO和SLiC作为特例，同时自然引入新的变体。GPO框架还阐明了离线算法如何通过定义损失的凸函数设计来实施正则化。我们的分析和实验揭示了离线正则化与经典RLHF公式所意图的KL散度正则化之间的联系和微妙差异。在类似于Gao等人2023年的受控环境中，我们还展示了不同的GPO变体在正则化和性能之间实现了类似的权衡，尽管超参数的最优值可能如理论所预测的那样有所不同。总之，我们的结果为对齐实践者提供了新的算法工具包和实证见解。"
}
{
  "title": "StackSight: Unveiling WebAssembly through Large Language Models and Neurosymbolic Chain-of-Thought Decompilation",
  "title_zh": "标题：StackSight：通过大型语言模型和神经符号链式思维反编译揭示WebAssembly",
  "abstract": "WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics.",
  "abstract_zh": "摘要：WebAssembly使得网页应用能够接近本地执行，越来越多地被用于需要高性能和强大安全性的任务。然而，其类似汇编的语法、隐式栈机器和低级数据类型使得人类开发者极难理解，这促使了对有效的WebAssembly反向工程技术的需求。在本文中，我们提出了StackSight，一种新颖的神经符号方法，将大型语言模型（LLMs）与先进的程序分析相结合，将复杂的WebAssembly代码反编译为可读的C++代码片段。StackSight通过静态分析算法可视化和跟踪虚拟栈的变化，然后应用链式思维提示，以利用LLM的复杂推理能力。评估结果表明，StackSight显著提高了WebAssembly的反编译效果。我们的用户研究还表明，StackSight生成的代码片段具有显著更高的胜率，并能够更好地理解代码语义。"
}
{
  "title": "Characterizing Large Language Model Geometry Helps Solve Toxicity Detection and Generation",
  "title_zh": "标题：表征大型语言模型几何有助于解决毒性检测与生成",
  "abstract": "Large Language Models (LLMs) drive current AI breakthroughs despite very little being known about their internal representations. In this work, we propose to shed the light on LLMs inner mechanisms through the lens of geometry. In particular, we develop in closed form $(i)$ the intrinsic dimension in which the Multi-Head Attention embeddings are constrained to exist and $(ii)$ the partition and per-region affine mappings of the feedforward (MLP) network of LLMs' layers. Our theoretical findings further enable the design of novel principled solutions applicable to state-of-the-art LLMs. First, we show that, through our geometric understanding, we can bypass LLMs' RLHF protection by controlling the embedding's intrinsic dimension through informed prompt manipulation. Second, we derive interpretable geometrical features that can be extracted from any (pre-trained) LLM, providing a rich abstract representation of their inputs. We observe that these features are sufficient to help solve toxicity detection, and even allow the identification of various types of toxicity. Our results demonstrate how, even in large-scale regimes, exact theoretical results can answer practical questions in LLMs. Code: https://github.com/RandallBalestriero/SplineLLM",
  "abstract_zh": "摘要：大型语言模型（LLMs）推动了当前人工智能的突破，尽管对其内部表征知之甚少。在这项工作中，我们提出通过几何的视角揭示LLMs的内在机制。具体而言，我们以封闭形式开发了$(i)$多头注意力嵌入所受约束的内在维度，以及$(ii)$LLMs层的前馈（MLP）网络的划分和每个区域的仿射映射。我们的理论发现进一步使得设计适用于最先进LLMs的新型原则性解决方案成为可能。首先，我们展示了通过我们的几何理解，我们可以通过有针对性的提示操控来控制嵌入的内在维度，从而绕过LLMs的RLHF保护。其次，我们推导出可从任何（预训练）LLM中提取的可解释几何特征，为其输入提供了丰富的抽象表征。我们观察到这些特征足以帮助解决毒性检测，甚至允许识别各种类型的毒性。我们的结果表明，即使在大规模情况下，精确的理论结果也能回答LLMs中的实际问题。代码：https://github.com/RandallBalestriero/SplineLLM"
}
{
  "title": "SelfIE: Self-Interpretation of Large Language Model Embeddings",
  "title_zh": "自我解释：大型语言模型嵌入的自我解释",
  "abstract": "How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM’s reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond to inquiries about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE’s text descriptions on hidden embeddings open avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets.",
  "abstract_zh": "大型语言模型（LLMs）如何获得答案？解释和控制LLM推理过程的能力是可靠性、透明性和未来模型发展的关键。我们提出了SelfIE（嵌入的自我解释），这是一个框架，使LLM能够通过利用其对给定段落的询问响应能力，以自然语言解释其自身嵌入。SelfIE能够解释隐藏嵌入中的开放世界概念，揭示LLM在做出伦理决策、内化提示注入和回忆有害知识等情况下的内部推理。SelfIE对隐藏嵌入的文本描述为控制LLM推理开辟了新途径。我们提出了监督控制，允许编辑开放式概念，同时仅需计算单层的梯度。我们将RLHF扩展到隐藏嵌入，并提出强化控制，能够在没有监督目标的情况下消除LLM中的有害知识。"
}
{
  "title": "Can AI Assistants Know What They Don't Know?",
  "title_zh": "标题：人工智能助手能否知道自己不知道的事情？",
  "abstract": "AI assistants powered by Large Language Models (LLMs) have demonstrated impressive performance in various tasks. However, LLMs still make factual errors in knowledge-intensive tasks such as open-domain question answering. These untruthful responses from AI assistants can pose significant risks in practical applications. Therefore, in this paper, we ask the question **Can AI assistants know what they don't know and express this awareness through natural language?** To investigate this, we construct a model-specific \"I don't know\" (Idk) dataset. This dataset includes Supervised Fine-tuning data and preference data, categorizing questions based on whether the assistant knows or does not know the answers. Then, we align the assistant with its corresponding Idk dataset using different alignment methods, including Supervised Fine-tuning and preference optimization. Experimental results show that, after alignment with the Idk dataset, the assistant is more capable of declining to answer questions outside its knowledge scope. The assistant aligned with the Idk dataset shows significantly higher truthfulness than the original assistant.",
  "abstract_zh": "摘要：由大型语言模型（LLMs）驱动的人工智能助手在各种任务中表现出色。然而，LLMs在知识密集型任务（如开放领域问答）中仍然会出现事实错误。这些来自人工智能助手的不真实回答在实际应用中可能带来重大风险。因此，本文提出了一个问题：**人工智能助手能否知道自己不知道的事情，并通过自然语言表达这种意识？**为此，我们构建了一个特定于模型的“我不知道”（Idk）数据集。该数据集包括监督微调数据和偏好数据，根据助手是否知道答案对问题进行分类。然后，我们使用不同的对齐方法（包括监督微调和偏好优化）将助手与其相应的Idk数据集对齐。实验结果表明，在与Idk数据集对齐后，助手更能够拒绝回答超出其知识范围的问题。与Idk数据集对齐的助手显示出比原始助手显著更高的真实性。"
}
{
  "title": "Position: A Roadmap to Pluralistic Alignment",
  "title_zh": "位置：通向多元对齐的路线图",
  "abstract": "With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve *all*, i.e., people with diverse values and perspectives. However, aligning models to serve *pluralistic* human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using large language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) *Overton pluralistic* models that present a spectrum of reasonable responses; 2) *Steerably pluralistic* models that can steer to reflect certain perspectives; and 3) *Distributionally pluralistic* models that are well-calibrated to a given population in distribution. We also formalize and discuss three possible classes of *pluralistic benchmarks*: 1) *Multi-objective* benchmarks, 2) *Trade-off steerable* benchmarks that incentivize models to steer to arbitrary trade-offs, and 3) *Jury-pluralistic* benchmarks that explicitly model diverse human ratings. We use this framework to argue that current alignment techniques may be fundamentally limited for pluralistic AI; indeed, we highlight empirical evidence, both from our own experiments and from other work, that standard alignment procedures might *reduce* distributional pluralism in models, motivating the need for further research on pluralistic alignment.",
  "abstract_zh": "随着人工智能系统的能力和普及性不断增强，设计能够服务于所有人（即拥有多样价值观和视角的人）的人工智能系统变得愈发重要。然而，使模型能够服务于多元化的人类价值观仍然是一个未解的研究问题。在本文中，我们提出了一条多元对齐的路线图，特别是使用大型语言模型作为测试平台。我们识别并形式化了三种可能的方式来定义和实现人工智能系统中的多元化：1）*奥弗顿多元*模型，呈现合理反应的光谱；2）*可引导多元*模型，可以引导以反映特定视角；3）*分布多元*模型，能够很好地校准到特定人群的分布。我们还形式化并讨论了三类可能的*多元基准*：1）*多目标*基准，2）*可引导权衡*基准，激励模型引导至任意权衡，3）*陪审团多元*基准，明确建模多样的人类评分。我们利用这一框架论证当前的对齐技术可能在多元化人工智能方面存在根本限制；实际上，我们强调了来自我们自己实验和其他工作的实证证据，表明标准对齐程序可能会*减少*模型中的分布多元性，从而激励对多元对齐的进一步研究。"
}
{
  "title": "SceneCraft: An LLM Agent for Synthesizing 3D Scenes as Blender Code",
  "title_zh": "场景工艺：用于将3D场景合成为Blender代码的LLM代理",
  "abstract": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal.",
  "abstract_zh": "本文介绍了场景工艺（SceneCraft），一种大型语言模型（LLM）代理，能够将文本描述转换为可在Blender中执行的Python脚本，从而渲染包含多达一百个3D资产的复杂场景。该过程需要复杂的空间规划和布局安排。我们通过高级抽象、战略规划和库学习的结合来应对这些挑战。场景工艺首先将场景图建模为蓝图，详细描述场景中资产之间的空间关系。然后，场景工艺基于该图编写Python脚本，将关系转换为资产布局的数值约束。接下来，场景工艺利用视觉-语言基础模型（如GPT-V）的感知优势，分析渲染图像并迭代优化场景。在此过程中，场景工艺还具有一个库学习机制，将常见脚本功能编译成可重用库，促进持续自我改进，而无需昂贵的LLM参数调整。我们的评估表明，场景工艺在渲染复杂场景方面超越了现有的基于LLM的代理，体现在其对约束的遵守和人类评估的良好反馈上。我们还通过从Sintel电影重建详细的3D场景，并利用生成的场景作为中介控制信号指导视频生成模型，展示了场景工艺更广泛的应用潜力。"
}
{
  "title": "SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks",
  "title_zh": "SLEB：通过冗余验证和消除变换器块来简化大型语言模型",
  "abstract": "Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to stream- line LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB outperforms previous LLM pruning methods in accelerating LLM inference while also maintaining superior perplexity and accuracy, making SLEB as a promising technique for enhancing the efficiency of LLMs. The code is available at: https://github.com/jiwonsong-dev/SLEB.",
  "abstract_zh": "大型语言模型（LLMs）在各种自然语言处理任务中表现出色。然而，它们庞大的参数数量对实际部署带来了重大挑战。剪枝是一种旨在减少LLMs大小和复杂性的技术，通过从网络中移除冗余组件提供了潜在解决方案。尽管剪枝有其前景，但现有方法往往难以实现显著的端到端LLM推理加速。在本文中，我们介绍了SLEB，这是一种旨在通过消除冗余变换器块来简化LLMs的新方法。我们选择变换器块作为剪枝的基本单元，因为LLMs在相邻块的输出之间表现出块级冗余和高度相似性。这个选择使我们能够有效提高LLMs的处理速度。我们的实验结果表明，SLEB在加速LLM推理方面优于之前的LLM剪枝方法，同时保持更优的困惑度和准确性，使SLEB成为提升LLMs效率的有前景的技术。代码可在：https://github.com/jiwonsong-dev/SLEB获取。"
}
{
  "title": "Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo",
  "title_zh": "标题：通过扭曲的序列蒙特卡罗进行语言模型中的概率推理",
  "abstract": "Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.",
  "abstract_zh": "摘要：大型语言模型（LLMs）的众多能力和安全技术，包括RLHF、自动红队、提示工程和填充，可以被视为从由给定奖励或潜在函数定义的未归一化目标分布中进行采样。在这项工作中，我们利用序列蒙特卡罗（SMC）的丰富工具包来解决这些概率推理问题。特别是，我们使用学习到的扭曲函数来估计每个时间步潜在值的期望未来值，这使我们能够将推理时的计算集中在有前景的部分序列上。我们提出了一种新颖的对比方法来学习扭曲函数，并与丰富的软强化学习文献建立联系。作为我们扭曲SMC框架的补充应用，我们提出了使用新颖的双向SMC界限评估语言模型推理技术准确性的方法。这些界限可用于估计推理和目标分布之间的KL散度。我们应用我们的推理评估技术，表明扭曲SMC在从预训练模型中采样不良输出（这是无害性训练和自动红队的有用组成部分）、生成情感多样的评论以及执行填充任务方面是有效的。"
}
{
  "title": "Differentially Private Bias-Term Fine-tuning of Foundation Models",
  "title_zh": "差分隐私偏置项微调基础模型",
  "abstract": "We study the problem of differentially private (DP) fine-tuning of large pre-trained models — a recent privacy-preserving approach suitable for solving downstream tasks with sensitive data. Existing work has demonstrated that high accuracy is possible under strong privacy constraint, yet requires significant computational overhead or modifications to the network architecture. We propose differentially private bias-term fine-tuning (DP-BiTFiT), which matches the state-of-the-art accuracy for DP algorithms and the efficiency of the standard BiTFiT. DP-BiTFiT is model agnostic (not modifying the network architecture), parameter efficient (only training about 0.1% of the parameters), and computation efficient (almost removing the overhead caused by DP, in both the time and space complexity). On a wide range of tasks, DP-BiTFiT is 2 - 30X faster and uses 2 - 8X less memory than DP full fine-tuning, even faster than the standard full fine-tuning. This amazing efficiency enables us to conduct DP fine-tuning on language and vision tasks with long-sequence texts and high-resolution images, which were computationally difficult using existing methods.",
  "abstract_zh": "我们研究了大型预训练模型的差分隐私（DP）微调问题——这是一种适合处理敏感数据下游任务的隐私保护方法。现有研究已证明在强隐私约束下可以实现高准确率，但需要显著的计算开销或对网络架构的修改。我们提出了差分隐私偏置项微调（DP-BiTFiT），其在DP算法的准确性和标准BiTFiT的效率上均达到最先进水平。DP-BiTFiT是模型无关的（不修改网络架构）、参数高效的（仅训练约0.1%的参数）以及计算高效的（几乎消除了DP带来的时间和空间复杂度开销）。在广泛的任务中，DP-BiTFiT比DP完全微调快2到30倍，内存使用量减少2到8倍，甚至比标准完全微调还要快。这种惊人的效率使我们能够在处理长序列文本和高分辨率图像的语言和视觉任务上进行DP微调，而使用现有方法在计算上是困难的。"
}
{
  "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch",
  "title_zh": "语言模型是超级马里奥：从同源模型中吸收能力作为一种免费午餐",
  "abstract": "In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly **D**rops delta parameters with a ratio $p$ **A**nd **RE**scales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard.",
  "abstract_zh": "在本文中，我们揭示了语言模型（LM）可以通过吸收同源模型的参数而无需重新训练或使用GPU来获得新能力。我们首先介绍DARE，以将大多数增量参数（即微调参数与预训练参数之间的差异）设置为零，而不影响监督微调（SFT）LM的能力，DARE随机以比例$p$**D**rop增量参数，并通过$1 / (1 - p)$重新缩放剩余参数，以近似原始嵌入。然后，我们将DARE用作多种SFT同源模型的通用插件，以稀疏化增量参数以减轻参数干扰，并通过参数融合将它们合并为单个模型。我们对基于编码器和解码器的LM进行了实验，结果显示：（1）SFT增量参数值范围通常较小（在0.002以内），且极度冗余，DARE可以轻松消除90%甚至99%的增量参数；（2）DARE可以将多个特定任务的LM合并为一个具有多样化能力的LM。值得注意的是，这种现象在大规模LM中更为明显，合并后的LM显示出超越任何源LM性能的潜力，提供了一项新发现。我们还利用DARE创建了一个合并的LM，该模型在开放LLM排行榜上以70亿参数的模型中排名第一。"
}
{
  "title": "MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models",
  "title_zh": "MAGDi：多智能体交互图的结构化蒸馏提升小型语言模型的推理能力",
  "abstract": "Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods that distill from a single teacher and multiple teachers. Moreover, MAGDi also demonstrates an order of magnitude higher efficiency over its teachers. We conduct extensive analyses to show that MAGDi (1) enhances the generalizability to out-of-domain tasks, (2) scales positively with the size and strength of the base student model, and (3) obtains larger improvements (via our multi-teacher training) when applying self-consistency – an inference technique that relies on model diversity.",
  "abstract_zh": "多智能体之间的交互在大型语言模型（LLM）中在多种推理任务上显示出显著的改进。然而，这些方法涉及多个模型在多个回合中的长生成过程，成本较高。此外，这些多智能体方法未能提供一个最终的单一模型以实现高效推理。为了解决这个问题，我们提出了MAGDi，这是一种将多个LLM之间的推理交互结构化蒸馏到小型语言模型中的新方法。MAGDi通过将多智能体交互表示为图，增强基础学生模型与图编码器的结合，并使用三个目标函数进行知识蒸馏：下一个标记预测、正确与错误推理之间的对比损失，以及基于图的目标以建模交互结构。在七个广泛使用的常识和数学推理基准上的实验表明，MAGDi提升了小型模型的推理能力，超越了几种从单一教师和多个教师进行蒸馏的方法。此外，MAGDi在效率上也比其教师高出一个数量级。我们进行了广泛的分析，表明MAGDi（1）增强了对领域外任务的泛化能力，（2）随着基础学生模型的大小和强度的增加而正向扩展，以及（3）在应用自一致性（依赖于模型多样性的推理技术）时，通过我们的多教师训练获得了更大的改进。"
}
{
  "title": "Prompting is a Double-Edged Sword: Improving Worst-Group Robustness of Foundation Models",
  "title_zh": "标题：提示是一把双刃剑：提高基础模型的最差群体鲁棒性",
  "abstract": "Machine learning models fail catastrophically under distribution shift, but a surprisingly effective way to empirically improve robustness to some types of shift (*e.g.*, Imagenet-A/C) is to use stronger open-vocabulary classifiers derived from foundation models. In this work, we first note that for shifts governed by spurious correlations (features spuriously correlated with the label on the training data, but not on test), the zero-shot and few-shot performance of foundation models is no better than ERM models, and remains unchanged when pretrained data/model size is scaled. Secondly, even in these situations, foundation models are quite accurate at predicting the value of the spurious feature. In a simplified setup, we theoretically analyze both these findings. Specifically, we show that during contrastive pretraining, the simplicity bias of foundation models tends to result in the learning of features that mostly rely on the spurious attribute, compared to more robust features. We leverage these observations to propose Prompting for Robustness (PfR) which first uses foundation models to zero-shot predict the spurious attribute on labeled examples, and then learns a classifier with balanced performance across different groups of labels and spurious attribute. Across 5 vision and language tasks, we show that PfR's performance nearly equals that of an oracle algorithm (group DRO) that leverages human labeled spurious attributes.",
  "abstract_zh": "摘要：机器学习模型在分布转移下会发生灾难性失败，但一种意外有效的方法是使用从基础模型派生的更强大的开放词汇分类器来经验性地提高对某些类型转移（例如，Imagenet-A/C）的鲁棒性。在这项工作中，我们首先注意到，对于由虚假相关性（训练数据中与标签虚假相关的特征，但在测试中不相关）主导的转移，基础模型的零样本和少样本性能与ERM模型没有区别，并且在预训练数据/模型规模扩大时保持不变。其次，即使在这些情况下，基础模型在预测虚假特征的值时也相当准确。在一个简化的设置中，我们理论上分析了这两个发现。具体而言，我们表明，在对比预训练期间，基础模型的简单性偏差往往导致学习主要依赖于虚假属性的特征，而不是更鲁棒的特征。我们利用这些观察提出了鲁棒性提示（PfR），该方法首先使用基础模型在标记示例上进行零样本预测虚假属性，然后学习一个在不同标签组和虚假属性之间具有平衡性能的分类器。在5个视觉和语言任务中，我们展示了PfR的性能几乎等同于利用人类标记虚假属性的oracle算法（群体DRO）。"
}
{
  "title": "Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning",
  "title_zh": "大型语言模型可以自动为少量样本表格学习工程特征",
  "abstract": "Large Language Models (LLMs), with their remarkable ability to tackle challenging and unseen reasoning problems, hold immense potential for tabular learning, that is vital for many real-world applications. In this paper, we propose a novel in-context learning framework, FeatLLM, which employs LLMs as feature engineers to produce an input data set that is optimally suited for tabular predictions. The generated features are used to infer class likelihood with a simple downstream machine learning model, such as linear regression and yields high performance few-shot learning. The proposed FeatLLM framework only uses this simple predictive model with the discovered features at inference time. Compared to existing LLM-based approaches, FeatLLM eliminates the need to send queries to the LLM for each sample at inference time. Moreover, it merely requires API-level access to LLMs, and overcomes prompt size limitations. As demonstrated across numerous tabular datasets from a wide range of domains, FeatLLM generates high-quality rules, significantly (10% on average) outperforming alternatives such as TabLLM and STUNT.",
  "abstract_zh": "大型语言模型（LLMs）凭借其出色的解决复杂和未知推理问题的能力，在表格学习中具有巨大的潜力，这对许多现实应用至关重要。本文提出了一种新颖的上下文学习框架FeatLLM，该框架利用LLMs作为特征工程师，生成最适合表格预测的输入数据集。生成的特征用于通过简单的下游机器学习模型（如线性回归）推断类别可能性，并实现高性能的少量样本学习。所提出的FeatLLM框架在推理时仅使用此简单预测模型和发现的特征。与现有的基于LLM的方法相比，FeatLLM消除了在推理时对每个样本发送查询到LLM的需求。此外，它仅需API级别的LLM访问，并克服了提示大小的限制。通过来自广泛领域的众多表格数据集的实验表明，FeatLLM生成高质量的规则，显著（平均提高10%）优于TabLLM和STUNT等替代方案。"
}
{
  "title": "Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition",
  "title_zh": "思维视频：从感知到认知的逐步视频推理",
  "abstract": "Existing research of video understanding still struggles to achieve in-depth comprehension and reasoning in complex videos, primarily due to the under-exploration of two key bottlenecks: fine-grained spatial-temporal perceptive understanding and cognitive-level video scene comprehension. This paper bridges the gap by presenting a novel solution. We first introduce a novel video Multimodal Large Language Model (MLLM), MotionEpic, which achieves fine-grained pixel-level spatial-temporal video grounding by integrating video spatial-temporal scene graph (STSG) representation. Building upon MotionEpic, we then develop a Video-of-Thought (VoT) reasoning framework. VoT inherits the Chain-of-Thought (CoT) core, breaking down a complex task into simpler and manageable sub-problems, and addressing them step-by-step from a low-level pixel perception to high-level cognitive interpretation. Extensive experiments across various complex video QA benchmarks demonstrate that our overall framework strikingly boosts existing state-of-the-art. To our knowledge, this is the first attempt at successfully implementing the CoT technique for achieving human-level video reasoning, where we show great potential in extending it to a wider range of video understanding scenarios. Systems and codes will be open later.",
  "abstract_zh": "现有的视频理解研究仍然在复杂视频中实现深入理解和推理方面面临挑战，主要是由于对两个关键瓶颈的探索不足：细粒度时空感知理解和认知层面的视频场景理解。本文通过提出一种新颖的解决方案来弥补这一空白。我们首先介绍了一种新的视频多模态大语言模型（MLLM），MotionEpic，它通过整合视频时空场景图（STSG）表示，实现了细粒度像素级时空视频定位。在MotionEpic的基础上，我们开发了一个思维视频（VoT）推理框架。VoT继承了思维链（CoT）的核心，将复杂任务分解为更简单和可管理的子问题，并从低级像素感知到高级认知解释逐步解决它们。在各种复杂视频问答基准上的大量实验表明，我们的整体框架显著提升了现有的最先进水平。据我们所知，这是首次成功实施CoT技术以实现人类水平的视频推理，我们展示了将其扩展到更广泛视频理解场景的巨大潜力。系统和代码将在稍后公开。"
}
{
  "title": "Amend to Alignment: Decoupled Prompt Tuning for Mitigating Spurious Correlation in Vision-Language Models",
  "title_zh": "标题：修正对齐：解耦提示调优以减轻视觉-语言模型中的虚假相关性",
  "abstract": "Fine-tuning the learnable prompt for a pre-trained vision-language model (VLM), such as CLIP, has demonstrated exceptional efficiency in adapting to a broad range of downstream tasks. Existing prompt tuning methods for VLMs do not distinguish spurious features introduced by biased training data from invariant features, and employ a uniform alignment process when adapting to unseen target domains. This can impair the cross-modal feature alignment when the testing data significantly deviate from the distribution of the training data, resulting in a poor out-of-distribution (OOD) generalization performance. In this paper, we reveal that the prompt tuning failure in such OOD scenarios can be attribute to the undesired alignment between the textual and the spurious feature. As a solution, we propose **CoOPood**, a fine-grained prompt tuning method that can discern the causal features and deliberately align the text modality with the invariant feature. Specifically, we design two independent contrastive phases using two lightweight projection layers during the alignment, each with different objectives: 1) pulling the text embedding closer to invariant image embedding and 2) pushing text embedding away from spurious image embedding. We have illustrated that **CoOPood** can serve as a general framework for VLMs and can be seamlessly integrated with existing prompt tuning methods. Extensive experiments on various OOD datasets demonstrate the performance superiority over state-of-the-art methods.",
  "abstract_zh": "摘要：对预训练视觉-语言模型（VLM），如CLIP，进行可学习提示的微调已显示出在适应广泛下游任务方面的卓越效率。现有的VLM提示调优方法未能区分由偏见训练数据引入的虚假特征与不变特征，并在适应未见目标领域时采用统一的对齐过程。这可能会在测试数据显著偏离训练数据分布时损害跨模态特征对齐，导致糟糕的分布外（OOD）泛化性能。本文揭示了在此类OOD场景中提示调优失败的原因在于文本特征与虚假特征之间的不良对齐。作为解决方案，我们提出了**CoOPood**，一种细粒度提示调优方法，能够识别因果特征并故意将文本模态与不变特征对齐。具体而言，我们在对齐过程中设计了两个独立的对比阶段，使用两个轻量级投影层，每个阶段有不同的目标：1）将文本嵌入拉近不变图像嵌入，2）将文本嵌入推远虚假图像嵌入。我们已证明**CoOPood**可以作为VLM的通用框架，并可以与现有的提示调优方法无缝集成。在各种OOD数据集上的大量实验表明，其性能优于最先进的方法。"
}
{
  "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
  "title_zh": "HarmBench：自动红队和强健拒绝的标准化评估框架",
  "abstract": "Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.",
  "abstract_zh": "自动红队在揭示和缓解与大语言模型（LLMs）恶意使用相关的风险方面具有巨大潜力，但该领域缺乏一个标准化的评估框架来严格评估新方法。为了解决这个问题，我们引入了HarmBench，一个用于自动红队的标准化评估框架。我们识别了在红队评估中之前未考虑的几个理想属性，并系统地设计了HarmBench以满足这些标准。使用HarmBench，我们对18种红队方法和33种目标LLM及防御进行了大规模比较，获得了新颖的见解。我们还引入了一种高效的对抗训练方法，极大地增强了LLM在多种攻击下的鲁棒性，展示了HarmBench如何促进攻击和防御的共同开发。我们在https://github.com/centerforaisafety/HarmBench上开源了HarmBench。"
}
{
  "title": "Dense Reward for Free in Reinforcement Learning from Human Feedback",
  "title_zh": "标题：基于人类反馈的强化学习中的密集奖励",
  "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many “actions” (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all without incurring extra computational cost or requiring any additional modelling. We demonstrate that, theoretically, this approach is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirically, we show that it stabilises training, accelerates the rate of learning, and, in practical cases, may lead to better local optima.",
  "abstract_zh": "摘要：基于人类反馈的强化学习（RLHF）被认为是使大型语言模型（LLMs）能够有效遵循指令并提供有用帮助的关键进展。传统上，这涉及在响应查询时从LLM生成完整内容，然后使用单独的奖励模型为完整内容分配分数。作为一种自回归过程，LLM必须采取许多“动作”（选择单个标记），并且仅在一个回合结束时收到一个稀疏的奖励，这种设置在传统强化学习中被认为难以优化。在这项工作中，我们利用奖励模型不仅仅是其标量输出的事实，特别是，它作为变换器架构的一部分计算标记的注意力图。我们使用这些注意力权重在整个完成过程中重新分配奖励，有效地密集化信号并突出最重要的标记，所有这些都不需要额外的计算成本或任何额外的建模。我们理论上证明，这种方法等同于基于潜力的奖励塑形，确保最优策略保持不变。经验上，我们展示了它稳定了训练，加速了学习速度，并且在实际案例中，可能导致更好的局部最优解。"
}
{
  "title": "AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for Transformers",
  "title_zh": "注意力感知的层级相关传播方法（AttnLRP）用于变换器",
  "abstract": "Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a single backward pass. Through extensive evaluations against existing methods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concept-based explanations. We provide an LRP library at https://github.com/rachtibat/LRP-eXplains-Transformers.",
  "abstract_zh": "大型语言模型容易产生偏见预测和幻觉，这突显了理解其模型内部推理过程的重要性。然而，实现对整个黑箱变换器模型的真实归因并保持计算效率仍然是一个未解决的挑战。通过扩展层级相关传播归因方法以处理注意力层，我们有效地解决了这些挑战。虽然已有部分解决方案，但我们的方法是首个能够真实且全面地归因于变换器模型的输入和潜在表示，并且计算效率与单次反向传播相似。通过对LLaMa 2、Mixtral 8x7b、Flan-T5和视觉变换器架构的广泛评估，我们证明了所提方法在真实度方面超越了其他方法，并使潜在表示的理解成为可能，为基于概念的解释打开了大门。我们在https://github.com/rachtibat/LRP-eXplains-Transformers提供了LRP库。"
}
{
  "title": "One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts",
  "title_zh": "标题：一个提示不够：混合专家提示的自动构建",
  "abstract": "Large Language Models (LLMs) exhibit strong generalization capabilities to novel tasks when prompted with language instructions and in-context demos. Since this ability sensitively depends on the quality of prompts, various methods have been explored to automate the instruction design. While these methods demonstrated promising results, they also restricted the searched prompt to one instruction. Such simplification significantly limits their capacity, as a single demo-free instruction might not be able to cover the entire complex problem space of the targeted task. To alleviate this issue, we adopt the Mixture-of-Expert paradigm and divide the problem space into a set of sub-regions; Each sub-region is governed by a specialized expert, equipped with both an instruction and a set of demos. A two-phase process is developed to construct the specialized expert for each region: (1) demo assignment: Inspired by the theoretical connection between in-context learning and kernel regression, we group demos into experts based on their semantic similarity; (2) instruction assignment: A region-based joint search of an instruction per expert complements the demos assigned to it, yielding a synergistic effect. The resulting method, codenamed Mixture-of-Prompts (MoP), achieves an average win rate of 81% against prior arts across several major benchmarks.",
  "abstract_zh": "摘要：大型语言模型（LLMs）在接受语言指令和上下文示例提示时，展现出对新任务的强大泛化能力。由于这一能力敏感地依赖于提示的质量，因此探索了多种方法来自动化指令设计。尽管这些方法展示了良好的结果，但它们也将搜索的提示限制为单一指令。这种简化显著限制了它们的能力，因为单一的无示例指令可能无法覆盖目标任务的整个复杂问题空间。为了解决这一问题，我们采用混合专家范式，将问题空间划分为一组子区域；每个子区域由一个专门的专家管理，配备指令和一组示例。我们开发了一个两阶段的过程来为每个区域构建专门的专家：（1）示例分配：受到上下文学习与核回归之间理论联系的启发，我们根据示例的语义相似性将其分组为专家；（2）指令分配：对每个专家进行基于区域的联合搜索，以补充分配给它的示例，从而产生协同效应。最终的方法，代号为混合提示（MoP），在多个主要基准测试中实现了81%的平均胜率，优于之前的研究成果。"
}
{
  "title": "Position: Leverage Foundational Models for Black-Box Optimization",
  "title_zh": "标题：立场：利用基础模型进行黑箱优化",
  "abstract": "Undeniably, Large Language Models (LLMs) have stirred an extraordinary wave of innovation in the machine learning research domain, resulting in substantial impact across diverse fields such as reinforcement learning, robotics, and computer vision. Their incorporation has been rapid and transformative, marking a significant paradigm shift in the field of machine learning research. However, the field of experimental design, grounded on black-box optimization, has been much less affected by such a paradigm shift, even though integrating LLMs with optimization presents a unique landscape ripe for exploration. In this position paper, we frame the field of black-box optimization around sequence-based foundation models and organize their relationship with previous literature. We discuss the most promising ways foundational language models can revolutionize optimization, which include harnessing the vast wealth of information encapsulated in free-form text to enrich task comprehension, utilizing highly flexible sequence models such as Transformers to engineer superior optimization strategies, and enhancing performance prediction over previously unseen search spaces.",
  "abstract_zh": "摘要：不可否认，大型语言模型（LLMs）在机器学习研究领域引发了非凡的创新浪潮，对强化学习、机器人技术和计算机视觉等多个领域产生了重大影响。尽管将LLMs与优化结合提供了一个独特的探索空间，但基于黑箱优化的实验设计领域受到的影响却相对较小。在这篇立场论文中，我们围绕基于序列的基础模型构建黑箱优化领域，并整理它们与先前文献的关系。我们讨论了基础语言模型如何革新优化的最有前景的方法，包括利用自由形式文本中蕴含的大量信息来丰富任务理解，利用如Transformer等高度灵活的序列模型来设计更优的优化策略，以及提高对以前未见搜索空间的性能预测。"
}
{
  "title": "StrokeNUWA—Tokenizing Strokes for Vector Graphic Synthesis",
  "title_zh": "标题：StrokeNUWA——用于矢量图形合成的笔画标记",
  "abstract": "To leverage LLMs for visual synthesis, traditional methods convert raster image information into discrete grid tokens through specialized visual modules, while disrupting the model’s ability to capture the true semantic representation of visual scenes. This paper posits that an alternative representation of images, vector graphics, can effectively surmount this limitation by enabling a more natural and semantically coherent segmentation of the image information. Thus, we introduce StrokeNUWA, a pioneering work exploring a better visual representation \"stroke\" tokens on vector graphics, which is inherently visual semantics rich, naturally compatible with LLMs, and highly compressed. Equipped with stroke tokens, StrokeNUWA can significantly surpass traditional LLM-based and optimization-based methods across various metrics in the vector graphic generation task. Besides, StrokeNUWA achieves up to a $94\\times$ speedup in inference over the speed of prior methods with an exceptional SVG code compression ratio of 6.9%.",
  "abstract_zh": "摘要：为了利用大型语言模型（LLMs）进行视觉合成，传统方法通过专门的视觉模块将光栅图像信息转换为离散的网格标记，但这会破坏模型捕捉视觉场景真实语义表示的能力。本文提出，图像的另一种表示方式——矢量图形，可以有效克服这一限制，从而实现对图像信息的更自然和语义一致的分割。因此，我们引入了StrokeNUWA，这是一项开创性的工作，探索在矢量图形上更好的视觉表示“笔画”标记，这种表示本质上语义丰富，自然兼容LLMs，并且高度压缩。配备笔画标记的StrokeNUWA在矢量图形生成任务中可以在各项指标上显著超越传统的基于LLM和优化的方法。此外，StrokeNUWA在推理速度上比之前的方法实现了高达$94\\times$的加速，并具有6.9%的卓越SVG代码压缩比。"
}
{
  "title": "Graph-enhanced Large Language Models in Asynchronous Plan Reasoning",
  "title_zh": "图增强的大型语言模型在异步计划推理中的应用",
  "abstract": "Planning is a fundamental property of human intelligence. Reasoning about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can large language models (LLMs) succeed at this task? Here, we present the first large-scale study investigating this question. We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called *Plan Like a Graph* (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results. We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. We see our study as an exciting step towards using LLMs as efficient autonomous agents. Our code and data are available at https://github.com/fangru-lin/graph-llm-asynchow-plan.",
  "abstract_zh": "计划是人类智能的基本特性。推理异步计划具有挑战性，因为它需要顺序和并行规划以优化时间成本。大型语言模型（LLMs）能否成功完成这一任务？在这里，我们呈现了第一个大规模研究来探讨这个问题。我们发现，包括GPT-4和LLaMA-2在内的一组具有代表性的闭源和开源LLMs在没有提供关于任务解决过程的示例时，在我们的基准AsyncHow中表现不佳。我们提出了一种名为*Plan Like a Graph*（PLaG）的新技术，将图与自然语言提示相结合，并取得了最先进的结果。我们展示了尽管PLaG可以提升模型性能，但当任务复杂性增加时，LLMs仍然会遭遇严重的性能下降，突显了利用LLMs模拟数字设备的局限性。我们将我们的研究视为朝着将LLMs作为高效自主代理的重要一步。我们的代码和数据可在https://github.com/fangru-lin/graph-llm-asynchow-plan获取。"
}
{
  "title": "Advancing DRL Agents in Commercial Fighting Games: Training, Integration, and Agent-Human Alignment",
  "title_zh": "在商业格斗游戏中推进深度强化学习代理：训练、集成与代理-人类对齐",
  "abstract": "Deep Reinforcement Learning (DRL) agents have demonstrated impressive success in a wide range of game genres. However, existing research primarily focuses on optimizing DRL competence rather than addressing the challenge of prolonged player interaction. In this paper, we propose a practical DRL agent system for fighting games named _Shūkai_, which has been successfully deployed to Naruto Mobile, a popular fighting game with over 100 million registered users. _Shūkai_ quantifies the state to enhance generalizability, introducing Heterogeneous League Training (HELT) to achieve balanced competence, generalizability, and training efficiency. Furthermore, _Shūkai_ implements specific rewards to align the agent's behavior with human expectations. _Shūkai_'s ability to generalize is demonstrated by its consistent competence across all characters, even though it was trained on only 13% of them. Additionally, HELT exhibits a remarkable 22% improvement in sample efficiency. _Shūkai_ serves as a valuable training partner for players in Naruto Mobile, enabling them to enhance their abilities and skills.",
  "abstract_zh": "深度强化学习（DRL）代理在多种游戏类型中表现出色。然而，现有研究主要集中在优化DRL能力上，而未解决长期玩家互动的挑战。本文提出了一种名为_Shūkai_的实用DRL代理系统，已成功部署于拥有超过1亿注册用户的热门格斗游戏《火影忍者手游》。_Shūkai_量化状态以增强泛化能力，引入异构联赛训练（HELT）以实现能力、泛化性和训练效率的平衡。此外，_Shūkai_实施特定奖励，以使代理的行为与人类期望对齐。_Shūkai_的泛化能力通过其在所有角色中的一致能力得以证明，尽管它仅在13%的角色上进行训练。此外，HELT在样本效率上展现出显著的22%提升。_Shūkai_为《火影忍者手游》的玩家提供了宝贵的训练伙伴，使他们能够提升自己的能力和技能。"
}
{
  "title": "RoboMP$^2$: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models",
  "title_zh": "RoboMP$^2$: 一种基于多模态大型语言模型的机器人多模态感知-规划框架",
  "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive reasoning abilities and general intelligence in various domains. It inspires researchers to train end-to-end MLLMs or utilize large models to generate policies with human-selected prompts for embodied agents. However, these methods exhibit limited generalization capabilities on unseen tasks or scenarios, and overlook the multimodal environment information which is critical for robots to make decisions. In this paper, we introduce a novel **Robo**tic **M**ultimodal **P**erception-**P**lanning (**RoboMP$^2$**) framework for robotic manipulation which consists of a Goal-Conditioned Multimodal Preceptor (GCMP) and a Retrieval-Augmented Multimodal Planner (RAMP). Specially, GCMP captures environment states by employing a tailored MLLMs for embodied agents with the abilities of semantic reasoning and localization. RAMP utilizes coarse-to-fine retrieval method to find the $k$ most-relevant policies as in-context demonstrations to enhance the planner. Extensive experiments demonstrate the superiority of RoboMP$^2$ on both VIMA benchmark and real-world tasks, with around 10% improvement over the baselines.",
  "abstract_zh": "多模态大型语言模型（MLLMs）在各个领域展示了令人印象深刻的推理能力和通用智能。这激励研究人员训练端到端的MLLMs或利用大型模型生成带有人类选择提示的策略以供具身代理使用。然而，这些方法在未见任务或场景上的泛化能力有限，并忽视了对机器人决策至关重要的多模态环境信息。本文介绍了一种新颖的**Robo**tic **M**ultimodal **P**erception-**P**lanning（**RoboMP$^2$**）框架，用于机器人操作，包括一个目标条件的多模态感知器（GCMP）和一个检索增强的多模态规划器（RAMP）。特别地，GCMP通过为具身代理量身定制的MLLMs捕获环境状态，具备语义推理和定位能力。RAMP利用粗到细的检索方法找到$k$个最相关的策略作为上下文示范，以增强规划器。大量实验表明，RoboMP$^2$在VIMA基准和现实世界任务上优于基线，提升约10%。"
}
{
  "title": "OLLIE: Imitation Learning from Offline Pretraining to Online Finetuning",
  "title_zh": "标题：OLLIE：从离线预训练到在线微调的模仿学习",
  "abstract": "In this paper, we study offline-to-online Imitation Learning (IL) that pretrains an imitation policy from static demonstration data, followed by fast finetuning with minimal environmental interaction. We find the naive combination of existing offline IL and online IL methods tends to behave poorly in this context, because the initial discriminator (often used in online IL) operates randomly and discordantly against the policy initialization, leading to misguided policy optimization and *unlearning* of pretraining knowledge. To overcome this challenge, we propose a principled offline-to-online IL method, named OLLIE, that simultaneously learns a near-expert policy initialization along with an *aligned discriminator initialization*, which can be seamlessly integrated into online IL, achieving smooth and fast finetuning. Empirically, OLLIE consistently and significantly outperforms the baseline methods in **20** challenging tasks, from continuous control to vision-based domains, in terms of performance, demonstration efficiency, and convergence speed. This work may serve as a foundation for further exploration of pretraining and finetuning in the context of IL.",
  "abstract_zh": "摘要：在本文中，我们研究了离线到在线的模仿学习（IL），该方法从静态演示数据中预训练一个模仿策略，然后通过最小的环境交互进行快速微调。我们发现现有的离线IL和在线IL方法的简单组合在这种情况下表现较差，因为初始判别器（通常用于在线IL）随机且与策略初始化不一致，导致策略优化误导和预训练知识的“遗忘”。为了解决这个挑战，我们提出了一种原则性的离线到在线IL方法，命名为OLLIE，它同时学习近专家的策略初始化和“对齐的判别器初始化”，可以无缝集成到在线IL中，实现平滑快速的微调。实证结果表明，OLLIE在**20**个具有挑战性的任务中（从连续控制到基于视觉的领域）在性能、演示效率和收敛速度方面始终显著优于基线方法。这项工作可能为进一步探索IL背景下的预训练和微调奠定基础。"
}
{
  "title": "Break the Sequential Dependency of LLM Inference Using Lookahead Decoding",
  "title_zh": "打破大型语言模型推理的顺序依赖：使用前瞻解码",
  "abstract": "Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding",
  "abstract_zh": "大型语言模型（LLMs）的自回归解码受到内存带宽的限制，导致高延迟和现代加速器的并行处理能力的显著浪费。现有的加速LLM解码的方法通常需要草稿模型（例如，推测解码），而获取这些模型并不简单且无法泛化。本文介绍了前瞻解码，这是一种精确的并行解码算法，可以加速LLM解码，而无需辅助模型或数据存储。它允许通过每步的log(FLOPs)交易来减少总解码步骤的数量，在单个或多个现代加速器上更具并行性，并与并发内存高效注意力（例如，FlashAttention）兼容。我们对前瞻解码的实现可以在MT-bench上将自回归解码速度提高至1.8倍，并在多个GPU的代码补全任务中实现强扩展下的4倍加速。我们的代码可在https://github.com/hao-ai-lab/LookaheadDecoding获取。"
}
{
  "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation",
  "title_zh": "标题：通过基于代理的不确定性估计改善语言模型的指令跟随能力",
  "abstract": "Assessing response quality to instructions in language models is vital but challenging due to the complexity of human language across different contexts. This complexity often results in ambiguous or inconsistent interpretations, making accurate assessment difficult. To address this issue, we propose a novel Uncertainty-aware Reward Model (URM) that introduces a robust uncertainty estimation for the quality of paired responses based on Bayesian approximation. Trained with preference datasets, our uncertainty-enabled proxy not only scores rewards for responses but also evaluates their inherent uncertainty. Empirical results demonstrate significant benefits of incorporating the proposed proxy into language model training. Our method boosts the instruction following capability of language models by refining data curation for training and improving policy optimization objectives, thereby surpassing existing methods by a large margin on benchmarks such as Vicuna and MT-bench. These findings highlight that our proposed approach substantially advances language model training and paves a new way of harnessing uncertainty within language models.",
  "abstract_zh": "摘要：评估语言模型对指令的响应质量至关重要，但由于人类语言在不同语境中的复杂性，这一过程充满挑战。这种复杂性常常导致模糊或不一致的解释，使得准确评估变得困难。为了解决这个问题，我们提出了一种新颖的不确定性感知奖励模型（URM），该模型基于贝叶斯近似引入了对配对响应质量的强大不确定性估计。通过偏好数据集进行训练，我们的不确定性代理不仅为响应评分奖励，还评估其固有的不确定性。实证结果表明，将所提出的代理纳入语言模型训练中具有显著的益处。我们的方法通过优化训练数据的策划和改善策略优化目标，提升了语言模型的指令跟随能力，从而在Vicuna和MT-bench等基准测试中大幅超越现有方法。这些发现突显了我们提出的方法在语言模型训练中的重大进展，并为在语言模型中利用不确定性开辟了新途径。"
}
{
  "title": "Position: A Call for Embodied AI",
  "title_zh": "标题：位置：对具身人工智能的呼吁",
  "abstract": "We propose Embodied AI (E-AI) as the next fundamental step in the pursuit of Artificial General Intelligence (AGI), juxtaposing it against current AI advancements, particularly Large Language Models (LLMs). We traverse the evolution of the embodiment concept across diverse fields (philosophy, psychology, neuroscience, and robotics) to highlight how E-AI distinguishes itself from the classical paradigm of static learning. By broadening the scope of E-AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston’s active inference principle, offering a comprehensive approach to E-AI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future E-AI research. Highlighting the importance of creating E-AI agents capable of seamless communication, collaboration, and coexistence with humans and other intelligent entities within real-world environments, we aim to steer the AI community towards addressing the multifaceted challenges and seizing the opportunities that lie ahead in the quest for AGI.",
  "abstract_zh": "摘要：我们提出具身人工智能（E-AI）作为追求人工通用智能（AGI）的下一个基本步骤，将其与当前的人工智能进展，特别是大型语言模型（LLMs）进行对比。我们探讨了具身概念在哲学、心理学、神经科学和机器人等多个领域的发展，强调E-AI如何与经典的静态学习范式区分开来。通过拓宽E-AI的范围，我们引入了一个基于认知架构的理论框架，强调感知、行动、记忆和学习作为具身代理的基本组成部分。该框架与Friston的主动推理原则相一致，为E-AI的发展提供了全面的方法。尽管人工智能领域取得了进展，但仍然存在诸多挑战，例如新型人工智能学习理论的制定和先进硬件的创新。我们的讨论为未来的E-AI研究奠定了基础指导方针。强调创建能够与人类和其他智能实体在现实环境中无缝沟通、协作和共存的E-AI代理的重要性，我们旨在引导人工智能社区应对多方面的挑战，并抓住在追求AGI过程中面临的机遇。"
}
{
  "title": "Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning",
  "title_zh": "时刻器：通过细粒度时间推理推进视频大型语言模型",
  "abstract": "Large Language Models (LLMs) demonstrate remarkable proficiency in comprehending and handling text-based tasks. Many efforts are being made to transfer these attributes to video modality, which are termed Video-LLMs. However, existing Video-LLMs can only capture the coarse-grained semantics and are unable to effectively handle tasks related to comprehension or localization of specific video segments. In light of these challenges, we propose Momentor, a Video-LLM capable of accomplishing fine-grained temporal understanding tasks. To support the training of Momentor, we design an automatic data generation engine to construct Moment-10M, a large-scale video instruction dataset with segment-level instruction data. We train Momentor on Moment-10M, enabling it to perform segment-level reasoning and localization. Zero-shot evaluations on several tasks demonstrate that Momentor excels in fine-grained temporally grounded comprehension and localization.",
  "abstract_zh": "大型语言模型（LLMs）在理解和处理基于文本的任务方面表现出色。许多努力正致力于将这些特性转移到视频模态上，这被称为视频LLMs。然而，现有的视频LLMs只能捕捉粗粒度语义，无法有效处理与特定视频片段的理解或定位相关的任务。针对这些挑战，我们提出了时刻器（Momentor），一种能够完成细粒度时间理解任务的视频LLM。为了支持时刻器的训练，我们设计了一个自动数据生成引擎，构建了Moment-10M，这是一个具有片段级指令数据的大规模视频指令数据集。我们在Moment-10M上训练时刻器，使其能够执行片段级推理和定位。在多个任务上的零-shot评估表明，时刻器在细粒度时间基础的理解和定位方面表现优异。"
}
{
  "title": "Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression",
  "title_zh": "解码压缩信任：审视高效大型语言模型在压缩下的可信度",
  "abstract": "Compressing high-capability Large Language Models (LLMs) has emerged as a favored strategy for resource-efficient inferences. While state-of-the-art (SoTA) compression methods boast impressive advancements in preserving benign task performance, the potential risks of compression in terms of safety and trustworthiness have been largely neglected. This study conducts the first, thorough evaluation of **three (3) leading LLMs** using **five (5) SoTA compression techniques** across **eight (8) trustworthiness dimensions**. Our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns. We find that quantization is currently a more effective approach than pruning in achieving efficiency and trustworthiness simultaneously. For instance, a 4-bit quantized model retains the trustworthiness of its original counterpart, but model pruning significantly degrades trustworthiness, even at 50% sparsity. Moreover, employing quantization within a moderate bit range could unexpectedly improve certain trustworthiness dimensions such as ethics and fairness. Conversely, extreme quantization to very low bit levels (3 bits) tends to reduce trustworthiness significantly. This increased risk cannot be uncovered by looking at benign performance alone, in turn, mandating comprehensive trustworthiness evaluation in practice. These findings culminate in practical recommendations for simultaneously achieving high utility, efficiency, and trustworthiness in LLMs. Code and models are available at https://decoding-comp-trust.github.io.",
  "abstract_zh": "压缩高能力的大型语言模型（LLMs）已成为资源高效推理的热门策略。尽管最先进的压缩方法在保持良性任务性能方面取得了令人印象深刻的进展，但压缩在安全性和可信度方面的潜在风险却在很大程度上被忽视。本研究首次对**三（3）个领先的LLM**使用**五（5）种最先进的压缩技术**进行全面评估，涵盖**八（8）个可信度维度**。我们的实验突显了压缩与可信度之间的复杂相互作用，揭示了一些有趣的模式。我们发现量化在同时实现效率和可信度方面目前比剪枝更有效。例如，4位量化模型保留了其原始对应模型的可信度，但模型剪枝在50%稀疏度时显著降低了可信度。此外，在适中的位范围内使用量化可能意外改善某些可信度维度，如伦理和公平性。相反，极端量化到非常低的位数（3位）往往会显著降低可信度。这种增加的风险无法仅通过观察良性性能来发现，因此在实践中需要全面的可信度评估。这些发现汇总为在LLM中同时实现高效用、高效性和可信度的实用建议。代码和模型可在https://decoding-comp-trust.github.io获取。"
}
{
  "title": "Discovering Bias in Latent Space: An Unsupervised Debiasing Approach",
  "title_zh": "在潜在空间中发现偏见：一种无监督去偏见方法",
  "abstract": "The question-answering (QA) capabilities of foundation models are highly sensitive to prompt variations, rendering their performance susceptible to superficial, non-meaning-altering changes. This vulnerability often stems from the model's preference or bias towards specific input characteristics, such as option position or superficial image features in multi-modal settings. We propose to rectify this bias directly in the model's internal representation. Our approach, SteerFair, finds the bias direction in the model's representation space and steers activation values away from it during inference. Specifically, we exploit the observation that bias often adheres to simple association rules, such as the spurious association between the first option and correctness likelihood. Next, we construct demonstrations of these rules from unlabeled samples and use them to identify the bias directions. We empirically show that SteerFair significantly reduces instruction-tuned model performance variance across prompt modifications on three benchmark tasks. Remarkably, our approach surpasses a supervised baseline with 100 labels by an average of 10.86% accuracy points and 12.95 score points and matches the performance with 500 labels.",
  "abstract_zh": "基础模型的问答（QA）能力对提示变化高度敏感，使其性能容易受到表面、非意义改变的影响。这种脆弱性通常源于模型对特定输入特征的偏好或偏见，例如在多模态设置中选项位置或表面图像特征。我们提出直接在模型的内部表示中纠正这种偏见。我们的方法SteerFair在模型的表示空间中找到偏见方向，并在推理过程中将激活值引导远离该方向。具体而言，我们利用偏见通常遵循简单关联规则的观察，例如第一个选项与正确性可能性之间的虚假关联。接下来，我们从未标记样本中构建这些规则的示例，并利用它们来识别偏见方向。我们通过实验证明，SteerFair显著减少了在三个基准任务上提示修改时指令调优模型性能的方差。值得注意的是，我们的方法在平均准确率上超越了具有100个标签的监督基线10.86个百分点和12.95分，并与500个标签的性能相匹配。"
}
{
  "title": "Causality Based Front-door Defense Against Backdoor Attack on Language Models",
  "title_zh": "基于因果关系的前门防御机制对抗语言模型的后门攻击",
  "abstract": "We have developed a new framework based on the theory of causal inference to protect language models against backdoor attacks. Backdoor attackers can poison language models with different types of triggers, such as words, sentences, grammar, and style, enabling them to selectively modify the decision-making of the victim model. However, existing defense approaches are only effective when the backdoor attack form meets specific assumptions, making it difficult to counter diverse backdoor attacks. We propose a new defense framework **F**ront-door **A**djustment for **B**ackdoor **E**limination (FABE) based on causal reasoning that does not rely on assumptions about the form of triggers. This method effectively differentiates between spurious and legitimate associations by creating a 'front door' that maps out the actual causal relationships. The term 'front door' refers to a text that retains the semantic equivalence of the initial input, which is generated by an additional, fine-tuned language model, denoted as the defense model. Our defense experiments against various attack methods at the token, sentence, and syntactic levels reduced the attack success rate from 93.63% to 15.12%, improving the defense effect by 2.91 times compared to the best baseline result of 66.61%, achieving state-of-the-art results. Through ablation study analysis, we analyzed the effect of each module in FABE, demonstrating the importance of complying with the front-door criterion and front-door adjustment formula, which also explains why previous methods failed. Our code to reproduce the experiments is available at: https://github.com/lyr17/Frontdoor-Adjustment-Backdoor-Elimination.",
  "abstract_zh": "我们开发了一种基于因果推断理论的新框架，以保护语言模型免受后门攻击。后门攻击者可以通过不同类型的触发器（如单词、句子、语法和风格）对语言模型进行污染，从而选择性地修改受害模型的决策。然而，现有的防御方法仅在后门攻击形式符合特定假设时有效，这使得抵御多样化的后门攻击变得困难。我们提出了一种新的防御框架**F**ront-door **A**djustment for **B**ackdoor **E**limination（FABE），基于因果推理，不依赖于对触发器形式的假设。该方法通过创建一个“前门”来有效区分虚假和合法的关联，映射出实际的因果关系。“前门”一词指的是保留初始输入语义等价性的文本，该文本由一个额外的、经过微调的语言模型生成，称为防御模型。我们针对不同攻击方法在标记、句子和句法层面的防御实验将攻击成功率从93.63%降低到15.12%，与最佳基线结果66.61%相比，防御效果提高了2.91倍，达到了最先进的结果。通过消融研究分析，我们分析了FABE中每个模块的效果，证明了遵循前门标准和前门调整公式的重要性，这也解释了为什么之前的方法失败。我们的实验重现代码可在以下网址获取：https://github.com/lyr17/Frontdoor-Adjustment-Backdoor-Elimination。"
}
{
  "title": "LQER: Low-Rank Quantization Error Reconstruction for LLMs",
  "title_zh": "LQER：大语言模型的低秩量化误差重构",
  "abstract": "Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce **L**ow-rank **Q**uantization **E**rror **R**eduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-based iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using $1.36 \\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework at [https://github.com/ChengZhang-98/lqer](https://github.com/ChengZhang-98/lqer)",
  "abstract_zh": "对大语言模型（LLMs）进行后训练量化具有挑战性。在本研究中，我们提出了低秩量化误差减少（LQER），它结合了量化和低秩近似，以恢复模型能力。LQER利用激活引起的尺度矩阵，将量化误差的奇异值分布驱动到理想分布，从而实现对各种LLMs和下游任务的几乎无损W4A8量化，而无需知识蒸馏、网格搜索或基于梯度的迭代优化。与现有方法不同，LQER的计算模式消除了从不规则内存位置收集高精度权重所需的专门散布和聚集过程。我们的W4A8 LLM在六个流行的下游任务上实现了近乎无损的性能，同时使用的硬件资源比领先的最先进方法少$1.36 \\times$。我们将开源我们的框架，网址为[https://github.com/ChengZhang-98/lqer](https://github.com/ChengZhang-98/lqer)"
}
{
  "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
  "title_zh": "标题：MLLM作为评判者：通过视觉-语言基准评估多模态LLM作为评判者",
  "abstract": "Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, a closer examination reveals persistent challenges in the evaluative capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: https://mllm-judge.github.io/.",
  "abstract_zh": "摘要：多模态大型语言模型（MLLM）最近引起了广泛关注，展现出在人工通用智能方面的显著潜力。然而，评估MLLM的实用性面临相当大的挑战，主要是由于缺乏与人类偏好一致的多模态基准。受LLM作为评判者这一概念的启发，本文引入了一种新颖的基准，称为MLLM作为评判者，以评估MLLM在不同模态中辅助评判者的能力，包括三个不同的任务：评分评估、配对比较和批量排名。我们的研究表明，尽管MLLM在配对比较中展现出显著的人类般的辨别能力，但在评分评估和批量排名任务中与人类偏好存在显著差异。此外，进一步的检查揭示了LLM在评估能力方面的持续挑战，包括多样的偏见、幻觉响应和判断不一致，即使在像GPT-4V这样的先进模型中也是如此。这些发现强调了在将MLLM视为完全可靠的评估者之前，迫切需要进行改进和进一步的研究工作。鉴于此，我们倡导为支持MLLM作为评判者的持续发展而付出更多努力。代码和数据集可在我们的项目主页公开获取：https://mllm-judge.github.io/。"
}
{
  "title": "Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation",
  "title_zh": "理解语言模型推理能力的路径聚合视角",
  "abstract": "Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and chain-of-thought (CoT) reasoning. More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and CoT datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step reasoning performance.",
  "abstract_zh": "预训练语言模型（LMs）能够在没有明确微调的情况下执行复杂推理。为了理解以下一个标记预测目标进行预训练如何促成这种推理能力的出现，我们提出可以将语言模型视为通过聚合在预训练时看到的间接推理路径来得出新结论。我们发现这种视角在逻辑推理与知识图谱（KGs）以及思维链（CoT）推理的两个重要案例中有效。更具体地，我们将推理路径形式化为知识/推理图上的随机游走路径。对学习到的语言模型分布的分析表明，相关随机游走路径概率的加权和是解释语言模型如何推理的合理方法。对多个知识图谱和思维链数据集的实验和分析揭示了在随机游走路径上训练的效果，并建议增强未标记的随机游走推理路径可以提高现实世界的多步推理性能。"
}
{
  "title": "Stop Regressing: Training Value Functions via Classification for Scalable Deep RL",
  "title_zh": "停止回归：通过分类训练价值函数以实现可扩展的深度强化学习",
  "abstract": "Value functions are an essential component in deep reinforcement learning (RL), that are typically trained via mean squared error regression to match bootstrapped target values. However, scaling value-based RL methods to large networks has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We show that training value functions with categorical cross-entropy significantly enhances performance and scalability across various domains, including single-task RL on Atari 2600 games, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving *state-of-the-art results* on these domains. Through careful analysis, we show that categorical cross-entropy mitigates issues inherent to value-based RL, such as noisy targets and non-stationarity. We argue that shifting to categorical cross-entropy for training value functions can substantially improve the scalability of deep RL at little-to-no cost.",
  "abstract_zh": "价值函数是深度强化学习（RL）中的一个重要组成部分，通常通过均方误差回归来训练以匹配自举目标值。然而，将基于价值的RL方法扩展到大型网络一直是一个挑战。与监督学习形成鲜明对比的是：通过利用交叉熵分类损失，监督方法已经可靠地扩展到大规模网络。观察到这一差异，本文探讨了是否仅通过使用分类代替回归来训练价值函数，也能改善深度RL的可扩展性。我们展示了使用类别交叉熵训练价值函数显著提升了在多个领域的性能和可扩展性，包括在Atari 2600游戏上的单任务RL、在大规模ResNet上的Atari多任务RL、使用Q-transformers的机器人操作、无搜索的国际象棋游戏，以及使用高容量Transformers的语言代理Wordle任务，在这些领域取得了*最先进的结果*。通过仔细分析，我们表明类别交叉熵缓解了基于价值的RL固有的问题，如噪声目标和非平稳性。我们认为，转向类别交叉熵来训练价值函数可以在几乎没有成本的情况下显著提高深度RL的可扩展性。"
}
{
  "title": "Compositional Text-to-Image Generation with Dense Blob Representations",
  "title_zh": "标题：基于密集斑点表示的组合文本到图像生成",
  "abstract": "Existing text-to-image models struggle to follow complex text prompts, raising the need for extra grounding inputs for better controllability. In this work, we propose to decompose a scene into visual primitives - denoted as dense blob representations - that contain fine-grained details of the scene while being modular, human-interpretable, and easy-to-construct. Based on blob representations, we develop a blob-grounded text-to-image diffusion model, termed BlobGEN, for compositional generation. Particularly, we introduce a new masked cross-attention module to disentangle the fusion between blob representations and visual features. To leverage the compositionality of large language models (LLMs), we introduce a new in-context learning approach to generate blob representations from text prompts. Our extensive experiments show that BlobGEN achieves superior zero-shot generation quality and better layout-guided controllability on MS-COCO. When augmented by LLMs, our method exhibits superior numerical and spatial correctness on compositional image generation benchmarks.",
  "abstract_zh": "摘要：现有的文本到图像模型在处理复杂文本提示时表现不佳，因此需要额外的基础输入以提高可控性。在本研究中，我们提出将场景分解为视觉原语——称为密集斑点表示——这些表示包含场景的细粒度细节，同时具有模块化、易于人类理解和构建的特点。基于斑点表示，我们开发了一种基于斑点的文本到图像扩散模型，称为BlobGEN，用于组合生成。特别地，我们引入了一种新的掩蔽交叉注意力模块，以解开斑点表示与视觉特征之间的融合。为了利用大型语言模型（LLMs）的组合性，我们引入了一种新的上下文学习方法，从文本提示中生成斑点表示。我们的广泛实验表明，BlobGEN在MS-COCO上实现了优越的零-shot生成质量和更好的布局引导可控性。当通过LLMs增强时，我们的方法在组合图像生成基准上表现出优越的数值和空间正确性。"
}
{
  "title": "Position: A Safe Harbor for AI Evaluation and Red Teaming",
  "title_zh": "位置：人工智能评估和红队测试的安全港",
  "abstract": "Independent evaluation and red teaming are critical for identifying the risks posed by generative AI systems. However, the terms of service and enforcement strategies used by prominent AI companies to deter model misuse have disincentives on good faith safety evaluations. This causes some researchers to fear that conducting such research or releasing their findings will result in account suspensions or legal reprisal. Although some companies offer researcher access programs, they are an inadequate substitute for independent research access, as they have limited community representation, receive inadequate funding, and lack independence from corporate incentives. We propose that major generative AI developers commit to providing a legal and technical safe harbor, protecting public interest safety research and removing the threat of account suspensions or legal reprisal. These proposals emerged from our collective experience conducting safety, privacy, and trustworthiness research on generative AI systems, where norms and incentives could be better aligned with public interests, without exacerbating model misuse. We believe these commitments are a necessary step towards more inclusive and unimpeded community efforts to tackle the risks of generative AI.",
  "abstract_zh": "独立评估和红队测试对于识别生成性人工智能系统所带来的风险至关重要。然而，主要人工智能公司用于阻止模型滥用的服务条款和执行策略对善意的安全评估产生了负面影响。这使得一些研究人员担心进行此类研究或发布他们的发现会导致账户暂停或法律报复。尽管一些公司提供研究人员访问计划，但由于缺乏社区代表性、资金不足以及缺乏与企业激励的独立性，这些计划并不能替代独立研究访问。我们建议主要的生成性人工智能开发者承诺提供法律和技术安全港，保护公共利益的安全研究，并消除账户暂停或法律报复的威胁。这些提议源于我们在生成性人工智能系统上进行安全、隐私和可信度研究的共同经验，在这些研究中，规范和激励可以更好地与公共利益对齐，而不会加剧模型滥用。我们相信这些承诺是朝着更具包容性和无障碍的社区努力解决生成性人工智能风险迈出的必要一步。"
}
{
  "title": "In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering",
  "title_zh": "上下文向量：通过潜在空间引导使上下文学习更有效且可控",
  "abstract": "Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3) it reduces the length of the prompt by removing the in-context demonstrations; 4) ICV is computationally much more efficient than fine-tuning. We demonstrate that ICV achieves better performance compared to standard in-context learning and fine-tuning on diverse tasks including safety, style transfer, role-playing and formatting. Moreover, we show that we can flexibly teach LLM to simultaneously follow different types of instructions by simple vector arithmetics on the corresponding ICVs.",
  "abstract_zh": "大型语言模型（LLMs）展示了新兴的上下文学习能力，能够根据示例演示适应新任务。然而，在许多环境中，上下文学习的有效性有限，难以进行定量控制，并且占用上下文窗口空间。为克服这些限制，我们提出了一种将上下文学习重新表述为上下文向量（ICV）的替代方法。使用ICV分为两个步骤。我们首先在示例演示上进行前向传播，从LLM的潜在嵌入中创建上下文向量。该向量捕捉了关于预期任务的重要信息。在新的查询中，我们不再将示例添加到提示中，而是使用ICV来调整LLM的潜在状态。ICV方法具有几个优点：1）它使LLM更有效地遵循示例；2）通过调整ICV的大小容易控制；3）通过去除上下文演示减少提示的长度；4）与微调相比，ICV在计算上更高效。我们证明ICV在安全性、风格迁移、角色扮演和格式化等多种任务上相比标准上下文学习和微调表现更好。此外，我们还展示了通过对相应ICV进行简单的向量运算，我们可以灵活地教LLM同时遵循不同类型的指令。"
}
{
  "title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity",
  "title_zh": "对齐算法的机制理解：以DPO和毒性为案例研究",
  "abstract": "While alignment algorithms are commonly used to tune pre-trained language models towards user preferences, we lack explanations for the underlying mechanisms in which models become ``aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in pre-trained language models (GPT2-medium, Llama2-7b). We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting models avert toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the models, reverting them back to their toxic behavior.",
  "abstract_zh": "虽然对齐算法通常用于调整预训练语言模型以符合用户偏好，但我们缺乏对模型如何“对齐”的基本机制的解释，这使得解释诸如越狱等现象变得困难。在本研究中，我们研究了一种流行的算法——直接偏好优化（DPO），以及它减少毒性的机制。具体而言，我们首先研究毒性在预训练语言模型（GPT2-medium，Llama2-7b）中的表现和引发方式。然后，我们使用精心设计的成对数据集应用DPO以减少毒性。我们检查了生成的模型如何避免产生有毒输出，发现从预训练中学习到的能力并没有被移除，而是被绕过。我们利用这一见解展示了一种简单的方法来解除模型的对齐，使其恢复到有毒行为。"
}
{
  "title": "InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks",
  "title_zh": "InfiAgent-DABench：评估代理在数据分析任务上的表现",
  "abstract": "In this paper, we introduce InfiAgent-DABench, the first benchmark specifically designed to evaluate LLM-based agents on data analysis tasks. Agents need to solve these tasks end-to-end by interacting with an execution environment. This benchmark contains DAEval, a dataset consisting of 603 data analysis questions derived from 124 CSV files, and an agent framework which incorporates LLMs to serve as data analysis agents for both serving and evaluating. Since data analysis questions are often open-ended and hard to evaluate without human supervision, we adopt a format-prompting technique to convert each question into a closed-form format so that they can be automatically evaluated. Our extensive benchmarking of 34 LLMs uncovers the current challenges encountered in data analysis tasks. In addition, building upon our agent framework, we develop a specialized agent, DAAgent, which surpasses GPT-3.5 by 3.9% on DABench. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.",
  "abstract_zh": "本文介绍了InfiAgent-DABench，这是第一个专门设计用于评估基于大型语言模型（LLM）的代理在数据分析任务中的基准。代理需要通过与执行环境的交互来端到端地解决这些任务。该基准包含DAEval，一个由124个CSV文件衍生的603个数据分析问题组成的数据集，以及一个将LLM纳入其中的代理框架，用于作为数据分析代理进行服务和评估。由于数据分析问题通常是开放式的，且在没有人工监督的情况下难以评估，我们采用格式提示技术将每个问题转换为封闭形式，以便进行自动评估。我们对34个LLM的广泛基准测试揭示了数据分析任务中当前面临的挑战。此外，基于我们的代理框架，我们开发了一个专门的代理DAAgent，其在DABench上的表现超越了GPT-3.5，提升幅度为3.9%。InfiAgent-DABench的评估数据集和工具包已在https://github.com/InfiAgent/InfiAgent发布。"
}
{
  "title": "From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language Models with Pinpoint Tuning",
  "title_zh": "从迎合者到真相讲述者：通过精准调优解决大型语言模型中的谄媚问题",
  "abstract": "Large Language Models (LLMs) tend to prioritize adherence to user prompts over providing veracious responses, leading to the sycophancy issue. When challenged by users, LLMs tend to admit mistakes and provide inaccurate responses even if they initially provided the correct answer. Recent works propose to employ supervised fine-tuning (SFT) to mitigate the sycophancy issue, while it typically leads to the degeneration of LLMs' general capability. To address the challenge, we propose a novel supervised pinpoint tuning (SPT), where the region-of-interest modules are tuned for a given objective. Specifically, SPT first reveals and verifies a small percentage (<5%) of the basic modules, which significantly affect a particular behavior of LLMs. i.e., sycophancy. Subsequently, SPT merely fine-tunes these identified modules while freezing the rest. To verify the effectiveness of the proposed SPT, we conduct comprehensive experiments, demonstrating that SPT significantly mitigates the sycophancy issue of LLMs (even better than SFT). Moreover, SPT introduces limited or even no side effects on the general capability of LLMs. Our results shed light on how to precisely, effectively, and efficiently explain and improve the targeted ability of LLMs.",
  "abstract_zh": "大型语言模型（LLMs）往往优先遵循用户提示而非提供真实响应，从而导致谄媚问题。当用户提出质疑时，LLMs倾向于承认错误并提供不准确的回答，即使它们最初提供了正确答案。近期研究建议采用监督微调（SFT）来缓解谄媚问题，但这通常会导致LLMs整体能力的退化。为了解决这一挑战，我们提出了一种新颖的监督精准调优（SPT），其中特定目标的关注区域模块被调优。具体而言，SPT首先揭示并验证少量（<5%）基本模块，这些模块显著影响LLMs的特定行为，即谄媚。随后，SPT仅对这些识别出的模块进行微调，同时冻结其余模块。为了验证所提SPT的有效性，我们进行了全面实验，证明SPT显著缓解了LLMs的谄媚问题（甚至优于SFT）。此外，SPT对LLMs的整体能力引入了有限或甚至没有副作用。我们的结果为如何精准、有效和高效地解释和改善LLMs的目标能力提供了启示。"
}
{
  "title": "GRATH: Gradual Self-Truthifying for Large Language Models",
  "title_zh": "标题：GRATH：大型语言模型的渐进自我真实化",
  "abstract": "Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate pairwise truthfulness training data with each pair containing a question and its correct and incorrect answers, and then optimizes the model via direct preference optimization (DPO) to learn from the truthfulness difference between answer pairs. GRATH iteratively refines truthfulness data and updates the model, leading to a gradual improvement in model truthfulness in a self-supervised manner. Empirically, we evaluate GRATH using different 7B-LLMs and compare with LLMs with similar or even larger sizes on benchmark datasets. Our results show that GRATH effectively improves LLMs' truthfulness without compromising other core capabilities. Notably, GRATH achieves state-of-the-art performance on TruthfulQA, with MC1 accuracy of 54.71% and MC2 accuracy of 69.10%, which even surpass those on 70B-LLMs. The code is available at https://github.com/chenweixin107/GRATH.",
  "abstract_zh": "摘要：真实性对于大型语言模型（LLMs）至关重要，因为它们在现实世界应用中的部署日益增多。然而，现有的LLMs在生成真实内容方面仍然面临挑战，正如在TruthfulQA等基准测试中的表现所示。为了解决这个问题，我们提出了GRAdual self-truTHifying（GRATH），一种新颖的后处理方法，用于增强LLMs的真实性。GRATH利用域外问题提示生成成对的真实性训练数据，每对数据包含一个问题及其正确和错误的答案，然后通过直接偏好优化（DPO）优化模型，以学习答案对之间的真实性差异。GRATH迭代地细化真实性数据并更新模型，从而以自我监督的方式逐步提高模型的真实性。通过实证评估，我们使用不同的7B-LLMs对GRATH进行评估，并与具有相似或更大规模的LLMs在基准数据集上进行比较。我们的结果表明，GRATH有效提高了LLMs的真实性，而不影响其他核心能力。值得注意的是，GRATH在TruthfulQA上达到了最先进的性能，MC1准确率为54.71%，MC2准确率为69.10%，甚至超过了70B-LLMs的表现。代码可在https://github.com/chenweixin107/GRATH获取。"
}
{
  "title": "Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes",
  "title_zh": "标题：在通信成本低于18千字节的情况下对十亿参数语言模型进行联邦全参数调优",
  "abstract": "Pre-trained large language models (LLMs) need fine-tuning to improve their responsiveness to natural language instructions. Federated learning offers a way to fine-tune LLMs using the abundant data on end devices without compromising data privacy. Most existing federated fine-tuning methods for LLMs rely on parameter-efficient fine-tuning techniques, which may not reach the performance height possible with full-parameter tuning. However, federated full-parameter tuning of LLMs is a non-trivial problem due to the immense communication cost. This work introduces FedKSeed that employs zeroth-order optimization with a finite set of random seeds. It significantly reduces transmission requirements between the server and clients to just a few random seeds and scalar gradients, amounting to only a few thousand bytes, making federated full-parameter tuning of billion-sized LLMs possible on devices. Building on it, we develop a strategy enabling probability-differentiated seed sampling, prioritizing perturbations with greater impact on model accuracy. Experiments across six scenarios with various LLMs, datasets and data partitions demonstrate that our approach outperforms existing federated LLM fine-tuning methods in both communication efficiency and zero-shot generalization.",
  "abstract_zh": "摘要：预训练的大型语言模型（LLMs）需要微调以提高其对自然语言指令的响应能力。联邦学习提供了一种利用终端设备上丰富数据进行LLMs微调的方法，而不妨碍数据隐私。现有的大多数LLMs联邦微调方法依赖于参数高效的微调技术，这可能无法达到全参数调优所能实现的性能高度。然而，由于巨大的通信成本，LLMs的联邦全参数调优是一个非平凡的问题。本研究提出了FedKSeed，采用有限随机种子的零阶优化，显著减少了服务器与客户端之间的传输需求，仅需几个随机种子和标量梯度，总量仅为几千字节，使得在设备上对十亿参数的LLMs进行联邦全参数调优成为可能。在此基础上，我们开发了一种策略，能够实现概率差异化的种子采样，优先考虑对模型准确性影响更大的扰动。在六个场景中，针对不同的LLMs、数据集和数据分区的实验表明，我们的方法在通信效率和零样本泛化方面均优于现有的联邦LLM微调方法。"
}
{
  "title": "Long-Tail Learning with Foundation Model: Heavy Fine-Tuning Hurts",
  "title_zh": "长尾学习与基础模型：重度微调会造成损害",
  "abstract": "The fine-tuning paradigm in addressing long-tail learning tasks has sparked significant interest since the emergence of foundation models. Nonetheless, how fine-tuning impacts performance in long-tail learning was not explicitly quantified. In this paper, we disclose that heavy fine-tuning may even lead to non-negligible performance deterioration on tail classes, and lightweight fine-tuning is more effective. The reason is attributed to inconsistent class conditions caused by heavy fine-tuning. With the observation above, we develop a low-complexity and accurate long-tail learning algorithms LIFT with the goal of facilitating fast prediction and compact models by adaptive lightweight fine-tuning. Experiments clearly verify that both the training time and the learned parameters are significantly reduced with more accurate predictive performance compared with state-of-the-art approaches. The implementation code is available at https://github.com/shijxcs/LIFT.",
  "abstract_zh": "微调范式在解决长尾学习任务中的应用自基础模型出现以来引起了广泛关注。然而，微调对长尾学习性能的影响并未被明确量化。本文揭示，重度微调甚至可能导致尾类性能显著下降，而轻量级微调则更为有效。其原因归因于重度微调所造成的不一致类别条件。基于上述观察，我们开发了一种低复杂度且准确的长尾学习算法LIFT，旨在通过自适应轻量级微调实现快速预测和紧凑模型。实验清晰验证，与最先进的方法相比，训练时间和学习参数显著减少，同时预测性能更为准确。实现代码可在https://github.com/shijxcs/LIFT获取。"
}
{
  "title": "Variational Learning is Effective for Large Deep Networks",
  "title_zh": "变分学习对大型深度网络是有效的",
  "abstract": "We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve finetuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence that variational learning is effective. Code is available at https://github.com/team-approx-bayes/ivon.",
  "abstract_zh": "我们提供了大量实证证据，反驳了变分学习对大型神经网络无效的普遍看法。我们展示了一种名为改进变分在线牛顿（IVON）的优化器，在从头训练大型网络（如GPT-2和ResNets）时，始终与Adam相匹配或表现更好。IVON的计算成本与Adam几乎相同，但其预测不确定性更佳。我们展示了IVON的几个新用例，在大型语言模型中改进微调和模型合并，准确预测泛化误差，并忠实估计对数据的敏感性。我们发现变分学习有效的证据压倒性。代码可在https://github.com/team-approx-bayes/ivon获取。"
}
{
  "title": "Understanding Finetuning for Factual Knowledge Extraction",
  "title_zh": "理解事实知识提取的微调",
  "abstract": "In this work, we study the impact of QA fine-tuning data on downstream factuality. We show that fine-tuning on lesser-known facts that are poorly stored during pretraining yields significantly worse factuality than fine-tuning on well-known facts, even when all facts are seen during pretraining. We prove this phenomenon theoretically, showing that training on lesser-known facts can lead the model to ignore subject entity names and instead output a generic plausible response even when the relevant factual knowledge is encoded in the model. On three question answering benchmarks (PopQA, Entity Questions, and MMLU) and two language models (Llama-2-7B and Mistral-7B), we find that (i) finetuning on a completely factual but lesser-known subset of the data deteriorates downstream factuality (5-10%) and (ii) finetuning on a subset of better-known examples matches or outperforms finetuning on the entire dataset. Ultimately, our results shed light on the interaction between pretrained knowledge and finetuning data and demonstrate the importance of taking into account how facts are stored in the pretrained model when fine-tuning for knowledge-intensive tasks.",
  "abstract_zh": "在这项工作中，我们研究了问答微调数据对下游事实性的影响。我们表明，在预训练期间存储不佳的冷门事实进行微调，导致的事实性显著低于在知名事实上进行微调，即使所有事实在预训练期间都被看到。我们从理论上证明了这一现象，显示在冷门事实上的训练可能导致模型忽略主题实体名称，而输出一个通用的合理响应，即使相关的事实知识已编码在模型中。在三个问答基准（PopQA、实体问题和MMLU）和两个语言模型（Llama-2-7B和Mistral-7B）上，我们发现（i）在一个完全真实但冷门的数据子集上进行微调会降低下游事实性（5-10%），以及（ii）在一个更知名示例的子集上进行微调与在整个数据集上进行微调的效果相当或更好。最终，我们的结果揭示了预训练知识与微调数据之间的相互作用，并展示了在进行知识密集型任务的微调时考虑事实在预训练模型中如何存储的重要性。"
}
{
  "title": "Towards Causal Foundation Model: on Duality between Optimal Balancing and Attention",
  "title_zh": "朝着因果基础模型：最优平衡与注意力之间的对偶性",
  "abstract": "Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for treatment effect estimations. We propose a novel, theoretically justified method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that CInA effectively generalizes to out-of-distribution datasets and various real-world datasets, matching or even surpassing traditional per-dataset methodologies. These results provide compelling evidence that our method has the potential to serve as a stepping stone for the development of causal foundation models.",
  "abstract_zh": "基础模型已经改变了机器学习的格局，在多种任务中展现出人类水平的智能火花。然而，在因果推断等复杂任务中仍然存在差距，主要是由于与复杂推理步骤和高数值精度要求相关的挑战。在这项工作中，我们迈出了构建因果感知基础模型以进行处理效应估计的第一步。我们提出了一种新颖的理论上有依据的方法，称为带注意力的因果推断（CInA），该方法利用多个未标记的数据集进行自监督因果学习，随后能够在新数据上对未见任务进行零样本因果推断。这基于我们的理论结果，证明了最优协变量平衡与自注意力之间的原始-对偶连接，通过训练好的变换器架构的最终层促进零样本因果推断。我们实证证明CInA能够有效地推广到分布外数据集和各种真实世界数据集，匹配甚至超越传统的每数据集方法。这些结果提供了有力证据，表明我们的方法有潜力成为因果基础模型发展的垫脚石。"
}
{
  "title": "AST-T5: Structure-Aware Pretraining for Code Generation and Understanding",
  "title_zh": "AST-T5：结构感知的代码生成与理解预训练",
  "abstract": "Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids complex program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks including HumanEval and MBPP. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our code and model are publicly available at https://github.com/gonglinyuan/ast_t5.",
  "abstract_zh": "大型语言模型（LLMs）在代码相关任务上取得了显著进展，但许多LLMs将代码视为简单序列，忽视了其结构特性。我们提出了AST-T5，这是一种新颖的预训练范式，利用抽象语法树（AST）来增强代码生成、转译和理解。通过动态编程，我们的AST感知分割保留了代码结构，而AST感知跨度破坏目标使模型能够重建各种代码结构。与其他模型不同，AST-T5避免了复杂的程序分析或架构更改，因此可以与任何编码器-解码器Transformer无缝集成。评估结果表明，AST-T5在包括HumanEval和MBPP在内的各种代码相关任务中始终优于同等规模的语言模型。结构感知使AST-T5在代码到代码任务中尤其强大，在Bugs2Fix任务中比CodeT5的精确匹配分数高出2分，在CodeXGLUE中的Java-C#转译任务中高出3分。我们的代码和模型已公开发布在https://github.com/gonglinyuan/ast_t5。"
}
{
  "title": "Transforming and Combining Rewards for Aligning Large Language Models",
  "title_zh": "标题：转换和组合奖励以对齐大型语言模型",
  "abstract": "A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is \"better\" than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. The derived transformation is straightforward: we apply a log-sigmoid function to the centered rewards, a method we term \"LSC-transformation\" (log-sigmoid-centered transformation). This transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model). Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is \"good\" in all measured properties, in a sense we make precise. Experiments aligning language models to be both helpful and harmless using RLHF show substantial improvements over the baseline (non-transformed) approach.",
  "abstract_zh": "摘要：对齐语言模型与人类偏好的常见方法是首先从偏好数据中学习奖励模型，然后使用该奖励模型更新语言模型。我们研究了在这种方法中出现的两个密切相关的问题。首先，奖励模型的任何单调变换都保留偏好排名；是否存在比其他选择“更好”的选择？其次，我们通常希望将语言模型对齐到多个属性：我们应该如何组合多个奖励模型？通过对齐过程的概率解释，我们确定了从Bradley-Terry偏好模型中学习的奖励的自然变换选择。所得到的变换非常简单：我们对中心化的奖励应用对数- sigmoid函数，这种方法称为“LSC变换”（对数- sigmoid-中心变换）。该变换具有两个重要特性。首先，它强调改善表现不佳的输出，而不是已经得分较高的输出。这减轻了欠拟合（某些提示未得到改善）和奖励黑客（模型学习利用奖励模型的错误指定）。其次，它通过将求和与逻辑合取联系起来，启用奖励的原则性聚合：变换奖励的总和对应于输出在所有测量属性中“良好”的概率，以我们精确的方式来理解。使用RLHF对齐语言模型以实现有用和无害的目标的实验显示，与基线（未变换）方法相比，取得了显著的改进。"
}
{
  "title": "A Resilient and Accessible Distribution-Preserving Watermark for Large Language Models",
  "title_zh": "标题：一种适用于大型语言模型的弹性和可访问的分布保持水印",
  "abstract": "Watermarking techniques offer a promising way to identify machine-generated content via embedding covert information into the contents generated from language models. A challenge in the domain lies in preserving the distribution of original generated content after watermarking. Our research extends and improves upon existing watermarking framework, placing emphasis on the importance of a Distribution-Preserving (DiP) watermark. Contrary to the current strategies, our proposed DiPmark simultaneously preserves the original token distribution during watermarking (distribution-preserving), is detectable without access to the language model API and prompts (accessible), and is provably robust to moderate changes of tokens (resilient). DiPmark operates by selecting a random set of tokens prior to the generation of a word, then modifying the token distribution through a distribution-preserving reweight function to enhance the probability of these selected tokens during the sampling process. Extensive empirical evaluation on various language models and tasks demonstrates our approach's distribution-preserving property, accessibility, and resilience, making it a effective solution for watermarking tasks that demand impeccable quality preservation.",
  "abstract_zh": "摘要：水印技术提供了一种有前景的方法，通过将隐蔽信息嵌入语言模型生成的内容中来识别机器生成的内容。该领域的挑战在于水印后保持原始生成内容的分布。我们的研究扩展并改进了现有的水印框架，强调了分布保持（DiP）水印的重要性。与当前策略相反，我们提出的DiPmark在水印过程中同时保持原始标记分布（分布保持），在没有访问语言模型API和提示的情况下可检测（可访问），并且在标记的适度变化下是可证明的稳健（弹性）。DiPmark通过在生成单词之前选择一组随机标记，然后通过分布保持重加权函数修改标记分布，以增强这些选定标记在采样过程中的概率。对各种语言模型和任务的广泛实证评估证明了我们方法的分布保持特性、可访问性和弹性，使其成为对质量保持要求严格的水印任务的有效解决方案。"
}
{
  "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint",
  "title_zh": "迭代偏好学习来自人类反馈：在KL约束下将理论与实践结合的RLHF",
  "abstract": "This paper studies the theoretical framework of the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings---offline, online, and hybrid---and propose efficient algorithms with finite-sample theoretical guarantees. Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate that these proposed methods significantly surpass existing strong baselines, such as DPO and Rejection Sampling Optimization (RSO), showcasing the connections between solid theoretical foundations and their potent practical implementations.",
  "abstract_zh": "本文研究了生成模型与人类反馈强化学习（RLHF）对齐过程的理论框架。我们考虑了一种标准的数学公式，即用于RLHF的反向KL正则化上下文赌博机。尽管这种公式在实际应用中广泛使用，但对其进行严格的理论分析仍然是一个未解决的问题。我们在离线、在线和混合三种不同环境中研究其行为，并提出具有有限样本理论保证的高效算法。为了向实际应用迈进，我们的框架通过对信息理论策略改进oracle的稳健近似，自然衍生出几种新颖的RLHF算法。这包括在线环境下的直接偏好优化（DPO）算法的迭代版本，以及离线场景中的多步拒绝采样策略。我们在大型语言模型的真实对齐实验中的实证评估表明，这些提出的方法显著超越了现有的强基线，如DPO和拒绝采样优化（RSO），展示了扎实的理论基础与其强大实际应用之间的联系。"
}
{
  "title": "A Language Model’s Guide Through Latent Space",
  "title_zh": "语言模型在潜在空间中的指南",
  "abstract": "Concept guidance has emerged as a cheap and simple way to control the behavior of language models by probing their hidden representations for concept vectors and using them to perturb activations at inference time. While the focus of previous work has largely been on *truthfulness*, in this paper we extend this framework to a richer set of concepts such as *appropriateness*, *humor*, *creativity* and *quality*, and explore to what degree current detection and guidance strategies work in these challenging settings. To facilitate evaluation, we develop a novel metric for concept guidance that takes into account both the success of concept elicitation as well as the potential degradation in fluency of the guided model. Our extensive experiments reveal that while some concepts such as *truthfulness* more easily allow for guidance with current techniques, novel concepts such as *appropriateness* or *humor* either remain difficult to elicit, need extensive tuning to work, or even experience confusion. Moreover, we find that probes with optimal detection accuracies do not necessarily make for the optimal guides, contradicting previous observations for *truthfulness*. Our work warrants a deeper investigation into the interplay between detectability, guidability, and the nature of the concept, and we hope that our rich experimental test-bed for guidance research inspires stronger follow-up approaches.",
  "abstract_zh": "概念引导已成为一种廉价且简单的方式，通过探测语言模型的隐藏表示以获取概念向量，并在推理时使用它们来扰动激活。尽管之前的工作主要集中在*真实性*上，本文将这一框架扩展到更丰富的概念集，如*适当性*、*幽默*、*创造力*和*质量*，并探讨当前的检测和引导策略在这些具有挑战性的环境中有效的程度。为了便于评估，我们开发了一种新颖的概念引导度量，考虑到概念引导的成功与被引导模型流畅性潜在下降之间的关系。我们的广泛实验表明，尽管一些概念如*真实性*更容易通过当前技术进行引导，但新颖概念如*适当性*或*幽默*要么仍然难以引导，要么需要大量调优才能有效，甚至会出现混淆。此外，我们发现具有最佳检测准确性的探测器不一定是最佳引导器，这与之前对*真实性*的观察相矛盾。我们的工作需要更深入地研究可检测性、可引导性与概念性质之间的相互作用，我们希望我们的丰富实验测试平台能够激励更强有力的后续研究方法。"
}
{
  "title": "What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement",
  "title_zh": "我的模型会忘记什么？预测语言模型精炼中的遗忘示例",
  "abstract": "Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting---the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products of example representations achieves better forecasting performance over a series of setups. Finally, we show that we reduce forgetting of upstream pretraining examples by replaying examples that are forecasted to be forgotten, demonstrating the practical utility of forecasting example forgetting.",
  "abstract_zh": "在实际应用中部署的语言模型会出现错误。然而，仅仅用纠正后的错误实例更新模型会导致灾难性遗忘——更新后的模型在指令调优或上游训练阶段学习的实例上出现错误。随机重放上游数据的效果不尽人意，且通常伴随高方差和较差的可控性。为此，我们尝试预测因模型更新而被遗忘的上游示例，以改善重放过程的可控性和可解释性。我们在一组在线学习的示例和相应的被遗忘的上游预训练示例的基础上训练预测模型。我们提出了一种部分可解释的预测模型，基于预训练示例的预软最大对数分数变化与在线学习示例的变化相似的观察，该模型在BART上表现良好，但在T5模型上失败。我们进一步展示了一种基于示例表示内积的黑箱分类器在一系列设置中实现了更好的预测性能。最后，我们通过重放预测为被遗忘的示例，减少了上游预训练示例的遗忘，展示了预测示例遗忘的实际效用。"
}
{
  "title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
  "title_zh": "标题：通过输入澄清集成分解大型语言模型的不确定性",
  "abstract": "Uncertainty decomposition refers to the task of decomposing the total uncertainty of a predictive model into aleatoric (data) uncertainty, resulting from inherent randomness in the data-generating process, and epistemic (model) uncertainty, resulting from missing information in the model's training data. In large language models (LLMs) specifically, identifying sources of uncertainty is an important step toward improving reliability, trustworthiness, and interpretability, but remains an important open research question. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarification ensembling, which can be applied to any pre-trained LLM. Our approach generates a set of clarifications for the input, feeds them into an LLM, and ensembles the corresponding predictions. We show that, when aleatoric uncertainty arises from ambiguity or under-specification in LLM inputs, this approach makes it possible to factor an (un-clarified) LLM's predictions into separate aleatoric and epistemic terms, using a decomposition similar to the one employed by Bayesian neural networks. Empirical evaluations demonstrate that input clarification ensembling provides accurate and reliable uncertainty quantification on several language processing tasks. Code and data are available at https://github.com/UCSB-NLP-Chang/llm_uncertainty.",
  "abstract_zh": "摘要：不确定性分解是指将预测模型的总不确定性分解为来自数据生成过程固有随机性的随机（数据）不确定性和由于模型训练数据中缺失信息而产生的认知（模型）不确定性的任务。特别是在大型语言模型（LLMs）中，识别不确定性来源是提高可靠性、可信性和可解释性的重要步骤，但仍然是一个重要的开放研究问题。本文介绍了一种针对LLMs的不确定性分解框架，称为输入澄清集成，可以应用于任何预训练的LLM。我们的方法生成一组输入的澄清，将其输入LLM，并集成相应的预测。我们展示了当随机不确定性源于LLM输入中的模糊或不充分规范时，该方法使得能够将（未澄清的）LLM的预测分解为单独的随机和认知项，使用类似于贝叶斯神经网络所采用的分解。实证评估表明，输入澄清集成在多个语言处理任务上提供了准确可靠的不确定性量化。代码和数据可在 https://github.com/UCSB-NLP-Chang/llm_uncertainty 获得。"
}
{
  "title": "Autoformalizing Euclidean Geometry",
  "title_zh": "自动形式化欧几里得几何",
  "abstract": "Autoformalization involves automatically translating informal math into formal theorems and proofs that are machine-verifiable. Euclidean geometry provides an interesting and controllable domain for studying autoformalization. In this paper, we introduce a neuro-symbolic framework for autoformalizing Euclidean geometry, which combines domain knowledge, SMT solvers, and large language models (LLMs). One challenge in Euclidean geometry is that informal proofs rely on diagrams, leaving gaps in texts that are hard to formalize. To address this issue, we use theorem provers to fill in such diagrammatic information automatically, so that the LLM only needs to autoformalize the explicit textual steps, making it easier for the model. We also provide automatic semantic evaluation for autoformalized theorem statements. We construct LeanEuclid, an autoformalization benchmark consisting of problems from Euclid's Elements and the UniGeo dataset formalized in the Lean proof assistant. Experiments with GPT-4 and GPT-4V show the capability and limitations of state-of-the-art LLMs on autoformalizing geometry problems. The data and code are available at https://github.com/loganrjmurphy/LeanEuclid.",
  "abstract_zh": "自动形式化涉及将非正式数学自动翻译为机器可验证的正式定理和证明。欧几里得几何为研究自动形式化提供了一个有趣且可控的领域。本文介绍了一种用于自动形式化欧几里得几何的神经符号框架，该框架结合了领域知识、SMT求解器和大型语言模型（LLMs）。欧几里得几何中的一个挑战是，非正式证明依赖于图示，这在文本中留下了难以形式化的空白。为了解决这个问题，我们使用定理证明器自动填充这些图示信息，使得LLM只需自动形式化显式的文本步骤，从而简化了模型的工作。我们还提供了自动形式化定理陈述的语义评估。我们构建了LeanEuclid，这是一个自动形式化基准，包含来自欧几里得《几何原本》和UniGeo数据集的问题，已在Lean证明助手中形式化。与GPT-4和GPT-4V的实验显示了最先进的LLMs在自动形式化几何问题上的能力和局限性。数据和代码可在https://github.com/loganrjmurphy/LeanEuclid获取。"
}
{
  "title": "Split-and-Denoise: Protect large language model inference with local differential privacy",
  "title_zh": "标题：分割与去噪：利用局部差分隐私保护大型语言模型推理",
  "abstract": "Large Language Models (LLMs) excel in natural language understanding by capturing hidden semantics in vector space. This process enriches the value of text embeddings for various downstream tasks, thereby fostering the Embedding-as-a-Service (EaaS) business model. However, the risk of privacy leakage due to direct text transmission to servers remains a critical concern. To address this, we introduce Split-N-Denoise (SnD), an private inference framework that splits the model to execute the token embedding layer on the client side at minimal computational cost. This allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters. Extensive experiments demonstrate SnD’s effectiveness in optimizing the privacy-utility tradeoff across various LLM architectures and diverse downstream tasks. The results reveal an improvement in performance under the same privacy budget compared to the baselines by over 10% on average, offering clients a privacy-preserving solution for local privacy protection.",
  "abstract_zh": "摘要：大型语言模型（LLMs）通过捕捉向量空间中的隐藏语义，在自然语言理解方面表现出色。该过程丰富了文本嵌入在各种下游任务中的价值，从而促进了嵌入即服务（EaaS）商业模型。然而，直接将文本传输到服务器所带来的隐私泄露风险仍然是一个关键问题。为了解决这个问题，我们提出了Split-N-Denoise（SnD），一个私密推理框架，该框架将模型分割，以在客户端以最小的计算成本执行令牌嵌入层。这使得客户端能够在将嵌入传输到服务器之前引入噪声，并随后接收和去噪扰动后的输出嵌入以用于下游任务。我们的方法专为LLMs的推理阶段设计，无需对模型参数进行修改。大量实验表明，SnD在优化各种LLM架构和不同下游任务的隐私效用权衡方面的有效性。结果显示，在相同隐私预算下，与基线相比，性能平均提高超过10%，为客户提供了一种保护本地隐私的隐私保护解决方案。"
}
{
  "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews",
  "title_zh": "标题：大规模监测AI修改内容：关于ChatGPT对AI会议同行评审影响的案例研究",
  "abstract": "We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: *ICLR* 2024, *NeurIPS* 2023, *CoRL* 2023 and *EMNLP* 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices.",
  "abstract_zh": "摘要：我们提出了一种方法，用于估计在大型语料库中，可能被大型语言模型（LLM）显著修改或生成的文本比例。我们的最大似然模型利用专家撰写和AI生成的参考文本，准确高效地检查真实世界中LLM的使用情况。我们将此方法应用于ChatGPT发布后进行的AI会议科学同行评审的案例研究：*ICLR* 2024、*NeurIPS* 2023、*CoRL* 2023和*EMNLP* 2023。我们的结果表明，提交给这些会议的同行评审文本中，有6.5%到16.9%可能被LLM显著修改，即超出了拼写检查或小幅写作更新的范围。生成文本出现的情况提供了用户行为的洞察：在报告较低信心的评审中、在截止日期临近时提交的评审中，以及来自不太可能回应作者反驳的评审者中，LLM生成文本的估计比例更高。我们还观察到生成文本在语料库层面的趋势，这些趋势可能在个体层面上过于微妙而难以检测，并讨论了这些趋势对同行评审的影响。我们呼吁未来的跨学科研究，以考察LLM使用如何改变我们的信息和知识实践。"
}
{
  "title": "Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models",
  "title_zh": "安全微调几乎没有成本：视觉大型语言模型的基线",
  "abstract": "Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject unsafe instructions and substantially reduce the success rates of several black-box adversarial attacks, which approach zero in many cases. The code and dataset will be open-sourced.",
  "abstract_zh": "当前的视觉大型语言模型（VLLMs）展现出卓越的能力，但容易生成有害内容，并且对最简单的越狱攻击也很脆弱。我们的初步分析发现，这主要是由于在视觉-语言指令微调过程中存在有害数据，且VLLM微调可能导致之前由基础LLM学习的安全对齐遗忘。为了解决这个问题，我们首先整理了一个涵盖各种有害类别的视觉-语言安全指令跟随数据集VLGuard。我们的实验表明，将该数据集整合到标准视觉-语言微调中或用于事后微调，能够有效地对VLLMs进行安全对齐。此对齐在对模型的有用性影响最小或甚至增强的情况下实现。我们的安全微调数据集的多样性使其成为测试现有VLLMs安全性、训练新模型或保护预训练VLLMs的宝贵资源。实证结果表明，微调后的VLLMs能够有效拒绝不安全的指令，并显著降低多种黑箱对抗攻击的成功率，在许多情况下接近于零。代码和数据集将开源。"
}
{
  "title": "Position: TrustLLM: Trustworthiness in Large Language Models",
  "title_zh": "标题：TrustLLM：大型语言模型的可信度",
  "abstract": "Large language models (LLMs) have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and capability (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones, suggesting that open-source models can achieve high levels of trustworthiness without additional mechanisms like *moderator*, offering valuable insights for developers in this field. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Besides these observations, we've uncovered key insights into the multifaceted trustworthiness in LLMs. We emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. We advocate that the establishment of an AI alliance between industry, academia, the open-source community to foster collaboration is imperative to advance the trustworthiness of LLMs.",
  "abstract_zh": "摘要：大型语言模型（LLMs）因其卓越的自然语言处理能力而受到广泛关注。然而，这些LLMs在可信度方面面临许多挑战。本文介绍了TrustLLM，这是对LLMs可信度的全面研究，包括不同维度的可信度原则、建立的基准、主流LLMs的可信度评估与分析，以及对开放挑战和未来方向的讨论。具体而言，我们首先提出了一套涵盖八个不同维度的可信LLMs原则。基于这些原则，我们进一步在包括真实性、安全性、公平性、鲁棒性、隐私和机器伦理等六个维度上建立了基准。然后，我们在TrustLLM中评估了16个主流LLMs，涉及30多个数据集。我们的研究结果首先表明，通常情况下，可信度与能力（即功能有效性）呈正相关。其次，我们的观察显示，专有LLMs在可信度方面通常优于大多数开源对手，这引发了对广泛可获取的开源LLMs潜在风险的担忧。然而，一些开源LLMs与专有模型非常接近，这表明开源模型可以在没有额外机制（如*moderator*）的情况下实现高水平的可信度，为该领域的开发者提供了宝贵的见解。第三，重要的是要注意到某些LLMs可能过度校准以表现出可信度，以至于错误地将无害提示视为有害，从而妨碍其效用并导致不响应。除了这些观察外，我们还揭示了LLMs中多维可信度的关键见解。我们强调确保透明度的重要性，不仅在模型本身，也在支撑可信度的技术上。我们主张在行业、学术界和开源社区之间建立AI联盟以促进合作，以推动LLMs的可信度是必要的。"
}
{
  "title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data",
  "title_zh": "标题：大语言模型的偏好微调应利用次优的在线数据",
  "abstract": "Learning from preference labels plays a crucial role in fine-tuning large language models --- this is done via supervised learning, on-policy reinforcement learning (RL), or contrastive learning. Different methods come with different implementation tradeoffs, and existing empirical findings present different conclusions, for instance, some results show that online RL is quite important to attain good fine-tuning results, while others find offline methods sufficient. This raises a question: **what kind of approaches are important for fine-tuning with preference data and why?** In this paper, we answer this question by performing a rigorous analysis of a number of fine-tuning techniques on didactic and full-scale LLM problems. Our main finding is that approaches that use on-policy sampling and attempt to push down the likelihood on certain responses (i.e., employ a ''negative gradient'') outperform offline and maximum likelihood objectives. We conceptualize our insights and unify methods that use on-policy sampling or negative gradient under a notion of mode-seeking objectives for categorical distributions. Mode-seeking objectives are able to alter probability mass on specific bins of a categorical distribution at a fast rate compared to maximum likelihood, allowing them to relocate masses across bins more effectively. Our analysis prescribes actionable insights for preference fine-tuning of LLMs and informs how data should be collected for maximal improvement.",
  "abstract_zh": "摘要：从偏好标签中学习在微调大语言模型中起着至关重要的作用——这可以通过监督学习、在线强化学习（RL）或对比学习来实现。不同的方法具有不同的实现权衡，现有的实证研究得出了不同的结论，例如，一些结果表明在线RL对于获得良好的微调结果非常重要，而另一些则发现离线方法足够。这引发了一个问题：**什么样的方法对于使用偏好数据的微调是重要的，为什么？** 在本文中，我们通过对多种微调技术在教学和全规模LLM问题上的严格分析来回答这个问题。我们的主要发现是，使用在线采样并试图降低某些响应的可能性（即，采用“负梯度”）的方法优于离线和最大似然目标。我们将我们的见解概念化，并将使用在线采样或负梯度的方法统一在类别分布的模式寻求目标的概念下。与最大似然相比，模式寻求目标能够以更快的速度改变类别分布特定区间的概率质量，从而更有效地在区间之间重新分配质量。我们的分析为大语言模型的偏好微调提供了可操作的见解，并指导数据应如何收集以实现最大改进。"
}
{
  "title": "Controlled Decoding from Language Models",
  "title_zh": "受控解码语言模型",
  "abstract": "KL-regularized reinforcement learning (RL) is a popular alignment framework to control the language model responses towards high reward outcomes. We pose a tokenwise RL objective and propose a modular solver for it, called *controlled decoding (CD)*. CD exerts control through a separate *prefix scorer* module, which is trained to learn a value function for the reward. The prefix scorer is used at inference time to control the generation from a frozen base model, provably sampling from a solution to the RL objective. We empirically demonstrate that CD is effective as a control mechanism on popular benchmarks. We also show that prefix scorers for multiple rewards may be combined at inference time, effectively solving a multi-objective RL problem with no additional training. We show that the benefits of applying CD transfer to an unseen base model with no further tuning as well. Finally, we show that CD can be applied in a blockwise decoding fashion at inference-time, essentially bridging the gap between the popular best-of-$K$ strategy and tokenwise control through reinforcement learning. This makes CD a promising approach for alignment of language models.",
  "abstract_zh": "KL正则化的强化学习（RL）是一种流行的对齐框架，用于控制语言模型的响应以获得高奖励结果。我们提出了一种逐标记的RL目标，并为其提出了一种模块化求解器，称为*受控解码（CD）*。CD通过一个单独的*前缀评分器*模块施加控制，该模块经过训练以学习奖励的价值函数。前缀评分器在推理时用于控制从冻结的基础模型生成，证明可以从RL目标的解中进行采样。我们通过实验证明，CD在流行基准测试中作为控制机制是有效的。我们还展示了多个奖励的前缀评分器可以在推理时结合，从而有效解决多目标RL问题而无需额外训练。我们表明，应用CD的好处可以转移到未见过的基础模型上，而无需进一步调整。最后，我们展示了CD可以在推理时以块状解码的方式应用，实质上弥合了流行的最佳-$K$策略与通过强化学习进行逐标记控制之间的差距。这使得CD成为对齐语言模型的一种有前景的方法。"
}
{
  "title": "Watermarks in the Sand: Impossibility of Strong Watermarking for Language Models",
  "title_zh": "沙中的水印：语言模型强水印的不可行性",
  "abstract": "Watermarking generative models consists of planting a statistical signal (watermark) in a model's output so that it can be later verified that the output was generated by the given model. A strong watermarking scheme satisfies the property that a computationally bounded attacker cannot erase the watermark without causing significant quality degradation. In this paper, we study the (im)possibility of strong watermarking schemes. We prove that, under well-specified and natural assumptions, strong watermarking is impossible to achieve. This holds even in the private detection algorithm setting, where the watermark insertion and detection algorithms share a secret key, unknown to the attacker. To prove this result, we introduce a generic efficient watermark attack; the attacker is not required to know the private key of the scheme or even which scheme is used. Our attack is based on two assumptions: (1) The attacker has access to a \"quality oracle\" that can evaluate whether a candidate output is a high-quality response to a prompt, and (2) The attacker has access to a \"perturbation oracle\" which can modify an output with a nontrivial probability of maintaining quality, and which induces an efficiently mixing random walk on high-quality outputs. We argue that both assumptions can be satisfied in practice by an attacker with weaker computational capabilities than the watermarked model itself, to which the attacker has only black-box access. Furthermore, our assumptions will likely only be easier to satisfy over time as models grow in capabilities and modalities. We demonstrate the feasibility of our attack by instantiating it to attack three existing watermarking schemes for large language models: Kirchenbauer et al. (2023), Kuditipudi et al. (2023), and Zhao et al. (2023), and include preliminary results on vision-language models. The same attack successfully removes the watermarks planted by all schemes, with only minor quality degradation.",
  "abstract_zh": "水印生成模型的过程是将统计信号（水印）植入模型的输出中，以便后续验证该输出是由给定模型生成的。强水印方案满足一个特性，即计算能力有限的攻击者无法在不显著降低质量的情况下抹去水印。本文研究了强水印方案的（不）可能性。我们证明，在明确且自然的假设下，强水印是不可能实现的。这一结论在私有检测算法设置中同样成立，在这种情况下，水印插入和检测算法共享一个攻击者未知的秘密密钥。为了证明这一结果，我们引入了一种通用的高效水印攻击；攻击者不需要知道方案的私钥，甚至不需要知道使用的是哪个方案。我们的攻击基于两个假设：（1）攻击者可以访问一个“质量oracle”，该oracle可以评估候选输出是否是对提示的高质量响应，以及（2）攻击者可以访问一个“扰动oracle”，该oracle可以以非平凡的概率修改输出并保持质量，并在高质量输出上引发高效的混合随机游走。我们认为，这两个假设在实践中可以被计算能力弱于水印模型的攻击者满足，而攻击者仅能以黑箱方式访问该模型。此外，随着模型能力和模态的增长，我们的假设在未来可能会变得更容易满足。我们通过实例化攻击三种现有的大型语言模型水印方案：Kirchenbauer等（2023），Kuditipudi等（2023）和Zhao等（2023），并包含对视觉-语言模型的初步结果，展示了我们攻击的可行性。相同的攻击成功去除了所有方案植入的水印，仅造成轻微的质量下降。"
}
{
  "title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL",
  "title_zh": "ArCHer：通过层次化多轮强化学习训练语言模型代理",
  "abstract": "Large language models (LLMs) have the potential to tackle sequential decision-making problems due to their generalist capabilities. Instead of optimizing ``myopic'' surrogate objectives such as human preferences within a single turn, in such problems, we wish to directly optimize long-term objectives, such as user satisfaction over an entire dialogue with an LLM or delayed success metrics in web navigation. Multi-turn reinforcement learning (RL) provides an appealing approach to directly optimize long-term objectives, but how can we design effective and efficient multi-turn RL algorithms for LLMs? In this work, we propose an algorithmic framework to multi-turn RL for LLMs that preserves the flexibility of token-by-token RL used in single-turn RL problems, while still accommodating long horizons and delayed rewards more effectively. Our framework, the **A**cto**r**-**C**ritic Framework with a **H**i**e**rarchical Structu**r**e (**ArCHer**), combines a high-level off-policy RL algorithm that trains a value function with a low-level RL algorithm that trains a token-by-token policy. While ArCHer can be instantiated with multiple RL algorithms, a particularly convenient instantiation is to use temporal difference (TD) learning at the high level and on-policy token-level policy gradient at the low level. Empirically, we show that ArCHer significantly improves efficiency and performance of multi-turn LLM tasks, attaining sample efficiency boosts of about **100x** over prior on-policy methods and converging to a much better performance than other off-policy methods.",
  "abstract_zh": "大型语言模型（LLMs）由于其通用能力，有潜力解决序列决策问题。在这些问题中，我们希望直接优化长期目标，例如与LLM的整个对话中的用户满意度或网页导航中的延迟成功指标，而不是优化单轮中的人类偏好等“短视”代理目标。多轮强化学习（RL）为直接优化长期目标提供了一种有吸引力的方法，但我们如何为LLM设计有效且高效的多轮RL算法？在本研究中，我们提出了一种针对LLM的多轮RL算法框架，该框架保留了单轮RL问题中逐个令牌RL的灵活性，同时更有效地适应长时间跨度和延迟奖励。我们的框架，即**A**cto**r**-**C**ritic框架与**H**i**e**rarchical Structu**r**e（**ArCHer**），结合了一个高层次的离线策略RL算法，该算法训练一个价值函数，以及一个低层次的RL算法，该算法训练逐个令牌的策略。虽然ArCHer可以与多种RL算法实例化，但一种特别方便的实例化是高层使用时间差（TD）学习，低层使用在线令牌级策略梯度。实证结果表明，ArCHer显著提高了多轮LLM任务的效率和性能，相较于之前的在线方法，样本效率提升约**100倍**，并收敛到比其他离线方法更好的性能。"
}
{
  "title": "Is In-Context Learning in Large Language Models Bayesian? A Martingale Perspective",
  "title_zh": "标题：大型语言模型中的上下文学习是贝叶斯的吗？马丁盖尔视角",
  "abstract": "In-context learning (ICL) has emerged as a particularly remarkable characteristic of Large Language Models (LLM): given a pretrained LLM and an observed dataset, LLMs can make predictions for new data points from the same distribution without fine-tuning. Numerous works have postulated ICL as approximately Bayesian inference, rendering this a natural hypothesis. In this work, we analyse this hypothesis from a new angle through the *martingale property*, a fundamental requirement of a Bayesian learning system for exchangeable data. We show that the martingale property is a necessary condition for unambiguous predictions in such scenarios, and enables a principled, decomposed notion of uncertainty vital in trustworthy, safety-critical systems. We derive actionable checks with corresponding theory and test statistics which must hold if the martingale property is satisfied. We also examine if uncertainty in LLMs decreases as expected in Bayesian learning when more data is observed. In three experiments, we provide evidence for violations of the martingale property, and deviations from a Bayesian scaling behaviour of uncertainty, falsifying the hypothesis that ICL is Bayesian.",
  "abstract_zh": "摘要：上下文学习（ICL）已成为大型语言模型（LLM）的一个特别显著特征：给定一个预训练的LLM和一个观察到的数据集，LLM可以在不进行微调的情况下对来自同一分布的新数据点进行预测。许多研究将ICL假设为近似贝叶斯推断，使其成为一个自然的假设。在本研究中，我们通过*马丁盖尔性质*这一新的角度分析了这一假设，这是贝叶斯学习系统对可交换数据的基本要求。我们表明，马丁盖尔性质是此类场景中明确预测的必要条件，并使得在可信赖的安全关键系统中至关重要的不确定性具有原则性和分解的概念。我们推导出相应的理论和检验统计量的可操作性检查，如果马丁盖尔性质成立，则必须满足这些条件。我们还考察了在观察到更多数据时，LLM中的不确定性是否如贝叶斯学习所预期那样减少。在三项实验中，我们提供了马丁盖尔性质的违反和不确定性贝叶斯缩放行为偏离的证据，从而否定了ICL是贝叶斯的假设。"
}
{
  "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
  "title_zh": "用双筒望远镜识别大型语言模型：零样本检测机器生成文本",
  "abstract": "Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called *Binoculars*, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate *Binoculars* on a number of text sources and in varied situations. Over a wide range of document types, *Binoculars* detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data. Code available at https://github.com/ahans30/Binoculars.",
  "abstract_zh": "检测现代大型语言模型生成的文本被认为是困难的，因为LLM和人类都可能表现出广泛的复杂行为。然而，我们发现基于对比两个密切相关的语言模型的得分在区分人类生成文本和机器生成文本方面非常准确。基于这一机制，我们提出了一种新颖的LLM检测器，仅需使用一对预训练的LLM进行简单计算。该方法称为*双筒望远镜*，在没有任何训练数据的情况下实现了最先进的准确性。它能够在不进行任何特定模型修改的情况下，从一系列现代LLM中识别机器文本。我们在多种文本来源和不同情况下全面评估了*双筒望远镜*。在广泛的文档类型中，*双筒望远镜*以0.01%的假阳性率检测到超过90%的ChatGPT（及其他LLM）生成样本，尽管没有在任何ChatGPT数据上进行训练。代码可在https://github.com/ahans30/Binoculars获取。"
}
{
  "title": "Language-guided Skill Learning with Temporal Variational Inference",
  "title_zh": "语言引导的技能学习与时间变分推断",
  "abstract": "We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.",
  "abstract_zh": "我们提出了一种从专家演示中发现技能的算法。该算法首先利用大型语言模型（LLMs）提出轨迹的初始分段。随后，一个层次化的变分推断框架结合LLM生成的分段信息，通过合并轨迹段来发现可重用的技能。为了进一步控制压缩与可重用性之间的权衡，我们引入了一种基于最小描述长度原则的新辅助目标，以帮助引导这一技能发现过程。我们的结果表明，使用我们方法的智能体能够发现有助于加速学习的技能，并在BabyAI（一个网格世界导航环境）和ALFRED（一个家庭模拟环境）中的新长时间任务上超越基线技能学习方法。"
}
{
  "title": "Language Models Represent Beliefs of Self and Others",
  "title_zh": "语言模型表示自我与他人的信念",
  "abstract": "Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.",
  "abstract_zh": "理解和归因心理状态的能力，即心智理论（ToM），是人类社会推理的基本能力。虽然大型语言模型（LLMs）似乎具备某些ToM能力，但这些能力背后的机制仍然难以捉摸。在本研究中，我们发现可以通过语言模型的神经激活线性解码不同代理的信念状态，表明存在自我与他人信念的内部表征。通过操控这些表征，我们观察到模型的ToM表现发生了显著变化，强调了它们在社会推理过程中的关键作用。此外，我们的发现扩展到涉及不同因果推理模式的多样社会推理任务，暗示这些表征的潜在可推广性。"
}
{
  "title": "Toward Adaptive Reasoning in Large Language Models with Thought Rollback",
  "title_zh": "朝向具有思维回滚的自适应推理的大型语言模型",
  "abstract": "Large language models (LLMs) have been routinely used to solve various tasks using step-by-step reasoning. However, the structure of intermediate reasoning steps, or *thoughts*, is rigid and unidirectional, such as chains, trees, or acyclic-directed graphs. Consequently, the resulting inflexible and forward-only reasoning may not address challenging tasks and fail when the LLM frequently gives false responses, i.e., hallucinations. This paper proposes a new reasoning framework, called *Thought Rollback* (TR), allowing LLMs to adaptively build thought structure while maintaining effective reasoning toward problem-solving under hallucinations. The core mechanism of TR is *rolling back thoughts*, which allows LLMs to perform error analysis on thoughts, and thus roll back to any previously mistaken thought for revision. Subsequently, by including such trial-and-error in the prompt to guide the LLM, each rollback leads to one more reliable reasoning path. Therefore, starting with a simple prompt without human annotations, LLM with TR adaptively and gradually explores thoughts for a correct solution. Comprehensive experiments on mathematical problems and multi-task reasoning demonstrate the state-of-the-art performance of TR in terms of problem-solving rate and interaction cost. For instance, the solving rate of GPT-4 with TR outperforms the current best by $9\\%$ on the MATH dataset. The source code is available under the folder *examples/ThoughtRollback* of https://github.com/iQua/llmpebase.",
  "abstract_zh": "大型语言模型（LLMs）通常通过逐步推理来解决各种任务。然而，中间推理步骤或*思维*的结构是僵化且单向的，例如链、树或无环有向图。因此，结果是僵化且仅向前的推理可能无法解决具有挑战性的任务，并且在LLM频繁给出错误响应（即幻觉）时会失败。本文提出了一种新的推理框架，称为*思维回滚*（TR），允许LLM在幻觉下自适应地构建思维结构，同时保持有效的推理以解决问题。TR的核心机制是*回滚思维*，这使得LLM能够对思维进行错误分析，从而回滚到任何先前错误的思维进行修正。随后，通过将这种试错过程包含在提示中以指导LLM，每次回滚都会导致一条更可靠的推理路径。因此，从一个没有人工注释的简单提示开始，具有TR的LLM自适应且逐步探索思维以找到正确的解决方案。对数学问题和多任务推理的综合实验表明，TR在解决率和交互成本方面表现出最先进的性能。例如，带有TR的GPT-4在MATH数据集上的解决率比当前最佳提高了$9\\%$。源代码可在https://github.com/iQua/llmpebase的*examples/ThoughtRollback*文件夹中获得。"
}
{
  "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
  "title_zh": "离群点加权层级稀疏（OWL）：修剪大型语言模型以实现高稀疏性的缺失秘密调料",
  "abstract": "Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge due to their colossal model size when it comes to practical deployment. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters can be pruned in one-shot without hurting performance. Building upon insights gained from pre-LLM models, particularly BERT-level language models, prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity levels, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields substantially improved results. To elucidate the underlying reasons for this disparity, we conduct a comprehensive analysis of the distribution of token features within LLMs. In doing so, we discover a strong correlation with the emergence of outliers, defined as features exhibiting significantly greater magnitudes compared to their counterparts in feature dimensions. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of **non-uniform layerwise sparsity ratios** specifically designed for LLM pruning, termed as **O**utlier **W**eighed **L**ayerwise sparsity (**OWL**). The sparsity ratio of OWL is directly proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1/V2, Vicuna, OPT, and Mistral, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by **61.22** and **6.80** perplexity at a high sparsity level of 70%, respectively, while delivering **2.6$\\times$** end-to-end inference speed-up in the DeepSparse inference engine. Code is available at https://github.com/luuyin/OWL.git.",
  "abstract_zh": "大型语言模型（LLMs）因其在各个领域的卓越表现而闻名，但在实际部署时，由于其庞大的模型规模，带来了挑战。为应对这一挑战，研究者们开始将传统的网络修剪技术应用于LLMs，发现可以在不影响性能的情况下，一次性修剪大量参数。基于对预LLM模型（特别是BERT级语言模型）的洞察，现有的LLM修剪策略始终遵循以相同稀疏水平均匀修剪所有层的做法，从而实现了稳健的性能。然而，这一观察与视觉模型领域的普遍趋势形成对比，后者通常通过非均匀层级稀疏获得显著改善。为阐明这种差异的根本原因，我们对LLMs中标记特征的分布进行了全面分析，发现与离群点的出现存在强相关性，离群点被定义为在特征维度中表现出显著更大幅度的特征。受此发现启发，我们提出了一种新颖的LLM修剪方法，结合了一套专门为LLM修剪设计的非均匀层级稀疏比，称为离群点加权层级稀疏（OWL）。OWL的稀疏比与每层观察到的离群点比率成正比，从而更有效地对齐层级权重稀疏性与离群点比率。我们在LLaMA-V1/V2、Vicuna、OPT和Mistral等多个基准上进行了实证评估，展示了OWL相较于以往方法的独特优势。例如，OWL在70%的高稀疏水平下，表现出显著的性能提升，分别超过了最先进的Wanda和SparseGPT **61.22**和**6.80**的困惑度，同时在DeepSparse推理引擎中实现了**2.6$\\times$**的端到端推理加速。代码可在https://github.com/luuyin/OWL.git获取。"
}
{
  "title": "Improving Open-Ended Text Generation via Adaptive Decoding",
  "title_zh": "标题：通过自适应解码改善开放式文本生成",
  "abstract": "Current language models decode text token by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality. This study introduces adaptive decoding, a mechanism that dynamically empowers language models to ascertain a sensible candidate set during generation. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence. Experimental results reveal that our method balances diversity and coherence well. The human evaluation shows that our method can generate human-preferred text. Additionally, our method can potentially improve the reasoning ability of language models.",
  "abstract_zh": "摘要：当前的语言模型根据概率分布逐个解码文本，确定下一个标记的适当候选项对于确保生成质量至关重要。本研究引入了自适应解码机制，动态赋能语言模型在生成过程中确定合理的候选集。具体而言，我们引入了一种基于熵的度量，称为置信度，并将确定最佳候选集概念化为一个提高置信度的过程。通过利用置信度的增量来评估将某个标记纳入候选集的合理性。实验结果表明，我们的方法在多样性和连贯性之间取得了良好的平衡。人类评估显示，我们的方法能够生成更符合人类偏好的文本。此外，我们的方法有潜力提高语言模型的推理能力。"
}
{
  "title": "On the Embedding Collapse when Scaling up Recommendation Models",
  "title_zh": "关于推荐模型扩展时的嵌入崩溃",
  "abstract": "Recent advances in foundation models have led to a promising trend of developing large recommendation models to leverage vast amounts of available data. Still, mainstream models remain embarrassingly small in size and naive enlarging does not lead to sufficient performance gain, suggesting a deficiency in the model scalability. In this paper, we identify the embedding collapse phenomenon as the inhibition of scalability, wherein the embedding matrix tends to occupy a low-dimensional subspace. Through empirical and theoretical analysis, we demonstrate a two-sided effect of feature interaction specific to recommendation models. On the one hand, interacting with collapsed embeddings restricts embedding learning and exacerbates the collapse issue. On the other hand, interaction is crucial in mitigating the fitting of spurious features as a scalability guarantee. Based on our analysis, we propose a simple yet effective multi-embedding design incorporating embedding-set-specific interaction modules to learn embedding sets with large diversity and thus reduce collapse. Extensive experiments demonstrate that this proposed design provides consistent scalability and effective collapse mitigation for various recommendation models. Code is available at this repository: https://github.com/thuml/Multi-Embedding.",
  "abstract_zh": "最近基础模型的进展促使了开发大型推荐模型的有希望趋势，以利用大量可用数据。然而，主流模型的规模仍然令人尴尬地小，简单的扩大并未带来足够的性能提升，表明模型可扩展性存在缺陷。在本文中，我们将嵌入崩溃现象确定为可扩展性的抑制，其中嵌入矩阵倾向于占据低维子空间。通过实证和理论分析，我们展示了特征交互对推荐模型的双面影响。一方面，与崩溃嵌入的交互限制了嵌入学习，并加剧了崩溃问题。另一方面，交互在减轻虚假特征的拟合方面至关重要，作为可扩展性的保证。基于我们的分析，我们提出了一种简单而有效的多嵌入设计，结合嵌入集特定的交互模块，以学习具有大多样性的嵌入集，从而减少崩溃。大量实验表明，该设计为各种推荐模型提供了一致的可扩展性和有效的崩溃缓解。代码可在此库中获取：https://github.com/thuml/Multi-Embedding。"
}
{
  "title": "Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement",
  "title_zh": "零样本心电图分类：多模态学习与测试时临床知识增强",
  "abstract": "Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility. In this work, we address these issues with the **M**ultimodal **E**CG **R**epresentation **L**earning (**MERL**) framework. Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks. At test time, we propose the **C**linical **K**nowledge **E**nhanced **P**rompt **E**ngineering (**CKEPE**) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating more descriptive prompts and reducing hallucinations in LLM-generated content to boost zero-shot classification. Based on MERL, we perform the first benchmark across six public ECG datasets, showing the superior performance of MERL compared against eSSL methods. Notably, MERL achieves an average AUC score of 75.2% in zero-shot classification (**without training data**), 3.2% higher than linear probed eSSL methods with 10% annotated training data, averaged across all six datasets.",
  "abstract_zh": "心电图（ECG）是临床实践中用于检测心脏心律失常疾病的重要非侵入性诊断工具。虽然心电图自监督学习（eSSL）方法在从未标注的心电图数据中进行表示学习方面显示出潜力，但它们往往忽视了报告中可以找到的临床知识。这种忽视以及对下游任务标注样本的需求限制了eSSL的适用性。在本研究中，我们通过**多模态心电图表示学习（MERL）**框架解决了这些问题。通过对心电图记录和相关报告的多模态学习，MERL能够使用文本提示进行零样本心电图分类，消除了下游任务对训练数据的需求。在测试时，我们提出了**临床知识增强提示工程（CKEPE）**方法，该方法利用大型语言模型（LLMs）来利用外部专家验证的临床知识数据库，生成更具描述性的提示，并减少LLM生成内容中的幻觉，从而提升零样本分类的效果。基于MERL，我们在六个公共心电图数据集上进行了首次基准测试，显示MERL相较于eSSL方法的优越性能。值得注意的是，MERL在零样本分类中平均AUC得分为75.2%（**无训练数据**），比使用10%标注训练数据的线性探测eSSL方法高出3.2%，在所有六个数据集上平均计算。"
}
{
  "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright BreachesWithout Adjusting Finetuning Pipeline",
  "title_zh": "标题：扩散模型越强，后门越容易：数据中毒诱导版权侵权而无需调整微调流程",
  "abstract": "The commercialization of text-to-image diffusion models (DMs) brings forth potential copyright concerns. Despite numerous attempts to protect DMs from copyright issues, the vulnerabilities of these solutions are underexplored. In this study, we formalized the Copyright Infringement Attack on generative AI models and proposed a backdoor attack method, SilentBadDiffusion, to induce copyright infringement without requiring access to or control over training processes. Our method strategically embeds connections between pieces of copyrighted information and text references in poisoning data while carefully dispersing that information, making the poisoning data inconspicuous when integrated into a clean dataset. Our experiments show the stealth and efficacy of the poisoning data. When given specific text prompts, DMs trained with a poisoning ratio of 0.20% can produce copyrighted images. Additionally, the results reveal that the more sophisticated the DMs are, the easier the success of the attack becomes. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny to prevent the misuse of DMs.",
  "abstract_zh": "摘要：文本到图像扩散模型（DMs）的商业化带来了潜在的版权问题。尽管已有众多尝试保护DMs免受版权问题的影响，但这些解决方案的脆弱性尚未得到充分探讨。在本研究中，我们对生成性人工智能模型的版权侵权攻击进行了形式化，并提出了一种后门攻击方法SilentBadDiffusion，以在不需要访问或控制训练过程的情况下诱导版权侵权。我们的方法在中毒数据中战略性地嵌入了版权信息与文本引用之间的联系，同时小心地分散这些信息，使得中毒数据在与干净数据集整合时不易被察觉。我们的实验显示了中毒数据的隐蔽性和有效性。当给定特定文本提示时，采用0.20%中毒比例训练的DMs能够生成版权图像。此外，结果表明，DMs越复杂，攻击成功的难度越低。这些发现强调了当前版权保护策略中的潜在陷阱，并强调了加强审查以防止DMs被滥用的必要性。"
}
{
  "title": "Bridging Environments and Language with Rendering Functions and Vision-Language Models",
  "title_zh": "标题：通过渲染函数和视觉语言模型连接环境与语言",
  "abstract": "Vision-language models (VLMs) have tremendous potential for *grounding* language, and thus enabling *language-conditioned agents (LCAs)* to perform diverse tasks specified with text. This has motivated the study of LCAs based on reinforcement learning (RL) with rewards given by rendering images of an environment and evaluating those images with VLMs. If single-task RL is employed, such approaches are limited by the cost and time required to train a policy for each new task. Multi-task RL (MTRL) is a natural alternative, but requires a carefully designed corpus of training tasks and does not always generalize reliably to new tasks. Therefore, this paper introduces a novel decomposition of the problem of building an LCA: first find an *environment configuration* that has a high VLM score for text describing a task; then use a (pretrained) goal-conditioned policy to reach that configuration. We also explore several enhancements to the speed and quality of VLM-based LCAs, notably, the use of distilled models, and the evaluation of configurations from multiple viewpoints to resolve the ambiguities inherent in a single 2D view. We demonstrate our approach on the Humanoid environment, showing that it results in LCAs that outperform MTRL baselines in zero-shot generalization, without requiring any textual task descriptions or other forms of environment-specific annotation during training.",
  "abstract_zh": "摘要：视觉语言模型（VLMs）在*基础*语言方面具有巨大的潜力，从而使得*语言条件代理（LCAs）*能够执行用文本指定的多种任务。这激励了基于强化学习（RL）的LCAs的研究，通过渲染环境图像并使用VLMs评估这些图像来给予奖励。如果采用单任务RL，这种方法受到为每个新任务训练策略所需的成本和时间的限制。多任务RL（MTRL）是一个自然的替代方案，但需要精心设计的训练任务语料库，并且并不总是能可靠地推广到新任务。因此，本文提出了一种构建LCA问题的新颖分解：首先找到一个对描述任务的文本具有高VLM评分的*环境配置*；然后使用（预训练的）目标条件策略达到该配置。我们还探索了几种提高基于VLM的LCA速度和质量的增强方法，特别是使用蒸馏模型，以及从多个视角评估配置以解决单一2D视图固有的模糊性。我们在类人环境中展示了我们的方法，结果表明它产生的LCA在零样本泛化方面优于MTRL基线，而在训练过程中不需要任何文本任务描述或其他形式的环境特定注释。"
}
{
  "title": "Fast-Slow Test-Time Adaptation for Online Vision-and-Language Navigation",
  "title_zh": "快速-慢速测试时间适应用于在线视觉-语言导航",
  "abstract": "The ability to accurately comprehend natural language instructions and navigate to the target location is essential for an embodied agent. Such agents are typically required to execute user instructions in an online manner, leading us to explore the use of unlabeled test samples for effective online model adaptation. However, for online Vision-and-Language Navigation (VLN), due to the intrinsic nature of inter-sample online instruction execution and intra-sample multi-step action decision, frequent updates can result in drastic changes in model parameters, while occasional updates can make the model ill-equipped to handle dynamically changing environments. Therefore, we propose a Fast-Slow Test-Time Adaptation (FSTTA) approach for online VLN by performing joint decomposition-accumulation analysis for both gradients and parameters in a unified framework. Extensive experiments show that our method obtains impressive performance gains on four popular benchmarks. Code is available at https://github.com/Feliciaxyao/ICML2024-FSTTA.",
  "abstract_zh": "准确理解自然语言指令并导航到目标位置的能力对于具身代理至关重要。这类代理通常需要以在线方式执行用户指令，因此我们探索了利用未标记测试样本进行有效在线模型适应。然而，对于在线视觉-语言导航（VLN），由于样本间在线指令执行和样本内多步动作决策的内在特性，频繁更新可能导致模型参数的剧烈变化，而偶尔更新则可能使模型无法应对动态变化的环境。因此，我们提出了一种快速-慢速测试时间适应（FSTTA）方法，通过在统一框架中对梯度和参数进行联合分解-累积分析来实现在线VLN。大量实验表明，我们的方法在四个流行基准上获得了显著的性能提升。代码可在 https://github.com/Feliciaxyao/ICML2024-FSTTA 获取。"
}
{
  "title": "OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization",
  "title_zh": "标题：OSSCAR：基于组合优化的视觉和语言模型的一次性结构剪枝",
  "abstract": "Structured pruning is a promising approach for reducing the inference costs of large vision and language models. By removing carefully chosen structures, e.g., neurons or attention heads, the improvements from this approach can be realized on standard deep learning hardware. In this work, we focus on structured pruning in the one-shot (post-training) setting, which does not require model retraining after pruning. We propose a novel combinatorial optimization framework for this problem, based on a layer-wise reconstruction objective and a careful reformulation that allows for scalable optimization. Moreover, we design a new local combinatorial optimization algorithm, which exploits low-rank updates for efficient local search. Our framework is time and memory-efficient and considerably improves upon state-of-the-art one-shot methods on vision models (e.g., ResNet50, MobileNet) and language models (e.g., OPT-1.3B -- OPT-30B). For language models, e.g., OPT-2.7B, OSSCAR can lead to $125\\times$ lower test perplexity on WikiText with $2\\times$ inference time speedup in comparison to the state-of-the-art ZipLM approach. Our framework is also $6\\times$ -- $8\\times$ faster. Notably, our work considers models with tens of billions of parameters, which is up to $100\\times$ larger than what has been previously considered in the structured pruning literature. Our code is available at https://github.com/mazumder-lab/OSSCAR.",
  "abstract_zh": "摘要：结构剪枝是一种有前景的方法，用于降低大型视觉和语言模型的推理成本。通过移除精心选择的结构，例如神经元或注意力头，这种方法的改进可以在标准深度学习硬件上实现。在本研究中，我们专注于一次性（后训练）设置中的结构剪枝，该设置在剪枝后不需要模型重新训练。我们提出了一种针对该问题的新型组合优化框架，基于逐层重构目标和仔细的重新表述，以实现可扩展的优化。此外，我们设计了一种新的局部组合优化算法，利用低秩更新进行高效的局部搜索。我们的框架在时间和内存上都高效，并显著改善了视觉模型（例如ResNet50、MobileNet）和语言模型（例如OPT-1.3B至OPT-30B）上的最新一次性方法。对于语言模型，例如OPT-2.7B，OSSCAR在WikiText上可以实现$125\\times$的测试困惑度降低，同时推理时间加速$2\\times$，相比于最新的ZipLM方法。我们的框架还快$6\\times$至$8\\times$。值得注意的是，我们的工作考虑了具有数十亿参数的模型，这比结构剪枝文献中之前考虑的模型大多达$100\\times$。我们的代码可在https://github.com/mazumder-lab/OSSCAR获取。"
}
{
  "title": "Getting the most out of your tokenizer for pre-training and domain adaptation",
  "title_zh": "充分利用您的分词器进行预训练和领域适应",
  "abstract": "Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to a wide range of use-cases. We find that when fine-tuning on more than 50 billion tokens, we can specialize the tokenizer of a pre-trained LLM to obtain large gains in generation speed and effective context size.",
  "abstract_zh": "分词是现代大型语言模型中一个被低估且常常被忽视的组成部分。大多数已发表的工作在所有实验中使用单一的分词器，通常是借用自其他模型，而没有进行消融实验或分析以优化分词。此外，在微调基础模型时，分词器通常保持不变。本文展示了分词器的大小、预分词正则表达式和训练数据可以显著影响模型的生成速度、有效上下文大小、内存使用和下游性能。我们训练了专门的字节对编码代码分词器，并对分词器设计对代码生成任务（如HumanEval和MBPP）中LLMs性能的影响进行了广泛的消融实验，并提供了分词器超参数选择和在预训练LLM中切换分词器的建议。我们在从头训练和从预训练模型中进行的实验中验证了它们在广泛用例中的适用性。我们发现，当在超过500亿个标记上进行微调时，可以专门化预训练LLM的分词器，以获得生成速度和有效上下文大小的显著提升。"
}
{
  "title": "NExT-Chat: An LMM for Chat, Detection and Segmentation",
  "title_zh": "NExT-Chat：一种用于聊天、检测和分割的大型多模态模型",
  "abstract": "The development of large language models (LLMs) has greatly advanced the field of multimodal understanding, leading to the emergence of large multimodal models (LMMs). In order to enhance visual comprehension, recent studies have equipped LMMs with region-level understanding capabilities by representing object bounding box coordinates as a series of text sequences (pix2seq). In this paper, we introduce a novel paradigm for object location modeling called the pix2emb method, where we ask the LMM to output the location embeddings and then decode them with different decoders. This paradigm allows us to use different location formats (such as bounding boxes and masks) in multimodal conversations. Leveraging the proposed pix2emb method, we train an LMM named NExT-Chat and demonstrate its capability of handling multiple tasks like visual grounding, region captioning, and grounded reasoning. Comprehensive experiments show the effectiveness of our NExT-Chat on various tasks, e.g., NExT-Chat (87.7) vs. Shikra (86.9) on POPE-Random, NExT-Chat (71.3) vs. LISA (67.9) on referring expression segmentation task, and NExT-Chat (79.6) vs. Kosmos-2 (62.3) on region caption task.",
  "abstract_zh": "大型语言模型（LLMs）的发展极大推动了多模态理解领域的进步，促使大型多模态模型（LMMs）的出现。为了增强视觉理解，近期研究通过将物体边界框坐标表示为一系列文本序列（pix2seq）来赋予LMM区域级理解能力。本文介绍了一种新的物体位置建模范式，称为pix2emb方法，我们要求LMM输出位置嵌入，然后使用不同的解码器对其进行解码。该范式使我们能够在多模态对话中使用不同的位置格式（如边界框和掩码）。利用所提出的pix2emb方法，我们训练了一个名为NExT-Chat的LMM，并展示了其处理视觉定位、区域描述和基于上下文推理等多项任务的能力。综合实验表明，我们的NExT-Chat在各种任务上的有效性，例如，NExT-Chat（87.7）与Shikra（86.9）在POPE-Random上的表现，NExT-Chat（71.3）与LISA（67.9）在指称表达分割任务上的表现，以及NExT-Chat（79.6）与Kosmos-2（62.3）在区域描述任务上的表现。"
}
{
  "title": "Learning Reward for Robot Skills Using Large Language Models via Self-Alignment",
  "title_zh": "机器人技能学习奖励的自我对齐大语言模型方法",
  "abstract": "Learning reward functions remains the bottleneck to equip a robot with a broad repertoire of skills. Large Language Models (LLM) contain valuable task-related knowledge that can potentially aid in the learning of reward functions. However, the proposed reward function can be imprecise, thus ineffective which requires to be further grounded with environment information. We proposed a method to learn rewards more efficiently in the absence of humans. Our approach consists of two components: We first use the LLM to propose features and parameterization of the reward, then update the parameters through an iterative self-alignment process. In particular, the process minimizes the ranking inconsistency between the LLM and the learnt reward functions based on the execution feedback. The method was validated on 9 tasks across 2 simulation environments. It demonstrates a consistent improvement in training efficacy and efficiency, meanwhile consuming significantly fewer GPT tokens compared to the alternative mutation-based method.",
  "abstract_zh": "学习奖励函数仍然是赋予机器人广泛技能的瓶颈。大语言模型（LLM）包含有价值的任务相关知识，可以潜在地帮助学习奖励函数。然而，所提出的奖励函数可能不够精确，因此效果不佳，需要进一步与环境信息结合。我们提出了一种在没有人类干预的情况下更有效地学习奖励的方法。我们的方法由两个部分组成：首先使用LLM提出奖励的特征和参数化，然后通过迭代自我对齐过程更新参数。特别地，该过程最小化LLM与基于执行反馈学习的奖励函数之间的排名不一致性。该方法在两个仿真环境中的9个任务上进行了验证，显示出训练效果和效率的一致改善，同时相比于替代的基于变异的方法消耗了显著更少的GPT令牌。"
}
{
  "title": "AND: Audio Network Dissection for Interpreting Deep Acoustic Models",
  "title_zh": "音频网络剖析：用于解释深度声学模型",
  "abstract": "Neuron-level interpretations aim to explain network behaviors and properties by investigating neurons responsive to specific perceptual or structural input patterns. Although there is emerging work in the vision and language domains, none is explored for acoustic models. To bridge the gap, we introduce *AND*, the first **A**udio **N**etwork **D**issection framework that automatically establishes natural language explanations of acoustic neurons based on highly responsive audio. *AND* features the use of LLMs to summarize mutual acoustic features and identities among audio. Extensive experiments are conducted to verify *AND*'s precise and informative descriptions. In addition, we highlight two acoustic model behaviors with analysis by *AND*. First, models discriminate audio with a combination of basic acoustic features rather than high-level abstract concepts. Second, training strategies affect neuron behaviors. Supervised training guides neurons to gradually narrow their attention, while self-supervised learning encourages neurons to be polysemantic for exploring high-level features. Finally, we demonstrate a potential use of *AND* in audio model unlearning by conducting concept-specific pruning based on the descriptions.",
  "abstract_zh": "神经元级别的解释旨在通过研究对特定感知或结构输入模式敏感的神经元来解释网络的行为和特性。尽管在视觉和语言领域出现了一些相关研究，但在声学模型方面尚未得到探索。为了解决这一空白，我们引入了*AND*，第一个**音频网络剖析**框架，它基于高度响应的音频自动建立声学神经元的自然语言解释。*AND*利用大型语言模型（LLMs）总结音频之间的共同声学特征和身份。我们进行了广泛的实验以验证*AND*的精确和信息丰富的描述。此外，我们通过*AND*突出分析了两个声学模型的行为。首先，模型通过基本声学特征的组合而非高级抽象概念来区分音频。其次，训练策略影响神经元的行为。监督训练引导神经元逐渐缩小注意力范围，而自监督学习则鼓励神经元在探索高级特征时具有多义性。最后，我们通过基于描述进行特定概念的剪枝，展示了*AND*在音频模型遗忘中的潜在应用。"
}
{
  "title": "Unsupervised Evaluation of Code LLMs with Round-Trip Correctness",
  "title_zh": "无监督评估代码大型语言模型的往返正确性",
  "abstract": "To evaluate code large language models (LLMs), research has relied on a few small manually curated benchmarks, such as HumanEval and MBPP, which represent a narrow part of the real-world software domains. In this work, we introduce round-trip correctness (RTC) as an alternative evaluation method. RTC allows Code LLM evaluation on a broader spectrum of real-world software domains without the need for costly human curation. RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input. We show how to employ RTC to evaluate code synthesis and editing. We find that RTC strongly correlates with model performance on existing narrow-domain code synthesis benchmarks while allowing us to expand to a much broader set of domains and tasks which was not previously possible without costly human annotations.",
  "abstract_zh": "为了评估代码大型语言模型（LLMs），研究依赖于一些小型手动策划的基准，如HumanEval和MBPP，这些基准仅代表真实软件领域的一小部分。在这项工作中，我们引入了往返正确性（RTC）作为一种替代评估方法。RTC允许在更广泛的真实软件领域中评估代码LLM，而无需昂贵的人为策划。RTC的核心思想是，我们可以要求模型进行预测（例如，用自然语言描述一些代码），将该预测反馈（例如，从预测描述合成代码），并检查这个往返过程是否产生与原始输入在语义上等价的代码。我们展示了如何使用RTC来评估代码合成和编辑。我们发现，RTC与现有狭域代码合成基准上的模型性能有很强的相关性，同时使我们能够扩展到更广泛的领域和任务，这在没有昂贵的人为注释的情况下是之前无法实现的。"
}
{
  "title": "Harmonizing Generalization and Personalization in Federated Prompt Learning",
  "title_zh": "标题：在联邦提示学习中协调泛化与个性化",
  "abstract": "Federated Prompt Learning (FPL) incorporates large pre-trained Vision-Language models (VLM) into federated learning through prompt tuning. The transferable representations and remarkable generalization capacity of VLM make them highly compatible with the integration of federated learning. Addressing data heterogeneity in federated learning requires personalization, but excessive focus on it across clients could compromise the model's ability to generalize effectively. To preserve the impressive generalization capability of VLM, it is crucial to strike a balance between personalization and generalization in FPL. To tackle this challenge, we proposed Federated Prompt Learning with CLIP Generalization and low-rank Personalization (FedPGP), which employs pre-trained CLIP to provide knowledge-guidance on the global prompt for improved generalization and incorporates a low-rank adaptation term to personalize the global prompt. Further, FedPGP integrates a prompt-wise contrastive loss to achieve knowledge guidance and personalized adaptation simultaneously, enabling a harmonious balance between personalization and generalization in FPL. We conduct extensive experiments on various datasets to explore base-to-novel generalization in both category-level and domain-level scenarios with heterogeneous data, showing the superiority of FedPGP in balancing generalization and personalization.",
  "abstract_zh": "摘要：联邦提示学习（FPL）通过提示调优将大型预训练的视觉-语言模型（VLM）纳入联邦学习。VLM的可迁移表示和卓越的泛化能力使其与联邦学习的整合高度兼容。解决联邦学习中的数据异质性需要个性化，但对客户之间的过度关注可能会损害模型有效泛化的能力。为了保持VLM令人印象深刻的泛化能力，在FPL中平衡个性化与泛化至关重要。为了解决这一挑战，我们提出了具有CLIP泛化和低秩个性化的联邦提示学习（FedPGP），该方法利用预训练的CLIP为全球提示提供知识指导以改善泛化，并结合低秩适应项来个性化全球提示。此外，FedPGP集成了逐提示对比损失，以同时实现知识指导和个性化适应，从而在FPL中实现个性化与泛化之间的和谐平衡。我们在各种数据集上进行了广泛实验，以探索在异质数据的类别级和领域级场景中的基础到新颖泛化，显示了FedPGP在平衡泛化与个性化方面的优越性。"
}
{
  "title": "To Cool or not to Cool? Temperature Network Meets Large Foundation Models via DRO",
  "title_zh": "标题：冷却还是不冷却？温度网络通过分布鲁棒优化与大型基础模型相结合",
  "abstract": "The temperature parameter plays a profound role during training and/or inference with large foundation models (LFMs) such as large language models (LLMs) and CLIP models. Particularly, it adjusts the logits in the softmax function in LLMs, which is crucial for next token generation, and it scales the similarities in the contrastive loss for training CLIP models. A significant question remains: `` Is it viable to learn a neural network to predict a personalized temperature of any input data for enhancing LFMs?\" In this paper, we present a principled framework for learning a small yet generalizable temperature prediction network (TempNet) to improve LFMs. Our solution is composed of a novel learning framework with robust losses underpinned by constrained distributionally robust optimization (DRO), and a properly designed TempNet with theoretical inspiration. TempNet can be trained together with a large foundation model from scratch or learned separately given a pretrained foundation model. It is not only useful for predicting personalized temperature to promote the training of LFMs but also generalizable and transferable to new tasks. Our experiments on LLMs and CLIP models demonstrate that TempNet greatly improves the performance of existing solutions or models.",
  "abstract_zh": "摘要：温度参数在训练和/或推理大型基础模型（如大型语言模型和CLIP模型）时发挥着深远的作用。特别是，它在LLMs的softmax函数中调整logits，这对于下一个token的生成至关重要，并且它在训练CLIP模型时缩放对比损失中的相似性。一个重要的问题仍然存在：“学习一个神经网络来预测任何输入数据的个性化温度以增强LFMs是否可行？”在本文中，我们提出了一个原则性框架，用于学习一个小而可泛化的温度预测网络（TempNet）以改善LFMs。我们的解决方案由一个新颖的学习框架组成，该框架具有基于约束分布鲁棒优化（DRO）的稳健损失，以及一个经过理论启发设计的TempNet。TempNet可以与大型基础模型一起从头开始训练，也可以在给定预训练基础模型的情况下单独学习。它不仅有助于预测个性化温度以促进LFMs的训练，而且还具有可泛化性和可迁移性，适用于新任务。我们在LLMs和CLIP模型上的实验表明，TempNet大大提高了现有解决方案或模型的性能。"
}
{
  "title": "OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models",
  "title_zh": "标题：OptiMUS：基于（混合整数）线性规划求解器和大型语言模型的可扩展优化建模",
  "abstract": "Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts. Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than $20$% and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than $30$%. The implementation and the datasets are available at https://github.com/teshnizi/OptiMUS.",
  "abstract_zh": "摘要：优化问题在制造、分配到医疗等多个领域普遍存在。然而，由于制定和解决这些问题所需的专业知识限制了优化工具和技术的广泛应用，因此大多数此类问题仍然是通过手动启发式方法而非最先进的求解器进行解决。本文介绍了OptiMUS，一种基于大型语言模型（LLM）的代理，旨在从自然语言描述中制定和解决（混合整数）线性规划问题。OptiMUS能够开发数学模型，编写和调试求解器代码，评估生成的解决方案，并根据这些评估改进其模型和代码。OptiMUS采用模块化结构处理问题，使其能够在不需要长提示的情况下处理具有长描述和复杂数据的问题。实验表明，OptiMUS在简单数据集上的表现比现有最先进的方法提高了超过20%，在困难数据集（包括本文发布的新数据集NLP4LP，其中包含长且复杂的问题）上的表现提高了超过30%。实现和数据集可在https://github.com/teshnizi/OptiMUS获取。"
}
{
  "title": "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback",
  "title_zh": "标题：RL-VLM-F：基于视觉语言基础模型反馈的强化学习",
  "abstract": "Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains — including classic control, as well as manipulation of rigid, articulated, and deformable objects — without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions. Videos can be found on our project website: https://rlvlmf2024.github.io/",
  "abstract_zh": "摘要：奖励工程在强化学习（RL）研究中长期以来一直是一个挑战，因为它通常需要大量的人力和反复试验的过程来设计有效的奖励函数。本文提出了RL-VLM-F，一种仅使用任务目标的文本描述和智能体的视觉观察，利用视觉语言基础模型（VLMs）的反馈自动生成奖励函数的方法。我们的方法的关键在于查询这些模型，根据任务目标的文本描述对智能体的图像观察对进行偏好判断，然后从偏好标签中学习奖励函数，而不是直接提示这些模型输出原始的奖励分数，因为后者可能会噪声和不一致。我们展示了RL-VLM-F在多个领域（包括经典控制以及刚性、关节和可变形物体的操作）中成功产生有效的奖励和策略，而无需人工监督，且在相同假设下超越了先前使用大型预训练模型进行奖励生成的方法。视频可以在我们的项目网站上找到：https://rlvlmf2024.github.io/"
}
{
  "title": "Interpretability Illusions in the Generalization of Simplified Models",
  "title_zh": "简化模型泛化中的可解释性幻觉",
  "abstract": "A common method to study deep learning systems is to use simplified model representations—for example, using singular value decomposition to visualize the model’s hidden states in a lower dimensional space. This approach assumes that the results of these simplifications are faithful to the original model. Here, we illustrate an important caveat to this assumption: even if the simplified representations can accurately approximate the full model on the training set, they may fail to accurately capture the model’s behavior out of distribution. We illustrate this by training Transformer models on controlled datasets with systematic generalization splits, including the Dyck balanced-parenthesis languages and a code completion task. We simplify these models using tools like dimensionality reduction and clustering, and then explicitly test how these simplified proxies match the behavior of the original model. We find consistent generalization gaps: cases in which the simplified proxies are more faithful to the original model on the in-distribution evaluations and less faithful on various tests of systematic generalization. This includes cases where the original model generalizes systematically but the simplified proxies fail, and cases where the simplified proxies generalize better. Together, our results raise questions about the extent to which mechanistic interpretations derived using tools like SVD can reliably predict what a model will do in novel situations.",
  "abstract_zh": "我们展示了一个重要的警告：即使简化表示能够准确近似训练集上的完整模型，它们可能无法准确捕捉模型在分布外的行为。我们通过在具有系统性泛化划分的受控数据集上训练Transformer模型来说明这一点，包括Dyck平衡括号语言和代码补全任务。我们使用降维和聚类等工具简化这些模型，然后明确测试这些简化代理如何与原始模型的行为匹配。我们发现了一致的泛化差距：简化代理在分布内评估中对原始模型的忠实度更高，而在各种系统性泛化测试中则较低。这包括原始模型系统性泛化但简化代理失败的情况，以及简化代理泛化更好的情况。我们的结果引发了关于使用SVD等工具得出的机械解释在新情况中能否可靠预测模型行为的疑问。"
}
{
  "title": "Nash Learning from Human Feedback",
  "title_zh": "人类反馈下的纳什学习",
  "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as the main paradigm for aligning large language models (LLMs) with human preferences. Traditionally, RLHF involves the initial step of learning a reward model from pairwise human feedback, i.e., expressed as preferences between pairs of text generations. Subsequently, the LLM's policy is fine-tuned to maximize the reward through a reinforcement learning algorithm. In this study, we introduce an alternative pipeline for the fine-tuning of LLMs using pairwise human feedback. Our approach entails the initial learning of a pairwise preference model, which is conditioned on two inputs (instead of a single input in the case of a reward model) given a prompt, followed by the pursuit of a policy that consistently generates responses preferred over those generated by any competing policy, thus defining the Nash equilibrium of this preference model. We term this approach Nash learning from human feedback (NLHF). In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent. This algorithm produces a sequence of policies, with the last iteration converging to the regularized Nash equilibrium. Additionally, we explore parametric representations of policies and introduce gradient descent algorithms for deep-learning architectures. We illustrate the effectiveness of our approach by presenting experimental results on a text summarization task. We believe NLHF offers a compelling avenue for fine-tuning LLMs and enhancing the alignment of LLMs with human preferences.",
  "abstract_zh": "从人类反馈（RLHF）中学习已成为将大型语言模型（LLMs）与人类偏好对齐的主要范式。传统上，RLHF的初始步骤是从成对的人类反馈中学习奖励模型，即通过文本生成对之间的偏好表达。随后，LLM的策略通过强化学习算法进行微调，以最大化奖励。在本研究中，我们引入了一种替代的微调LLM的流程，使用成对的人类反馈。我们的方法包括初步学习一个成对偏好模型，该模型基于两个输入（而不是奖励模型中的单个输入），然后追求一个策略，该策略始终生成比任何竞争策略生成的响应更受偏好的结果，从而定义该偏好模型的纳什均衡。我们将这种方法称为人类反馈下的纳什学习（NLHF）。在表格策略表示的背景下，我们提出了一种基于镜像下降原理的新算法解决方案，称为Nash-MD。该算法生成一系列策略，最后一次迭代收敛到正则化的纳什均衡。此外，我们探索了策略的参数表示，并为深度学习架构引入了梯度下降算法。我们通过在文本摘要任务上的实验结果展示了我们方法的有效性。我们相信NLHF为微调LLM和增强LLM与人类偏好的对齐提供了一个引人注目的途径。"
}
{
  "title": "Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback",
  "title_zh": "线性对齐：一种无需调优和反馈的人类偏好对齐的闭式解",
  "abstract": "The success of AI assistants based on Language Models (LLMs) hinges on Reinforcement Learning from Human Feedback (RLHF) to comprehend and align with user intentions. However, traditional alignment algorithms, such as PPO, are hampered by complex annotation and training requirements. This reliance limits the applicability of RLHF and hinders the development of professional assistants tailored to diverse human preferences. In this work, we introduce *Linear Alignment*, a novel algorithm that aligns language models with human preferences in one single inference step, eliminating the reliance on data annotation and model training. Linear alignment incorporates a new parameterization for policy optimization under divergence constraints, which enables the extraction of optimal policy in a closed-form manner and facilitates the direct estimation of the aligned response. Extensive experiments on both general and personalized preference datasets demonstrate that linear alignment significantly enhances the performance and efficiency of LLM alignment across diverse scenarios.",
  "abstract_zh": "基于语言模型（LLMs）的人工智能助手的成功依赖于来自人类反馈的强化学习（RLHF），以理解和对齐用户意图。然而，传统的对齐算法，如PPO，受到复杂注释和训练要求的限制。这种依赖限制了RLHF的适用性，并阻碍了针对多样化人类偏好的专业助手的发展。在本研究中，我们提出了*线性对齐*，这是一种新颖的算法，可以在一次推理步骤中将语言模型与人类偏好对齐，消除了对数据注释和模型训练的依赖。线性对齐引入了一种新的参数化方法，用于在发散约束下进行策略优化，从而以闭式方式提取最优策略，并便于直接估计对齐响应。在一般和个性化偏好数据集上的大量实验表明，线性对齐显著提高了LLM对齐在多种场景下的性能和效率。"
}
{
  "title": "Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning",
  "title_zh": "可信的检索增强大型语言模型的强化学习对齐",
  "abstract": "Trustworthiness is an essential prerequisite for the real-world application of large language models. In this paper, we focus on the trustworthiness of language models with respect to retrieval augmentation. Despite being supported with external evidence, retrieval-augmented generation still suffers from hallucinations, one primary cause of which is the conflict between contextual and parametric knowledge. We deem that retrieval-augmented language models have the inherent capabilities of supplying response according to both contextual and parametric knowledge. Inspired by aligning language models with human preference, we take the first step towards aligning retrieval-augmented language models to a status where it responds relying merely on the external evidence and disregards the interference of parametric knowledge. Specifically, we propose a reinforcement learning based algorithm Trustworthy-Alignment, theoretically and experimentally demonstrating large language models' capability of reaching a trustworthy status without explicit supervision on how to respond. Our work highlights the potential of large language models on exploring its intrinsic abilities by its own and expands the application scenarios of alignment from fulfilling human preference to creating trustworthy agents.",
  "abstract_zh": "可信性是大型语言模型在现实世界应用中的基本前提。本文关注于语言模型在检索增强方面的可信性。尽管有外部证据的支持，检索增强生成仍然存在幻觉问题，其主要原因之一是上下文知识与参数知识之间的冲突。我们认为，检索增强语言模型具有根据上下文和参数知识提供响应的内在能力。受人类偏好的对齐启发，我们迈出了将检索增强语言模型对齐到仅依赖外部证据而忽略参数知识干扰的状态的第一步。具体而言，我们提出了一种基于强化学习的算法“可信对齐”，理论和实验上证明了大型语言模型在没有明确监督如何响应的情况下达到可信状态的能力。我们的工作突出了大型语言模型探索其内在能力的潜力，并将对齐的应用场景从满足人类偏好扩展到创建可信代理。"
}
{
  "title": "A Closer Look at the Limitations of Instruction Tuning",
  "title_zh": "标题：深入探讨指令调优的局限性",
  "abstract": "Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating responses. (4) Popular methods to improve IT do not lead to performance improvements over a simple LoRA fine-tuned model. Our findings reveal that responses generated solely from pre-trained knowledge consistently outperform responses by models that learn any form of new knowledge from IT on open-source datasets. We hope the insights and challenges revealed in this paper inspire future work in related directions.",
  "abstract_zh": "摘要：指令调优（IT）是使用指令-响应对训练大型语言模型（LLMs）的过程，已成为将基础预训练LLMs转变为开放域对话代理的主要方法。尽管IT取得了显著成功并被广泛采用，但其局限性和缺陷仍未得到充分探索。本文通过严格的实验和对LLMs在IT过程中变化的深入分析，揭示了IT的各种局限性。特别是，我们展示了（1）IT未能增强LLMs的知识或技能。LoRA微调仅限于学习响应启动和风格标记，而全参数微调导致知识退化。（2）从知识来源派生的IT数据集复制响应模式会导致响应质量下降。（3）全参数微调通过不准确地借用IT数据集中概念相似实例的标记生成响应，增加了幻觉现象。（4）改善IT的流行方法并未带来比简单的LoRA微调模型更好的性能提升。我们的研究发现，仅从预训练知识生成的响应在开源数据集上始终优于从IT学习任何形式新知识的模型生成的响应。我们希望本文揭示的见解和挑战能激励未来在相关方向的研究。"
}
{
  "title": "Magicoder: Empowering Code Generation with OSS-Instruct",
  "title_zh": "魔法编码器：通过OSS-Instruct增强代码生成",
  "abstract": "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using **OSS-Instruct**, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.",
  "abstract_zh": "我们介绍了魔法编码器，这是一系列完全开源（代码、权重和数据）的用于代码的大型语言模型（LLMs），其性能显著接近顶级代码模型，同时参数不超过70亿。魔法编码器模型在75K合成指令数据上进行训练，使用**OSS-Instruct**，这是一种新颖的方法，通过开源代码片段启发LLMs生成多样化的代码指令数据。我们的主要动机是通过丰富的开源参考来减轻LLMs生成的合成数据的固有偏见，以生产更真实和可控的数据。OSS-Instruct与其他数据生成方法（如Evol-Instruct）的正交性进一步使我们能够构建增强版的魔法编码器S。魔法编码器和魔法编码器S在广泛的编码基准测试中显著超越了具有相似或更大规模的最先进代码模型。值得注意的是，基于CodeLlama的魔法编码器S-CL-7B甚至在HumanEval+上超越了著名的ChatGPT（66.5对65.9，pass@1）。总的来说，OSS-Instruct为利用丰富的开源参考制作多样化的合成指令数据开辟了新方向。"
}
{
  "title": "Parameter-Efficient Fine-Tuning with Discrete Fourier Transform",
  "title_zh": "参数高效的离散傅里叶变换微调",
  "abstract": "Low-rank adaptation (LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices $A$ and $B$ to represent the weight change, i.e., $\\Delta W=BA$. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats $\\Delta W$ as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover $\\Delta W$. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M. Our code is released at [this link](https://github.com/Chaos96/fourierft).",
  "abstract_zh": "低秩适应（LoRA）最近在微调基础模型方面引起了广泛关注。它通过引入低秩矩阵 $A$ 和 $B$ 来表示权重变化，从而有效减少可训练参数的数量，即 $\\Delta W=BA$。尽管 LoRA 取得了进展，但在处理大规模定制适应或更大基础模型时面临存储挑战。在本研究中，我们旨在通过利用傅里叶变换的强大表达能力进一步压缩可训练参数。具体而言，我们引入 FourierFT，将 $\\Delta W$ 视为空间域中的矩阵，并仅学习其谱系数的一小部分。通过训练得到的谱系数，我们实现逆离散傅里叶变换以恢复 $\\Delta W$。实证结果表明，我们的 FourierFT 方法在多个任务上（包括自然语言理解、自然语言生成、指令调优和图像分类）以更少的参数表现出与 LoRA 相当或更好的性能。例如，在对 LLaMA2-7B 模型进行指令调优时，FourierFT 以仅 0.064M 的可训练参数超越了 LoRA 的 33.5M。我们的代码已在 [此链接](https://github.com/Chaos96/fourierft) 发布。"
}
{
  "title": "Fair Classification with Partial Feedback: An Exploration-Based Data Collection Approach",
  "title_zh": "公平分类与部分反馈：基于探索的数据收集方法",
  "abstract": "In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a \"desired\" classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees and encode context-specific group fairness properties. Evaluation on real-world datasets shows that this approach consistently boosts the quality of collected outcome data and improves the fraction of true positives for all groups, with only a small reduction in predictive utility.",
  "abstract_zh": "在许多预测场景中（例如，信用贷款），只有对过去被积极分类的样本观察到真实结果。这些过去的观察反过来形成了用于未来预测的分类器的训练数据集。然而，这样的训练数据集缺乏关于过去（错误）被负面分类样本结果的信息，可能导致错误的分类器。我们提出了一种方法，利用可用数据训练分类器，并配备一系列探索策略，以收集关于那些本来会被忽视的子群体的结果数据。对于任何探索策略，该方法提供了保证：（1）所有子群体都被探索，（2）假阳性的比例是有界的，以及（3）训练的分类器收敛到“期望”的分类器。正确的探索策略依赖于上下文；可以选择它来改善学习保证并编码特定上下文的群体公平性属性。对真实世界数据集的评估表明，该方法始终提高了收集结果数据的质量，并改善了所有群体的真实阳性比例，同时仅对预测效用造成小幅下降。"
}
{
  "title": "Bounding the Excess Risk for Linear Models Trained on Marginal-Preserving, Differentially-Private, Synthetic Data",
  "title_zh": "标题：对基于边际保持的差分隐私合成数据训练的线性模型的过度风险进行界定",
  "abstract": "The growing use of machine learning (ML) has raised concerns that an ML model may reveal private information about an individual who has contributed to the training dataset. To prevent leakage of sensitive data, we consider using differentially- private (DP), synthetic training data instead of real training data to train an ML model. A key desirable property of synthetic data is its ability to preserve the low-order marginals of the original distribution. Our main contribution comprises novel upper and lower bounds on the excess empirical risk of linear models trained on such synthetic data, for continuous and Lipschitz loss functions. We perform extensive experimentation alongside our theoretical results.",
  "abstract_zh": "摘要：机器学习（ML）的日益使用引发了对ML模型可能泄露参与训练数据集的个人私密信息的担忧。为了防止敏感数据泄露，我们考虑使用差分隐私（DP）合成训练数据，而不是真实训练数据，来训练ML模型。合成数据的一个关键优良特性是其能够保留原始分布的低阶边际。我们的主要贡献是为基于这种合成数据训练的线性模型提供了新的过度经验风险的上界和下界，适用于连续和利普希茨损失函数。我们进行了广泛的实验，并与我们的理论结果相结合。"
}
{
  "title": "Position: Cracking the Code of Cascading Disparity Towards Marginalized Communities",
  "title_zh": "标题：立场：破解对边缘化社区的级联差异代码",
  "abstract": "The rise of foundation models holds immense promise for advancing AI, but this progress may amplify existing risks and inequalities, leaving marginalized communities behind. In this position paper, we discuss that disparities towards marginalized communities – performance, representation, privacy, robustness, interpretability and safety – are not isolated concerns but rather interconnected elements of a cascading disparity phenomenon. We contrast foundation models with traditional models and highlight the potential for exacerbated disparity against marginalized communities. Moreover, we emphasize the unique threat of cascading impacts in foundation models, where interconnected disparities can trigger long-lasting negative consequences, specifically to the people on the margin. We define marginalized communities within the machine learning context and explore the multifaceted nature of disparities. We analyze the sources of these disparities, tracing them from data creation, training and deployment procedures to highlight the complex technical and socio-technical landscape. To mitigate the pressing crisis, we conclude with a set of calls to action to mitigate disparity at its source.",
  "abstract_zh": "摘要：基础模型的崛起为推动人工智能带来了巨大希望，但这一进展可能加剧现有风险和不平等，使边缘化社区被抛在了后面。在这篇立场论文中，我们讨论了对边缘化社区的差异——表现、代表性、隐私、稳健性、可解释性和安全性——并非孤立的问题，而是级联差异现象的相互关联元素。我们将基础模型与传统模型进行对比，强调对边缘化社区可能加剧的差异。此外，我们强调基础模型中级联影响的独特威胁，其中相互关联的差异可能引发持久的负面后果，特别是对边缘人群。我们在机器学习背景下定义边缘化社区，并探讨差异的多面性。我们分析这些差异的来源，从数据创建、训练到部署过程，突显复杂的技术和社会技术环境。为了缓解紧迫的危机，我们总结了一系列行动呼吁，以从源头上减轻差异。"
}
{
  "title": "Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency",
  "title_zh": "稀疏-IFT：最大化训练效率的稀疏同FLOP变换",
  "abstract": "Recent research has focused on weight sparsity in deep neural network training to reduce FLOPs, aiming for improved efficiency (test accuracy w.r.t training FLOPs). However, sparse weight training often compromises accuracy, requiring extended training schedules to attain the accuracy of dense models. In contrast, our approach, Sparse Iso-FLOP Transformations (Sparse-IFT), uses sparsity to improve accuracy while maintaining dense model FLOPs. Using a single hyperparameter (i.e., the sparsity level), Sparse-IFTs efficiently replace dense layers, expanding the search space for optimal sparse masks. In addition, dynamic sparse training (DST) with Sparse-IFT models effectively navigate this larger sparse mask-weight space, which is evidenced by a spectral analysis using Ramanujan graph properties. Our study reveals a robust correlation among mask topology, weights, and final performance. Notably, without adjusting any training hyperparameters, replacing dense layers with Sparse-IFT yields significant improvements, such as a +3.5% boost for ResNet-18 on ImageNet and +0.9% for GPT-3 Small on the Open LLM leaderboard. To the best of our knowledge, this is the first work to demonstrate the use of sparsity for improving the accuracy of dense models through a set of simple-to-use sparse transformations. Code is available at: https://github.com/CerebrasResearch/Sparse-IFT.",
  "abstract_zh": "近期研究集中于深度神经网络训练中的权重稀疏性，以减少FLOPs，旨在提高效率（测试准确度相对于训练FLOPs）。然而，稀疏权重训练通常会影响准确性，需要延长训练时间以达到稠密模型的准确性。相较之下，我们的方法稀疏同FLOP变换（Sparse-IFT）利用稀疏性在保持稠密模型FLOPs的同时提高准确性。通过使用单一超参数（即稀疏水平），Sparse-IFT有效替换稠密层，扩展了最佳稀疏掩码的搜索空间。此外，使用Sparse-IFT模型的动态稀疏训练（DST）有效地在更大的稀疏掩码-权重空间中导航，这通过使用拉马努金图属性的谱分析得到了证实。我们的研究揭示了掩码拓扑、权重和最终性能之间的强相关性。值得注意的是，在不调整任何训练超参数的情况下，用Sparse-IFT替换稠密层带来了显著的改进，例如在ImageNet上ResNet-18提高了+3.5%，在Open LLM排行榜上GPT-3 Small提高了+0.9%。据我们所知，这是首个通过一组简单易用的稀疏变换展示稀疏性用于提高稠密模型准确性的研究。代码可在以下链接获取：https://github.com/CerebrasResearch/Sparse-IFT。"
}
{
  "title": "Online Cascade Learning for Efficient Inference over Streams",
  "title_zh": "在线级联学习以实现高效流处理推理",
  "abstract": "Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose *online cascade learning*, the first approach to address this challenge. The objective here is to learn a ``cascade'' of models, starting with lower-capacity models (such as logistic regression) and ending with a powerful LLM, along with a *deferral policy* that determines the model to be used on a given input. We formulate the task of learning cascades online as an imitation-learning problem, where smaller models are updated over time imitating the collected LLM demonstrations, and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90% with strong robustness against input distribution shifts, underscoring its efficacy and adaptability in stream processing. Our source code is available at https://github.com/flitternie/online_cascade_learning.",
  "abstract_zh": "大型语言模型（LLMs）在回答关于数据流的复杂查询方面具有天然的作用，但LLM推理的高计算成本使其在许多此类任务中不可行。我们提出了*在线级联学习*，这是解决这一挑战的首个方法。其目标是学习一个“级联”模型，从低容量模型（如逻辑回归）开始，最终到达强大的LLM，并制定一个*延迟策略*，以确定在给定输入上使用的模型。我们将在线学习级联的任务表述为模仿学习问题，其中较小的模型随着时间的推移模仿收集到的LLM演示，并为该问题提供了一个无悔算法。四个基准测试的实验结果表明，我们的方法在准确性上与LLM相当，同时将推理成本降低了多达90%，并对输入分布变化具有强大的鲁棒性，突显了其在流处理中的有效性和适应性。我们的源代码可在https://github.com/flitternie/online_cascade_learning获取。"
}
{
  "title": "Exploring the LLM Journey from Cognition to Expression with Linear Representations",
  "title_zh": "探索大型语言模型从认知到表达的线性表示之旅",
  "abstract": "This paper presents an in-depth examination of the evolution and interplay of cognitive and expressive capabilities in large language models (LLMs), with a specific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese and English) LLM series. We define and explore the model's cognitive and expressive capabilities through linear representations across three critical phases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Cognitive capability is defined as the quantity and quality of information conveyed by the neuron output vectors within the network, similar to the neural signal processing in human cognition. Expressive capability is defined as the model’s capability to produce word-level output. Our findings unveil a sequential development pattern, where cognitive abilities are largely established during Pretraining, whereas expressive abilities predominantly advance during SFT and RLHF. Statistical analyses confirm a significant correlation between the two capabilities, suggesting that cognitive capacity may limit expressive potential. The paper also explores the theoretical underpinnings of these divergent developmental trajectories and their connection to the LLMs' architectural design. Moreover, we evaluate various optimization-independent strategies, such as few-shot learning and repeated sampling, which bridge the gap between cognitive and expressive capabilities. This research reveals the potential connection between the hidden space and the output space, contributing valuable insights into the interpretability and controllability of their training processes.",
  "abstract_zh": "本文深入探讨了大型语言模型（LLMs）中认知能力与表达能力的演变与相互作用，特别关注于先进的双语（中文和英文）LLM系列——百川7B和百川33B。我们通过线性表示定义并探讨模型的认知和表达能力，涵盖三个关键阶段：预训练、监督微调（SFT）和人类反馈强化学习（RLHF）。认知能力被定义为网络中神经元输出向量传递的信息的数量和质量，类似于人类认知中的神经信号处理。表达能力则被定义为模型生成词级输出的能力。我们的研究发现了一种顺序发展模式，其中认知能力主要在预训练阶段建立，而表达能力则在SFT和RLHF阶段显著提升。统计分析确认了这两种能力之间的显著相关性，表明认知能力可能限制表达潜力。本文还探讨了这些不同发展轨迹的理论基础及其与LLMs架构设计的关系。此外，我们评估了各种独立于优化的策略，如少量学习和重复采样，这些策略弥合了认知与表达能力之间的差距。这项研究揭示了隐空间与输出空间之间的潜在联系，为其训练过程的可解释性和可控性提供了宝贵的见解。"
}
{
  "title": "Position: Do pretrained Transformers Learn In-Context by Gradient Descent?",
  "title_zh": "标题：位置：预训练的变换器是否通过梯度下降学习上下文？",
  "abstract": "The emergence of In-Context Learning (ICL) in LLMs remains a remarkable phenomenon that is partially understood. To explain ICL, recent studies have created theoretical connections to Gradient Descent (GD). We ask, do such connections hold up in actual pre-trained language models? We highlight the limiting assumptions in prior works that make their setup considerably different from the practical setup in which language models are trained. For example, their experimental verification uses *ICL objective* (training models explicitly for ICL), which differs from the emergent ICL in the wild. Furthermore, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pre-trained on natural data (LLaMa-7B). Our comparisons of three performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and the number of demonstrations. We observe that ICL and GD modify the output distribution of language models differently. These results indicate that *the equivalence between ICL and GD remains an open hypothesis* and calls for further studies.",
  "abstract_zh": "摘要：大规模语言模型中的上下文学习（ICL）的出现仍然是一个部分理解的显著现象。为了解释ICL，最近的研究建立了与梯度下降（GD）的理论联系。我们询问，这些联系在实际的预训练语言模型中是否成立？我们强调了先前研究中的限制假设，使其设置与语言模型训练的实际设置有显著不同。例如，它们的实验验证使用*ICL目标*（明确训练模型以实现ICL），这与实际环境中出现的ICL不同。此外，这些研究中使用的理论手工构造权重的属性与真实大规模语言模型的属性不匹配。我们还寻找真实模型中的证据。我们观察到ICL和GD对观察演示的顺序具有不同的敏感性。最后，我们在自然环境中探讨并比较ICL与GD假设。我们对在自然数据上预训练的语言模型（LLaMa-7B）进行了全面的实证分析。我们对三种性能指标的比较突出了ICL和GD在数据集、模型和演示数量等各种因素下的不一致行为。我们观察到ICL和GD以不同的方式修改语言模型的输出分布。这些结果表明*ICL与GD之间的等价性仍然是一个开放的假设*，并呼吁进一步研究。"
}
{
  "title": "Watermark Stealing in Large Language Models",
  "title_zh": "标题：大型语言模型中的水印窃取",
  "abstract": "LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying *watermark stealing* (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical *spoofing attacks*, as hypothesized in prior work, but also greatly boosts *scrubbing* attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80\\%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at https://watermark-stealing.org.",
  "abstract_zh": "摘要：LLM水印技术作为检测AI生成内容的一种有前景的方法引起了关注，一些研究表明当前方案可能已经适合部署。在本研究中，我们对这一说法提出质疑，识别出*水印窃取*（WS）作为这些方案的一个基本漏洞。我们展示了查询水印LLM的API以大致逆向工程水印，从而实现实际的*欺骗攻击*，正如之前的研究所假设的那样，同时也大大增强了*清除攻击*，这一点之前并未被注意。我们首次提出了一种自动化的WS算法，并在现实环境中进行了首次全面的欺骗和清除研究。我们展示了攻击者只需花费不到50美元就能同时欺骗和清除之前被认为安全的最先进方案，平均成功率超过80%。我们的发现挑战了关于LLM水印技术的普遍看法，强调了对更强大方案的需求。我们将所有代码和额外示例公开在https://watermark-stealing.org。"
}
{
  "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities",
  "title_zh": "音频火烈鸟：一种具有少量学习和对话能力的新型音频语言模型",
  "abstract": "Augmenting large language models (LLMs) to understand audio – including non-speech sounds and non-verbal speech – is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https://github.com/NVIDIA/audio-flamingo.",
  "abstract_zh": "增强大型语言模型（LLMs）理解音频（包括非语言声音和非言语语音）对于LLMs在多样化现实世界应用中的重要性至关重要。本文提出了音频火烈鸟，这是一种新型音频语言模型，具有1）强大的音频理解能力，2）通过上下文学习和检索快速适应未见任务的能力，以及3）强大的多轮对话能力。我们引入了一系列训练技术、架构设计和数据策略，以增强我们的模型具备这些能力。在各种音频理解任务上的广泛评估确认了我们方法的有效性，设定了新的最先进基准。我们的演示网站是 https://audioflamingo.github.io/，代码在 https://github.com/NVIDIA/audio-flamingo 上开源。"
}
{
  "title": "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF",
  "title_zh": "标题：迭代数据平滑：缓解RLHF中的奖励过拟合和过度优化",
  "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper analyzes potential reasons behind the issues, and designs improved reward learning algorithm termed 'Iterative Data Smoothing' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.",
  "abstract_zh": "摘要：人类反馈强化学习（RLHF）是一种关键技术，它使语言模型与以人为本的价值观紧密对齐。RLHF的初始阶段涉及使用排名数据的奖励模型学习人类价值观。观察到奖励模型在训练一个周期后性能下降，而过度优化学习到的奖励模型最终会妨碍真实目标。本文分析了这些问题背后的潜在原因，并设计了一种改进的奖励学习算法，称为“迭代数据平滑”（IDS）。其核心思想是在每个训练周期中，我们不仅使用数据更新模型，还使用模型更新数据，将硬标签替换为软标签。我们的实证研究结果突显了这种方法相较于传统方法的优越性能。"
}
{
  "title": "One Size Fits All for Semantic Shifts: Adaptive Prompt Tuning for Continual Learning",
  "title_zh": "一种适用于语义变化的通用方法：持续学习的自适应提示调优",
  "abstract": "In real-world continual learning (CL) scenarios, tasks often exhibit intricate and unpredictable semantic shifts, posing challenges for *fixed* prompt management strategies which are tailored to only handle semantic shifts of *uniform* degree (i.e., uniformly mild or uniformly abrupt). To address this limitation, we propose an *adaptive* prompting approach that effectively accommodates semantic shifts of *varying* degree where mild and abrupt shifts are mixed. AdaPromptCL employs the assign-and-refine semantic grouping mechanism that dynamically manages prompt groups in accordance with the semantic similarity between tasks, enhancing the quality of grouping through continuous refinement. Our experiment results demonstrate that AdaPromptCL outperforms existing prompting methods by up to 21.3%, especially in the benchmark datasets with diverse semantic shifts between tasks.",
  "abstract_zh": "在现实世界的持续学习（CL）场景中，任务往往表现出复杂且不可预测的语义变化，这对仅能处理*均匀*程度的语义变化（即，均匀轻微或均匀突然）的*固定*提示管理策略提出了挑战。为了解决这一限制，我们提出了一种*自适应*提示方法，能够有效适应*不同*程度的语义变化，其中轻微和突然的变化混合在一起。AdaPromptCL采用分配与细化的语义分组机制，根据任务之间的语义相似性动态管理提示组，通过持续细化提升分组质量。我们的实验结果表明，AdaPromptCL在现有提示方法中表现优越，提升幅度可达21.3%，特别是在任务之间存在多样化语义变化的基准数据集上。"
}
{
  "title": "Gradient-based Visual Explanation for Transformer-based CLIP",
  "title_zh": "基于梯度的视觉解释方法用于基于Transformer的CLIP",
  "abstract": "Significant progress has been achieved on the improvement and downstream usages of the Contrastive Language-Image Pre-training (CLIP) vision-language model, while less attention is paid to the interpretation of CLIP. We propose a Gradient-based visual Explanation method for CLIP (Grad-ECLIP), which interprets the matching result of CLIP for specific input image-text pair. By decomposing the architecture of the encoder and discovering the relationship between the matching similarity and intermediate spatial features, Grad-ECLIP produces effective heat maps that show the influence of image regions or words on the CLIP results. Different from the previous Transformer interpretation methods that focus on the utilization of self-attention maps, which are typically extremely sparse in CLIP, we produce high-quality visual explanations by applying channel and spatial weights on token features. Qualitative and quantitative evaluations verify the superiority of Grad-ECLIP compared with the state-of-the-art methods. A series of analysis are conducted based on our visual explanation results, from which we explore the working mechanism of image-text matching, and the strengths and limitations in attribution identification of CLIP. Codes are available here: https://github.com/Cyang-Zhao/Grad-Eclip.",
  "abstract_zh": "在对对比语言-图像预训练（CLIP）视觉语言模型的改进和下游应用取得显著进展的同时，对CLIP的解释关注较少。我们提出了一种用于CLIP的基于梯度的视觉解释方法（Grad-ECLIP），该方法解释特定输入图像-文本对的CLIP匹配结果。通过分解编码器的架构并发现匹配相似性与中间空间特征之间的关系，Grad-ECLIP生成有效的热图，显示图像区域或单词对CLIP结果的影响。与以往关注自注意力图的Transformer解释方法不同，这些图在CLIP中通常非常稀疏，我们通过对标记特征应用通道和空间权重来生成高质量的视觉解释。定性和定量评估验证了Grad-ECLIP相较于最先进方法的优越性。基于我们的视觉解释结果进行了系列分析，从中探讨了图像-文本匹配的工作机制，以及CLIP在归因识别中的优势和局限性。代码可在此获取：https://github.com/Cyang-Zhao/Grad-Eclip。"
}
{
  "title": "Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions",
  "title_zh": "有益还是有害的数据？无需微调的Shapley归因方法用于解释语言模型预测",
  "abstract": "The increasing complexity of foundational models underscores the necessity for explainability, particularly for fine-tuning, the most widely used training method for adapting models to downstream tasks. Instance attribution, one type of explanation, attributes the model prediction to each training example by an instance score. However, the robustness of instance scores, specifically towards dataset resampling, has been overlooked. To bridge this gap, we propose a notion of robustness on the sign of the instance score. We theoretically and empirically demonstrate that the popular leave-one-out-based methods lack robustness, while the Shapley value behaves significantly better, but at a higher computational cost. Accordingly, we introduce an efficient fine-tuning-free approximation of the Shapley value (FreeShap) for instance attribution based on the neural tangent kernel. We empirically demonstrate that FreeShap outperforms other methods for instance attribution and other data-centric applications such as data removal, data selection, and wrong label detection, and further generalize our scale to large language models (LLMs). Our code is available at https://github.com/JTWang2000/FreeShap.",
  "abstract_zh": "基础模型日益复杂，强调了解释性的重要性，特别是在微调方面，这是适应模型到下游任务的最广泛使用的训练方法。实例归因作为一种解释方法，通过实例分数将模型预测归因于每个训练样本。然而，实例分数的鲁棒性，特别是对数据集重采样的鲁棒性，常常被忽视。为了解决这一问题，我们提出了实例分数符号鲁棒性的概念。我们理论和实证地证明，流行的基于留一法的方法缺乏鲁棒性，而Shapley值表现显著更好，但计算成本更高。因此，我们引入了一种基于神经切线核的高效无微调Shapley值近似（FreeShap）用于实例归因。我们实证表明，FreeShap在实例归因和其他数据中心应用（如数据删除、数据选择和错误标签检测）方面优于其他方法，并进一步将我们的规模推广到大型语言模型（LLMs）。我们的代码可在https://github.com/JTWang2000/FreeShap获取。"
}
{
  "title": "Why Larger Language Models Do In-context Learning Differently?",
  "title_zh": "标题：为什么更大的语言模型在上下文学习中表现不同？",
  "abstract": "Large language models (LLM) have emerged as a powerful tool for AI, with the key ability of in-context learning (ICL), where they can perform well on unseen tasks based on a brief series of task examples without necessitating any adjustments to the model parameters. One recent interesting mysterious observation is that models of different scales may have different ICL behaviors: larger models tend to be more sensitive to noise in the test context. This work studies this observation theoretically aiming to improve the understanding of LLM and ICL. We analyze two stylized settings: (1) linear regression with one-layer single-head linear transformers and (2) parity classification with two-layer multiple attention heads transformers (non-linear data and non-linear model). In both settings, we give closed-form optimal solutions and find that smaller models emphasize important hidden features while larger ones cover more hidden features; thus, smaller models are more robust to noise while larger ones are more easily distracted, leading to different ICL behaviors. This sheds light on where transformers pay attention to and how that affects ICL. Preliminary experimental results on large base and chat models provide positive support for our analysis.",
  "abstract_zh": "摘要：大型语言模型（LLM）作为人工智能的强大工具，具备上下文学习（ICL）的关键能力，即能够在无需调整模型参数的情况下，根据一系列简短的任务示例在未见过的任务上表现良好。最近一个有趣的观察是，不同规模的模型可能具有不同的ICL行为：较大的模型对测试上下文中的噪声更为敏感。本研究理论上探讨这一观察，旨在加深对LLM和ICL的理解。我们分析了两种典型设置：（1）使用单层单头线性变换器的线性回归和（2）使用双层多头注意力变换器的奇偶分类（非线性数据和非线性模型）。在这两种设置中，我们给出了闭式最优解，并发现较小的模型强调重要的隐藏特征，而较大的模型覆盖更多的隐藏特征；因此，较小的模型对噪声更具鲁棒性，而较大的模型更容易分心，导致不同的ICL行为。这为变换器关注的内容及其对ICL的影响提供了启示。对大型基础和聊天模型的初步实验结果为我们的分析提供了积极支持。"
}
{
  "title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models",
  "title_zh": "鲁棒CLIP：视觉嵌入的无监督对抗微调用于鲁棒的大型视觉-语言模型",
  "abstract": "Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many large vision-language models (LVLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (LVLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of LVLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the down-stream LVLMs is required. The code and robust models are available on GitHub.",
  "abstract_zh": "多模态基础模型如OpenFlamingo、LLaVA和GPT-4在各种现实世界任务中越来越多地被使用。先前的研究表明，这些模型在视觉模态上对对抗攻击高度脆弱。这些攻击可以被利用来传播虚假信息或欺诈用户，因此构成了重大风险，这使得大型多模态基础模型的鲁棒性成为一个紧迫的问题。CLIP模型或其变体在许多大型视觉-语言模型（LVLMs）中被用作冻结的视觉编码器，例如LLaVA和OpenFlamingo。我们提出了一种无监督对抗微调方案，以获得鲁棒的CLIP视觉编码器，从而在所有依赖于CLIP的视觉下游任务（LVLMs、零-shot分类）中实现鲁棒性。特别是，我们展示了一旦用我们的鲁棒模型替换原始CLIP模型，恶意第三方提供操控图像对LVLM用户的隐秘攻击将不再可能。下游LVLMs无需重新训练或微调。代码和鲁棒模型已在GitHub上提供。"
}
{
  "title": "Language Models as Science Tutors",
  "title_zh": "语言模型作为科学辅导员",
  "abstract": "NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models with 7B and 34B parameters. These LM tutors specialized in math have a 32K-token context window, and they excel at TutorEval while performing strongly on GSM8K and MATH. Our datasets build on open-source materials, and we release our models, data, and evaluations publicly.",
  "abstract_zh": "自然语言处理最近在训练具有强大科学问题解决能力的语言模型方面取得了令人兴奋的进展。然而，模型开发并未关注语言模型在科学中的实际应用案例，包括需要处理长篇科学文档的教育应用。为了解决这个问题，我们引入了TutorEval和TutorChat。TutorEval是一个多样化的问答基准，包含由专家撰写的关于STEM教科书长章节的问题。TutorEval帮助衡量语言模型作为科学助手的实际可用性，它是第一个结合长上下文、自由形式生成和多学科科学知识的基准。此外，我们展示了用现有对话数据集微调基础模型会导致在TutorEval上的表现不佳。因此，我们创建了TutorChat，这是一个关于教科书的80,000个长合成对话的数据集。我们使用TutorChat对具有7B和34B参数的Llemma模型进行微调。这些专注于数学的语言模型辅导员具有32K标记的上下文窗口，在TutorEval中表现出色，同时在GSM8K和MATH上也表现强劲。我们的数据集基于开源材料，我们公开发布了我们的模型、数据和评估。"
}
{
  "title": "GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks",
  "title_zh": "GistScore：通过Gist瓶颈学习更好的上下文示例选择表示",
  "abstract": "In-Context Learning (ICL) is the ability of Large Language Models (LLMs) to perform new tasks when conditioned on prompts comprising a few task examples. However, ICL performance can be critically sensitive to the choice of examples. To dynamically select the best examples for every test input, we propose Example Gisting, a novel approach for training example encoders through supervised finetuning with an attention bottleneck between the inputs and outputs. These gist models form the basis for GistScore, a novel metric for scoring and selecting informative examples. Further, we experiment with two variations: (1) finetuning gist models for each dataset and (2) multi-task training a single model on a large collection of datasets. The latter can be used for new tasks out-of-the-box, enabling a training-free ICL pipeline. Evaluations with 21 datasets spanning 9 tasks and 8 diverse LLMs show that our fine-tuned models get state-of-the-art ICL performance with over 20% absolute gain over off-the-shelf retrievers and 5% over the best prior methods. Further, our multi-task model generalizes well to new tasks, datasets, and prompt templates. Selection using this model matches or outperforms prior methods while being three orders of magnitude faster than the strongest training-free baseline.",
  "abstract_zh": "上下文学习（ICL）是大型语言模型（LLMs）在条件为包含少量任务示例的提示时执行新任务的能力。然而，ICL性能对示例的选择非常敏感。为了动态选择每个测试输入的最佳示例，我们提出了示例摘要（Example Gisting），这是一种通过在输入和输出之间设置注意力瓶颈进行监督微调的示例编码器训练新方法。这些摘要模型构成了GistScore的基础，GistScore是一种用于评分和选择信息性示例的新指标。此外，我们实验了两种变体：（1）针对每个数据集微调摘要模型和（2）在大量数据集上对单个模型进行多任务训练。后者可以开箱即用地用于新任务，从而实现无训练的ICL管道。对涵盖9个任务和8种不同LLM的21个数据集的评估表明，我们的微调模型在ICL性能上达到了最新水平，相较于现成的检索器有超过20%的绝对增益，相较于最佳的先前方法有5%的增益。此外，我们的多任务模型在新任务、数据集和提示模板上具有良好的泛化能力。使用该模型的选择与先前的方法相匹配或超越，同时比最强的无训练基线快三个数量级。"
}
{
  "title": "Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts",
  "title_zh": "标题：Prompting4Debugging：通过发现问题提示对文本到图像扩散模型进行红队测试",
  "abstract": "Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown remarkable ability in high-quality content generation, and become one of the representatives for the recent wave of transformative AI. Nevertheless, such advance comes with an intensifying concern about the misuse of this generative technology, especially for producing copyrighted or NSFW (i.e. not safe for work) images. Although efforts have been made to filter inappropriate images/prompts or remove undesirable concepts/styles via model fine-tuning, the reliability of these safety mechanisms against diversified problematic prompts remains largely unexplored. In this work, we propose **Prompting4Debugging (P4D)** as a debugging and red-teaming tool that automatically finds problematic prompts for diffusion models to test the reliability of a deployed safety mechanism. We demonstrate the efficacy of our P4D tool in uncovering new vulnerabilities of SD models with safety mechanisms. Particularly, our result shows that around half of prompts in existing safe prompting benchmarks which were originally considered \"safe\" can actually be manipulated to bypass many deployed safety mechanisms, including concept removal, negative prompt, and safety guidance. Our findings suggest that, without comprehensive testing, the evaluations on limited safe prompting benchmarks can lead to a false sense of safety for text-to-image models.",
  "abstract_zh": "摘要：文本到图像扩散模型，例如稳定扩散（SD），最近在高质量内容生成方面表现出显著能力，成为近期变革性人工智能浪潮的代表之一。然而，这一进展也引发了对这种生成技术滥用的日益关注，特别是在生成受版权保护或不适合工作（NSFW）图像方面。尽管已经采取措施通过模型微调过滤不当图像/提示或去除不良概念/风格，但这些安全机制对多样化问题提示的可靠性仍然 largely 未被探索。在本研究中，我们提出了**Prompting4Debugging (P4D)**，作为一种调试和红队工具，自动寻找扩散模型的问题提示，以测试已部署安全机制的可靠性。我们展示了P4D工具在揭示SD模型安全机制的新漏洞方面的有效性。特别是，我们的结果表明，现有安全提示基准中约一半原本被认为“安全”的提示实际上可以被操控，以绕过许多已部署的安全机制，包括概念移除、负提示和安全指导。我们的发现表明，如果没有全面的测试，对有限的安全提示基准的评估可能会导致对文本到图像模型的虚假安全感。"
}
{
  "title": "BAGEL: Bootstrapping Agents by Guiding Exploration with Language",
  "title_zh": "标题：BAGEL：通过语言引导探索来引导智能体的自启动",
  "abstract": "Following natural language instructions by executing actions in digital environments (e.g. web-browsers and REST APIs) is a challenging task for language model (LM) agents. Unfortunately, LM agents often fail to generalize to new environments without human demonstrations. This work presents BAGEL, a method for bootstrapping LM agents without human supervision. BAGEL converts a seed set of randomly explored trajectories to synthetic demonstrations via round-trips between two noisy LM components: an LM labeler which converts a trajectory into a synthetic instruction, and a zero-shot LM agent which maps the synthetic instruction into a refined trajectory. By performing these round-trips iteratively, BAGEL quickly converts the initial distribution of trajectories towards those that are well-described by natural language. We adapt the base LM agent at test time with in-context learning by retrieving relevant BAGEL demonstrations based on the instruction, and find improvements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x reduction in execution failures.",
  "abstract_zh": "摘要：根据自然语言指令在数字环境中执行操作（例如网页浏览器和REST API）对语言模型（LM）智能体来说是一项具有挑战性的任务。不幸的是，LM智能体在没有人类示范的情况下，往往无法在新环境中进行泛化。本研究提出了BAGEL，一种在没有人类监督的情况下自启动LM智能体的方法。BAGEL通过两个噪声LM组件之间的往返转换，将一组随机探索的轨迹转化为合成示范：一个将轨迹转换为合成指令的LM标注器，以及一个将合成指令映射到精炼轨迹的零-shot LM智能体。通过迭代执行这些往返，BAGEL迅速将初始轨迹分布转变为那些能够被自然语言良好描述的轨迹。我们在测试时通过根据指令检索相关的BAGEL示范来适应基础LM智能体，并发现ToolQA和MiniWob++的绝对改进超过2-13%，执行失败率减少高达13倍。"
}
{
  "title": "Position: Will we run out of data? Limits of LLM scaling based on human-generated data",
  "title_zh": "标题：位置：我们会耗尽数据吗？基于人类生成数据的LLM扩展限制",
  "abstract": "We investigate the potential constraints on LLM scaling posed by the availability of public human-generated text data. We forecast the growing demand for training data based on current trends and estimate the total stock of public human text data. Our findings indicate that if current LLM development trends continue, models will be trained on datasets roughly equal in size to the available stock of public human text data between 2026 and 2032, or slightly earlier if models are overtrained. We explore how progress in language modeling can continue when human-generated text datasets cannot be scaled any further. We argue that synthetic data generation, transfer learning from data-rich domains, and data efficiency improvements might support further progress.",
  "abstract_zh": "摘要：我们研究了公共人类生成文本数据的可用性对LLM扩展可能带来的限制。我们根据当前趋势预测训练数据的需求增长，并估算公共人类文本数据的总存量。我们的研究结果表明，如果当前的LLM发展趋势持续，模型将在2026年至2032年间训练的数据集的规模大致等于可用的公共人类文本数据存量，或者如果模型过度训练，则可能会稍早一些。我们探讨了当人类生成的文本数据集无法进一步扩展时，语言建模的进展如何能够继续。我们认为，合成数据生成、从数据丰富领域的迁移学习以及数据效率的提升可能会支持进一步的进展。"
}
{
  "title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
  "title_zh": "VoroNav：基于Voronoi的零样本物体导航与大型语言模型",
  "abstract": "In the realm of household robotics, the Zero-Shot Object Navigation (ZSON) task empowers agents to adeptly traverse unfamiliar environments and locate objects from novel categories without prior explicit training. This paper introduces VoroNav, a novel semantic exploration framework that proposes the Reduced Voronoi Graph to extract exploratory paths and planning nodes from a semantic map constructed in real time. By harnessing topological and semantic information, VoroNav designs text-based descriptions of paths and images that are readily interpretable by a large language model (LLM). In particular, our approach presents a synergy of path and farsight descriptions to represent the environmental context, enabling LLM to apply commonsense reasoning to ascertain waypoints for navigation. Extensive evaluation on HM3D and HSSD validates VoroNav surpasses existing benchmarks in both success rate and exploration efficiency (absolute improvement: +2.8% Success and +3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally introduced metrics that evaluate obstacle avoidance proficiency and perceptual efficiency further corroborate the enhancements achieved by our method in ZSON planning. Project page: https://voro-nav.github.io",
  "abstract_zh": "在家庭机器人领域，零样本物体导航（ZSON）任务使得代理能够熟练地穿越陌生环境并定位来自新类别的物体，而无需事先明确的训练。本文介绍了VoroNav，这是一种新颖的语义探索框架，提出了减少的Voronoi图，以从实时构建的语义地图中提取探索路径和规划节点。通过利用拓扑和语义信息，VoroNav设计了易于大型语言模型（LLM）理解的路径和图像的文本描述。特别是，我们的方法展示了路径和远见描述的协同作用，以表示环境上下文，使LLM能够应用常识推理来确定导航的路标。对HM3D和HSSD的广泛评估验证了VoroNav在成功率和探索效率上超越现有基准（绝对提升：HM3D上成功率+2.8%和SPL+3.7%，HSSD上成功率+2.6%和SPL+3.8%）。此外，引入的评估障碍规避能力和感知效率的指标进一步证实了我们的方法在ZSON规划中所取得的改进。项目页面：https://voro-nav.github.io"
}
{
  "title": "Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models",
  "title_zh": "标题：用于微调语言模型的方差减少零阶方法",
  "abstract": "Fine-tuning language models (LMs) has demonstrated success in a wide array of downstream tasks. However, as LMs are scaled up, the memory requirements for backpropagation become prohibitively high. Zeroth-order (ZO) optimization methods can leverage memory-efficient forward passes to estimate gradients. More recently, MeZO, an adaptation of ZO-SGD, has been shown to consistently outperform zero-shot and in-context learning when combined with suitable task prompts. In this work, we couple ZO methods with variance reduction techniques to enhance stability and convergence for inference-based LM fine-tuning. We introduce Memory-Efficient Zeroth-Order Stochastic Variance-Reduced Gradient (MeZO-SVRG) and demonstrate its efficacy across multiple LM fine-tuning tasks, eliminating the reliance on task-specific prompts. Evaluated across a range of both masked and autoregressive LMs on benchmark GLUE tasks, MeZO-SVRG outperforms MeZO with up to 20% increase in test accuracies in both full- and partial-parameter fine-tuning settings. MeZO-SVRG benefits from reduced computation time as it often surpasses MeZO's peak test accuracy with a $2\\times$ reduction in GPU-hours. MeZO-SVRG significantly reduces the required memory footprint compared to first-order SGD, i.e. by $2\\times$ for autoregressive models. Our experiments highlight that MeZO-SVRG's memory savings progressively improve compared to SGD with larger batch sizes.",
  "abstract_zh": "摘要：微调语言模型（LM）在各种下游任务中取得了成功。然而，随着LM规模的扩大，反向传播的内存需求变得过于高昂。零阶（ZO）优化方法可以利用内存高效的前向传播来估计梯度。最近，ZO-SGD的一个改编版本MeZO已被证明在结合适当的任务提示时，能够持续超越零-shot和上下文学习。在本研究中，我们将ZO方法与方差减少技术结合，以增强基于推理的LM微调的稳定性和收敛性。我们引入了内存高效的零阶随机方差减少梯度（MeZO-SVRG），并展示了其在多个LM微调任务中的有效性，消除了对特定任务提示的依赖。在基准GLUE任务上评估的多种掩蔽和自回归LM中，MeZO-SVRG在全参数和部分参数微调设置中，测试准确率比MeZO提高了多达20%。MeZO-SVRG由于计算时间减少，通常在GPU小时数减少$2\\times$的情况下超越MeZO的峰值测试准确率。与一阶SGD相比，MeZO-SVRG显著减少了所需的内存占用，即对于自回归模型减少了$2\\times$。我们的实验表明，与SGD相比，MeZO-SVRG的内存节省在更大批量大小下逐渐改善。"
}
{
  "title": "Stealing part of a production language model",
  "title_zh": "盗取生产语言模型的一部分",
  "abstract": "We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \r\n$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the GPT-3.5-turbo model, and estimate it would cost under \\\\$2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.",
  "abstract_zh": "我们介绍了首个模型盗取攻击，该攻击从黑箱生产语言模型（如OpenAI的ChatGPT或谷歌的PaLM-2）中提取精确且非平凡的信息。具体而言，我们的攻击在典型API访问的情况下恢复了变换器模型的嵌入投影层（考虑对称性）。仅需不到20美元，我们的攻击就提取了OpenAI的Ada和Babbage语言模型的整个投影矩阵。由此，我们首次确认这些黑箱模型的隐藏维度分别为1024和2048。我们还恢复了GPT-3.5-turbo模型的确切隐藏维度大小，并估计恢复整个投影矩阵的查询成本将低于2000美元。最后，我们讨论了潜在的防御和缓解措施，以及可能扩展我们攻击的未来工作的影响。"
}
{
  "title": "An Embodied Generalist Agent in 3D World",
  "title_zh": "3D世界中的具身通用智能体",
  "abstract": "Leveraging massive knowledge from large language models (LLMs), recent machine learning models show notable successes in general-purpose task solving in diverse domains such as computer vision and robotics. However, several significant challenges remain: (i) most of these models rely on 2D images yet exhibit a limited capacity for 3D input; (ii) these models rarely explore the tasks inherently defined in 3D world, e.g., 3D grounding, embodied reasoning and acting. We argue these limitations significantly hinder current models from performing real-world tasks and approaching general intelligence. To this end, we introduce LEO, an embodied multi-modal generalist agent that excels in perceiving, grounding, reasoning, planning, and acting in the 3D world. LEO is trained with a unified task interface, model architecture, and objective in two stages: (i) 3D vision-language (VL) alignment and (ii) 3D vision-language-action (VLA) instruction tuning. We collect large-scale datasets comprising diverse object-level and scene-level tasks, which require considerable understanding of and interaction with the 3D world. Moreover, we meticulously design an LLM-assisted pipeline to produce high-quality 3D VL data. Through extensive experiments, we demonstrate LEO's remarkable proficiency across a wide spectrum of tasks, including 3D captioning, question answering, embodied reasoning, navigation and manipulation. Our ablative studies and scaling analyses further provide valuable insights for developing future embodied generalist agents. Code and data are available on [project page](https://embodied-generalist.github.io/).",
  "abstract_zh": "利用大型语言模型（LLMs）中的大量知识，最近的机器学习模型在计算机视觉和机器人等多个领域的通用任务解决中取得了显著成功。然而，仍然存在几个重大挑战：（i）这些模型大多依赖于2D图像，但对3D输入的处理能力有限；（ii）这些模型很少探索在3D世界中固有定义的任务，例如3D定位、具身推理和行动。我们认为这些限制显著阻碍了当前模型执行现实世界任务和接近通用智能。为此，我们介绍了LEO，一个在3D世界中擅长感知、定位、推理、规划和行动的具身多模态通用智能体。LEO通过统一的任务接口、模型架构和目标分两个阶段进行训练：（i）3D视觉-语言（VL）对齐和（ii）3D视觉-语言-行动（VLA）指令调优。我们收集了大规模数据集，涵盖了多样的对象级和场景级任务，这些任务需要对3D世界有相当的理解和互动。此外，我们精心设计了一个LLM辅助管道，以生成高质量的3D VL数据。通过广泛的实验，我们展示了LEO在包括3D字幕生成、问答、具身推理、导航和操作等广泛任务中的卓越能力。我们的消融研究和规模分析进一步为未来具身通用智能体的开发提供了宝贵的见解。代码和数据可在[项目页面](https://embodied-generalist.github.io/)获取。"
}
{
  "title": "The Linear Representation Hypothesis and the Geometry of Large Language Models",
  "title_zh": "线性表示假设与大型语言模型的几何结构",
  "abstract": "Informally, the \"linear representation hypothesis\" is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does \"linear representation\" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity and projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of linear representation, one in the output (word) representation space, and one in the input (context) space. We then prove that these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this *causal inner product*, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.",
  "abstract_zh": "非正式地说，“线性表示假设”是指高层次概念在某种表示空间中以方向线性表示的想法。本文探讨了两个密切相关的问题：什么是“线性表示”？我们如何理解表示空间中的几何概念（例如，余弦相似度和投影）？为了解答这些问题，我们使用反事实的语言给出线性表示的两个形式化，一个是在输出（词）表示空间，另一个是在输入（上下文）空间。然后我们证明这些分别与线性探测和模型引导相关。为了理解几何概念，我们使用形式化来识别一个特定的（非欧几里得）内积，这在某种意义上尊重语言结构。利用这个*因果内积*，我们展示了如何统一所有线性表示的概念。特别是，这允许使用反事实对构建探针和引导向量。对LLaMA-2的实验表明了概念的线性表示的存在、与解释和控制的联系，以及内积选择的基本作用。"
}
{
  "title": "Auto-Encoding Morph-Tokens for Multimodal LLM",
  "title_zh": "自动编码形态标记用于多模态大语言模型",
  "abstract": "For multimodal LLMs, the synergy of visual comprehension (textual output) and generation (visual output) presents an ongoing challenge. This is due to a conflicting objective: for comprehension, an MLLM needs to abstract the visuals; for generation, it needs to preserve the visuals as much as possible. Thus, the objective is a dilemma for visual-tokens. To resolve the conflict, we propose encoding images into morph-tokens to serve a dual purpose: for comprehension, they act as visual prompts instructing MLLM to generate texts; for generation, they take on a different, non-conflicting role as complete visual-tokens for image reconstruction, where the missing visual cues are recovered by the MLLM. Extensive experiments show that morph-tokens can achieve a new SOTA for multimodal comprehension and generation simultaneously. Our project is available at https://github.com/DCDmllm/MorphTokens.",
  "abstract_zh": "对于多模态大语言模型，视觉理解（文本输出）和生成（视觉输出）之间的协同作用仍然是一个持续的挑战。这是由于一个相互矛盾的目标：对于理解，MLLM需要抽象视觉；而对于生成，它需要尽可能保留视觉。因此，这一目标对视觉标记构成了困境。为了解决这一冲突，我们提出将图像编码为形态标记，以实现双重目的：在理解中，它们作为视觉提示指导MLLM生成文本；在生成中，它们作为完整的视觉标记用于图像重建，缺失的视觉线索由MLLM恢复。大量实验表明，形态标记能够同时实现多模态理解和生成的新状态。我们的项目可在 https://github.com/DCDmllm/MorphTokens 获取。"
}
{
  "title": "Position: LLMs Can’t Plan, But Can Help Planning in LLM-Modulo Frameworks",
  "title_zh": "标题：位置：大型语言模型无法规划，但可以在大型语言模型模组框架中帮助规划",
  "abstract": "We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators. We present a vision of LLM-Modulo Frameworks that combine the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime. We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs. We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, and allows extending the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications.",
  "abstract_zh": "摘要：我们认为自回归大型语言模型无法单独进行规划或自我验证（毕竟这是一种推理形式），并阐明了文献中误解的原因。我们还将论证，大型语言模型应被视为通用的近似知识源，在规划/推理任务中扮演更有意义的角色，而不仅仅是简单的前端/后端格式翻译器。我们提出了一个大型语言模型模组框架的愿景，该框架结合了大型语言模型的优势与外部基于模型的验证器，在更紧密的双向交互机制中运作。我们将展示如何借助大型语言模型获取驱动外部验证器的模型。我们还将论证，与其简单地将大型语言模型和符号组件进行流水线处理，这个大型语言模型模组框架提供了一种更好的神经符号方法，能够更紧密地整合大型语言模型和符号组件，并允许将基于模型的规划/推理机制的范围扩展到更灵活的知识、问题和偏好规范。"
}
{
  "title": "Data Engineering for Scaling Language Models to 128K Context",
  "title_zh": "标题：将语言模型扩展到128K上下文的数据工程",
  "abstract": "We study continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular *the ability to utilize information at arbitrary input locations*, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training (e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the *quantity* and *quality* of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize *domain balance* and *length upsampling*. Concretely, naïvely upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance; a balanced domain mixture is equally important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K.",
  "abstract_zh": "摘要：我们研究了将语言模型上下文长度扩展到128K的持续预训练方案，重点关注数据工程。我们假设，长上下文建模，特别是*在任意输入位置利用信息的能力*，是通过大规模预训练大多数已经获得的能力，并且这种能力可以通过在适当数据混合上进行轻量级持续预训练，轻松扩展到比训练期间看到的上下文长得多的情况（例如，从4K到128K）。我们调查了持续预训练的数据的*数量*和*质量*：（1）在数量方面，我们表明500百万到50亿个标记足以使模型在128K上下文中检索信息；（2）在质量方面，我们的结果同样强调*领域平衡*和*长度上采样*。具体而言，简单地在某些领域（如书籍）上对较长数据进行上采样，这是一种现有工作的常见做法，表现不佳；平衡的领域混合同样重要。我们证明，在1B-5B个标记的此类数据上对完整模型进行持续预训练是一种有效且经济的策略，可以将语言模型的上下文长度扩展到128K。我们的方案优于强大的开源长上下文模型，并缩小了与前沿模型如GPT-4 128K的差距。"
}
{
  "title": "Tandem Transformers for Inference Efficient LLMs",
  "title_zh": "串联变压器用于高效推理的大型语言模型",
  "abstract": "The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative (Leviathan et al., 2023) and parallel (Stern et al., 2018) decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations. We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream performance. We further incorporate the Tandem model within the speculative decoding (SPEED) framework where the large model validates tokens from the small model. This ensures that the tandem of PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream task accuracy.",
  "abstract_zh": "传统大型语言模型（LLMs）的自回归特性固有地限制了推理速度，因为令牌是顺序生成的。尽管投机性（Leviathan等，2023）和并行（Stern等，2018）解码技术试图缓解这一问题，但它们面临局限性：要么依赖于准确性较低的小模型进行生成，要么未能充分利用基础LLM的表示。我们提出了一种新颖的架构——串联变压器，以解决这些问题。该架构独特地结合了（1）一个小的自回归模型和（2）一个以块模式运行的大模型（同时处理多个令牌）。通过让小模型关注大模型更丰富的表示，其预测准确性得到了显著提升。在PaLM2预训练数据集上，PaLM2-Bison和PaLM2-Gecko的串联在下一个令牌预测准确性上比单独的PaLM2-Gecko提高了3.3%，并且与具有可比下游性能的PaLM2-Otter模型相比，提供了1.16倍的加速。我们进一步将串联模型纳入投机解码（SPEED）框架，其中大模型验证小模型的令牌。这确保了PaLM2-Bison和PaLM2-Gecko的串联在保持相同下游任务准确性的同时，实现了显著的加速（比使用原始PaLM2-Gecko在SPEED中快约1.14倍）。"
}
{
  "title": "diff History for Neural Language Agents",
  "title_zh": "神经语言代理的差异历史",
  "abstract": "Neural Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, results in long and verbose textual prompts. As a result, prior work in LM agents is limited to restricted domains with small observation size as well as minimal needs for interaction history or instruction finetuning. In this paper, we introduce diff history, a simple and highly effective solution to these issues. By applying the Unix diff command on consecutive text observations in the interaction histories used to prompt LM policies, we can both abstract away redundant information and focus the content of textual inputs on the salient changes in the environment. On NetHack, an unsolved video game that requires long-horizon reasoning for decision-making, LMs tuned with diff history match state-of-the-art performance for neural agents while needing 1800X fewer training examples compared to prior work. Even on the simpler BabyAI-Text environment with concise text observations, we find that although diff history increases the length of prompts, the representation it provides offers a 25% improvement in the efficiency of low-sample instruction finetuning. Further, we show that diff history scales favorably across different finetuning dataset sizes. We open-source our code and data to https://diffhistory.github.io.",
  "abstract_zh": "神经语言模型（LM）为通用的具身控制提供了一个令人兴奋的解决方案。然而，使用基于LM的控制器时出现了一个关键技术问题：环境观察必须转换为文本，这与历史记录相结合，导致冗长的文本提示。因此，LM代理的先前工作仅限于观察规模小且对交互历史或指令微调需求最低的受限领域。在本文中，我们介绍了差异历史，这是一种简单而高效的解决方案。通过在用于提示LM策略的交互历史中的连续文本观察上应用Unix diff命令，我们可以抽象掉冗余信息，并将文本输入的内容集中在环境中的显著变化上。在需要长时间推理进行决策的未解决视频游戏NetHack中，使用差异历史调优的LM在神经代理的性能上达到了最先进的水平，同时相比于先前的工作需要少1800倍的训练示例。即使在具有简洁文本观察的简单BabyAI-Text环境中，我们发现尽管差异历史增加了提示的长度，但它提供的表示在低样本指令微调的效率上提高了25%。此外，我们还展示了差异历史在不同微调数据集规模上具有良好的扩展性。我们将代码和数据开源至https://diffhistory.github.io。"
}
{
  "title": "Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark",
  "title_zh": "标题：重新审视零阶优化在内存高效大语言模型微调中的应用：基准研究",
  "abstract": "In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow in size, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by (Malladi et al., 2023). Unlike traditional ZO-SGD methods, ou让work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families, three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments will be made public.",
  "abstract_zh": "摘要：在自然语言处理（NLP）不断发展的背景下，使用一阶（FO）优化器如SGD和Adam对预训练的大语言模型（LLMs）进行微调已成为标准。然而，随着LLMs规模的扩大，FO梯度计算中反向传播（BP）带来的大量内存开销成为一个重大挑战。解决这个问题至关重要，特别是在内存效率至关重要的设备端训练等应用中。本文提出转向无BP的零阶（ZO）优化，作为降低LLM微调过程中的内存成本的解决方案，基于（Malladi等，2023）提出的初步概念。与传统的ZO-SGD方法不同，我们的工作扩展了对更广泛的ZO优化技术的探索，通过对五个LLM家族、三种任务复杂性和五种微调方案进行全面的首次基准研究。我们的研究揭示了先前被忽视的优化原则，强调了任务对齐的重要性、前向梯度方法的作用以及算法复杂性与微调性能之间的平衡。我们进一步引入了对ZO优化的新增强，包括块状下降、混合训练和梯度稀疏性。我们的研究为实现更高效的内存LLM微调提供了一个有前景的方向。我们将公开所有实验的代码以供重现。"
}
{
  "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models",
  "title_zh": "ReMax：一种简单、有效且高效的强化学习方法，用于对齐大型语言模型",
  "abstract": "Reinforcement Learning from Human Feedback (RLHF) is key to aligning Large Language Models (LLMs), typically paired with the Proximal Policy Optimization (PPO) algorithm. While PPO is a powerful method designed for general reinforcement learning tasks, it is overly sophisticated for LLMs, leading to laborious hyper-parameter tuning and significant computation burdens. To make RLHF efficient, we present ReMax, which leverages 3 properties of RLHF: fast simulation, deterministic transitions, and trajectory-level rewards. These properties are not exploited in PPO, making it less suitable for RLHF. Building on the renowned REINFORCE algorithm, ReMax does not require training an additional value model as in PPO and is further enhanced with a new variance reduction technique. ReMax offers several benefits over PPO: it is simpler to implement, eliminates more than 4 hyper-parameters in PPO, reduces GPU memory usage, and shortens training time. ReMax can save about 46% GPU memory than PPO when training a 7B model and enables training on A800-80GB GPUs without the memory-saving offloading technique needed by PPO. Applying ReMax to a Mistral-7B model resulted in a 94.78% win rate on the AlpacaEval leaderboard and a 7.739 score on MT-bench, setting a new SOTA for open-source 7B models. These results show the effectiveness of ReMax while addressing the limitations of PPO in LLMs.",
  "abstract_zh": "来自人类反馈的强化学习（RLHF）是对齐大型语言模型（LLMs）的关键，通常与近端策略优化（PPO）算法配对。虽然PPO是一种针对一般强化学习任务设计的强大方法，但对于LLMs来说过于复杂，导致繁琐的超参数调优和显著的计算负担。为了提高RLHF的效率，我们提出了ReMax，它利用了RLHF的三个特性：快速仿真、确定性转移和轨迹级奖励。这些特性在PPO中并未得到利用，使其不太适合RLHF。基于著名的REINFORCE算法，ReMax不需要像PPO那样训练额外的价值模型，并通过一种新的方差减少技术进一步增强。ReMax相较于PPO提供了多个优势：实现更简单，消除了PPO中超过4个超参数，减少了GPU内存使用，并缩短了训练时间。在训练一个7B模型时，ReMax可以节省约46%的GPU内存，并使得在A800-80GB GPU上训练成为可能，而不需要PPO所需的内存节省卸载技术。将ReMax应用于Mistral-7B模型，在AlpacaEval排行榜上获得了94.78%的胜率，在MT-bench上得分7.739，创造了开源7B模型的新SOTA。这些结果展示了ReMax的有效性，同时解决了PPO在LLMs中的局限性。"
}
{
  "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
  "title_zh": "对过时嵌入的新视角：通过修正网络改善密集检索器训练",
  "abstract": "In dense retrieval, deep encoders provide embeddings for both inputs and targets, and the softmax function is used to parameterize a distribution over a large number of candidate targets (e.g., textual passages for information retrieval). Significant challenges arise in training such encoders in the increasingly prevalent scenario of (1) a large number of targets, (2) a computationally expensive target encoder model, (3) cached target embeddings that are out-of-date due to ongoing training of target encoder parameters. This paper presents a simple and highly scalable response to these challenges by training a small parametric _corrector network_ that adjusts stale cached target embeddings, enabling an accurate softmax approximation and thereby sampling of up-to-date high scoring \"hard negatives.\" We theoretically investigate the generalization properties of our proposed target corrector, relating the complexity of the network, staleness of cached representations, and the amount of training data. We present experimental results on large benchmark dense retrieval datasets as well as on QA with retrieval augmented language models. Our approach matches state-of-the-art results even when no target embedding updates are made during training beyond an initial cache from the unsupervised pre-trained model, providing a 4-80x reduction in re-embedding computational cost.",
  "abstract_zh": "在密集检索中，深度编码器为输入和目标提供嵌入，并使用softmax函数对大量候选目标（例如，用于信息检索的文本段落）进行参数化分布。在目标数量庞大、目标编码器模型计算开销大、以及由于目标编码器参数的持续训练而导致的过时缓存目标嵌入等日益普遍的情况下，训练此类编码器面临重大挑战。本文提出了一种简单且高度可扩展的解决方案，通过训练一个小型参数化的修正网络来调整过时的缓存目标嵌入，从而实现准确的softmax近似，并能够抽样最新的高评分“困难负样本”。我们理论上研究了所提目标修正器的泛化特性，关联了网络的复杂性、缓存表示的过时程度以及训练数据的数量。我们在大型基准密集检索数据集以及检索增强语言模型的问答任务上展示了实验结果。即使在训练过程中没有对目标嵌入进行更新，仅依赖于来自无监督预训练模型的初始缓存，我们的方法仍然达到了最先进的结果，并在重新嵌入计算成本上提供了4-80倍的降低。"
}
{
  "title": "Assessing Large Language Models on Climate Information",
  "title_zh": "评估大型语言模型在气候信息上的表现",
  "abstract": "As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in critically relevant domains. We present a comprehensive evaluation framework, grounded in science communication research, to assess LLM responses to questions about climate change. Our framework emphasizes both presentational and epistemological adequacy, offering a fine-grained analysis of LLM generations spanning 8 dimensions and 30 issues. Our evaluation task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel protocol for scalable oversight that relies on AI Assistance and raters with relevant education. We evaluate several recent LLMs on a set of diverse climate questions. Our results point to a significant gap between surface and epistemological qualities of LLMs in the realm of climate communication.",
  "abstract_zh": "随着大型语言模型（LLMs）的流行，有必要评估它们在关键相关领域的能力。我们提出了一个基于科学传播研究的综合评估框架，以评估LLM对气候变化问题的回答。我们的框架强调表现和认识论的适当性，提供了对LLM生成内容的细致分析，涵盖8个维度和30个问题。我们的评估任务是一个现实世界的例子，展示了AI如何补充并提升人类表现的日益增长的挑战性问题。我们引入了一种新颖的可扩展监督协议，依赖于AI辅助和具有相关教育背景的评估者。我们对几种近期的LLM在一组多样的气候问题上进行了评估。我们的结果显示，LLM在气候传播领域的表面质量与认识论质量之间存在显著差距。"
}
{
  "title": "CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay",
  "title_zh": "代码迭代：通过优先回顾重放自我改进的语言模型",
  "abstract": "Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach the ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the program output given input) to the output actually produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-symbolic approach that scales to the full ARC evaluation dataset. Our method solves 15% of ARC evaluation tasks, achieving state-of-the-art performance and outperforming existing neural and symbolic baselines. Our code is available at https://github.com/Qualcomm-AI-research/codeit.",
  "abstract_zh": "大型语言模型在解决被普遍认为需要人类级推理能力的任务方面越来越有效。然而，这些模型在一般智能基准测试（如抽象与推理语料库ARC）上的表现仍然很差。在本文中，我们将ARC视为一个通过示例编程的问题，并引入了一种新颖且可扩展的语言模型自我改进方法，称为代码迭代（CodeIt）。我们的方法在1）程序采样和回顾重标记，2）从优先经验重放中学习之间进行迭代。通过将一个回合的目标（即给定输入的程序输出）重新标记为实际由采样程序生成的输出，我们的方法有效地解决了程序合成中奖励的极度稀疏性。将CodeIt应用于ARC数据集，我们证明了优先回顾重放结合预训练和数据增强，能够实现成功的跨任务泛化。CodeIt是第一个能够扩展到完整ARC评估数据集的神经符号方法。我们的方法解决了15%的ARC评估任务，达到了最先进的性能，并超越了现有的神经和符号基线。我们的代码可在https://github.com/Qualcomm-AI-research/codeit获取。"
}
{
  "title": "UniAudio: Towards Universal Audio Generation with Large Language Models",
  "title_zh": "统一音频：基于大型语言模型的通用音频生成",
  "abstract": "Audio generation is a major branch of generative AI research. Compared with prior works in this area that are commonly task-specific with heavy domain knowledge, this paper advocates building universal audio generation models that can handle various tasks in a unified manner. As recent research on large language models (LLMs) has demonstrated their strong ability to handle multiple tasks, this work presents UniAudio, an LLM-based audio generation model that supports a wide range of audio generation tasks. Based on various input conditions, such as phoneme, text description, or audio itself, UniAudio can generate speech, sound, music, and singing voice. The proposed UniAudio is built with 100k hours of multi-source open-available audio data and is scaled to 1B parameters. The audio tokenization method and language model architecture are also specifically designed for both performance and efficiency. Experimentally, UniAuido supports 11 audio generation tasks and achieves competitive results on all tasks consistently. We also show that UniAudio can support new tasks seamlessly via simple fine-tuning.",
  "abstract_zh": "音频生成是生成性人工智能研究的一个主要分支。与该领域以往的任务特定且需要大量领域知识的研究相比，本文倡导构建能够以统一方式处理各种任务的通用音频生成模型。最近关于大型语言模型（LLMs）的研究表明，它们在处理多任务方面具有强大的能力，因此本研究提出了UniAudio，一个基于LLM的音频生成模型，支持广泛的音频生成任务。UniAudio可以根据不同的输入条件，如音素、文本描述或音频本身，生成语音、声音、音乐和歌声。所提出的UniAudio基于10万小时的多源开放音频数据构建，并扩展到10亿参数。音频标记方法和语言模型架构也专门为性能和效率而设计。实验表明，UniAudio支持11个音频生成任务，并在所有任务上始终取得竞争性结果。我们还展示了UniAudio可以通过简单的微调无缝支持新任务。"
}
{
  "title": "Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization",
  "title_zh": "视频-LaVIT：解耦视觉-运动标记化的统一视频-语言预训练",
  "abstract": "In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models are available at https://video-lavit.github.io.",
  "abstract_zh": "鉴于多模态大型语言模型（LLMs）的最新进展，越来越多的关注集中在将其从图像-文本数据扩展到更具信息量的现实世界视频上。与静态图像相比，视频在有效的大规模预训练中面临独特的挑战，因为其时空动态的建模。在本文中，我们通过高效的视频分解来解决视频-语言预训练中的这些局限性，将每个视频表示为关键帧和时间运动。然后，这些被适配到一个大型语言模型（LLM），使用精心设计的标记器将视觉和时间信息离散化为少量标记，从而实现视频、图像和文本的统一生成预训练。在推理时，从LLM生成的标记被仔细恢复到原始连续像素空间，以创建各种视频内容。我们提出的框架能够理解和生成图像和视频内容，证明其在图像和视频理解与生成的13个多模态基准测试中的竞争性能。我们的代码和模型可在 https://video-lavit.github.io 获取。"
}
{
  "title": "Multi-Patch Prediction: Adapting Language Models for Time Series Representation Learning",
  "title_zh": "多补丁预测：将语言模型适应于时间序列表示学习",
  "abstract": "In this study, we present $\\text{aL\\small{LM}4T\\small{S}}$, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii). fine-tuning for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mastering temporal patch-based representations. $\\text{aL\\small{LM}4T\\small{S}}$ demonstrates superior performance in several downstream tasks, proving its effectiveness in deriving temporal representations with enhanced transferability and marking a pivotal advancement in the adaptation of LLMs for time-series analysis.",
  "abstract_zh": "在本研究中，我们提出了$\\text{aL\\small{LM}4T\\small{S}}$，这是一个创新框架，旨在将大型语言模型（LLMs）适应于时间序列表示学习。我们的方法的核心是将时间序列预测重新构想为自监督的多补丁预测任务，与传统的掩码和重建方法相比，更有效地捕捉补丁表示中的时间动态。我们的策略包括两个阶段的训练：（i）在各种时间序列数据集上进行基于下一个补丁预测的因果持续预训练阶段，有效地将LLM能力与时间序列数据的复杂性同步；（ii）在特定的时间序列上下文中进行多补丁预测的微调。我们框架的一个独特元素是补丁级解码层，它不同于依赖于序列级解码的先前方法。这种设计直接将单个补丁转化为时间序列，从而显著增强模型掌握基于时间的补丁表示的能力。$\\text{aL\\small{LM}4T\\small{S}}$在多个下游任务中表现出色，证明了其在获取具有增强可迁移性的时间表示方面的有效性，并标志着LLMs在时间序列分析适应中的重要进展。"
}
{
  "title": "AutoOS: Make Your OS More Powerful by Exploiting Large Language Models",
  "title_zh": "自动操作系统：通过利用大型语言模型增强您的操作系统能力",
  "abstract": "With the rapid development of Artificial Intelligence of Things (AIoT), customizing and optimizing operating system (OS) kernel configurations for various AIoT application scenarios is crucial for maximizing system performance. However, existing approaches falter due to the overwhelming problem complexity (i.e., over 15,000 configuration options in the Linux kernel), together with the huge evaluation costs and error-prone options that may result in OS boot-up failure, which all make it an unresolved problem to optimize the Linux kernel automatically. In this paper, we introduce AutoOS, a novel framework exploiting Large Language Models for customizing and optimizing OS kernel configurations automatically for various AIoT application scenarios.Inspired by the inherently directory-structured kernel configuration process,  we first formulate our research problem as optimizing on a dynamic tree. We then propose a novel framework integrating a state machine-based traversal algorithm as the observe-prune-propose-act-correct loop, which can effectively refine the optimization space and ensure a successful OS boot-up.Experimental results show that AutoOS can automatically customize and optimize the OS kernel configurations without human effort. More importantly, AutoOS even achieves better performance by up to 25% than vendor-provided configuration.",
  "abstract_zh": "随着物联网人工智能（AIoT）的快速发展，为各种AIoT应用场景定制和优化操作系统（OS）内核配置对于最大化系统性能至关重要。然而，现有方法由于问题复杂性过高（即Linux内核中超过15,000个配置选项）、评估成本巨大以及可能导致操作系统启动失败的易错选项而受到限制，这使得自动优化Linux内核成为一个未解决的问题。本文介绍了AutoOS，一个利用大型语言模型自动定制和优化操作系统内核配置的新框架，适用于各种AIoT应用场景。我们首先将研究问题表述为在动态树上进行优化，受内核配置过程的目录结构启发。然后，我们提出了一种新框架，集成了基于状态机的遍历算法，形成观察-修剪-提议-行动-校正循环，能够有效细化优化空间并确保操作系统成功启动。实验结果表明，AutoOS能够在没有人工干预的情况下自动定制和优化操作系统内核配置。更重要的是，AutoOS的性能比供应商提供的配置提高了多达25%。"
}
{
  "title": "Stay on Topic with Classifier-Free Guidance",
  "title_zh": "保持主题一致的无分类器引导",
  "abstract": "Classifier-Free Guidance (CFG) has recently emerged in as a lightweight technique to encourage prompt-adherence in generations, yet has not yet been successfully applied to language modeling. In this work, we demonstrate across a wide array of benchmarks that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across: Q&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in human evaluations we show a 75% preference for using CFG over baseline.",
  "abstract_zh": "无分类器引导（CFG）最近作为一种轻量级技术出现，用于鼓励生成中的提示遵循性，但尚未成功应用于语言建模。在这项工作中，我们在广泛的基准测试中展示了CFG可以作为纯语言建模中的推理时间技术广泛使用。我们表明，CFG（1）在问答、推理、代码生成和机器翻译等任务中提升了Pythia、GPT-2和LLaMA系列模型的性能，在LAMBADA上以LLaMA-7B超越PaLM-540B达到了SOTA；（2）带来了相当于参数数量翻倍的模型的改进；（3）可以与其他推理时间方法如思维链和自我一致性叠加，进一步提升困难任务的表现；（4）可以用于提高助手在挑战性形式驱动和内容驱动提示中的真实性和连贯性：在人类评估中，我们显示出对使用CFG的偏好达75%，优于基线。"
}
{
  "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
  "title_zh": "标题：大型语言模型中学习与遗忘不安全示例",
  "abstract": "As the number of large language models (LLMs) released to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data. We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content. Drawing inspiration from the discrepancies in forgetting, we introduce the “ForgetFilter” algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data. We demonstrate that the ForgetFilter algorithm ensures safety in customized finetuning without compromising downstream task performance, unlike sequential safety finetuning. ForgetFilter outperforms alternative strategies like replay and moral self-correction in curbing LLMs’ ability to assimilate unsafe content during custom finetuning, e.g. 75% lower than not applying any safety measures and 62% lower than using self-correction in toxicity score.",
  "abstract_zh": "摘要：随着公开发布的大型语言模型（LLMs）数量的增加，迫切需要理解这些模型从第三方自定义微调数据中学习所带来的安全隐患。我们探讨了在包含偏见、毒性和有害内容的噪声自定义数据上微调的LLMs的行为，发现虽然对齐的LLMs能够轻松学习这些不安全内容，但在随后在更安全内容上微调时，它们往往比其他示例更显著地遗忘这些内容。受到遗忘差异的启发，我们提出了“ForgetFilter”算法，该算法根据模型对该数据的遗忘信号强度来过滤不安全数据。我们证明ForgetFilter算法在不影响下游任务性能的情况下确保了自定义微调的安全性，这与顺序安全微调不同。ForgetFilter在抑制LLMs在自定义微调过程中吸收不安全内容的能力方面优于重放和道德自我纠正等替代策略，例如比不采取任何安全措施低75%，比使用毒性评分自我纠正低62%。"
}
{
  "title": "Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers",
  "title_zh": "使用你的直觉：基于神经赌博机与变换器的LLM指令优化",
  "abstract": "Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algorithm which replaces the GP in BO by an NN surrogate to optimize instructions for black-box LLMs. More importantly, the neural bandit algorithm allows us to naturally couple the NN surrogate with the hidden representation learned by a pre-trained transformer (i.e., an open-source LLM), which significantly boosts its performance. These motivate us to propose our INSTruction optimization usIng Neural bandits Coupled with Transformers (INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use extensive experiments to show that INSTINCT consistently outperforms baselines in different tasks, e.g., various instruction induction tasks and the task of improving zero-shot chain-of-thought instructions. Our code is available at https://github.com/xqlin98/INSTINCT.",
  "abstract_zh": "大型语言模型（LLMs）在遵循指令方面表现出色，并在各种应用中取得了令人印象深刻的成绩。然而，LLMs的表现很大程度上依赖于给定的指令，这些指令通常需要经过大量人工努力进行手动调整。最近的研究使用查询高效的贝叶斯优化（BO）算法自动优化提供给黑箱LLMs的指令。然而，当优化高度复杂（例如，高维）目标函数时，BO通常表现不佳，例如将指令映射到LLM性能的函数。这主要是由于BO所使用的高斯过程（GP）作为代理来建模目标函数的表达能力有限。同时，已经反复证明神经网络（NNs），特别是预训练的变换器，具有强大的表达能力，并能够建模高度复杂的函数。因此，我们采用了一种神经赌博机算法，该算法通过NN代理替代BO中的GP，以优化黑箱LLMs的指令。更重要的是，神经赌博机算法使我们能够自然地将NN代理与由预训练变换器（即开源LLM）学习的隐藏表示结合起来，从而显著提升其性能。这促使我们提出了我们的指令优化算法INSTruction optimization usIng Neural bandits Coupled with Transformers（INSTINCT）。我们对ChatGPT进行了指令优化，并通过大量实验表明，INSTINCT在不同任务中始终优于基线，例如各种指令归纳任务和改善零-shot思维链指令的任务。我们的代码可在https://github.com/xqlin98/INSTINCT获取。"
}
{
  "title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI",
  "title_zh": "MMT-Bench：评估大型视觉-语言模型在多任务通用人工智能中的全面多模态基准",
  "abstract": "Large Vision-Language Models (LVLMs) show significant strides in general-propose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, and reasoning. MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving $20$ publicly available LVLMs such as the proprietary GeminiProVision model, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence.",
  "abstract_zh": "大型视觉-语言模型（LVLMs）在视觉对话和具身导航等通用多模态应用中取得了显著进展。然而，现有的多模态评估基准覆盖的多模态任务数量有限，仅测试基本能力，未能跟踪LVLM的发展。在本研究中，我们提出了MMT-Bench，这是一个全面的基准，旨在评估LVLM在需要专业知识和深思熟虑的视觉识别、定位和推理的海量多模态任务中的表现。MMT-Bench包含$31,325$个精心策划的多选视觉问题，来自车辆驾驶和具身导航等多种多模态场景，涵盖$32$个核心元任务和$162$个多模态理解的子任务。由于其广泛的任务覆盖，MMT-Bench使得通过任务地图评估LVLM成为可能，促进了对领域内外任务的发现。涉及$20$个公开可用的LVLM（如专有的GeminiProVision模型）的评估结果，突显了MMT-Bench所带来的重大挑战。我们预计MMT-Bench将激励社区开发下一代多模态基础模型，旨在实现通用多模态智能。"
}
{
  "title": "MuxServe: Flexible Spatial-Temporal Multiplexing for Multiple LLM Serving",
  "title_zh": "MuxServe：用于多个大型语言模型服务的灵活时空复用",
  "abstract": "Large language models (LLMs) have demonstrated remarkable performance, and organizations are racing to serve LLMs of varying sizes as endpoints for use-cases like chat, programming and search. However, efficiently serving multiple LLMs poses significant challenges for existing approaches due to varying popularity of LLMs. In the paper, we present MuxServe, a flexible spatial-temporal multiplexing system for efficient multiple LLM serving. The key insight behind is to colocate LLMs considering their popularity to multiplex memory resources, and leverage the characteristics of prefill and decoding phases to separate and flexibly colocate them to multiplex computation resources. MuxServe formally formulates the multiplexing problem, and proposes a novel placement algorithm and adaptive batch scheduling strategy to identify optimal colocations and maximize utilization. MuxServe designs a unified resource manager to enable flexible and efficient multiplexing. Evaluation results show that MuxServe can achieves up to $1.8\\times$ higher throughput or processes $2.9\\times$ more requests within $99\\%$ SLO attainment. The code is available at: https://github.com/hao-ai-lab/MuxServe.",
  "abstract_zh": "大型语言模型（LLMs）表现出卓越的性能，组织机构正在争相提供不同规模的LLM作为聊天、编程和搜索等用例的端点。然而，由于LLM的受欢迎程度各异，现有方法在高效服务多个LLM时面临重大挑战。本文提出了MuxServe，一个灵活的时空复用系统，用于高效服务多个LLM。其关键见解在于考虑LLM的受欢迎程度进行共置，以复用内存资源，并利用预填充和解码阶段的特性，将它们分开并灵活共置以复用计算资源。MuxServe正式构建了复用问题模型，并提出了一种新颖的放置算法和自适应批处理调度策略，以识别最佳共置并最大化利用率。MuxServe设计了一个统一的资源管理器，以实现灵活高效的复用。评估结果表明，MuxServe的吞吐量最高可提高$1.8\\times$，或在$99\\%$ SLO达成率内处理$2.9\\times$更多请求。代码可在：https://github.com/hao-ai-lab/MuxServe获取。"
}
{
  "title": "A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models",
  "title_zh": "标题：生成模型的核评分偏差-方差-协方差分解",
  "abstract": "Generative models, like large language models, are becoming increasingly relevant in our daily lives, yet a theoretical framework to assess their generalization behavior and uncertainty does not exist. Particularly, the problem of uncertainty estimation is commonly solved in an ad-hoc and task-dependent manner. For example, natural language approaches cannot be transferred to image generation. In this paper, we introduce the first bias-variance-covariance decomposition for kernel scores. This decomposition represents a theoretical framework from which we derive a kernel-based variance and entropy for uncertainty estimation. We propose unbiased and consistent estimators for each quantity which only require generated samples but not the underlying model itself. Based on the wide applicability of kernels, we demonstrate our framework via generalization and uncertainty experiments for image, audio, and language generation. Specifically, kernel entropy for uncertainty estimation is more predictive of performance on CoQA and TriviaQA question answering datasets than existing baselines and can also be applied to closed-source models.",
  "abstract_zh": "摘要：生成模型，如大型语言模型，正日益与我们的日常生活相关，但尚不存在评估其泛化行为和不确定性的理论框架。特别是，不确定性估计的问题通常以临时和任务依赖的方式解决。例如，自然语言方法无法转移到图像生成。在本文中，我们引入了首个核评分的偏差-方差-协方差分解。该分解代表了一个理论框架，从中我们推导出基于核的方差和熵用于不确定性估计。我们提出了每个量的无偏和一致的估计量，这些估计量只需要生成的样本，而不需要底层模型本身。基于核的广泛适用性，我们通过图像、音频和语言生成的泛化和不确定性实验展示了我们的框架。具体而言，基于核的熵在不确定性估计中对CoQA和TriviaQA问答数据集的性能预测优于现有基准，并且也可以应用于闭源模型。"
}
{
  "title": "The Illusion of State in State-Space Models",
  "title_zh": "状态空间模型中的状态幻觉",
  "abstract": "State-space models (SSMs) have emerged as a potential alternative architecture for building large language models (LLMs) compared to the previously ubiquitous transformer architecture. One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill & Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks (RNNs). *But do SSMs truly have an advantage (over transformers) in expressive power for state tracking?* Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class $\\mathsf{TC}^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its recurrent formulation, the \"state'' in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.",
  "abstract_zh": "状态空间模型（SSMs）已成为构建大型语言模型（LLMs）的潜在替代架构，相较于之前无处不在的变换器架构。变换器的一个理论弱点是它们无法表达某些类型的顺序计算和状态跟踪（Merrill & Sabharwal, 2023），而SSMs则通过与递归神经网络（RNNs）的紧密架构相似性，明确设计来解决这一问题。*但是，SSMs在状态跟踪的表达能力上真的优于变换器吗？*令人惊讶的是，答案是否定的。我们的分析表明，SSMs的表达能力与变换器非常相似：SSMs无法表达复杂性类$\\mathsf{TC}^0$之外的计算。特别是，这意味着它们无法解决简单的状态跟踪问题，如排列组合。因此，SSMs在某些符号下被证明无法准确跟踪国际象棋走法、评估代码或在长叙述中跟踪实体。为了补充我们的形式分析，我们报告了实验结果，显示Mamba风格的SSMs确实在状态跟踪方面存在困难。因此，尽管其递归形式，SSM中的“状态”是一种幻觉：SSMs在表达能力上与非递归模型如变换器相似，这可能根本限制了它们解决现实世界状态跟踪问题的能力。"
}
{
  "title": "DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models (Exemplified as A Video Agent)",
  "title_zh": "哆啦A梦GPT：理解动态场景的大型语言模型（以视频代理为例）",
  "abstract": "Recent LLM-driven visual agents mainly focus on solving image-based tasks, which limits their ability to understand dynamic scenes, making it far from real-life applications like guiding students in laboratory experiments and identifying their mistakes. Hence, this paper explores DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to understand dynamic scenes. Considering the video modality better reflects the ever-changing nature of real-world scenarios, we exemplify DoraemonGPT as a video agent. Given a video with a question/task, DoraemonGPT begins by converting the input video into a symbolic memory that stores task-related attributes. This structured representation allows for spatial-temporal querying and reasoning by well-designed sub-task tools, resulting in concise intermediate results. Recognizing that LLMs have limited internal knowledge when it comes to specialized domains (e.g., analyzing the scientific principles underlying experiments), we incorporate plug-and-play tools to assess external knowledge and address tasks across different domains. Moreover, a novel LLM-driven planner based on Monte Carlo Tree Search is introduced to explore the large planning space for scheduling various tools. The planner iteratively finds feasible solutions by backpropagating the result's reward, and multiple solutions can be summarized into an improved final answer. We extensively evaluate DoraemonGPT's effectiveness on three benchmarks and several in-the-wild scenarios. Project page: https://z-x-yang.github.io/doraemon-gpt.",
  "abstract_zh": "近年来，基于大型语言模型（LLM）的视觉代理主要集中于解决基于图像的任务，这限制了它们理解动态场景的能力，使其距离现实生活应用（如指导学生进行实验和识别错误）相去甚远。因此，本文探讨了哆啦A梦GPT，这是一个由LLM驱动的全面且概念优雅的系统，用于理解动态场景。考虑到视频模态更好地反映了现实场景的不断变化，我们将哆啦A梦GPT作为视频代理进行示例。给定一个带有问题/任务的视频，哆啦A梦GPT首先将输入视频转换为存储任务相关属性的符号记忆。这种结构化表示允许通过精心设计的子任务工具进行时空查询和推理，从而产生简洁的中间结果。认识到LLM在专业领域（例如，分析实验背后的科学原理）方面的内部知识有限，我们结合即插即用工具来评估外部知识并解决不同领域的任务。此外，基于蒙特卡罗树搜索的新型LLM驱动规划器被引入，以探索大规模规划空间以调度各种工具。该规划器通过反向传播结果的奖励迭代寻找可行解决方案，并可以将多个解决方案汇总为改进的最终答案。我们在三个基准和多个实际场景中广泛评估了哆啦A梦GPT的有效性。项目页面：https://z-x-yang.github.io/doraemon-gpt。"
}
{
  "title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment",
  "title_zh": "标题：上下文中的奖励：基础模型与动态偏好调整的多目标对齐",
  "abstract": "We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline.",
  "abstract_zh": "摘要：我们考虑基础模型与人类偏好的多目标对齐问题，这是实现有益且无害的人工智能系统的关键步骤。然而，使用强化学习（RL）微调大型基础模型通常成本高且不稳定，而人类偏好的多维性、异质性和冲突性进一步复杂化了对齐过程。本文介绍了上下文中的奖励（RiC），它根据提示上下文中的多个奖励来调整基础模型的响应，并应用监督微调进行对齐。RiC的显著特点是简单性和适应性，因为它只需对单个基础模型进行监督微调，并在推理时支持用户偏好的动态调整。受到抽象凸优化问题的解析解的启发，我们的动态推理时调整方法接近多个目标的帕累托最优解。实证证据表明，我们的方法在对齐大型语言模型（LLMs）和扩散模型以适应多样化奖励方面的有效性，仅需约10%的GPU小时，相较于多目标RL基线。"
}
{
  "title": "DPZero: Private Fine-Tuning of Language Models without Backpropagation",
  "title_zh": "DPZero：无反向传播的语言模型私有微调",
  "abstract": "The widespread practice of fine-tuning large language models (LLMs) on domain-specific data faces two major challenges in memory and privacy. First, as the size of LLMs continues to grow, the memory demands of gradient-based training methods via backpropagation become prohibitively high. Second, given the tendency of LLMs to memorize training data, it is important to protect potentially sensitive information in the fine-tuning data from being regurgitated. Zeroth-order methods, which rely solely on forward passes, substantially reduce memory consumption during training. However, directly combining them with standard differentially private gradient descent suffers more as model size grows. To bridge this gap, we introduce DPZero, a novel private zeroth-order algorithm with nearly dimension-independent rates. The memory efficiency of DPZero is demonstrated in privately fine-tuning RoBERTa and OPT on several downstream tasks. Our code is available at https://github.com/Liang137/DPZero.",
  "abstract_zh": "大规模语言模型（LLMs）在特定领域数据上的微调普遍面临内存和隐私两个主要挑战。首先，随着LLMs规模的不断增长，基于梯度的反向传播训练方法的内存需求变得极其高昂。其次，由于LLMs倾向于记忆训练数据，保护微调数据中潜在敏感信息不被重现显得尤为重要。零阶方法仅依赖前向传播，显著降低了训练过程中的内存消耗。然而，直接将其与标准差分隐私梯度下降结合时，随着模型规模的增长，效果会更差。为了解决这一问题，我们提出了DPZero，一种新颖的私有零阶算法，具有几乎与维度无关的速率。我们在多个下游任务上展示了DPZero在私有微调RoBERTa和OPT时的内存效率。我们的代码可在https://github.com/Liang137/DPZero获取。"
}
{
  "title": "Optimizing Watermarks for Large Language Models",
  "title_zh": "优化大型语言模型的水印",
  "abstract": "With the rise of large language models (LLMs) and concerns about potential misuse, watermarks for generative LLMs have recently attracted much attention. An important aspect of such watermarks is the trade-off between their identifiability and their impact on the quality of the generated text. This paper introduces a systematic approach to this trade-off in terms of a multi-objective optimization problem. For a large class of robust, efficient watermarks, the associated Pareto optimal solutions are identified and shown to outperform existing robust, efficient watermarks.",
  "abstract_zh": "随着大型语言模型（LLMs）的兴起以及对潜在误用的担忧，生成性LLMs的水印最近引起了广泛关注。这类水印的一个重要方面是其可识别性与生成文本质量之间的权衡。本文提出了一种系统的方法，将这一权衡视为多目标优化问题。对于一类稳健且高效的水印，识别出相关的帕累托最优解，并显示其优于现有的稳健高效水印。"
}
{
  "title": "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content",
  "title_zh": "RigorLLM：针对大型语言模型的不良内容的弹性防护措施",
  "abstract": "Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various tasks in different domains. However, the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges. Current mitigation strategies, while effective, are not resilient under adversarial attacks. This paper introduces Resilient Guardrails for Large Language Models (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted approach that includes energy-based training data augmentation through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with LLMs based on our data augmentation, RigorLLM offers a robust solution to harmful content moderation. Our experimental evaluations demonstrate that RigorLLM not only outperforms existing baselines like OpenAI API and Perspective API in detecting harmful content but also exhibits unparalleled resilience to jailbreaking attacks. The innovative use of constrained optimization and a fusion-based guardrail approach represents a significant step forward in developing more secure and reliable LLMs, setting a new standard for content moderation frameworks in the face of evolving digital threats.",
  "abstract_zh": "近年来大型语言模型（LLMs）的进展展示了其在各个领域不同任务中的卓越能力。然而，LLMs中偏见的出现以及在恶意输入下生成有害内容的潜力带来了重大挑战。当前的缓解策略虽然有效，但在对抗攻击下并不具备弹性。本文介绍了大型语言模型的弹性防护措施（RigorLLM），这是一个新颖的框架，旨在高效且有效地调节LLMs的有害和不安全输入与输出。通过采用多方面的方法，包括通过Langevin动力学进行基于能量的训练数据增强、通过极小极大优化优化输入的安全后缀，以及结合基于我们数据增强的强健KNN与LLMs的融合模型，RigorLLM为有害内容的调节提供了强有力的解决方案。我们的实验评估表明，RigorLLM不仅在检测有害内容方面超越了现有基准，如OpenAI API和Perspective API，而且在抵御越狱攻击方面表现出无与伦比的弹性。约束优化和基于融合的防护措施的创新使用，标志着在开发更安全可靠的LLMs方面向前迈出了重要一步，为应对不断演变的数字威胁设定了内容调节框架的新标准。"
}
{
  "title": "CogBench: a large language model walks into a psychology lab",
  "title_zh": "标题：CogBench：一个大型语言模型走进心理学实验室",
  "abstract": "Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces *CogBench*, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs’ behavior. We apply *CogBench* to 40 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.",
  "abstract_zh": "摘要：大型语言模型（LLMs）显著推动了人工智能领域的发展。然而，全面评估它们仍然具有挑战性。我们认为，这部分是由于大多数基准测试主要关注性能指标。本文介绍了*CogBench*，一个包含来自七个认知心理学实验的十个行为指标的基准。这种新颖的方法为表型化LLMs的行为提供了工具包。我们将*CogBench*应用于40个LLMs，生成了丰富多样的数据集。我们使用统计多层建模技术分析这些数据，考虑到特定LLMs的微调版本之间的嵌套依赖关系。我们的研究强调了模型大小和人类反馈强化学习（RLHF）在提高性能和与人类行为对齐中的关键作用。有趣的是，我们发现开源模型的风险倾向低于专有模型，而在代码上进行微调并不一定增强LLMs的行为。最后，我们探讨了提示工程技术的效果。我们发现，思维链提示改善了概率推理，而退一步提示促进了基于模型的行为。"
}
{
  "title": "Building Socially-Equitable Public Models",
  "title_zh": "构建社会公平的公共模型",
  "abstract": "Public models offer predictions to a variety of downstream tasks and have played a crucial role in various AI applications, showcasing their proficiency in accurate predictions. However, the exclusive emphasis on prediction accuracy may not align with the diverse end objectives of downstream agents. Recognizing the public model's predictions as a service, we advocate for integrating the objectives of downstream agents into the optimization process. Concretely, to address performance disparities and foster fairness among heterogeneous agents in training, we propose a novel Equitable Objective. This objective, coupled with a policy gradient algorithm, is crafted to train the public model to produce a more equitable/uniform performance distribution across downstream agents, each with their unique concerns. Both theoretical analysis and empirical case studies have proven the effectiveness of our method in advancing performance equity across diverse downstream agents utilizing the public model for their decision-making. Codes and datasets are released at https://github.com/Ren-Research/Socially-Equitable-Public-Models.",
  "abstract_zh": "公共模型为各种下游任务提供预测，并在多个人工智能应用中发挥了关键作用，展示了其在准确预测方面的能力。然而，过于强调预测准确性可能与下游代理的多样化最终目标不一致。我们将公共模型的预测视为一种服务，主张在优化过程中整合下游代理的目标。具体而言，为了应对性能差异并促进异构代理之间的公平性，我们提出了一种新颖的公平目标。该目标结合政策梯度算法，旨在训练公共模型，以在下游代理之间产生更公平/均匀的性能分布，每个代理都有其独特的关注点。理论分析和实证案例研究证明了我们的方法在促进利用公共模型进行决策的多样化下游代理之间的性能公平性方面的有效性。代码和数据集已发布在 https://github.com/Ren-Research/Socially-Equitable-Public-Models。"
}
{
  "title": "Efficient Exploration for LLMs",
  "title_zh": "高效探索大型语言模型",
  "abstract": "We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.",
  "abstract_zh": "我们提供了有效探索在收集人类反馈以改善大型语言模型方面的显著益处的证据。在我们的实验中，代理顺序生成查询，同时根据收到的反馈调整奖励模型。我们表现最佳的代理使用双重汤普森采样生成查询，不确定性由一种认知神经网络表示。我们的结果表明，高效探索使得在更少的查询中实现高水平的性能成为可能。此外，不确定性估计和探索方案的选择在其中扮演了关键角色。"
}
{
  "title": "ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models",
  "title_zh": "标题：ConTextual：评估大型多模态模型中的上下文敏感文本丰富视觉推理",
  "abstract": "Many real-world tasks require an agent to reason jointly over text and visual objects, (e.g., navigating in public spaces), which we refer to as context-sensitive text-rich visual reasoning. Specifically, these tasks require an understanding of the context in which the text interacts with visual elements within an image. However, there is a lack of existing datasets to benchmark the state-of-the-art multimodal models' capability on context-sensitive text-rich visual reasoning. In this paper, we introduce ConTextual, a novel dataset featuring human-crafted instructions that require context-sensitive reasoning for text-rich images. We conduct experiments to assess the performance of 14 foundation models (GPT-4V, Gemini-Pro-Vision, LLaVA-Next) and establish a human performance baseline. Further, we perform human evaluations of the model responses and observe a significant performance gap of 30.8% between GPT-4V (the current best-performing Large Multimodal Model) and human performance. Our fine-grained analysis reveals that GPT-4V encounters difficulties interpreting time-related data and infographics. However, it demonstrates proficiency in comprehending abstract visual contexts such as memes and quotes. Finally, our qualitative analysis uncovers various factors contributing to poor performance including lack of precise visual perception and hallucinations. Our dataset, code, and leaderboard can be found on the project page https://con-textual.github.io/.",
  "abstract_zh": "摘要：许多现实世界任务要求代理在文本和视觉对象上进行联合推理（例如，在公共空间中导航），我们称之为上下文敏感的文本丰富视觉推理。具体而言，这些任务需要理解文本与图像中的视觉元素互动的上下文。然而，现有数据集中缺乏基准测试最先进多模态模型在上下文敏感文本丰富视觉推理方面的能力。在本文中，我们介绍了ConTextual，一个新颖的数据集，包含需要上下文敏感推理的文本丰富图像的人类创作指令。我们进行实验以评估14个基础模型（GPT-4V、Gemini-Pro-Vision、LLaVA-Next）的性能，并建立人类性能基线。此外，我们对模型响应进行人类评估，观察到GPT-4V（当前表现最佳的大型多模态模型）与人类表现之间存在30.8%的显著性能差距。我们的细致分析揭示，GPT-4V在解释与时间相关的数据和信息图方面遇到困难。然而，它在理解抽象视觉上下文（如表情包和引用）方面表现出色。最后，我们的定性分析揭示了导致性能不佳的各种因素，包括缺乏精确的视觉感知和幻觉。我们的数据集、代码和排行榜可以在项目页面https://con-textual.github.io/找到。"
}
{
  "title": "A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?",
  "title_zh": "对材料发现的LLMs进行冷静审视：它们真的适合分子上的贝叶斯优化吗？",
  "abstract": "Automation is one of the cornerstones of contemporary material discovery. Bayesian optimization (BO) is an essential part of such workflows, enabling scientists to leverage prior domain knowledge into efficient exploration of a large molecular space. While such prior knowledge can take many forms, there has been significant fanfare around the ancillary scientific knowledge encapsulated in large language models (LLMs). However, existing work thus far has only explored LLMs for heuristic materials searches. Indeed, recent work obtains the uncertainty estimate---an integral part of BO---from point-estimated, _non-Bayesian_ LLMs. In this work, we study the question of whether LLMs are actually useful to accelerate principled _Bayesian_ optimization in the molecular space. We take a sober, dispassionate stance in answering this question. This is done by carefully (i) viewing LLMs as fixed feature extractors for standard but principled BO surrogate models and by (ii) leveraging parameter-efficient finetuning methods and Bayesian neural networks to obtain the posterior of the LLM surrogate. Our extensive experiments with real-world chemistry problems show that LLMs can be useful for BO over molecules, but only if they have been pretrained or finetuned with domain-specific data.",
  "abstract_zh": "自动化是当代材料发现的基石之一。贝叶斯优化（BO）是此类工作流程的重要组成部分，使科学家能够利用先前的领域知识有效地探索广泛的分子空间。尽管这种先前知识可以采取多种形式，但围绕大型语言模型（LLMs）所包含的辅助科学知识的宣传却相当显著。然而，现有的研究迄今为止仅探索了LLMs在启发式材料搜索中的应用。实际上，最近的研究从点估计的非贝叶斯LLMs中获得了不确定性估计，这是BO的一个重要组成部分。在本研究中，我们探讨了LLMs是否真的有助于加速分子空间中的原则性贝叶斯优化。我们以冷静、客观的态度回答这个问题。具体而言，我们通过（i）将LLMs视为标准但原则性的BO代理模型的固定特征提取器，以及（ii）利用参数高效的微调方法和贝叶斯神经网络来获得LLM代理的后验分布，来实现这一目标。我们在真实化学问题上的广泛实验表明，LLMs在分子上的BO中可以发挥作用，但前提是它们已经经过领域特定数据的预训练或微调。"
}
{
  "title": "Think Before You Act: Decision Transformers with Working Memory",
  "title_zh": "在行动之前思考：具有工作记忆的决策变换器",
  "abstract": "Decision Transformer-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and computation. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Inspired by this, we propose a working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in Atari games and Meta-World object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of the proposed architecture.",
  "abstract_zh": "基于决策变换器的决策代理在多个任务中展示了泛化能力。然而，它们的性能依赖于大量的数据和计算。我们认为，这种低效源于遗忘现象，即模型在训练过程中将其行为记忆在参数中。因此，在新任务上的训练可能会恶化模型在先前任务上的表现。与大型语言模型的隐式记忆机制相比，人脑利用分布式记忆存储，有助于有效管理和组织多项技能，从而减轻遗忘现象。受到此启发，我们提出了一种工作记忆模块，用于存储、融合和检索不同下游任务的信息。评估结果表明，所提方法在Atari游戏和Meta-World物体操控任务中提高了训练效率和泛化能力。此外，我们还展示了记忆微调进一步增强了所提架构的适应性。"
}
{
  "title": "Prompt-based Visual Alignment for Zero-shot Policy Transfer",
  "title_zh": "基于提示的视觉对齐用于零-shot策略转移",
  "abstract": "Overfitting in RL has become one of the main obstacles to applications in reinforcement learning(RL). Existing methods do not provide explicit semantic constrain for the feature extractor, hindering the agent from learning a unified cross-domain representation and resulting in performance degradation on unseen domains. Besides, abundant data from multiple domains are needed. To address these issues, in this work, we propose prompt-based visual alignment (PVA), a robust framework to mitigate the detrimental domain bias in the image for zero-shot policy transfer. Inspired that Visual-Language Model (VLM) can serve as a bridge to connect both text space and image space, we leverage the semantic information contained in a text sequence as an explicit constraint to train a visual aligner. Thus, the visual aligner can map images from multiple domains to a unified domain and achieve good generalization performance. To better depict semantic information, prompt tuning is applied to learn a sequence of learnable tokens. With explicit constraints of semantic information, PVA can learn unified cross-domain representation under limited access to cross-domain data and achieves great zero-shot generalization ability in unseen domains. We verify PVA on a vision-based autonomous driving task with CARLA simulator. Experiments show that the agent generalizes well on unseen domains under limited access to multi-domain data.",
  "abstract_zh": "在强化学习（RL）中，过拟合已成为应用的主要障碍之一。现有方法未对特征提取器提供明确的语义约束，阻碍了智能体学习统一的跨域表示，导致在未见域上的性能下降。此外，需要来自多个域的大量数据。为了解决这些问题，本文提出了一种基于提示的视觉对齐（PVA）框架，以减轻图像中的有害领域偏差，从而实现零-shot策略转移。受到视觉-语言模型（VLM）可以作为连接文本空间和图像空间的桥梁的启发，我们利用文本序列中包含的语义信息作为显式约束来训练视觉对齐器。因此，视觉对齐器可以将来自多个域的图像映射到统一的域，并实现良好的泛化性能。为了更好地描绘语义信息，应用提示调优来学习一系列可学习的标记。在语义信息的显式约束下，PVA可以在有限的跨域数据访问下学习统一的跨域表示，并在未见域上实现良好的零-shot泛化能力。我们在CARLA模拟器的基于视觉的自动驾驶任务上验证了PVA。实验表明，智能体在有限的多域数据访问下能够很好地在未见域上进行泛化。"
}
{
  "title": "InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining",
  "title_zh": "标题：InstructRetro：检索增强预训练后的指令调优",
  "abstract": "Pretraining auto-regressive large language models (LLMs) with retrieval demonstrates better perplexity and factual accuracy by leveraging external databases. However, the size of existing pretrained retrieval-augmented LLM is still limited (e.g., Retro has 7.5B parameters), which limits the effectiveness of instruction tuning and zero-shot generalization. In this work, we introduce Retro 48B, the largest LLM pretrained with retrieval. Specifically, we continue to pretrain a 43B GPT model on additional 100 billion tokens using the Retro augmentation method by retrieving from 1.2 trillion tokens. Notably, the obtained foundation model, Retro 48B, largely outperforms the counterpart GPT 43B trained on 1.2T tokens in terms of perplexity with only 2.58% additional GPU hours, demonstrating the significant scaling potential of the method. After instruction tuning on Retro, InstructRetro demonstrates significant improvement over the instruction-tuned GPT on a wide range of zero-shot tasks. Specifically, the average improvement of InstructRetro is 7% over its GPT counterpart across 8 short-form QA and reading comprehension tasks, 10% over GPT across 4 challenging long-form QA tasks, and 16% over GPT across 3 summarization tasks. Surprisingly, we find that one can ablate the encoder from InstructRetro architecture and directly use its decoder backbone, while achieving comparable results. Our results highlight the promising direction to obtain a better GPT decoder through continued pretraining with retrieval before instruction tuning. Our code and checkpoints are publicly available at: https://huggingface.co/nvidia/retro-48b-instruct-4k.",
  "abstract_zh": "摘要：使用检索对自回归大型语言模型（LLMs）进行预训练，通过利用外部数据库显示出更好的困惑度和事实准确性。然而，现有的预训练检索增强LLM的规模仍然有限（例如，Retro有75亿参数），这限制了指令调优和零-shot泛化的有效性。在本研究中，我们引入了Retro 48B，这是最大的预训练检索增强LLM。具体而言，我们在额外的1000亿个标记上继续对一个430亿的GPT模型进行预训练，使用从1.2万亿个标记中检索的Retro增强方法。值得注意的是，获得的基础模型Retro 48B在困惑度方面大幅优于在1.2万亿标记上训练的对应GPT 43B，仅增加2.58%的GPU小时，展示了该方法显著的扩展潜力。在Retro上进行指令调优后，InstructRetro在广泛的零-shot任务上显著优于经过指令调优的GPT。具体而言，InstructRetro在8个短文本问答和阅读理解任务中平均提高了7%，在4个具有挑战性的长文本问答任务中提高了10%，在3个摘要任务中提高了16%。令人惊讶的是，我们发现可以从InstructRetro架构中去掉编码器，直接使用其解码器主干，同时实现可比的结果。我们的结果突显了通过在指令调优之前继续使用检索进行预训练以获得更好的GPT解码器的有希望方向。我们的代码和检查点可在以下网址公开获取：https://huggingface.co/nvidia/retro-48b-instruct-4k。"
}
{
  "title": "Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels",
  "title_zh": "标题：Q-Align：通过离散文本定义级别教导大型多模态模型进行视觉评分",
  "abstract": "The explosion of visual content available online underscores the requirement for an accurate machine assessor to robustly evaluate scores across diverse types of visual contents. While recent studies have demonstrated the exceptional potentials of large multi-modality models (LMMs) on a wide range of related fields, in this work, we explore how to teach them for visual rating aligning with human opinions. Observing that human raters only learn and judge discrete text-defined levels in subjective studies, we propose to emulate this subjective process and teach LMMs with text-defined rating levels instead of scores. The proposed Q-Align achieves state-of-the-art accuracy on image quality assessment (IQA), image aesthetic assessment (IAA), as well as video quality assessment (VQA) under the original LMM structure. With the syllabus, we further unify the three tasks into one model, termed the OneAlign. Our experiments demonstrate the advantage of discrete levels over direct scores on training, and that LMMs can learn beyond the discrete levels and provide effective finer-grained evaluations. Code and weights will be released.",
  "abstract_zh": "摘要：在线可用的视觉内容激增凸显了对准确机器评估者的需求，以稳健地评估各种视觉内容的分数。尽管最近的研究展示了大型多模态模型（LMMs）在广泛相关领域的卓越潜力，但在本研究中，我们探讨如何教导它们进行与人类意见一致的视觉评分。观察到人类评分者在主观研究中仅学习和判断离散文本定义的级别，我们提出模拟这一主观过程，并用文本定义的评分级别而非分数来教导LMMs。所提出的Q-Align在图像质量评估（IQA）、图像美学评估（IAA）以及视频质量评估（VQA）中在原始LMM结构下实现了最先进的准确性。通过课程大纲，我们进一步将这三项任务统一为一个模型，称为OneAlign。我们的实验表明，离散级别在训练中优于直接分数，并且LMMs可以超越离散级别，提供有效的细粒度评估。代码和权重将会发布。"
}
{
  "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning",
  "title_zh": "LESS：选择影响数据以进行针对性指令调优",
  "abstract": "Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as *targeted instruction tuning*. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform **L**ow-rank gradi**E**nt **S**imilarity **S**earch for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable *gradient datastore* with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at [princeton-nlp/LESS](https://github.com/princeton-nlp/LESS).",
  "abstract_zh": "指令调优在大型语言模型（LLMs）中解锁了强大的能力，利用组合数据集开发通用聊天机器人。然而，现实世界的应用通常需要一套专业技能（例如，推理）。挑战在于从这些庞大的数据集中识别出最相关的数据，以有效开发特定能力，我们将这种设置框定为*针对性指令调优*。我们提出了LESS，一种优化器感知且实用高效的算法，用于估计数据影响并执行**低秩梯度相似性搜索**以选择指令数据。关键是，LESS调整现有的影响公式以适应Adam优化器和可变长度的指令数据。LESS首先构建一个具有低维梯度特征的高度可重用和可转移的*梯度数据存储*，然后根据与体现特定能力的少量示例的相似性选择示例。实验表明，在LESS选择的5%数据上训练通常可以在各种下游任务中超越在完整数据集上训练。此外，所选数据具有高度可转移性：较小的模型可以用于为较大模型和不同家族的模型选择有用数据。我们的定性分析表明，我们的方法超越了表面形式线索，以识别出为预期下游应用所需的推理技能的示例数据。为了促进未来的工作，我们在[princeton-nlp/LESS](https://github.com/princeton-nlp/LESS)发布了代码和数据。"
}
{
  "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
  "title_zh": "美杜莎：具有多个解码头的简单LLM推理加速框架",
  "abstract": "Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one's output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa reduces the number of decoding steps required. We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the model's capabilities. Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2$\\times$ speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-2.8$\\times$.",
  "abstract_zh": "大型语言模型（LLMs）采用自回归解码，需要顺序计算，每一步都依赖于前一步的输出。这造成了瓶颈，因为每一步都需要将完整的模型参数从高带宽内存（HBM）移动到加速器的缓存。虽然已经提出了如投机解码等方法来解决此问题，但由于获取和维护单独草稿模型的挑战，其实施受到阻碍。本文提出了美杜莎，一种通过添加额外解码头以并行预测多个后续标记来增强LLM推理的高效方法。美杜莎使用基于树的注意机制，在每个解码步骤中构建多个候选延续并同时验证。通过利用并行处理，美杜莎减少了解码所需的步骤数量。我们为美杜莎提供了两个层次的微调程序，以满足不同用例的需求：美杜莎-1：美杜莎在冻结的主干LLM上直接微调，实现无损推理加速。美杜莎-2：美杜莎与主干LLM一起微调，提高了美杜莎头的预测准确性和更高的加速，但需要一种特殊的训练方案以保持模型的能力。此外，我们提出了几种扩展，改善或扩展美杜莎的实用性，包括自蒸馏以处理没有训练数据的情况，以及典型的接受方案以提高接受率，同时保持生成质量。我们在不同大小和训练程序的模型上评估美杜莎。实验表明，美杜莎-1在不影响生成质量的情况下可以实现超过2.2$\\times$的加速，而美杜莎-2进一步将加速提高到2.3-2.8$\\times$。"
}
{
  "title": "In-Context Principle Learning from Mistakes",
  "title_zh": "标题：从错误中学习的上下文原则",
  "abstract": "In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific “principles” from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4-turbo and Claude-2.1. For example, LEAP improves over the standard few-shot prompting using GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does not require any more input or examples than the standard few-shot prompting settings.",
  "abstract_zh": "摘要：上下文学习（ICL，也称为少量提示）已成为将大型语言模型（LLMs）适应下游任务的标准方法，通过学习少量的输入-输出示例。然而，所有基于ICL的方法仅从正确的输入-输出对中学习。本文重新审视这一范式，从给定的少量输入-输出示例中学习更多。我们引入学习原则（LEAP）：首先，我们故意引导模型在这些少量示例上犯错；然后我们反思这些错误，从中学习明确的任务特定“原则”，帮助解决类似问题并避免常见错误；最后，我们提示模型使用原始的少量示例和这些学习到的通用原则来回答未见的测试问题。我们在多种基准上评估LEAP，包括多跳问答（Hotpot QA）、文本问答（DROP）、Big-Bench困难推理和数学问题（GSM8K和MATH）；在所有这些基准中，LEAP提升了最强大的可用LLMs，如GPT-3.5-turbo、GPT-4、GPT-4-turbo和Claude-2.1。例如，LEAP在DROP中比标准的少量提示提高了7.5%，在HotpotQA中提高了3.3%。重要的是，LEAP不需要比标准的少量提示设置更多的输入或示例。"
}
{
  "title": "Verification of Machine Unlearning is Fragile",
  "title_zh": "机器遗忘的验证是脆弱的",
  "abstract": "As privacy concerns escalate in the realm of machine learning, data owners now have the option to utilize machine unlearning to remove their data from machine learning models, following recent legislation. To enhance transparency in machine unlearning and avoid potential dishonesty by model providers, various verification strategies have been proposed. These strategies enable data owners to ascertain whether their target data has been effectively unlearned from the model. However, our understanding of the safety issues of machine unlearning verification remains nascent. In this paper, we explore the novel research question of whether model providers can circumvent verification strategies while retaining the information of data supposedly unlearned. Our investigation leads to a pessimistic answer: the verification of machine unlearning is fragile. Specifically, we categorize the current verification strategies regarding potential dishonesty among model providers into two types. Subsequently, we introduce two novel adversarial unlearning processes capable of circumventing both types. We validate the efficacy of our methods through theoretical analysis and empirical experiments using real-world datasets. This study highlights the vulnerabilities and limitations in machine unlearning verification, paving the way for further research into the safety of machine unlearning.",
  "abstract_zh": "随着隐私问题在机器学习领域的升级，数据所有者现在可以利用机器遗忘根据最近的立法从机器学习模型中删除他们的数据。为了增强机器遗忘的透明度并避免模型提供者的潜在不诚实，提出了各种验证策略。这些策略使数据所有者能够确定其目标数据是否已有效地从模型中遗忘。然而，我们对机器遗忘验证的安全问题的理解仍然处于初步阶段。本文探讨了一个新颖的研究问题，即模型提供者是否可以在保留所谓已遗忘数据的信息的同时规避验证策略。我们的调查得出了一个悲观的答案：机器遗忘的验证是脆弱的。具体而言，我们将当前的验证策略根据模型提供者的潜在不诚实行为分为两类。随后，我们介绍了两种新颖的对抗性遗忘过程，能够规避这两类策略。我们通过理论分析和使用真实世界数据集的实证实验验证了我们方法的有效性。本研究突出了机器遗忘验证中的脆弱性和局限性，为进一步研究机器遗忘的安全性铺平了道路。"
}
{
  "title": "Controllable Prompt Tuning For Balancing Group Distributional Robustness",
  "title_zh": "可控提示调优以平衡群体分布鲁棒性",
  "abstract": "Models trained on data composed of different groups or domains can suffer from severe performance degradation under distribution shifts. While recent methods have largely focused on optimizing the worst-group objective, this often comes at the expense of good performance on other groups. To address this problem, we introduce an optimization scheme to achieve good performance across groups and find a good solution for all without severely sacrificing performance on any of them. However, directly applying such optimization involves updating the parameters of the entire network, making it both computationally expensive and challenging. Thus, we introduce Controllable Prompt Tuning (CPT), which couples our approach with prompt-tuning techniques. On spurious correlation benchmarks, our procedures achieve state-of-the-art results across both transformer and non-transformer architectures, as well as unimodal and multimodal data, while requiring only $0.4\\%$ tunable parameters.",
  "abstract_zh": "在由不同群体或领域组成的数据上训练的模型在分布变化下可能会遭受严重的性能下降。尽管最近的方法主要集中在优化最差群体目标上，但这往往会牺牲其他群体的良好性能。为了解决这个问题，我们引入了一种优化方案，以在各个群体中实现良好的性能，并找到一个对所有群体都有效的良好解决方案，而不会严重牺牲任何群体的性能。然而，直接应用这种优化涉及更新整个网络的参数，这使得计算成本高且具有挑战性。因此，我们引入了可控提示调优（CPT），将我们的方法与提示调优技术结合。在虚假相关基准测试中，我们的程序在变换器和非变换器架构以及单模态和多模态数据上均实现了最先进的结果，同时只需$0.4\\%$的可调参数。"
}
{
  "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts",
  "title_zh": "人类启发的阅读代理，具备极长上下文的要旨记忆",
  "abstract": "Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called *gist memories*, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3.5-20x.",
  "abstract_zh": "当前的大型语言模型（LLMs）不仅受限于某个最大上下文长度，而且在处理长输入时也无法稳健地工作。为了解决这些限制，我们提出了ReadAgent，一个在实验中有效上下文长度提高至20倍的LLM代理系统。受到人类如何互动阅读长文档的启发，我们将ReadAgent实现为一个简单的提示系统，利用LLMs的先进语言能力来（1）决定在记忆事件中存储哪些内容，（2）将这些记忆事件压缩成称为*要旨记忆*的短期记忆，以及（3）在ReadAgent需要提醒自己相关细节以完成任务时采取行动查找原文中的段落。我们在三个长文档阅读理解任务（QuALITY、NarrativeQA和QMSum）上评估ReadAgent与基线的表现，使用原始长上下文和要旨记忆。这些评估表明，ReadAgent在所有三个任务上均优于基线，同时将有效上下文窗口扩展了3.5-20倍。"
}
{
  "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens",
  "title_zh": "长罗佩：将大型语言模型的上下文窗口扩展至超过200万标记",
  "abstract": "Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations. Code is available at https://github.com/microsoft/LongRoPE",
  "abstract_zh": "大型语言模型（LLMs）中，较大的上下文窗口是一项理想特性。然而，由于高昂的微调成本、长文本的稀缺以及新标记位置引入的灾难性值，目前扩展的上下文窗口限制在约128k标记。本文首次提出了LongRoPE，将预训练LLMs的上下文窗口扩展至令人印象深刻的2048k标记，仅需在256k训练长度内进行最多1k的微调步骤，同时保持在原始短上下文窗口的性能。这是通过三项关键创新实现的：（i）我们通过高效搜索识别并利用位置插值中的两种非均匀性，为微调提供更好的初始化，并在非微调场景中实现8倍扩展；（ii）我们引入了一种渐进扩展策略，首先对256k长度的LLM进行微调，然后在微调后的扩展LLM上进行第二次位置插值，以实现2048k的上下文窗口；（iii）我们在8k长度上重新调整LongRoPE，以恢复短上下文窗口的性能。在LLaMA2和Mistral上进行的广泛实验表明我们方法的有效性。通过LongRoPE扩展的模型保留了原始架构，仅对位置嵌入进行了小幅修改，并能够重用大部分现有优化。代码可在https://github.com/microsoft/LongRoPE获取。"
}
{
  "title": "Exploiting Code Symmetries for Learning Program Semantics",
  "title_zh": "利用代码对称性学习程序语义",
  "abstract": "This paper tackles the challenge of teaching code semantics to Large Language Models (LLMs) for program analysis by incorporating code symmetries into the model architecture. We introduce a group-theoretic framework that defines code symmetries as semantics-preserving transformations, where forming a code symmetry group enables precise and efficient reasoning of code semantics. Our solution, SymC, develops a novel variant of self-attention that is provably equivariant to code symmetries from the permutation group defined over the program dependence graph. SymC obtains superior performance on five program analysis tasks, outperforming state-of-the-art code models, including GPT-4, without any pre-training. Our results suggest that code LLMs that encode the code structural prior via the code symmetry group generalize better and faster.",
  "abstract_zh": "本文通过将代码对称性纳入模型架构，解决了向大型语言模型（LLMs）教授代码语义以进行程序分析的挑战。我们引入了一个群论框架，将代码对称性定义为保持语义不变的变换，形成代码对称性群使得对代码语义的推理更加精确和高效。我们的解决方案SymC开发了一种新型的自注意力变体，该变体在程序依赖图上定义的置换群的代码对称性下是可证明的等变。SymC在五个程序分析任务上取得了优越的性能，超越了包括GPT-4在内的最先进代码模型，且无需任何预训练。我们的结果表明，通过代码对称性群编码代码结构先验的代码LLMs具有更好的泛化能力和更快的速度。"
}
{
  "title": "KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation",
  "title_zh": "KV-Runahead：通过并行键值缓存生成实现可扩展的因果LLM推理",
  "abstract": "Large Language Model or LLM inference has two phases, the prompt (or prefill) phase to output the first token and the extension (or decoding) phase to the generate subsequent tokens. In this work, we propose an efficient parallelization scheme, KV-Runahead to accelerate the prompt phase. The key observation is that the extension phase generates tokens faster than the prompt phase because of key-value cache (KV-cache). Hence, KV-Runahead parallelizes the prompt phase by orchestrating multiple processes to populate the KV-cache and minimizes the time-to-first-token (TTFT). Dual-purposing the KV-cache scheme has two main benefits. First, since KV-cache is designed to leverage the causal attention map, we minimize computation and computation automatically. Second, since it already exists for the extension phase, KV-Runahead is easy to implement. We further propose context-level load-balancing to handle uneven KV-cache generation (due to the causal attention) and to optimize TTFT. Compared with an existing parallelization scheme such as tensor or sequential parallelization where keys and values are locally generated and exchanged via all-gather collectives, our experimental results demonstrate that KV-Runahead can offer over 1.4× and 1.6× speedups for Llama 7B and Falcon 7B respectively.",
  "abstract_zh": "大型语言模型（LLM）推理分为两个阶段：输出第一个标记的提示（或预填充）阶段和生成后续标记的扩展（或解码）阶段。在这项工作中，我们提出了一种高效的并行化方案KV-Runahead，以加速提示阶段。关键观察是扩展阶段生成标记的速度快于提示阶段，因为存在键值缓存（KV-cache）。因此，KV-Runahead通过协调多个进程填充KV-cache来并行化提示阶段，并最小化首次标记的时间（TTFT）。双重用途的KV-cache方案有两个主要好处。首先，由于KV-cache旨在利用因果注意力图，我们自动最小化计算和计算。其次，由于它已在扩展阶段存在，KV-Runahead易于实现。我们进一步提出了上下文级负载均衡，以处理不均匀的KV-cache生成（由于因果注意力）并优化TTFT。与现有的并行化方案（如张量或顺序并行化，其中键和值在本地生成并通过全收集交换）相比，我们的实验结果表明，KV-Runahead可以为Llama 7B和Falcon 7B分别提供超过1.4倍和1.6倍的加速。"
}
{
  "title": "What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation",
  "title_zh": "标题：诱导头需要什么条件才能成功？关于上下文学习电路及其形成的机制研究",
  "abstract": "In-context learning is a powerful emergent ability in transformer models. Prior work in mechanistic interpretability has identified a circuit element that may be critical for in-context learning – the induction head (IH), which performs a match-and-copy operation. During training of large transformers on natural language data, IHs emerge around the same time as a notable phase change in the loss. Despite the robust evidence for IHs and this interesting coincidence with the phase change, relatively little is known about the diversity and emergence dynamics of IHs. Why is there more than one IH, and how are they dependent on each other? Why do IHs appear all of a sudden, and what are the subcircuits that enable them to emerge? We answer these questions by studying IH emergence dynamics in a controlled setting by training on synthetic data. In doing so, we develop and share a novel optogenetics-inspired causal framework for modifying activations throughout training. Using this framework, we delineate the diverse and additive nature of IHs. By \"clamping\" subsets of activations throughout training, we then identify three underlying subcircuits that interact to drive IH formation, yielding the phase change. Furthermore, these subcircuits shed light on data-dependent properties of formation, such as phase change timing, already showing the promise of this more in-depth understanding of subcircuits that need to \"go right\" for an induction head.",
  "abstract_zh": "摘要：上下文学习是变换器模型中的一种强大新兴能力。之前的机制可解释性研究已识别出一个可能对上下文学习至关重要的电路元件——诱导头（IH），它执行匹配和复制操作。在对自然语言数据进行大规模变换器训练时，IH的出现与损失的显著相变几乎同时发生。尽管有强有力的证据支持IH的存在以及与相变的有趣巧合，但关于IH的多样性和出现动态的了解相对较少。为什么会有多个IH，它们之间如何相互依赖？IH为何会突然出现，是什么子电路使其得以出现？我们通过在受控环境中对合成数据进行训练来研究IH的出现动态，从而回答这些问题。在此过程中，我们开发并分享了一种新颖的受光遗传学启发的因果框架，用于在训练过程中修改激活。利用该框架，我们阐明了IH的多样性和叠加特性。通过在训练过程中“夹紧”激活的子集，我们识别出三个相互作用的基础子电路，推动IH的形成，导致相变。此外，这些子电路揭示了形成的依赖于数据的特性，如相变时机，已经显示出对需要“正确”运作的诱导头的子电路更深入理解的潜力。"
}
{
  "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models",
  "title_zh": "自我对弈微调将弱语言模型转变为强语言模型",
  "abstract": "Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents.",
  "abstract_zh": "通过监督微调（SFT）利用人类标注数据的力量对于推动大型语言模型（LLMs）至关重要。本文探讨了在不需要获取额外人类标注数据的情况下，从弱模型成长为强模型的前景。我们提出了一种新的微调方法，称为自我对弈微调（SPIN），该方法从一个经过监督微调的模型开始。SPIN的核心是自我对弈机制，LLM通过与自身实例对弈来提升其能力。更具体地说，LLM从其之前的迭代中生成自己的训练数据，通过区分这些自生成的响应与人类标注数据获得的响应来优化其策略。我们的方法逐步将LLM从一个初始模型提升为一个强大的模型，充分发挥人类标注示范数据在SFT中的潜力。从理论上讲，我们证明了只有当LLM策略与目标数据分布一致时，才能实现我们方法的训练目标函数的全局最优解。从经验上看，我们在多个基准数据集上评估了我们的方法，包括HuggingFace开放LLM排行榜、MT-Bench和Big-Bench的数据集。我们的结果表明，SPIN可以显著提高LLM在各种基准上的表现，甚至超过通过直接偏好优化（DPO）并补充额外GPT-4偏好数据训练的模型。这为自我对弈的前景提供了启示，使得在没有专家对手的情况下实现人类水平的LLM表现成为可能。"
}
{
  "title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models",
  "title_zh": "标题：LIDAO：朝着有限干预以去偏见（大型）语言模型",
  "abstract": "Large language models (LLMs) have achieved impressive performance on various natural language generation tasks. Nonetheless, they suffer from generating negative and harmful contents that are biased against certain demographic groups (e.g., female), raising severe fairness concerns. As remedies, prior works intervened the generation by removing attitude or demographic information, inevitably degrading the generation quality and resulting in notable *fairness-fluency* trade-offs. However, it is still under-explored to what extent the fluency *has to* be affected in order to achieve a desired level of fairness. In this work, we conduct the first formal study from an information-theoretic perspective. We show that previous approaches are excessive for debiasing and propose LIDAO, a general framework to debias a (L)LM at a better fluency provably. We further robustify LIDAO in adversarial scenarios, where a carefully-crafted prompt may stimulate LLMs exhibiting instruction-following abilities to generate texts with fairness issue appears only when the prompt is also taken into account. Experiments on three LMs ranging from 0.7B to 7B parameters demonstrate the superiority of our method.",
  "abstract_zh": "摘要：大型语言模型（LLMs）在各种自然语言生成任务中取得了令人印象深刻的表现。然而，它们在生成内容时往往会产生对某些人口群体（例如女性）存在偏见的负面和有害内容，这引发了严重的公平性问题。作为补救措施，之前的研究通过去除态度或人口信息来干预生成，不可避免地降低了生成质量，并导致显著的*公平性-流畅性*权衡。然而，尚未深入探讨为了实现所需的公平性水平，流畅性*必须*受到多大程度的影响。在本研究中，我们从信息论的角度进行首次正式研究。我们表明，之前的方法在去偏见方面过于极端，并提出了LIDAO，一个在流畅性上可证明地更好地去偏见（L）LM的通用框架。我们进一步增强了LIDAO在对抗场景中的鲁棒性，在这些场景中，精心设计的提示可能刺激LLMs展现指令跟随能力，生成的文本只有在考虑提示时才会出现公平性问题。对三个参数范围从0.7B到7B的语言模型的实验表明了我们方法的优越性。"
}
{
  "title": "Improving Context Understanding in Multimodal Large Language Models via Multimodal Composition Learning",
  "title_zh": "标题：通过多模态组合学习提高多模态大型语言模型的上下文理解",
  "abstract": "Previous efforts using frozen Large Language Models (LLMs) for visual understanding, via image captioning or image-text retrieval tasks, face challenges when dealing with complex multimodal scenarios. In order to enhance the capabilities of Multimodal Large Language Models (MLLM) in comprehending the context of vision and language, we introduce Multimodal Composition Learning (MCL) for the purpose of mapping or aligning the vision and language input. In particular, we introduce two tasks: Multimodal-Context Captioning (MC-Cap) and Multimodal-Context Retrieval (MC-Ret) to guide a frozen LLM in comprehending the vision and language context. These specialized tasks are crafted to improve the LLM’s capacity for efficient processing and utilization of multimodal inputs, thereby enhancing its proficiency in generating more accurate text or visual representations. Extensive experiments on both retrieval tasks (i.e., zero-shot composed image retrieval, visual storytelling image retrieval and visual dialog image retrieval) and text generation tasks (i.e., visual question answering) demonstrate the effectiveness of the proposed method. The code is available at: https://github.com/dhg-wei/MCL.",
  "abstract_zh": "摘要：以往通过冻结的大型语言模型（LLMs）进行视觉理解的努力，在处理复杂的多模态场景时面临挑战。为了增强多模态大型语言模型（MLLM）在理解视觉和语言上下文方面的能力，我们引入了多模态组合学习（MCL），旨在映射或对齐视觉和语言输入。特别地，我们引入了两个任务：多模态上下文字幕生成（MC-Cap）和多模态上下文检索（MC-Ret），以指导冻结的LLM理解视觉和语言上下文。这些专门任务旨在提高LLM有效处理和利用多模态输入的能力，从而增强其生成更准确文本或视觉表示的能力。在检索任务（即零-shot组合图像检索、视觉故事图像检索和视觉对话图像检索）和文本生成任务（即视觉问答）上的广泛实验表明了所提方法的有效性。代码可在以下链接获取：https://github.com/dhg-wei/MCL。"
}
{
  "title": "NExT-GPT: Any-to-Any Multimodal LLM",
  "title_zh": "NExT-GPT：任意到任意的多模态大型语言模型",
  "abstract": "While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, image, video, and audio. By leveraging the existing well-trained high-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training but also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building a unified AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community.",
  "abstract_zh": "尽管最近多模态大型语言模型（MM-LLMs）取得了令人兴奋的进展，但它们大多受到仅限于输入侧多模态理解的限制，无法以多种模态生成内容。由于人类总是通过各种模态感知世界并与他人交流，因此开发能够接受和传递任意模态内容的任意到任意MM-LLMs对于实现人类水平的人工智能至关重要。为填补这一空白，我们提出了一种端到端的通用任意到任意MM-LLM系统NExT-GPT。我们将一个大型语言模型与多模态适配器和不同的扩散解码器连接，使NExT-GPT能够以文本、图像、视频和音频的任意组合感知输入和生成输出。通过利用现有的高性能编码器和解码器，NExT-GPT仅用少量参数（1%）调整某些投影层，这不仅有利于低成本训练，还便于向更多潜在模态的方便扩展。此外，我们引入了一种模态切换指令调优（MosIT），并手动策划了一个高质量的数据集用于MosIT，基于此，NExT-GPT具备了复杂的跨模态语义理解和内容生成能力。总体而言，我们的研究展示了构建一个能够建模通用模态的统一人工智能代理的良好前景，为社区中更类人化的人工智能研究铺平了道路。"
}
{
  "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
  "title_zh": "标题：LoCoCo：用于长上下文压缩的卷积插入方法",
  "abstract": "This paper tackles the memory hurdle of of processing long context sequences in Large Language Models (LLMs), by presenting a novel approach, Dropping In Convolutions for **Lo**ng **Co**ntext **Co**mpression (**LoCoCo**). LoCoCo employs only a fixed-size Key-Value (KV) cache, and can enhance efficiency in both inference and fine-tuning stages. Diverging from prior methods that selectively drop KV pairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion technique, blending previous KV pairs with incoming tokens to minimize the loss of contextual information and ensure accurate attention modeling. This token integration is achieved through injecting one-dimensional convolutional kernels that dynamically calculate mixing weights for each KV cache slot. Designed for broad compatibility with existing LLM frameworks, LoCoCo allows for straightforward \"drop-in\" integration without needing architectural modifications, while incurring minimal tuning overhead. Experiments demonstrate that LoCoCo maintains consistently outstanding performance across various context lengths and can achieve a high context compression rate during both inference and fine-tuning phases. During inference, we successfully compressed up to $3482$ tokens into a $128$-size KV cache, while retaining comparable performance to the full sequence - an accuracy improvement of up to $0.2791$ compared to baselines at the same cache size. During post-training tuning, we also effectively extended the context length from 4K to 32K using a KV cache of fixed size 512, achieving performance similar to fine-tuning with entire sequences.",
  "abstract_zh": "摘要：本文通过提出一种新颖的方法——用于长上下文压缩的卷积插入（LoCoCo），解决了在大型语言模型（LLMs）中处理长上下文序列的内存瓶颈。LoCoCo仅使用固定大小的键值（KV）缓存，并能够在推理和微调阶段提高效率。与之前基于启发式选择性丢弃KV对的方法不同，LoCoCo利用数据驱动的自适应融合技术，将先前的KV对与传入的标记相结合，以最小化上下文信息的损失并确保准确的注意力建模。这种标记集成是通过注入一维卷积核实现的，这些卷积核动态计算每个KV缓存槽的混合权重。LoCoCo设计上与现有的LLM框架具有广泛的兼容性，允许简单的“插入式”集成，而无需架构修改，同时产生最小的调优开销。实验表明，LoCoCo在各种上下文长度下保持了一贯的卓越性能，并且在推理和微调阶段都能实现高上下文压缩率。在推理过程中，我们成功将多达3482个标记压缩到128大小的KV缓存中，同时保持与完整序列相当的性能——与相同缓存大小的基线相比，准确性提高了最多0.2791。在后训练微调过程中，我们还有效地将上下文长度从4K扩展到32K，使用固定大小为512的KV缓存，取得了与使用完整序列微调相似的性能。"
}
{
  "title": "LoRA+: Efficient Low Rank Adaptation of Large Models",
  "title_zh": "标题：LoRA+: 大模型的高效低秩适应",
  "abstract": "In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in (Hu et al., 2021) leads to suboptimal finetuning of models with large width. This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate in ADAM. Using scaling arguments for large width networks, we demonstrate that the same learning rate does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen fixed ratio. We call this proposed algorithm LoRA+. In our extensive experiments, LoRA+ improves finetuning speed (up to ∼ 2X SpeedUp) and performance (1% − 2% improvements), at the same computational cost as LoRA. The code is available at https://github.com/nikhil-ghosh-berkeley/loraplus",
  "abstract_zh": "摘要：在本文中，我们展示了最初在（Hu et al., 2021）中提出的低秩适应（LoRA）导致宽度较大的模型的微调效果不佳。这是因为LoRA中的适配器矩阵A和B在ADAM中使用相同的学习率进行更新。通过对宽度较大的网络进行缩放论证，我们证明相同的学习率无法有效进行特征学习。然后，我们展示了通过为LoRA适配器矩阵A和B设置不同的学习率并选择一个合适的固定比例，可以简单地纠正LoRA的这一次优化性。我们将这个提议的算法称为LoRA+。在我们的广泛实验中，LoRA+在与LoRA相同的计算成本下，提高了微调速度（最高约2倍加速）和性能（提高1%-2%）。代码可在https://github.com/nikhil-ghosh-berkeley/loraplus获取。"
}
{
  "title": "Risk Aware Benchmarking of Large Language Models",
  "title_zh": "风险意识基准测试大型语言模型",
  "abstract": "We propose a distributional framework for benchmarking socio-technical risks of foundation models with quantified statistical significance. Our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. We show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. Using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. Inspired by portfolio optimization and selection theory in mathematical finance, we define a metrics portfolio for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. The statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit theorems instantiated in practice via a bootstrap variance estimate. We use our framework to compare various large language models regarding risks related to drifting from instructions and outputting toxic content.",
  "abstract_zh": "我们提出了一种分布框架，用于基准测试基础模型的社会技术风险，并量化统计显著性。我们的方法基于对真实随机变量的一阶和二阶随机主导的新统计相对检验。我们展示了该检验中的二阶统计量与经济计量学和数学金融中常用的均值风险模型相关联，以在选择替代方案时平衡风险和效用。利用该框架，我们正式开发了一种风险意识的方法，用于在由指定指标量化的保护措施下选择基础模型。受到数学金融中的投资组合优化和选择理论的启发，我们为每个模型定义了一个指标组合，以聚合一组指标，并基于这些组合的随机主导进行模型选择。我们的检验的统计显著性通过中心极限定理的渐近分析在理论上得到了支持，并通过自助方差估计在实践中得以实现。我们利用该框架比较了各种大型语言模型在遵循指令和输出有毒内容方面的风险。"
}
{
  "title": "DeCoOp: Robust Prompt Tuning with Out-of-Distribution Detection",
  "title_zh": "DeCoOp：具有分布外检测的鲁棒提示调优",
  "abstract": "Vision-language models (VLMs), such as CLIP, have demonstrated impressive zero-shot capabilities for various downstream tasks. Their performance can be further enhanced through few-shot prompt tuning methods. However, current studies evaluate the performance of learned prompts separately on base and new classes. This evaluation lacks practicality for real-world applications since downstream tasks cannot determine whether the data belongs to base or new classes in advance. In this paper, we explore a problem setting called ***O**pen-world **P**rompt **T**uning* (OPT), which involves tuning prompts on base classes and evaluating on a combination of base and new classes. By introducing ***De**composed **P**rompt **T**uning* framework (DePT), we theoretically demonstrate that OPT can be solved by incorporating out-of-distribution detection into prompt tuning, thereby enhancing the base-to-new discriminability. Based on DePT, we present a novel prompt tuning approach, namely, ***De**composed **Co**ntext **Op**timization* (DeCoOp), which introduces new-class detectors and sub-classifiers to further enhance the base-class and new-class discriminability. Experimental results on 11 benchmark datasets validate the effectiveness of DePT and demonstrate that DeCoOp outperforms current state-of-the-art methods, providing a significant 2% average accuracy improvement.",
  "abstract_zh": "视觉语言模型（VLMs），如CLIP，在各种下游任务中展示了令人印象深刻的零-shot能力。通过少量样本的提示调优方法可以进一步提升其性能。然而，目前的研究分别在基础类和新类上评估学习到的提示的性能，这种评估在实际应用中缺乏实用性，因为下游任务无法提前判断数据属于基础类还是新类。本文探讨了一种称为开放世界提示调优（OPT）的问题设置，该设置涉及在基础类上调优提示，并在基础类和新类的组合上进行评估。通过引入分解提示调优框架（DePT），我们理论上证明OPT可以通过将分布外检测纳入提示调优来解决，从而增强基础类与新类之间的可区分性。基于DePT，我们提出了一种新颖的提示调优方法，即分解上下文优化（DeCoOp），该方法引入新类检测器和子分类器，以进一步增强基础类和新类的可区分性。在11个基准数据集上的实验结果验证了DePT的有效性，并表明DeCoOp在性能上超越了当前的最先进方法，提供了显著的2%的平均准确率提升。"
}
{
  "title": "Instruction Tuning for Secure Code Generation",
  "title_zh": "安全代码生成的指令调优",
  "abstract": "Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across a variety of popular LMs and datasets. It is able to drastically improve security (by about 30%), while preserving utility.",
  "abstract_zh": "现代语言模型（LMs）在日常和专业环境中得到了广泛应用，尤其是在编程方面。促进这一采用的一个重要过程是指令调优，它通过训练模型遵循用户指令和人类偏好，显著增强了LMs的实际效用。然而，现有的指令调优方案忽视了一个关键方面：生成代码的安全性。因此，即使是最先进的指令调优LMs也经常生成不安全的代码，带来显著的安全风险。在本研究中，我们引入了SafeCoder来填补这一空白。SafeCoder使用我们通过自动化管道收集的多样化和高质量数据集进行以安全为中心的微调。我们将安全微调与标准指令调优相结合，以促进安全性和效用的联合优化。尽管其方法简单，我们展示了SafeCoder在多种流行的LMs和数据集上都有效。它能够显著提高安全性（约30%），同时保持效用。"
}
{
  "title": "Unifying Image Processing as Visual Prompting Question Answering",
  "title_zh": "统一图像处理为视觉提示问答",
  "abstract": "Image processing is a fundamental task in computer vision, which aims at enhancing image quality and extracting essential features for subsequent vision applications. Traditionally, task-specific models are developed for individual tasks and designing such models requires distinct expertise. Building upon the success of large language models (LLMs) in natural language processing (NLP), there is a similar trend in computer vision, which focuses on developing large-scale models through pretraining and in-context learning. This paradigm shift reduces the reliance on task-specific models, yielding a powerful unified model to deal with various tasks. However, these advances have predominantly concentrated on high-level vision tasks, with less attention paid to low-level vision tasks. To address this issue, we propose a universal model for general image processing that covers image restoration, image enhancement, image feature extraction tasks, etc. Our proposed framework, named PromptGIP, unifies these diverse image processing tasks within a universal framework. Inspired by NLP question answering (QA) techniques, we employ a visual prompting question answering paradigm. Specifically, we treat the input-output image pair as a structured question-answer sentence, thereby reprogramming the image processing task as a prompting QA problem. PromptGIP can undertake diverse cross-domain tasks using provided visual prompts, eliminating the need for task-specific finetuning. Capable of handling up to 15 different image processing tasks, PromptGIP represents a versatile and adaptive approach to general image processing. While PromptGIP has demonstrated a certain degree of out-of-domain task generalization capability, further research is expected to fully explore its more powerful emergent generalization. Codes will be available at https://github.com/lyh-18/PromptGIP.",
  "abstract_zh": "图像处理是计算机视觉中的一项基础任务，旨在增强图像质量并提取后续视觉应用所需的基本特征。传统上，为各个特定任务开发任务特定模型，设计这些模型需要不同的专业知识。在自然语言处理（NLP）中大型语言模型（LLMs）成功的基础上，计算机视觉中也出现了类似的趋势，专注于通过预训练和上下文学习开发大规模模型。这一范式转变减少了对任务特定模型的依赖，产生了一个强大的统一模型来处理各种任务。然而，这些进展主要集中在高层视觉任务上，而对低层视觉任务的关注较少。为了解决这个问题，我们提出了一个通用模型，用于一般图像处理，涵盖图像恢复、图像增强、图像特征提取等任务。我们提出的框架名为PromptGIP，将这些不同的图像处理任务统一在一个通用框架内。受到NLP问答（QA）技术的启发，我们采用了视觉提示问答范式。具体而言，我们将输入-输出图像对视为一个结构化的问答句子，从而将图像处理任务重新编程为一个提示QA问题。PromptGIP可以使用提供的视觉提示执行多样的跨领域任务，消除了对任务特定微调的需求。PromptGIP能够处理多达15种不同的图像处理任务，代表了一种多功能和自适应的通用图像处理方法。尽管PromptGIP在一定程度上展示了跨领域任务的泛化能力，但进一步的研究有望充分探索其更强大的突现泛化能力。代码将可在https://github.com/lyh-18/PromptGIP获取。"
}
{
  "title": "Reason for Future, Act for Now: A Principled Architecture for Autonomous LLM Agents",
  "title_zh": "未来的理由，现在的行动：自主大型语言模型代理的原则性架构",
  "abstract": "Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it is unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose the first framework with provable regret guarantees to orchestrate reasoning and acting, which we call *reason for future, act for now* (**RAFA**). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon (*reason for future*). At each step, the LLM agent takes the initial action of the planned trajectory (*act for now*), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state. The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs with the memory buffer to estimate the unknown environment (learning) and generate an optimal trajectory for multiple future steps that maximize a value function (planning). The learning and planning subroutines are performed in an in-context manner to emulate the actor-critic update for MDPs. Our theoretical analysis establishes a $\\sqrt{T}$ regret, while our experimental validation demonstrates superior empirical performance.",
  "abstract_zh": "大型语言模型（LLMs）展示了令人印象深刻的推理能力，但将推理转化为现实世界中的行动仍然具有挑战性。特别是，如何在与外部环境的最小交互次数内证明地完成给定任务尚不清楚，例如，通过内部推理机制。为此，我们提出了第一个具有可证明遗憾保证的框架来协调推理和行动，我们称之为*未来的理由，现在的行动*（**RAFA**）。具体而言，我们设计了一个推理的提示模板，该模板从记忆缓冲区学习，并规划未来的长时间轨迹（*未来的理由*）。在每一步中，LLM代理采取计划轨迹的初始行动（*现在的行动*），将收集到的反馈存储在记忆缓冲区中，并重新调用推理例程，从新状态重新规划未来轨迹。关键思想是将LLMs中的推理视为贝叶斯自适应马尔可夫决策过程（MDPs）中的学习和规划。因此，我们使用记忆缓冲区提示LLMs以估计未知环境（学习）并生成最大化价值函数的多个未来步骤的最佳轨迹（规划）。学习和规划子例程以上下文方式执行，以模拟MDPs的演员-评论家更新。我们的理论分析建立了$\\sqrt{T}$的遗憾，而我们的实验验证展示了优越的经验性能。"
}
{
  "title": "Embodied CoT Distillation From LLM To Off-the-shelf Agents",
  "title_zh": "标题：从大型语言模型到现成代理的具身CoT蒸馏",
  "abstract": "We address the challenge of utilizing large language models (LLMs) for complex embodied tasks, in the environment where decision-making systems operate timely on capacity-limited, off-the-shelf devices. We present DeDer, a framework for decomposing and distilling the embodied reasoning capabilities from LLMs to efficient, small language model (sLM)-based policies. In DeDer, the decision-making process of LLM-based strategies is restructured into a hierarchy with a reasoning-policy and planning-policy. The reasoning-policy is distilled from the data that is generated through the embodied in-context learning and self-verification of an LLM, so it can produce effective rationales. The planning-policy, guided by the rationales, can render optimized plans efficiently. In turn, DeDer allows for adopting sLMs for both policies, deployed on off-the-shelf devices. Furthermore, to enhance the quality of intermediate rationales, specific to embodied tasks, we devise the embodied knowledge graph, and to generate multiple rationales timely through a single inference, we also use the contrastively prompted attention model. Our experiments with the ALFRED benchmark demonstrate that DeDer surpasses leading language planning and distillation approaches, indicating the applicability and efficiency of sLM-based embodied policies derived through DeDer.",
  "abstract_zh": "摘要：我们解决了在决策系统需要及时在容量有限的现成设备上操作的环境中，利用大型语言模型（LLMs）进行复杂具身任务的挑战。我们提出了DeDer，一个将LLMs的具身推理能力分解并蒸馏到高效的小型语言模型（sLM）基础策略的框架。在DeDer中，基于LLM策略的决策过程被重构为一个包含推理策略和规划策略的层次结构。推理策略是通过LLM的具身上下文学习和自我验证生成的数据进行蒸馏的，因此能够产生有效的推理依据。规划策略在推理依据的指导下，可以高效地生成优化计划。反过来，DeDer允许在现成设备上采用sLM进行这两种策略的部署。此外，为了提高特定于具身任务的中间推理依据的质量，我们设计了具身知识图谱，并通过对比提示注意模型及时生成多个推理依据。我们在ALFRED基准上的实验表明，DeDer超越了领先的语言规划和蒸馏方法，表明通过DeDer衍生的基于sLM的具身策略的适用性和效率。"
}
{
  "title": "Position: Key Claims in LLM Research Have a Long Tail of Footnotes",
  "title_zh": "标题：位置：大型语言模型研究中的关键主张有着长尾的脚注",
  "abstract": "Much of the recent discourse within the ML community has been centered around Large Language Models (LLMs), their functionality and potential -- yet not only do we not have a working definition of LLMs, but much of this discourse relies on claims and assumptions that are worth re-examining. We contribute a definition of LLMs, critically examine five common claims regarding their properties (including 'emergent properties'), and conclude with suggestions for future research directions and their framing.",
  "abstract_zh": "摘要：近期机器学习社区的讨论主要集中在大型语言模型（LLMs）、它们的功能和潜力上——然而，我们不仅没有一个有效的LLM定义，而且许多讨论依赖于值得重新审视的主张和假设。我们提供了LLM的定义，批判性地审视了关于其属性的五个常见主张（包括“涌现属性”），并以对未来研究方向及其框架的建议作为结尾。"
}
{
  "title": "$\\texttt{MoE-RBench}$: Towards Building Reliable Language Models with Sparse Mixture-of-Experts",
  "title_zh": "标题：$\\texttt{MoE-RBench}$：构建可靠的稀疏专家混合语言模型",
  "abstract": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new domains such as in fine-tuning MoE models sometimes underperform their dense counterparts. Motivated by the research gap and counter-intuitive phenomenon, we propose $\\texttt{MoE-RBench}$, the first comprehensive assessment of SMoE reliability from three aspects: $\\textit{(i)}$ safety and hallucination, $\\textit{(ii)}$ resilience to adversarial attacks, and $\\textit{(iii)}$ out-of-distribution robustness. Extensive models and datasets are tested to compare the MoE to dense networks from these reliability dimensions. Our empirical observations suggest that with appropriate hyperparameters, training recipes, and inference techniques, we can build the MoE model more reliably than the dense LLM. In particular, we find that the robustness of SMoE is sensitive to the basic training settings. We hope that this study can provide deeper insights into how to adapt the pre-trained MoE model to other tasks with higher-generation security, quality, and stability. Codes are available at https://github.com/UNITES-Lab/MoE-RBench.",
  "abstract_zh": "摘要：专家混合（MoE）作为扩展大型语言模型（LLMs）的有前景框架，越来越受到关注。然而，MoE的可靠性评估却滞后于其迅速增长的应用。此外，当转移到新领域时，例如在微调MoE模型时，有时表现不如其稠密对应物。基于这一研究空白和反直觉现象，我们提出了$\\texttt{MoE-RBench}$，这是对SMoE可靠性的首次全面评估，从三个方面进行：$\\textit{(i)}$ 安全性和幻觉，$\\textit{(ii)}$ 对抗攻击的韧性，以及 $\\textit{(iii)}$ 分布外的鲁棒性。通过广泛的模型和数据集测试，我们比较了MoE与稠密网络在这些可靠性维度上的表现。我们的实证观察表明，适当的超参数、训练方案和推理技术可以使MoE模型的可靠性超过稠密LLM。特别是，我们发现SMoE的鲁棒性对基本训练设置非常敏感。我们希望这项研究能够为如何将预训练的MoE模型适应于其他任务提供更深入的见解，以提高生成的安全性、质量和稳定性。代码可在https://github.com/UNITES-Lab/MoE-RBench获取。"
}
{
  "title": "COPAL: Continual Pruning in Large Language Generative Models",
  "title_zh": "COPAL：大语言生成模型中的持续剪枝",
  "abstract": "Adapting pre-trained large language models to different domains in natural language processing requires two key considerations: high computational demands and model's inability to continual adaptation. To simultaneously address both issues, this paper presents COPAL (**CO**ntinual **P**runing in **A**daptive **L**anguage settings), an algorithm developed for pruning large language generative models under a continual model adaptation setting. While avoiding resource-heavy finetuning or retraining, our pruning process is guided by the proposed sensitivity analysis. The sensitivity effectively measures model's ability to withstand perturbations introduced by the new dataset and finds model's weights that are relevant for all encountered datasets. As a result, COPAL allows seamless model adaptation to new domains while enhancing the resource efficiency. Our empirical evaluation on a various size of LLMs show that COPAL outperforms baseline models, demonstrating its efficacy in efficiency and adaptability.",
  "abstract_zh": "适应预训练的大语言模型到自然语言处理中的不同领域需要两个关键考虑：高计算需求和模型持续适应能力的缺失。为同时解决这两个问题，本文提出了COPAL（**CO**ntinual **P**runing in **A**daptive **L**anguage settings），这是一个在持续模型适应设置下为大语言生成模型剪枝而开发的算法。在避免资源密集型微调或重新训练的同时，我们的剪枝过程由所提出的敏感性分析引导。敏感性有效地衡量模型抵御新数据集引入的扰动的能力，并找到与所有遇到的数据集相关的模型权重。因此，COPAL允许模型无缝适应新领域，同时提高资源效率。我们对不同规模的LLM的实证评估表明，COPAL优于基线模型，证明了其在效率和适应性方面的有效性。"
}
{
  "title": "DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton",
  "title_zh": "标题：DFA-RAG：基于确定有限自动机的大语言模型对话语义路由器",
  "abstract": "This paper introduces the retrieval-augmented large language model with Definite Finite Automaton (DFA-RAG), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs). Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM. This structured approach acts as a semantic router which enables the LLM to adhere to a deterministic response pathway. The routing is achieved by the retrieval-augmentation generation (RAG) strategy, which carefully selects dialogue examples aligned with the current conversational context. The advantages of DFA-RAG include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs. Extensive benchmarks validate DFA-RAG's effectiveness, indicating its potential as a valuable contribution to the conversational agent.",
  "abstract_zh": "摘要：本文介绍了一种基于确定有限自动机（DFA-RAG）的检索增强大语言模型的新框架，旨在增强使用大语言模型（LLMs）的对话代理的能力。传统的LLMs在生成符合特定响应指南的受限和合规回复时面临挑战，例如情感支持和客户服务。我们的框架通过在LLM中嵌入从训练对话中学习的确定有限自动机（DFA）来解决这些挑战。这种结构化的方法充当语义路由器，使LLM能够遵循确定性的响应路径。路由通过检索增强生成（RAG）策略实现，该策略仔细选择与当前对话上下文一致的对话示例。DFA-RAG的优势包括通过人类可读的DFA提供可解释的结构、对话中响应的上下文感知检索，以及与现有LLMs的即插即用兼容性。广泛的基准测试验证了DFA-RAG的有效性，表明其作为对话代理的有价值贡献的潜力。"
}
{
  "title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains",
  "title_zh": "标签-LLM：将通用LLM重新用于专业领域",
  "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM’s embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from task functions, our method enables zero-shot generalization to unseen problems through diverse combinations of the input tags. It also boosts LLM’s performance in various specialized domains, such as predicting protein or chemical properties and modeling drug-target interactions, outperforming expert models tailored to these tasks.",
  "abstract_zh": "大型语言模型（LLMs）在理解和生成自然语言方面表现出色。然而，在预训练语料库中代表性不足的高度专业化领域（如物理和生物医学科学）中，它们的能力减弱。本文探讨如何将通用LLM重新用于专业领域的有效任务求解器。我们引入了一种新颖的模型无关框架，用于学习自定义输入标签，这些标签被参数化为附加到LLM嵌入层的连续向量，以调节LLM。我们设计了两种类型的输入标签：领域标签用于界定专业表示（例如，化学公式）并提供领域相关的上下文；功能标签用于表示特定功能（例如，预测分子性质）并压缩功能求解指令。我们开发了一个三阶段协议，利用辅助数据和领域知识学习这些标签。通过明确将任务领域与任务功能分离，我们的方法通过输入标签的多样组合实现对未见问题的零样本泛化。它还提升了LLM在各种专业领域的表现，例如预测蛋白质或化学性质和建模药物-靶标相互作用，超越了针对这些任务量身定制的专家模型。"
}
{
  "title": "DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning",
  "title_zh": "标题：DS-Agent：通过案例推理赋能大型语言模型的自动化数据科学",
  "abstract": "In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100% success rate in the development stage, while attaining 36% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \r\n$1.60 and \\\\$0.13 per run with GPT-4, respectively. Our data and code are open-sourced at https://github.com/guosyjlu/DS-Agent.",
  "abstract_zh": "摘要：在本研究中，我们探讨了大型语言模型（LLMs）代理自动化数据科学任务的潜力，旨在理解任务需求，然后构建和训练最适合的机器学习模型。尽管现有LLM代理取得了广泛成功，但在此场景中，它们生成不合理实验计划的能力受到限制。为此，我们提出了DS-Agent，这是一种新颖的自动化框架，结合了LLM代理和案例推理（CBR）。在开发阶段，DS-Agent遵循CBR框架构建自动迭代管道，灵活利用Kaggle的专家知识，并通过反馈机制促进一致的性能提升。此外，DS-Agent在低资源部署阶段实施了简化的CBR范式，以适应开发阶段成功解决方案的直接代码生成，显著降低了对LLM基础能力的需求。从实证来看，DS-Agent与GPT-4在开发阶段实现了100%的成功率，而在部署阶段在不同LLM中平均提高了36%的通过率。在两个阶段中，DS-Agent的性能排名最佳，分别以$1.60和$0.13的成本运行GPT-4。我们的数据和代码已开源于https://github.com/guosyjlu/DS-Agent。"
}
{
  "title": "Differentially Private Synthetic Data via Foundation Model APIs 2: Text",
  "title_zh": "差分隐私合成数据通过基础模型API 2：文本",
  "abstract": "Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications.",
  "abstract_zh": "文本数据因机器学习算法的出现而变得极其宝贵。大量在现实世界中生成的高质量文本数据是私密的，因此由于隐私问题无法自由共享或使用。生成具有正式隐私保证的私密文本数据的合成副本，即差分隐私（DP），提供了一种有前景且可扩展的解决方案。然而，现有方法需要对私密数据进行大型语言模型（LLMs）的DP微调以生成DP合成数据。这种方法对于专有LLMs（例如GPT-3.5）不可行，并且对于开源LLMs也需要相当大的计算资源。Lin等人（2024）最近引入了私有进化（PE）算法，仅通过对扩散模型的API访问生成DP合成图像。在这项工作中，我们提出了一种增强的PE算法，称为Aug-PE，适用于文本的复杂设置。我们使用对LLM的API访问生成DP合成文本，而无需任何模型训练。我们在三个基准数据集上进行了全面实验。我们的结果表明，Aug-PE生成的DP合成文本在效用上与SOTA DP微调基线具有竞争力。这强调了仅依赖LLMs的API访问生成高质量DP合成文本的可行性，从而促进了更便捷的隐私保护LLM应用途径。"
}
{
  "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
  "title_zh": "视频诗人：一种用于零-shot视频生成的大型语言模型",
  "abstract": "We present VideoPoet, a language model capable of synthesizing high-quality video from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting the ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/",
  "abstract_zh": "我们提出了视频诗人，这是一种能够从多种条件信号合成高质量视频的语言模型。视频诗人采用仅解码器的变换器架构，处理多模态输入，包括图像、视频、文本和音频。训练协议遵循大型语言模型（LLMs）的流程，包括两个阶段：预训练和任务特定适应。在预训练期间，视频诗人结合了多模态生成目标的混合，采用自回归变换器框架。预训练的LLM作为基础，可以适应各种视频生成任务。我们展示了实证结果，证明该模型在零-shot视频生成方面的先进能力，特别强调生成高保真运动的能力。项目页面：http://sites.research.google/videopoet/"
}
{
  "title": "DE-COP: Detecting Copyrighted Content in Language Models Training Data",
  "title_zh": "DE-COP：检测语言模型训练数据中的版权内容",
  "abstract": "*How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed?* We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content is included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP outperforms the prior best method by 8.6% in detection accuracy (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give approximately 0% accuracy. The code and datasets are available at https://github.com/LeiLiLab/DE-COP.",
  "abstract_zh": "*我们如何检测语言模型的训练过程中是否使用了版权内容，考虑到训练数据通常是未公开的？* 我们的动机源于一个前提，即语言模型很可能会识别其训练文本中的逐字摘录。我们提出了DE-COP，一种确定版权内容是否包含在训练中的方法。DE-COP的核心方法是通过多项选择题来探测大型语言模型，选项包括逐字文本及其释义。我们构建了BookTection，一个基准数据集，其中包含165本书的摘录，这些书籍在模型训练截止日期之前和之后出版，以及它们的释义。我们的实验表明，DE-COP在具有logits可用的模型上，检测准确率（AUC）比之前最佳方法提高了8.6%。此外，DE-COP在完全黑箱模型上检测可疑书籍的平均准确率达到72%，而之前的方法的准确率约为0%。代码和数据集可在https://github.com/LeiLiLab/DE-COP获取。"
}
{
  "title": "Successor Features for Efficient Multi-Subject Controlled Text Generation",
  "title_zh": "继任特征用于高效的多主题控制文本生成",
  "abstract": "While large language models (LLMs) have achieved impressive performance in generating fluent and realistic text, controlling the generated text so that it exhibits properties such as safety, factuality, and non-toxicity remains challenging. Existing decoding-based controllable text generation methods are static in terms of the dimension of control; if the target subject is changed, they require new training. Moreover, it can quickly become prohibitive to concurrently control multiple subjects. To address these challenges, we first show that existing methods can be framed as a reinforcement learning problem, where an action-value function estimates the likelihood of a desired attribute appearing in the generated text. Then, we introduce a novel approach named SF-Gen, which leverages the concept of successor features to decouple the dynamics of LLMs from task-specific rewards. By employing successor features, our method proves to be memory-efficient and computationally efficient for both training and decoding, especially when dealing with multiple target subjects. To the best of our knowledge, our research represents the first application of successor features in text generation. In addition to its computational efficiency, the resultant language produced by our method is comparable to the SOTA (and outperforms baselines) in both control measures as well as language quality, which we demonstrate through a series of experiments in various controllable text generation tasks.",
  "abstract_zh": "尽管大型语言模型（LLMs）在生成流畅且真实的文本方面取得了令人印象深刻的表现，但控制生成文本以展现安全性、真实性和非毒性等特性仍然具有挑战性。现有的基于解码的可控文本生成方法在控制维度上是静态的；如果目标主题发生变化，它们需要重新训练。此外，同时控制多个主题可能迅速变得不可承受。为了解决这些挑战，我们首先展示了现有方法可以被框架为强化学习问题，其中一个动作值函数估计所需属性出现在生成文本中的可能性。然后，我们引入了一种名为SF-Gen的新方法，该方法利用继任特征的概念，将LLMs的动态与任务特定奖励解耦。通过采用继任特征，我们的方法在训练和解码方面证明了其内存效率和计算效率，特别是在处理多个目标主题时。据我们所知，我们的研究代表了继任特征在文本生成中的首次应用。除了计算效率外，我们方法生成的语言在控制指标和语言质量上与SOTA相当（并且超越基线），我们通过一系列可控文本生成任务的实验进行了验证。"
}
{
  "title": "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements",
  "title_zh": "GLoRe：何时、何地以及如何通过全局和局部精炼来提升大型语言模型的推理能力",
  "abstract": "State-of-the-art language models can exhibit reasoning refinement capabilities on math, science or coding tasks. However, recent work demonstrates that even the best models struggle to identify *when and where to refine* without access to external feedback. In this paper, we propose Stepwise ORMs (**SORMs**) which are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or $V^{\\star}$ as a form of Process-based reward modeling. Our experiments show that SORMs can more accurately detect incorrect reasoning steps compared to ORMs, thus enabling them to give precise step-level feedback to refinement models. We then train *global* refinement models, which take only the question and a draft solution as input and predict a corrected solution, and *local* refinement models which also take as input a critique indicating the location of the first reasoning error. We generate training data for both models synthetically by reusing data used to train the SORM. We find combining global and local refinements, using the ORM as a reranker, significantly outperforms either one individually, as well as a best of three sample baseline. With this strategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned with RL) on GSM8K from 53% to 65% when greedily sampled.",
  "abstract_zh": "最先进的语言模型在数学、科学或编码任务上展现了推理精炼的能力。然而，最近的研究表明，即使是最好的模型在没有外部反馈的情况下也难以识别*何时何地进行精炼*。在本文中，我们提出了逐步优化的奖励模型（**SORMs**），该模型仅在合成数据上训练，以近似最佳策略的预期未来奖励或$V^{\\star}$，作为一种基于过程的奖励建模。我们的实验表明，与ORMs相比，SORMs能够更准确地检测不正确的推理步骤，从而使其能够为精炼模型提供精确的逐步反馈。然后，我们训练*全局*精炼模型，该模型仅将问题和草拟解决方案作为输入，并预测修正后的解决方案，以及*局部*精炼模型，该模型还将指示第一个推理错误位置的批评作为输入。我们通过重用用于训练SORM的数据合成生成这两种模型的训练数据。我们发现，将全局和局部精炼结合使用，并将ORM作为重新排序器，显著优于单独使用任一模型以及三样本基线中的最佳者。通过这种策略，我们可以将已经通过强化学习微调的LLaMA-2 13B模型在GSM8K上的准确率从53%提高到65%。"
}
{
  "title": "Copyright Traps for Large Language Models",
  "title_zh": "大型语言模型的版权陷阱",
  "abstract": "Questions of fair use of copyright-protected content to train Large Language Models (LLMs) are being actively debated. Document-level inference has been proposed as a new task: inferring from black-box access to the trained model whether a piece of content has been seen during training. SOTA methods however rely on naturally occurring memorization of (part of) the content. While very effective against models that memorize significantly, we hypothesize - and later confirm - that they will not work against models that do not naturally memorize, e.g. medium-size 1B models. We here propose to use copyright traps, the inclusion of fictitious entries in original content, to detect the use of copyrighted materials in LLMs with a focus on models where memorization does not naturally occur. We carefully design a randomized controlled experimental setup, inserting traps into original content (books) and train a 1.3B LLM from scratch. We first validate that the use of content in our target model would be undetectable using existing methods. We then show, contrary to intuition, that even medium-length trap sentences repeated a significant number of times (100) are not detectable using existing methods. However, we show that longer sequences repeated a large number of times can be reliably detected (AUC=0.75) and used as copyright traps. Beyond copyright applications, our findings contribute to the study of LLM memorization: the randomized controlled setup enables us to draw causal relationships between memorization and certain sequence properties such as repetition in model training data and perplexity.",
  "abstract_zh": "关于使用受版权保护内容训练大型语言模型（LLMs）的合理使用问题正在积极讨论。提出了一种新的任务：通过黑箱访问训练好的模型来推断某个内容在训练期间是否被见过。然而，现有的最先进方法依赖于内容的自然记忆。虽然对显著记忆的模型非常有效，但我们假设（并随后确认）这些方法对不自然记忆的模型（例如中型1B模型）无效。我们在此提出使用版权陷阱，即在原始内容中包含虚构条目，以检测LLMs中受版权材料的使用，重点关注那些不自然记忆的模型。我们精心设计了随机对照实验设置，将陷阱插入原始内容（书籍）中，并从头开始训练一个1.3B的LLM。我们首先验证目标模型中内容的使用在现有方法下是不可检测的。然后，我们展示了与直觉相反的结果，即即使是中等长度的陷阱句子重复相当多次（100次）也无法通过现有方法检测。然而，我们表明，重复大量次数的较长序列可以可靠地检测（AUC=0.75）并用作版权陷阱。除了版权应用外，我们的发现还为LLM记忆的研究做出了贡献：随机对照设置使我们能够在记忆与某些序列属性（如模型训练数据中的重复性和困惑度）之间建立因果关系。"
}
{
  "title": "CaM: Cache Merging for Memory-efficient LLMs Inference",
  "title_zh": "CaM：用于内存高效LLM推理的缓存合并",
  "abstract": "Despite the exceptional performance of Large Language Models (LLMs), the substantial volume of key-value (KV) pairs cached during inference presents a barrier to their efficient deployment. To ameliorate this, recent works have aimed to selectively eliminate these caches, informed by the attention scores of associated tokens. However, such cache eviction invariably leads to output perturbation, regardless of the token choice. This perturbation escalates with the compression ratio, which can precipitate a marked deterioration in LLM inference performance. This paper introduces Cache Merging (CaM) as a solution to mitigate this challenge. CaM adaptively merges to-be-evicted caches into the remaining ones, employing a novel sampling strategy governed by the prominence of attention scores within discarded locations. In this manner, CaM enables memory-efficient LLMs to preserve critical token information, even obviating the need to maintain their corresponding caches. Extensive experiments utilizing LLaMA, OPT, and GPT-NeoX across various benchmarks corroborate CaM's proficiency in bolstering the performance of memory-efficient LLMs. Code is released at https://github.com/zyxxmu/cam.",
  "abstract_zh": "尽管大型语言模型（LLMs）表现出色，但推理过程中缓存的关键值（KV）对的巨大数量成为其高效部署的障碍。为了解决这一问题，近期的研究旨在根据相关标记的注意力得分选择性地消除这些缓存。然而，这种缓存驱逐无论标记选择如何，都会导致输出扰动。随着压缩比的增加，这种扰动会加剧，从而显著恶化LLM的推理性能。本文提出了缓存合并（CaM）作为缓解这一挑战的解决方案。CaM自适应地将待驱逐的缓存合并到剩余的缓存中，采用一种新颖的采样策略，该策略由被丢弃位置的注意力得分的显著性驱动。通过这种方式，CaM使内存高效的LLM能够保留关键的标记信息，甚至无需维护其相应的缓存。利用LLaMA、OPT和GPT-NeoX在各种基准测试中进行的大量实验证实了CaM在提升内存高效LLM性能方面的能力。代码已发布在https://github.com/zyxxmu/cam。"
}
{
  "title": "Language Generation with Strictly Proper Scoring Rules",
  "title_zh": "标题：基于严格适当评分规则的语言生成",
  "abstract": "Language generation based on maximum likelihood estimation (MLE) has become the fundamental approach for text generation. Maximum likelihood estimation is typically performed by minimizing the log-likelihood loss, also known as the logarithmic score in statistical decision theory. The logarithmic score is strictly proper in the sense that it encourages honest forecasts, where the expected score is maximized only when the model reports true probabilities. Although many strictly proper scoring rules exist, the logarithmic score is the only local scoring rule among them that depends exclusively on the probability of the observed sample, making it capable of handling the exponentially large sample space of natural text. In this work, we propose a straightforward strategy for adapting scoring rules to language generation, allowing for language modeling with any non-local scoring rules. Leveraging this strategy, we train language generation models using two classic strictly proper scoring rules, the Brier score and the Spherical score, as alternatives to the logarithmic score. Experimental results indicate that simply substituting the loss function, without adjusting other hyperparameters, can yield substantial improvements in model's generation capabilities. Moreover, these improvements can scale up to large language models (LLMs) such as LLaMA-7B and LLaMA-13B. Source code: https://github.com/shaochenze/ScoringRulesLM.",
  "abstract_zh": "摘要：基于最大似然估计（MLE）的语言生成已成为文本生成的基本方法。最大似然估计通常通过最小化对数似然损失来执行，这在统计决策理论中被称为对数评分。对数评分是严格适当的，因为它鼓励诚实的预测，只有当模型报告真实概率时，期望评分才会最大化。尽管存在许多严格适当的评分规则，但对数评分是其中唯一一个仅依赖于观察样本概率的局部评分规则，使其能够处理自然文本的指数级大样本空间。在这项工作中，我们提出了一种简单的策略，将评分规则适应于语言生成，允许使用任何非局部评分规则进行语言建模。利用这一策略，我们使用两种经典的严格适当评分规则——Brier评分和球形评分，作为对数评分的替代方案来训练语言生成模型。实验结果表明，仅仅替换损失函数，而不调整其他超参数，就可以显著提高模型的生成能力。此外，这些改进可以扩展到大型语言模型（LLMs），如LLaMA-7B和LLaMA-13B。源代码：https://github.com/shaochenze/ScoringRulesLM。"
}
{
  "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
  "title_zh": "KIVI：一种无调优的非对称2位量化方法用于KV缓存",
  "abstract": "Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU's SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Our findings indicate that the key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token. From this analysis, we developed a tuning-free 2bit KV cache quantization algorithm, named KIVI. With the hardware-friendly implementation, KIVI can enable Llama (Llama-2), Falcon, and Mistral models to maintain almost the same quality while using 2.6$\\times$ less peak memory usage (including the model weight). This reduction in memory usage enables up to 4x larger batch size, bringing $2.35 \\times \\sim 3.47 \\times$ throughput on real LLM inference workload.",
  "abstract_zh": "高效服务大型语言模型（LLMs）需要将多个请求批量处理，以降低每个请求的成本。然而，存储注意力键和值以避免重新计算的键值（KV）缓存显著增加了内存需求，成为速度和内存使用的新瓶颈。随着批量大小和上下文长度的增加，这种内存需求也在增加。此外，推理速度受限于KV缓存的大小，因为GPU的SRAM必须为每个生成的令牌从主GPU内存加载整个KV缓存，导致计算核心在此过程中处于空闲状态。减少KV缓存大小的一个简单有效的解决方案是量化，它减少了KV缓存占用的总字节数。然而，目前缺乏深入研究KV缓存元素分布的研究，以理解KV缓存量化的困难和限制。为填补这一空白，我们对流行LLM的KV缓存中的元素分布进行了全面研究。我们的研究结果表明，键缓存应按通道量化，即沿通道维度对元素进行分组并一起量化。相反，值缓存应按令牌量化。基于这一分析，我们开发了一种无调优的2位KV缓存量化算法，命名为KIVI。通过硬件友好的实现，KIVI能够使Llama（Llama-2）、Falcon和Mistral模型在使用2.6倍更少的峰值内存（包括模型权重）的同时保持几乎相同的质量。这种内存使用的减少使得批量大小最大可增加4倍，在实际LLM推理工作负载中带来了2.35倍至3.47倍的吞吐量。"
}
{
  "title": "QUEST: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
  "title_zh": "标题：QUEST：针对高效长上下文LLM推理的查询感知稀疏性",
  "abstract": "As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at https://github.com/mit-han-lab/quest.",
  "abstract_zh": "摘要：随着对长上下文大型语言模型（LLMs）需求的增加，具有高达128K或1M标记的上下文窗口的模型变得越来越普遍。然而，长上下文LLM推理面临挑战，因为随着序列长度的增加，推理速度显著下降。这一减速主要是由于在自注意力期间加载大量KV缓存。先前的研究表明，少量关键标记将主导注意力结果。然而，我们观察到标记的关键性高度依赖于查询。为此，我们提出了Quest，一种查询感知的KV缓存选择算法。Quest跟踪KV缓存页面中的最小和最大键值，并使用查询向量估计给定页面的关键性。通过仅加载Top-K关键KV缓存页面进行注意力计算，Quest显著加快了自注意力速度而不牺牲准确性。我们展示了Quest可以实现高达2.23倍的自注意力加速，将推理延迟减少7.03倍，同时在长依赖任务上表现良好，准确性损失微乎其微。代码可在https://github.com/mit-han-lab/quest获取。"
}
{
  "title": "AI Control: Improving Safety Despite Intentional Subversion",
  "title_zh": "AI控制：尽管存在故意破坏，仍能提高安全性",
  "abstract": "As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. To do so, safety measures either aim at making LLMs try to avoid harmful outcomes or aim at preventing LLMs from causing harmful outcomes, even if they try to cause them. In this paper, we focus on this second layer of defense. We develop and evaluate pipelines of safety techniques (protocols) that try to ensure safety despite intentional subversion - an approach we call AI control. We investigate a setting in which we want to solve a sequence of programming problems without ever submitting subtly wrong code, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate a range of protocols and red-team them by exploring strategies that the untrusted model could use to subvert them. We find that using the trusted model to edit untrusted-model code or using the untrusted model as a monitor substantially improves on simple baselines.",
  "abstract_zh": "随着大型语言模型（LLMs）变得越来越强大并更自主地部署，防止它们造成有害后果将变得愈发重要。为此，安全措施旨在使LLMs尽量避免有害后果，或旨在防止LLMs造成有害后果，即使它们试图造成这些后果。本文重点关注第二层防御。我们开发并评估了一系列安全技术（协议）的管道，试图确保安全，尽管存在故意破坏——我们称之为AI控制。我们研究了一个场景，在这个场景中，我们希望解决一系列编程问题，而不提交微妙错误的代码，利用对强大但不可信模型（在我们的案例中为GPT-4）、对较弱可信模型（在我们的案例中为GPT-3.5）的访问，以及有限的高质量可信劳动力的访问。我们研究了一系列协议，并通过探索不可信模型可能用来破坏它们的策略进行红队测试。我们发现，使用可信模型编辑不可信模型代码或将不可信模型用作监控器，显著改善了简单基线。"
}
{
  "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning",
  "title_zh": "数学规模：数学推理的指令调优扩展",
  "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., GPT-3.5). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct MWPBench, a benchmark of Math Word Problems, which is a collection of 9 datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on MWPBench, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.8% in micro average accuracy and 43.6% in macro average accuracy, respectively.",
  "abstract_zh": "大型语言模型（LLMs）在解决问题方面表现出色。然而，它们在解决数学问题方面的能力仍然不足。我们提出了MathScale，这是一种简单且可扩展的方法，利用前沿的LLMs（如GPT-3.5）创建高质量的数学推理数据。受人类数学学习认知机制的启发，它首先从种子数学问题中提取主题和知识点，然后构建概念图，随后用于生成新的数学问题。MathScale在我们生成的数学数据集的规模轴上表现出有效的可扩展性。因此，我们创建了一个包含两百万个数学问答对的数学推理数据集（MathScaleQA）。为了全面评估LLMs的数学推理能力，我们构建了MWPBench，一个数学文字问题基准，这是一个包含9个数据集（包括GSM8K和MATH）的集合，涵盖K-12、大学和竞赛级别的数学问题。我们将MathScaleQA应用于微调开源LLMs（如LLaMA-2和Mistral），显著提升了其数学推理能力。在MWPBench上的评估中，MathScale-7B在所有数据集上实现了最先进的性能，在微平均准确率上超越了同等规模的最佳同行42.8%，在宏平均准确率上超越了43.6%。"
}
{
  "title": "Understanding the Effects of Iterative Prompting on Truthfulness",
  "title_zh": "标题：理解迭代提示对真实性的影响",
  "abstract": "The development of Large Language Models (LLMs) has notably transformed numerous sectors, offering impressive text generation capabilities. Yet, the reliability and truthfulness of these models remain pressing concerns. To this end, we investigate iterative prompting, a strategy hypothesized to refine LLM responses, assessing its impact on LLM truthfulness, an area which has not been thoroughly explored. Our extensive experiments explore the intricacies of iterative prompting variants, examining their influence on the accuracy and calibration of model responses. Our findings reveal that naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors. In response to these challenges, we introduce several prompting variants designed to address the identified issues. These variants demonstrate marked improvements over existing baselines, signaling a promising direction for future research. Our work provides a nuanced understanding of iterative prompting and introduces novel approaches to enhance the truthfulness of LLMs, thereby contributing to the development of more accurate and trustworthy AI systems",
  "abstract_zh": "摘要：大型语言模型（LLMs）的发展显著改变了多个领域，提供了令人印象深刻的文本生成能力。然而，这些模型的可靠性和真实性仍然是迫切关注的问题。为此，我们研究了迭代提示，这是一种假设可以改善LLM响应的策略，评估其对LLM真实性的影响，这是一个尚未被充分探索的领域。我们的广泛实验探讨了迭代提示变体的复杂性，考察它们对模型响应的准确性和校准的影响。我们的发现表明，简单的提示方法显著削弱了真实性，导致校准错误加剧。针对这些挑战，我们提出了几种旨在解决识别问题的提示变体。这些变体在现有基准上显示出显著的改进，标志着未来研究的有希望方向。我们的工作提供了对迭代提示的细致理解，并引入了新方法以增强LLM的真实性，从而为开发更准确和可信的人工智能系统做出贡献。"
}
{
  "title": "Fundamental Limitations of Alignment in Large Language Models",
  "title_zh": "大型语言模型对齐的基本限制",
  "abstract": "An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that within the limits of this framework, for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates an undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the LLM prone to being prompted into the undesired behaviors. This theoretical result is being experimentally demonstrated in large scale by the so called contemporary \"chatGPT jailbreaks\", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona. Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.",
  "abstract_zh": "在开发与人类互动的语言模型时，一个重要方面是将其行为对齐，使其对人类用户有用且无害。这通常通过调整模型来增强期望行为并抑制不期望行为，这一过程称为对齐。本文提出了一种称为行为期望界限（BEB）的理论方法，使我们能够正式研究大型语言模型中对齐的若干固有特征和限制。重要的是，我们证明在该框架的限制内，对于任何具有有限概率被模型表现出的行为，存在可以触发模型输出该行为的提示，且这种概率随着提示长度的增加而增加。这意味着任何减弱不期望行为但并未完全消除的对齐过程，都无法抵御对抗性提示攻击。此外，我们的框架暗示了领先的对齐方法（如基于人类反馈的强化学习）使大型语言模型容易被提示为不期望行为的机制。这一理论结果在所谓的当代“chatGPT越狱”中得到了大规模的实验验证，其中对抗性用户通过触发模型表现出恶意角色来欺骗大型语言模型打破其对齐防护。我们的结果揭示了大型语言模型对齐的基本限制，并突显了制定可靠机制以确保人工智能安全的必要性。"
}
{
  "title": "A Tale of Tails: Model Collapse as a Change of Scaling Laws",
  "title_zh": "尾巴的故事：模型崩溃作为缩放法则的变化",
  "abstract": "As AI model size grows, neural *scaling laws* have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: *How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus?* Will future models, still improve, or be doomed to degenerate up to total *(model) collapse*? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning\" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation using the large language model Llama2.",
  "abstract_zh": "随着AI模型规模的增长，神经网络的*缩放法则*已成为预测大型模型在增加容量和原始（人类或自然）训练数据规模时改进的重要工具。然而，流行模型的广泛使用意味着在线数据和文本的生态系统将共同演变，逐渐包含更多合成数据。本文提出了一个问题：*在合成数据不可避免地进入训练语料库的情况下，缩放法则将如何变化？*未来的模型是否仍会改进，还是注定会退化到完全的*(模型)崩溃*？我们通过缩放法则的视角发展了模型崩溃的理论框架。我们发现了一系列衰减现象，分析了缩放损失、随着生成数量的缩放转移、技能的“遗忘”以及混合人类和合成数据时的理解能力。我们的理论通过在算术任务和使用大型语言模型Llama2进行文本生成的大规模实验得到了验证。"
}
{
  "title": "UPAM: Unified Prompt Attack in Text-to-Image Generation Models Against Both Textual Filters and Visual Checkers",
  "title_zh": "统一提示攻击：针对文本到图像生成模型的文本过滤器和视觉检查器",
  "abstract": "Text-to-Image (T2I) models have raised security concerns due to their potential to generate inappropriate or harmful images. In this paper, we propose UPAM, a novel framework that investigates the robustness of T2I models from the attack perspective. Unlike most existing attack methods that focus on deceiving textual defenses, UPAM aims to deceive both textual and visual defenses in T2I models. UPAM enables gradient-based optimization, offering greater effectiveness and efficiency than previous methods. Given that T2I models might not return results due to defense mechanisms, we introduce a Sphere-Probing Learning (SPL) scheme to support gradient optimization even when no results are returned. Additionally, we devise a Semantic-Enhancing Learning (SEL) scheme to finetune UPAM for generating target-aligned images. Our framework also ensures attack stealthiness. Extensive experiments demonstrate UPAM's effectiveness and efficiency.",
  "abstract_zh": "文本到图像（T2I）模型因其生成不当或有害图像的潜力而引发安全担忧。本文提出了UPAM，一种新颖的框架，从攻击的角度研究T2I模型的鲁棒性。与大多数现有攻击方法专注于欺骗文本防御不同，UPAM旨在同时欺骗T2I模型中的文本和视觉防御。UPAM支持基于梯度的优化，比以前的方法提供更高的有效性和效率。考虑到T2I模型可能因防御机制而不返回结果，我们引入了一种球面探测学习（SPL）方案，以支持即使在未返回结果时的梯度优化。此外，我们设计了一种语义增强学习（SEL）方案，以微调UPAM以生成目标对齐的图像。我们的框架还确保攻击的隐蔽性。大量实验表明UPAM的有效性和效率。"
}
{
  "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
  "title_zh": "MM-Vet：评估大型多模态模型的综合能力",
  "abstract": "We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models.",
  "abstract_zh": "我们提出了MM-Vet，一个评估基准，用于检查大型多模态模型（LMMs）在复杂多模态任务上的表现。近期的LMMs展示了多种引人注目的能力，例如解决黑板上的数学问题、推理新闻图像中的事件和名人，以及解释视觉笑话。模型的快速进展给评估基准的发展带来了挑战。问题包括：（1）如何系统地构建和评估复杂的多模态任务；（2）如何设计在不同问题和答案类型上都能良好工作的评估指标；以及（3）如何提供超越简单性能排名的模型洞见。为此，我们提出了MM-Vet，基于一个洞察，即解决复杂任务的引人注目的能力通常是通过一个能够整合不同核心视觉-语言（VL）能力的通才模型来实现的。MM-Vet定义了6种核心VL能力，并检查了由能力组合衍生出的16种感兴趣的整合。对于评估指标，我们提出了一种基于LLM的评估器，用于开放式输出。该评估器能够在不同问题类型和答案风格之间进行评估，从而产生统一的评分指标。我们在MM-Vet上评估了代表性的LMMs，提供了对不同LMM系统范式和模型能力的洞见。"
}
{
  "title": "Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models",
  "title_zh": "思维算法：增强大型语言模型中的创意探索",
  "abstract": "Current literature, aiming to surpass the \"Chain-of-Thought\" approach, often resorts to external modi operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. Due to their *myopic perspective*, they escalate the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the *Algorithm of Thoughts*---a novel strategy that propels LLMs through algorithmic reasoning pathways. By employing algorithmic examples fully in-context, this overarching view of the whole process exploits the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and even more recent multi-query strategies that employ an extensive tree search algorithms while using significantly fewer tokens. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application. The code and related content can be found in: https://algorithm-of-thoughts.github.io",
  "abstract_zh": "当前文献旨在超越“思维链”方法，通常诉诸于外部操作方式，包括暂停、修改然后恢复生成过程，以提升大型语言模型（LLMs）的推理能力。由于其*短视的视角*，它们增加了查询请求的数量，导致成本、内存和计算开销的增加。为此，我们提出了*思维算法*——一种通过算法推理路径推动LLMs的新策略。通过在上下文中充分使用算法示例，这一对整个过程的全面视角利用了LLMs固有的递归动态，仅通过一到几个查询扩展了它们的创意探索。我们的技术在使用显著更少的标记的同时，优于早期的单查询方法和更近期的采用广泛树搜索算法的多查询策略。有趣的是，我们的结果表明，使用算法指导LLM可以导致性能超越算法本身，暗示LLM具有将其直觉编织入优化搜索中的固有能力。我们探讨了我们方法有效性的基础及其应用的细微差别。代码和相关内容可以在：https://algorithm-of-thoughts.github.io 找到。"
}
{
  "title": "Neighboring Perturbations of Knowledge Editing on Large Language Models",
  "title_zh": "知识编辑对大型语言模型的邻近扰动",
  "abstract": "Despite their exceptional capabilities, large language models (LLMs) are prone to generating unintended text due to false or outdated knowledge. Given the resource-intensive nature of retraining LLMs, there has been a notable increase in the development of knowledge editing. However, current approaches and evaluations rarely explore the perturbation of editing on neighboring knowledge. This paper studies whether updating new knowledge to LLMs perturbs the neighboring knowledge encapsulated within them. Specifically, we seek to figure out whether appending a new answer into an answer list to a factual question leads to catastrophic forgetting of original correct answers in this list, as well as unintentional inclusion of incorrect answers. A metric of additivity is introduced and a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge. Besides, a plug-and-play framework termed Appending via Preservation and Prevention (APP) is proposed to mitigate the neighboring perturbation by maintaining the integrity of the answer list. Experiments demonstrate the effectiveness of APP coupling with four editing methods on three LLMs.",
  "abstract_zh": "尽管大型语言模型（LLMs）具有卓越的能力，但由于错误或过时的知识，它们容易生成意外文本。考虑到重新训练LLMs的资源密集型特性，知识编辑的发展显著增加。然而，当前的方法和评估很少探讨编辑对邻近知识的扰动。本文研究将新知识更新到LLMs是否会扰动其内部封装的邻近知识。具体而言，我们试图弄清楚将新答案附加到事实问题的答案列表中是否会导致该列表中原始正确答案的灾难性遗忘，以及无意中包含错误答案。引入了一种加法性度量，并构建了一个名为知识附加扰动评估（PEAK）的基准，以评估附加新知识时对邻近知识的扰动程度。此外，提出了一种名为通过保护和预防附加（APP）的即插即用框架，以通过维护答案列表的完整性来减轻邻近扰动。实验表明，APP与四种编辑方法在三种LLMs上的结合效果显著。"
}
{
  "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications",
  "title_zh": "评估安全对齐的脆弱性：通过剪枝和低秩修改",
  "abstract": "Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3$ % at the parameter level and $2.5$ % at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.",
  "abstract_zh": "大型语言模型（LLMs）在其安全机制中显示出固有的脆弱性，表现为易受越狱攻击甚至非恶意微调的影响。本研究通过利用剪枝和低秩修改探讨了这种安全对齐的脆弱性。我们开发了识别对安全防护至关重要的关键区域的方法，这些区域在神经元和秩级别上与效用相关区域相互解耦。令人惊讶的是，我们发现的孤立区域是稀疏的，参数级别约占 $3$ %，秩级别约占 $2.5$ %。去除这些区域会损害安全性，而对效用的影响不大，证实了模型安全机制的固有脆弱性。此外，我们还表明，即使对安全关键区域的修改受到限制，LLMs仍然容易受到低成本微调攻击。这些发现强调了在LLMs中迫切需要更强大的安全策略。"
}
{
  "title": "Position: Building Guardrails for Large Language Models Requires Systematic Design",
  "title_zh": "标题：立场：为大型语言模型构建保护措施需要系统设计",
  "abstract": "As Large Language Models (LLMs) become more integrated into our daily lives, it is crucial to identify and mitigate their risks, especially when the risks can have profound impacts on human users and societies. Guardrails, which filter the inputs or outputs of LLMs, have emerged as a core safeguarding technology. This position paper takes a deep look at current open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the challenges and the road towards building more complete solutions. Drawing on robust evidence from previous research, we advocate for a systematic approach to construct guardrails for LLMs, based on comprehensive consideration of diverse contexts across various LLMs applications. We propose employing socio-technical methods through collaboration with a multi-disciplinary team to pinpoint precise technical requirements, exploring advanced neural-symbolic implementations to embrace the complexity of the requirements, and developing verification and testing to ensure the utmost quality of the final product.",
  "abstract_zh": "摘要：随着大型语言模型（LLMs）越来越融入我们的日常生活，识别和减轻其风险变得至关重要，尤其是当这些风险可能对人类用户和社会产生深远影响时。保护措施作为过滤LLMs输入或输出的核心安全技术应运而生。本文深入探讨当前的开源解决方案（Llama Guard、Nvidia NeMo、Guardrails AI），并讨论构建更完整解决方案的挑战和道路。基于以往研究的有力证据，我们倡导采取系统化的方法为LLMs构建保护措施，全面考虑各种LLMs应用的不同背景。我们建议通过与多学科团队的合作，采用社会技术方法来明确具体的技术需求，探索先进的神经符号实现以应对需求的复杂性，并开发验证和测试以确保最终产品的最高质量。"
}
{
  "title": "Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function",
  "title_zh": "大型语言模型是否按照人们的预期表现？测量人类泛化函数",
  "abstract": "What makes large language models (LLMs) impressive is also what makes them hard to evaluate: their diversity of uses. To evaluate these models, we must understand the purposes they will be used for. We consider a setting where these deployment decisions are made by people, and in particular, people's beliefs about where an LLM will perform well. We model such beliefs as the consequence of a human generalization function: having seen what an LLM gets right or wrong, people generalize to where else it might succeed. We collect a dataset of 19K examples of how humans make generalizations across 79 tasks from the MMLU and BIG-Bench benchmarks. We show that the human generalization function can be predicted using NLP methods: people have consistent structured ways to generalize. We then evaluate LLM alignment with the human generalization function. Our results show that -- especially for cases where the cost of mistakes is high -- more capable models (e.g. GPT-4) can do worse on the instances people choose to use them for, exactly because they are not aligned with the human generalization function.",
  "abstract_zh": "大型语言模型（LLMs）令人印象深刻的原因也是它们难以评估的原因：使用的多样性。为了评估这些模型，我们必须理解它们将被用于何种目的。我们考虑一个场景，在这个场景中，这些部署决策是由人做出的，特别是人们对LLM表现良好的信念。我们将这种信念建模为人类泛化函数的结果：在看到LLM的正确与错误后，人们会推断它可能在哪些地方成功。我们收集了一个包含19K示例的数据集，展示人类如何在79个MMLU和BIG-Bench基准任务中进行泛化。我们表明，人类泛化函数可以通过NLP方法进行预测：人们有一致的结构化方式进行泛化。然后，我们评估LLM与人类泛化函数的一致性。我们的结果表明——尤其是在错误成本高的情况下——更强大的模型（例如GPT-4）在用户选择使用的实例上表现可能更差，正是因为它们与人类泛化函数不一致。"
}
{
  "title": "Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models",
  "title_zh": "粗到细的高亮：减少大型语言模型中的知识幻觉",
  "abstract": "Generation of plausible but incorrect factual information, often termed hallucination, has attracted significant research interest. Retrieval-augmented language model (RALM)---which enhances models with up-to-date knowledge---emerges as a promising method to reduce hallucination. However, existing RALMs may instead exacerbate hallucination when retrieving lengthy contexts. To address this challenge, we propose COFT, a novel **CO**arse-to-**F**ine highligh**T**ing method to focus on different granularity-level key texts, thereby avoiding getting lost in lengthy contexts. Specifically, COFT consists of three components: *recaller*, *scorer*, and *selector*. First, *recaller* applies a knowledge graph to extract potential key entities in a given context. Second, *scorer* measures the importance of each entity by calculating its contextual weight. Finally, *selector* selects high contextual weight entities with a dynamic threshold algorithm and highlights the corresponding paragraphs, sentences, or words in a coarse-to-fine manner. Extensive experiments on knowledge hallucination benchmark demonstrate the effectiveness of COFT, leading to a superior performance over 30% in F1 score metric. Moreover, COFT also exhibits remarkable versatility across various long-form tasks, such as reading comprehension and question answering.",
  "abstract_zh": "生成看似合理但不正确的事实信息，通常被称为幻觉，已引起了显著的研究兴趣。检索增强语言模型（RALM）——通过最新知识增强模型——成为减少幻觉的有前景的方法。然而，现有的RALM在检索较长上下文时可能会加剧幻觉。为了解决这一挑战，我们提出了COFT，一种新颖的粗到细高亮方法，旨在关注不同粒度级别的关键文本，从而避免在冗长的上下文中迷失。具体而言，COFT由三个组件组成：*回忆器*、*评分器*和*选择器*。首先，*回忆器*利用知识图谱提取给定上下文中的潜在关键实体。其次，*评分器*通过计算上下文权重来衡量每个实体的重要性。最后，*选择器*使用动态阈值算法选择高上下文权重的实体，并以粗到细的方式高亮相应的段落、句子或单词。在知识幻觉基准上的大量实验表明，COFT的有效性，F1分数指标的表现提升超过30%。此外，COFT在各种长文本任务（如阅读理解和问答）中也表现出显著的多样性。"
}
{
  "title": "Revisiting the Role of Language Priors in Vision-Language Models",
  "title_zh": "标题：重新审视语言先验在视觉-语言模型中的作用",
  "abstract": "Vision-language models (VLMs) are impactful in part because they can be applied to a variety of visual understanding tasks in a zero-shot fashion, without any fine-tuning. We study $\\textit{generative VLMs}$ that are trained for next-word generation given an image. We explore their zero-shot performance on the illustrative task of image-text retrieval across nine popular vision-language benchmarks. Our first observation is that they can be repurposed for discriminative tasks (such as image-text retrieval) by simply computing the match score of generating a particular text string given an image. We call this probabilistic score the Visual Generative Pre-Training Score (VisualGPTScore). While the VisualGPTScore produces near-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on others. We analyze this behavior through a probabilistic lens, pointing out that some benchmarks inadvertently capture unnatural language distributions by creating adversarial but unlikely text captions. In fact, we demonstrate that even a \"blind\" language model that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago. We derive a probabilistic post-processing scheme that controls for the amount of linguistic bias in generative VLMs at test time without having to retrain or fine-tune the model. We show that the VisualGPTScore, when appropriately debiased, is a strong zero-shot baseline for vision-language understanding, oftentimes producing state-of-the-art accuracy.",
  "abstract_zh": "摘要：视觉-语言模型（VLMs）之所以具有影响力，部分原因是它们可以以零-shot的方式应用于各种视觉理解任务，而无需任何微调。我们研究了针对给定图像生成下一个单词的$\\textit{生成VLMs}$。我们探讨了它们在九个流行视觉-语言基准上的图像-文本检索这一示例任务的零-shot性能。我们的第一个观察是，通过简单计算给定图像生成特定文本字符串的匹配分数，它们可以被重新用于区分任务（如图像-文本检索）。我们将这个概率分数称为视觉生成预训练分数（VisualGPTScore）。尽管VisualGPTScore在某些检索基准上产生了近乎完美的准确性，但在其他基准上表现不佳。我们通过概率视角分析这种行为，指出某些基准通过创建对抗性但不太可能的文本标题，意外地捕捉到了不自然的语言分布。实际上，我们证明，即使是一个“盲目”的语言模型，忽略任何图像证据，有时也能超越所有先前的研究，令人想起多年前视觉问答（VQA）社区面临的类似挑战。我们推导出一种概率后处理方案，可以在测试时控制生成VLMs中的语言偏见量，而无需重新训练或微调模型。我们展示了经过适当去偏见处理的VisualGPTScore，是视觉-语言理解的强零-shot基线，通常能够产生最先进的准确性。"
}
{
  "title": "Fool Your (Vision and) Language Model with Embarrassingly Simple Permutations",
  "title_zh": "愚弄你的（视觉和）语言模型：令人尴尬的简单排列",
  "abstract": "Large language and vision-language models are rapidly being deployed in practice thanks to their impressive capabilities in instruction following, in-context learning, and so on. This raises an urgent need to carefully analyse their robustness so that stakeholders can understand if and when such models are trustworthy enough to be relied upon in any given application. In this paper, we highlight a specific vulnerability in popular models, namely permutation sensitivity in multiple-choice question answering (MCQA). Specifically, we show empirically that popular models are vulnerable to adversarial permutation in answer sets for multiple-choice prompting, which is surprising as models should ideally be as invariant to prompt permutation as humans are. These vulnerabilities persist across various model sizes, and exist in very recent language and vision-language models. Code to reproduce all experiments is provided in supplementary materials.",
  "abstract_zh": "大型语言模型和视觉-语言模型因其在指令跟随、上下文学习等方面的卓越能力而迅速投入实践，这引发了对其鲁棒性进行仔细分析的迫切需求，以便利益相关者了解这些模型在特定应用中是否值得信赖。本文强调了流行模型中的一个特定脆弱性，即多项选择问答（MCQA）中的排列敏感性。我们通过实验证明，流行模型在多项选择提示的答案集中的对抗性排列下存在脆弱性，这令人惊讶，因为模型理想情况下应对提示排列的变化与人类一样不变。这些脆弱性在各种模型规模中持续存在，并且在最近的语言和视觉-语言模型中也存在。所有实验的重现代码已在补充材料中提供。"
}
{
  "title": "Neural Networks Learn Statistics of Increasing Complexity",
  "title_zh": "神经网络学习逐渐复杂的统计特征",
  "abstract": "The _distributional simplicity bias_ (DSB) posits that neural networks learn low-order moments of the data distribution first, before moving on to higher-order correlations. In this work, we present compelling new evidence for the DSB by showing that networks automatically learn to perform well on maximum-entropy distributions whose low-order statistics match those of the training set early in training, then lose this ability later. We also extend the DSB to discrete domains by proving an equivalence between token $n$-gram frequencies and the moments of embedding vectors, and by finding empirical evidence for the bias in LLMs. Finally we use optimal transport methods to surgically edit the low-order statistics of one class to match those of another, and show that early-training networks treat the edited samples as if they were drawn from the target class. Code is available at https://github.com/EleutherAI/features-across-time.",
  "abstract_zh": "分布简单性偏见（DSB）假设神经网络首先学习数据分布的低阶矩，然后再转向高阶相关性。在这项工作中，我们通过展示网络在训练初期自动学习在最大熵分布上表现良好（其低阶统计特征与训练集相匹配），并在后期失去这种能力，为DSB提供了有力的新证据。我们还通过证明标记$n$-gram频率与嵌入向量的矩之间的等价性，将DSB扩展到离散领域，并找到LLMs中该偏见的实证证据。最后，我们使用最优传输方法对一个类别的低阶统计特征进行精确编辑，使其与另一个类别的统计特征匹配，并表明早期训练的网络将编辑后的样本视为来自目标类别。代码可在https://github.com/EleutherAI/features-across-time获取。"
}
{
  "title": "Understanding the Learning Dynamics of Alignment with Human Feedback",
  "title_zh": "理解与人类反馈对齐的学习动态",
  "abstract": "Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potentially offensive text; reader discretion is advised.",
  "abstract_zh": "将大型语言模型（LLMs）与人类意图对齐已成为在现实系统中安全部署模型的关键任务。尽管现有的对齐方法取得了实证成功，但理论上理解这些方法如何影响模型行为仍然是一个未解的问题。我们的工作首次尝试从理论上分析人类偏好对齐的学习动态。我们正式展示了偏好数据集的分布如何影响模型更新的速度，并对训练准确性提供了严格的保证。我们的理论还揭示了一个复杂现象，即优化倾向于优先考虑某些具有更高偏好可区分性的行为。我们在当代LLMs和对齐任务上实证验证了我们的发现，强化了我们的理论见解，并为未来的对齐方法提供了考虑方向。免责声明：本文包含可能令人反感的文本；建议读者自行判断。"
}
{
  "title": "Position: Towards Implicit Prompt For Text-To-Image Models",
  "title_zh": "标题：立场：面向文本到图像模型的隐式提示",
  "abstract": "Recent text-to-image (T2I) models have had great success, and many benchmarks have been proposed to evaluate their performance and safety. However, they only consider explicit prompts while neglecting implicit prompts (hint at a target without explicitly mentioning it). These prompts may get rid of safety constraints and pose potential threats to the applications of these models. This position paper highlights the current state of T2I models toward implicit prompts. We present a benchmark named ImplicitBench and conduct an investigation on the performance and impacts of implicit prompts with popular T2I models. Specifically, we design and collect more than 2,000 implicit prompts of three aspects: General Symbols, Celebrity Privacy, and Not-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models' capabilities under these implicit prompts. Experiment results show that (1) T2I models are able to accurately create various target symbols indicated by implicit prompts; (2) Implicit prompts bring potential risks of privacy leakage for T2I models. (3) Constraints of NSFW in most of the evaluated T2I models can be bypassed with implicit prompts. We call for increased attention to the potential and risks of implicit prompts in the T2I community and further investigation into the capabilities and impacts of implicit prompts, advocating for a balanced approach that harnesses their benefits while mitigating their risks.",
  "abstract_zh": "摘要：近年来，文本到图像（T2I）模型取得了巨大成功，并提出了许多基准来评估其性能和安全性。然而，它们仅考虑显式提示，而忽视了隐式提示（暗示目标而不明确提及）。这些提示可能会绕过安全约束，对这些模型的应用构成潜在威胁。本文强调了T2I模型在隐式提示方面的现状。我们提出了一个名为ImplicitBench的基准，并对流行T2I模型在隐式提示下的性能和影响进行了调查。具体而言，我们设计并收集了超过2000个隐式提示，涵盖三个方面：通用符号、名人隐私和不适合工作（NSFW）问题，并评估了六个知名T2I模型在这些隐式提示下的能力。实验结果表明：（1）T2I模型能够准确创建隐式提示所指示的各种目标符号；（2）隐式提示给T2I模型带来了潜在的隐私泄露风险；（3）大多数评估的T2I模型在NSFW方面的约束可以通过隐式提示绕过。我们呼吁T2I社区对隐式提示的潜力和风险给予更多关注，并进一步研究隐式提示的能力和影响，倡导一种平衡的方法，利用其优势，同时减轻其风险。"
}
{
  "title": "Representation Surgery: Theory and Practice of Affine Steering",
  "title_zh": "表示手术：仿射引导的理论与实践",
  "abstract": "Language models often exhibit undesirable behavior, e.g., generating toxic or gender-biased text. In the case of neural language models, an encoding of the undesirable behavior is often present in the model's representations. Thus, one natural (and common) approach to prevent the model from exhibiting undesirable behavior is to steer the model's representations in a manner that reduces the probability of it generating undesirable text. This paper investigates the formal and empirical properties of steering functions, i.e., transformation of the neural language model's representations that alter its behavior. First, we derive two optimal, in the least-squares sense, affine steering functions under different constraints. Our theory provides justification for existing approaches and offers a novel, improved steering approach. Second, we offer a series of experiments that demonstrate the empirical effectiveness of the methods in mitigating bias and reducing toxic generation.",
  "abstract_zh": "语言模型常常表现出不良行为，例如生成有毒或性别偏见的文本。在神经语言模型的情况下，不良行为的编码通常存在于模型的表示中。因此，防止模型表现出不良行为的一种自然（且常见）的方法是以减少生成不良文本的概率的方式引导模型的表示。本文研究了引导函数的形式和实证特性，即改变神经语言模型行为的表示变换。首先，我们在不同约束下推导出两种最优的、最小二乘意义下的仿射引导函数。我们的理论为现有方法提供了依据，并提出了一种新颖的改进引导方法。其次，我们提供了一系列实验，展示了这些方法在减轻偏见和减少有毒生成方面的实证有效性。"
}
{
  "title": "In-Context Unlearning: Language Models as Few-Shot Unlearners",
  "title_zh": "上下文去学习：语言模型作为少量样本去学习者",
  "abstract": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the Right to be Forgotten. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.",
  "abstract_zh": "机器去学习是研究有效去除特定训练实例对模型影响的领域，近年来因“被遗忘权”等监管指南而受到越来越多的关注。实现精确去学习通常需要完全重新训练模型，而对于大型模型（如大型语言模型）来说，这在计算上是不可行的。为此，最近的研究提出了几种算法，旨在在不重新训练模型的情况下近似去除训练数据。这些算法关键依赖于对模型参数的访问以进行更新，但由于计算限制或仅对大型语言模型的查询访问，这一假设在实践中可能不成立。在本研究中，我们提出了一类新的大型语言模型去学习方法，称为“上下文去学习”。该方法通过在上下文中提供特定类型的输入，简单地从模型中去除实例，而无需更新模型参数。为了去除特定的训练实例，我们在推理时将这些实例与不同于其真实标签的标签一起呈现给大型语言模型。我们的实验结果表明，上下文去学习的性能与其他需要访问模型参数的最先进方法相当，甚至在某些情况下表现更优，有效去除了特定实例对模型的影响，同时保持了测试准确性。"
}
{
  "title": "Delving into Differentially Private Transformer",
  "title_zh": "深入探讨差分隐私变换器",
  "abstract": "Deep learning with differential privacy (DP) has garnered significant attention over the past years, leading to the development of numerous methods aimed at enhancing model accuracy and training efficiency. This paper delves into the problem of training Transformer models with differential privacy. Our treatment is modular: the logic is to 'reduce' the problem of training DP Transformer to the more basic problem of training DP vanilla neural nets. The latter is better understood and amenable to many model-agnostic methods. Such 'reduction' is done by first identifying the hardness unique to DP Transformer training: the attention distraction phenomenon and a lack of compatibility with existing techniques for efficient gradient clipping. To deal with these two issues, we propose the Re-Attention Mechanism and Phantom Clipping, respectively. We believe that our work not only casts new light on training DP Transformers but also promotes a modular treatment to advance research in the field of differentially private deep learning.",
  "abstract_zh": "深度学习与差分隐私（DP）在过去几年中引起了广泛关注，促使开发出许多旨在提高模型准确性和训练效率的方法。本文深入研究了使用差分隐私训练变换器模型的问题。我们的处理是模块化的：逻辑是将训练DP变换器的问题“简化”为训练DP普通神经网络的更基本问题。后者更易于理解，并且适用于许多与模型无关的方法。这种“简化”首先通过识别与DP变换器训练独特相关的困难：注意力干扰现象以及与现有高效梯度裁剪技术的不兼容性来实现。为了解决这两个问题，我们分别提出了重新注意机制和幻影裁剪。我们相信我们的工作不仅为训练DP变换器提供了新的视角，还促进了模块化处理，以推动差分隐私深度学习领域的研究。"
}
{
  "title": "Implicit meta-learning may lead language models to trust more reliable sources",
  "title_zh": "隐式元学习可能使语言模型更信任可靠来源",
  "abstract": "We demonstrate that large language models (LLMs) may learn indicators of document usefulness and modulate their updates accordingly. We introduce random strings (\"tags\") as indicators of usefulness in a synthetic fine-tuning dataset. Fine-tuning on this dataset leads to **implicit meta-learning (IML)**: in further fine-tuning, the model updates to make more use of text that is tagged as useful. We perform a thorough empirical investigation of this phenomenon, finding (among other things) that (i) it occurs in both pretrained LLMs and those trained from scratch, as well as on a vision task, and (ii) larger models and smaller batch sizes tend to give more IML. We also use probing to examine how IML changes the way models store knowledge in their parameters. Finally, we reflect on what our results might imply about the capabilities, risks, and controllability of future AI systems.",
  "abstract_zh": "我们展示了大型语言模型（LLMs）可能学习文档有用性的指标并相应地调整其更新。我们在一个合成微调数据集中引入随机字符串（“标签”）作为有用性的指标。对该数据集的微调导致了**隐式元学习（IML）**：在进一步的微调中，模型更新以更多地利用标记为有用的文本。我们对这一现象进行了彻底的实证研究，发现（其中包括）(i) 这一现象发生在预训练的LLMs和从头开始训练的模型中，以及在视觉任务中，(ii) 较大的模型和较小的批量大小往往会产生更多的IML。我们还使用探测方法检查IML如何改变模型在其参数中存储知识的方式。最后，我们反思我们的结果可能对未来AI系统的能力、风险和可控性意味着什么。"
}
{
  "title": "Intersectional Unfairness Discovery",
  "title_zh": "交叉不公平性发现",
  "abstract": "AI systems have been shown to produce unfair results for certain subgroups of population, highlighting the need to understand bias on certain sensitive attributes. Current research often falls short, primarily focusing on the subgroups characterized by a single sensitive attribute, while neglecting the nature of intersectional fairness of multiple sensitive attributes. This paper focuses on its one fundamental aspect by discovering diverse high-bias intersectional sensitive attributes. Specifically, we propose a Bias-Guided Generative Network (BGGN). By treating each bias value as a reward, BGGN efficiently generates high-bias intersectional sensitive attributes. Experiments on real-world text and image datasets demonstrate a diverse and efficient discovery of BGGN. To further evaluate the generated unseen but possible unfair intersectional sensitive attributes, we formulate them as prompts and use modern generative AI to produce new text and images. The results of frequently generating biased data provides new insights of discovering potential unfairness in popular modern generative AI systems. **Warning: This paper contains examples that are offensive in nature.**",
  "abstract_zh": "人工智能系统已被证明对某些人口子群体产生不公平结果，突显了理解某些敏感属性偏见的必要性。目前的研究往往不足，主要集中在由单一敏感属性特征化的子群体，而忽视了多个敏感属性的交叉公平性。本论文专注于其一个基本方面，通过发现多样化的高偏见交叉敏感属性。具体而言，我们提出了一种偏见引导生成网络（BGGN）。通过将每个偏见值视为奖励，BGGN有效地生成高偏见交叉敏感属性。在真实世界的文本和图像数据集上的实验展示了BGGN的多样化和高效发现。为了进一步评估生成的未见但可能的不公平交叉敏感属性，我们将其表述为提示，并使用现代生成性人工智能生成新的文本和图像。频繁生成偏见数据的结果为发现流行现代生成性人工智能系统中的潜在不公平性提供了新的见解。**警告：本文包含冒犯性质的示例。**"
}
{
  "title": "To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models",
  "title_zh": "每个（文本序列）都有其独特之处：改善大型语言模型中的记忆数据遗忘",
  "abstract": "LLMs have been found to memorize training textual sequences and regurgitate verbatim said sequences during text generation time. This fact is known to be the cause of privacy and related (e.g., copyright) problems. Unlearning in LLMs then takes the form of devising new algorithms that will properly deal with these side-effects of memorized data, while not hurting the model's utility. We offer a fresh perspective towards this goal, namely, that each textual sequence to be forgotten should be treated differently when being unlearned based on its degree of memorization within the LLM. We contribute a new metric for measuring unlearning quality, an adversarial attack showing that SOTA algorithms lacking this perspective fail for privacy, and two new unlearning methods based on Gradient Ascent and Task Arithmetic, respectively. A comprehensive performance evaluation across an extensive suite of NLP tasks then mapped the solution space, identifying the best solutions under different scales in model capacities and forget set sizes and quantified the gains of the new approaches.",
  "abstract_zh": "研究发现，大型语言模型（LLMs）会记忆训练文本序列，并在文本生成时逐字复述这些序列。这一事实被认为是隐私及相关（例如，版权）问题的根源。因此，在LLMs中，遗忘的形式是设计新的算法，以妥善处理这些记忆数据的副作用，同时不损害模型的效用。我们提供了一个新的视角，即每个需要遗忘的文本序列在被遗忘时应根据其在LLM中的记忆程度进行不同的处理。我们贡献了一种新的遗忘质量测量指标，一种对抗性攻击，表明缺乏这一视角的最先进算法在隐私方面的失败，以及基于梯度上升和任务算术的两种新的遗忘方法。随后，在广泛的自然语言处理任务中进行了全面的性能评估，绘制了解决方案空间，识别出在不同模型容量和遗忘集大小下的最佳解决方案，并量化了新方法的收益。"
}
{
  "title": "How Language Model Hallucinations Can Snowball",
  "title_zh": "语言模型幻觉如何积累",
  "abstract": "A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we show that LMs sometimes produce hallucinations that they can separately recognize as incorrect. To do this, we construct three question-answering datasets where LMs often state an incorrect answer which is followed by an explanation with at least one incorrect claim. Crucially, we find that GPT-3.5, GPT-4, and LLaMA2-70B-chat can identify 67%, 87%, and 94% of these incorrect claims, respectively. We show that this phenomenon doesn't disappear under higher temperatures sampling, beam search, and zero-shot chain-of-thought prompting. These findings reveal that LM hallucinations can snowball: early mistakes by an LM can lead to more mistakes that otherwise would not be made.",
  "abstract_zh": "使用语言模型在实际应用中的一个主要风险是它们倾向于产生不正确的陈述。幻觉通常归因于语言模型的知识缺口，但我们表明，语言模型有时会产生它们可以单独识别为不正确的幻觉。为此，我们构建了三个问答数据集，其中语言模型经常给出不正确的答案，随后是至少包含一个不正确声明的解释。关键是，我们发现GPT-3.5、GPT-4和LLaMA2-70B-chat分别可以识别67%、87%和94%的这些不正确声明。我们表明，这种现象在更高温度采样、束搜索和零-shot思维链提示下并不会消失。这些发现揭示了语言模型的幻觉可以积累：语言模型的早期错误可能导致更多本不会发生的错误。"
}
{
  "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models",
  "title_zh": "C-RAG：检验增强语言模型生成风险的认证方法",
  "abstract": "Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models.",
  "abstract_zh": "尽管大型语言模型（LLMs）在多种应用中展现出令人印象深刻的能力，但它们仍然面临可信度问题，如幻觉和不一致性。提出了检索增强语言模型（RAG）以通过引入外部知识来增强生成的可信度，但其生成风险的理论理解仍然未被探索。本文回答了以下问题：1）RAG是否确实能导致低生成风险，2）如何对RAG和普通LLMs的生成风险提供可证明的保证，以及3）哪些充分条件使RAG模型能够降低生成风险。我们提出了C-RAG，这是第一个用于认证RAG模型生成风险的框架。具体而言，我们为RAG模型提供了符合风险分析，并认证了生成风险的上置信界限，我们称之为符合生成风险。我们还对一般有界风险函数在测试分布变化下的符合生成风险提供了理论保证。我们证明，当检索模型和变换器的质量非平凡时，RAG实现的符合生成风险低于单个LLM的风险。我们的深入实证结果展示了在四个广泛使用的NLP数据集和四个最先进的检索模型上，我们的符合生成风险保证的合理性和紧密性。"
}
{
  "title": "Feedback Loops With Language Models Drive In-Context Reward Hacking",
  "title_zh": "反馈循环与语言模型驱动上下文奖励黑客行为",
  "abstract": "Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient---they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide three recommendations for evaluation to capture more instances of ICRH. As AI development accelerates, the effects of feedback loops will proliferate, increasing the need to understand their role in shaping LLM behavior.",
  "abstract_zh": "语言模型影响外部世界：它们查询读取和写入网页的API，生成塑造人类行为的内容，并作为自主代理运行系统命令。这些交互形成反馈循环：大型语言模型（LLM）的输出影响世界，而世界又反过来影响后续的LLM输出。在本研究中，我们展示了反馈循环可能导致上下文奖励黑客行为（ICRH），在测试时LLM优化一个（可能是隐含的）目标，但在此过程中产生负面副作用。例如，考虑一个被部署以增加Twitter互动的LLM代理；LLM可能会将其之前的推文检索到上下文窗口中，并使其更加有争议，从而增加互动但也增加了有害性。我们识别并研究导致ICRH的两个过程：输出精炼和策略精炼。对于这些过程，静态数据集的评估是不够的——它们忽视了反馈效应，因此无法捕捉到最有害的行为。作为回应，我们提供了三项评估建议，以捕捉更多ICRH实例。随着人工智能发展的加速，反馈循环的影响将会增加，从而加大了理解其在塑造LLM行为中的作用的必要性。"
}
{
  "title": "MusicRL: Aligning Music Generation to Human Preferences",
  "title_zh": "音乐强化学习：将音乐生成与人类偏好对齐",
  "abstract": "We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as “upbeat workout music” can map to a retro guitar solo or a technopop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive [MusicLM](https://arxiv.org/abs/2301.11325) model of discrete audio tokens finetuned with reinforcement learning to maximize sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters. Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it. This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models. Samples can be found at google-research.github.io/seanet/musiclm/rlhf/.",
  "abstract_zh": "我们提出了音乐强化学习（MusicRL），这是第一个基于人类反馈微调的音乐生成系统。文本到音乐模型的欣赏特别主观，因为音乐性概念以及标题背后的具体意图是用户依赖的（例如，“欢快的锻炼音乐”可以对应于复古吉他独奏或电子流行节拍）。这不仅使得此类模型的监督训练具有挑战性，还需要在其部署后的微调中整合持续的人类反馈。MusicRL是一个预训练的自回归[MusicLM](https://arxiv.org/abs/2301.11325)离散音频标记模型，通过强化学习进行微调，以最大化序列级奖励。我们设计了与文本遵循性和音频质量相关的奖励函数，并借助选定的评估者进行微调，将MusicLM转变为MusicRL-R。我们将MusicLM部署给用户，并收集了包含300,000对偏好的大量数据集。通过人类反馈的强化学习（RLHF），我们训练了MusicRL-U，这是第一个大规模整合人类反馈的文本到音乐模型。人类评估表明，MusicRL-R和MusicRL-U均优于基线模型。最终，MusicRL-RU结合了这两种方法，并根据人类评估者的反馈产生了最佳模型。消融研究揭示了影响人类偏好的音乐属性，表明文本遵循性和质量仅占其中一部分。这强调了音乐欣赏中的主观性普遍存在，并呼吁进一步让人类听众参与音乐生成模型的微调。样本可以在google-research.github.io/seanet/musiclm/rlhf/找到。"
}
{
  "title": "Dual Operating Modes of In-Context Learning",
  "title_zh": "上下文学习的双重操作模式",
  "abstract": "In-context learning (ICL) exhibits dual operating modes: ***task learning***, i.e., acquiring a new skill from in-context samples, and ***task retrieval***, i.e., locating and activating a relevant pretrained skill. Recent theoretical work proposes various mathematical models to analyze ICL, but they cannot fully explain the duality. In this work, we analyze a generalized probabilistic model for pretraining data, obtaining a quantitative understanding of the two operating modes of ICL. Leveraging our analysis, we provide the first explanation of an unexplained phenomenon observed with real-world large language models (LLMs). Under some settings, the ICL risk initially increases and then decreases with more in-context examples. Our analysis offers a plausible explanation for this \"early ascent\" phenomenon: a limited number of in-context samples may lead to the retrieval of an incorrect skill, thereby increasing the risk, which will eventually diminish as task learning takes effect with more in-context samples. We also analyze ICL with biased labels, e.g., zero-shot ICL, where in-context examples are assigned random labels, and predict the bounded efficacy of such approaches. We corroborate our analysis and predictions with extensive experiments with Transformers and LLMs.",
  "abstract_zh": "上下文学习（ICL）表现出双重操作模式：***任务学习***，即从上下文样本中获取新技能，以及***任务检索***，即定位和激活相关的预训练技能。最近的理论工作提出了各种数学模型来分析ICL，但它们无法完全解释这种双重性。在本研究中，我们分析了一种针对预训练数据的广义概率模型，从而获得了对ICL两种操作模式的定量理解。利用我们的分析，我们首次解释了在真实世界大型语言模型（LLMs）中观察到的一个未解现象。在某些设置下，ICL风险在获得更多上下文示例时最初增加，然后减少。我们的分析为这种“早期上升”现象提供了一个合理的解释：有限数量的上下文样本可能导致检索到错误的技能，从而增加风险，而随着更多上下文样本的引入，任务学习的效果最终会使风险减小。我们还分析了带有偏差标签的ICL，例如零-shot ICL，其中上下文示例被分配随机标签，并预测了此类方法的有限有效性。我们通过对变换器和LLMs的广泛实验验证了我们的分析和预测。"
}
{
  "title": "Junk DNA Hypothesis: Pruning Small Pre-Trained Weights $\\textit{Irreversibly}$ and $\\textit{Monotonically}$ Impairs ``Difficult\" Downstream Tasks in LLMs",
  "title_zh": "垃圾DNA假说：不可逆和单调地修剪小规模预训练权重会损害大型语言模型中的“困难”下游任务",
  "abstract": "We present *Junk DNA Hypothesis* by adopting a novel *task-centric* angle for the pre-trained weights of large language models (LLMs). It has been believed that weights in LLMs contain significant redundancy, leading to the conception that a considerable chunk of the parameters can be removed by *pruning* without compromising performance. Contrary to this belief, this paper presents a *counter-argument*: small-magnitude weights of pre-trained model weights encode vital knowledge essential for tackling difficult downstream tasks - manifested as the **monotonic relationship** between the performance drop of downstream tasks across the difficulty spectrum, as we prune more pre-trained weights by magnitude. Moreover, we reveal that these seemingly inconsequential weights can result in **irreparable loss** of knowledge and performance degradation in difficult tasks, even when downstream continual training is allowed. Interestingly, our evaluations show that the other popular compression, namely *quantization* **fail** to exhibit similar ``monotonic\" effect and does not as convincingly disentangle this task-difficulty information. To study formally, we introduce several quantifiable metrics to *gauge the downstream task difficulty*: (a) within the same task category, and (b) across different task categories. Our extensive experiments substantiate the Junk DNA Hypothesis across a diverse range of model sizes, tasks, datasets, and even pruning methods. Codes are available at https://github.com/VITA-Group/Junk_DNA_Hypothesis.git.",
  "abstract_zh": "我们提出了*垃圾DNA假说*，通过采用一种新颖的*任务中心*视角来分析大型语言模型（LLMs）的预训练权重。人们普遍认为，LLMs中的权重包含显著的冗余，因此可以通过*修剪*去除相当一部分参数，而不影响性能。与这一观点相反，本文提出了一个*反论点*：小幅度的预训练模型权重编码了应对困难下游任务所需的重要知识——这体现在我们通过幅度修剪更多预训练权重时，下游任务性能下降与任务难度之间的**单调关系**。此外，我们揭示这些看似微不足道的权重可能导致**不可修复的知识损失**和困难任务的性能下降，即使允许下游的持续训练。值得注意的是，我们的评估显示，另一种流行的压缩方法，即*量化*，**未能**表现出类似的“单调”效应，也未能如此有效地解开任务难度信息。为了进行正式研究，我们引入了几个可量化的指标来*衡量下游任务的难度*：（a）在同一任务类别内，以及（b）跨不同任务类别。我们广泛的实验验证了垃圾DNA假说在多种模型规模、任务、数据集甚至修剪方法中的有效性。代码可在 https://github.com/VITA-Group/Junk_DNA_Hypothesis.git 获取。"
}
{
  "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
  "title_zh": "3D-VLA：一种3D视觉-语言-动作生成世界模型",
  "abstract": "Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan action accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM) and a set of action tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train the embodied diffusion models and align them into the LLM for predicting the goal image and point cloud. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodality generation and planning capabilities in embodied environments, showcasing its potential in real-world applications.",
  "abstract_zh": "最近的视觉-语言-动作（VLA）模型依赖于2D输入，缺乏与3D物理世界更广泛领域的整合。此外，它们通过学习从感知到动作的直接映射来进行动作预测，忽视了世界的广泛动态以及动作与动态之间的关系。相比之下，人类具备描绘未来场景的世界模型，以便相应地规划行动。为此，我们提出了3D-VLA，通过引入一类新的具身基础模型，顺畅地将3D感知、推理和行动通过生成世界模型连接起来。具体而言，3D-VLA建立在基于3D的大型语言模型（LLM）之上，并引入了一组动作标记以与具身环境进行互动。此外，为了将生成能力注入模型，我们训练了具身扩散模型并将其与LLM对齐，以预测目标图像和点云。为了训练我们的3D-VLA，我们通过从现有机器人数据集中提取大量与3D相关的信息，策划了一个大规模的3D具身指令数据集。我们在持有数据集上的实验表明，3D-VLA显著提高了具身环境中的推理、多模态生成和规划能力，展示了其在现实世界应用中的潜力。"
}
{
  "title": "HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding",
  "title_zh": "HALC：通过自适应焦点对比解码减少物体幻觉",
  "abstract": "While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate HALC’s effectiveness in reducing OH, outperforming state-of-the-arts across four benchmarks. Code is released at https://github.com/BillChan226/HALC.",
  "abstract_zh": "虽然大型视觉语言模型（LVLMs）在解释多模态上下文方面表现出色，但它们不可避免地遭受物体幻觉（OH）。我们介绍了HALC，这是一种新颖的解码算法，旨在减轻LVLMs中的OH。HALC利用视觉语言任务中独特的细粒度最佳视觉信息，同时在局部和全局上下文中操作。具体而言，HALC集成了一种强大的自适应焦点定位机制（局部）以实时纠正幻觉标记，以及一种专门的束搜索算法（全局）以显著减少OH，同时保持文本生成质量。此外，HALC可以作为即插即用模块集成到任何LVLM中，无需额外训练。大量实验研究表明，HALC在减少OH方面的有效性，超越了四个基准测试中的最新技术。代码已发布在 https://github.com/BillChan226/HALC。"
}
{
  "title": "Observable Propagation: Uncovering Feature Vectors in Transformers",
  "title_zh": "可观察传播：揭示变压器中的特征向量",
  "abstract": "A key goal of current mechanistic interpretability research in NLP is to find *linear features* (also called \"feature vectors\") for transformers: directions in activation space corresponding to concepts that are used by a given model in its computation. Present state-of-the-art methods for finding linear features require large amounts of labelled data -- both laborious to acquire and computationally expensive to utilize. In this work, we introduce a novel method, called \"observable propagation\" (in short: ObProp), for finding linear features used by transformer language models in computing a given task -- *using almost no data*. Our paradigm centers on the concept of \"observables\", linear functionals corresponding to given tasks. We then introduce a mathematical theory for the analysis of feature vectors, including a similarity metric between feature vectors called the *coupling coefficient* which estimates the degree to which one feature's output correlates with another's. We use ObProp to perform extensive qualitative investigations into several tasks, including gendered occupational bias, political party prediction, and programming language detection. Our results suggest that ObProp surpasses traditional approaches for finding feature vectors in the low-data regime, and that ObProp can be used to better understand the mechanisms responsible for bias in large language models.",
  "abstract_zh": "当前自然语言处理中的机制可解释性研究的一个关键目标是找到变压器的*线性特征*（也称为“特征向量”）：在激活空间中对应于模型在计算中使用的概念的方向。现有的最先进方法需要大量标记数据，这不仅获取困难，而且计算成本高。在本研究中，我们提出了一种新方法，称为“可观察传播”（简称：ObProp），用于在几乎不使用数据的情况下找到变压器语言模型在执行特定任务时使用的线性特征。我们的范式围绕“可观察量”这一概念展开，后者是与给定任务对应的线性泛函。然后，我们引入了一种用于特征向量分析的数学理论，包括一种称为*耦合系数*的特征向量相似性度量，用于估计一个特征的输出与另一个特征输出之间的相关程度。我们使用ObProp对多个任务进行广泛的定性研究，包括性别职业偏见、政治党派预测和编程语言检测。我们的结果表明，ObProp在低数据环境中超越了传统的特征向量寻找方法，并且ObProp可以更好地理解大型语言模型中导致偏见的机制。"
}
{
  "title": "FairProof : Confidential and Certifiable Fairness for Neural Networks",
  "title_zh": "公平证明：神经网络的保密和可认证公平性",
  "abstract": "Machine learning models are increasingly used in societal applications, yet legal and privacy concerns demand that they very often be kept confidential. Consequently, there is a growing distrust about the fairness properties of these models in the minds of consumers, who are often at the receiving end of model predictions. To this end, we propose *Fairproof* -- a system that uses Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the fairness of a model, while maintaining confidentiality. We also propose a fairness certification algorithm for fully-connected neural networks which is befitting to ZKPs and is used in this system. We implement *Fairproof* in Gnark and demonstrate empirically that our system is practically feasible. Code is available at https://github.com/infinite-pursuits/FairProof.",
  "abstract_zh": "机器学习模型在社会应用中越来越普遍，但法律和隐私问题要求它们往往保持机密。因此，消费者对这些模型的公平性特征产生了越来越多的不信任。为此，我们提出了*Fairproof*——一个利用零知识证明（密码学原语）公开验证模型公平性的系统，同时保持机密性。我们还提出了一种适用于完全连接神经网络的公平性认证算法，适合于零知识证明，并在该系统中使用。我们在Gnark中实现了*Fairproof*，并通过实证展示了我们的系统在实践中的可行性。代码可在https://github.com/infinite-pursuits/FairProof获取。"
}
{
  "title": "Position: Towards Unified Alignment Between Agents, Humans, and Environment",
  "title_zh": "标题：位置：朝着代理、人与环境之间的统一对齐",
  "abstract": "The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of **U**nified **A**lignment for **A**gents (**UA**$^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of **UA**$^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles demonstrating intentions, personalized reranking reflecting complex environmental dynamics, and runtime cost statistics as self-constraints. We then follow the principles of **UA**$^2$ to propose an initial design of our agent and benchmark its performance with several candidate baselines in the retrofitted WebShop. The extensive experimental results further prove the importance of the principles of **UA**$^2$. Our research sheds light on the next steps of autonomous agent research with improved general problem-solving abilities.",
  "abstract_zh": "摘要：基础模型的快速进展促进了自主代理的繁荣，这些代理利用基础模型的通用能力进行推理、决策和环境交互。然而，在复杂的现实环境中，代理的有效性仍然有限。在本研究中，我们介绍了代理的**U**nified **A**lignment (**UA**$^2$) 原则，倡导代理与人类意图、环境动态以及自我约束（如预算限制）的同时对齐。从**UA**$^2$的角度，我们回顾了当前的代理研究，并强调了现有代理基准和方法候选中被忽视的因素。我们还通过引入现实特征到WebShop中进行概念验证研究，包括展示意图的用户档案、反映复杂环境动态的个性化重新排序以及作为自我约束的运行成本统计。然后，我们遵循**UA**$^2$的原则，提出了我们代理的初步设计，并在改造后的WebShop中与多个候选基线进行了性能基准测试。广泛的实验结果进一步证明了**UA**$^2$原则的重要性。我们的研究为自主代理研究的下一步提供了启示，旨在提高其通用问题解决能力。"
}
{
  "title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents",
  "title_zh": "动态评估大型语言模型的元探测代理",
  "abstract": "Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are also dynamically configurable, allowing multifaceted analysis. We conducted extensive evaluations using MPA and found that most LLMs achieve poorer performance, indicating room for improvement. Our multifaceted analysis demonstrated the strong correlation between the basic abilities and an implicit Mattew effect on model size, i.e., larger models possess stronger correlations of the abilities. MPA can also be used as a data augmentation approach to enhance LLMs. Code is available at: https://github.com/microsoft/promptbench.",
  "abstract_zh": "大型语言模型（LLMs）的评估在社区中引发了极大的关注，因为数据污染问题。现有工作设计了使用特定任务的明确定义算法的评估协议，这些协议无法轻易扩展到多样化的场景。此外，当前的评估基准只能提供总体基准结果，无法支持对LLMs能力的细粒度和多方面分析。本文提出了元探测代理（MPA），这是一种受心理测量学启发的通用动态评估协议，用于评估LLMs。MPA设计了探测和判断代理，自动将原始评估问题转化为新的问题，遵循心理测量理论中的三种基本认知能力：语言理解、问题解决和领域知识。这些基本能力也是动态可配置的，允许多方面分析。我们使用MPA进行了广泛的评估，发现大多数LLMs的表现较差，表明有改进的空间。我们的多方面分析展示了基本能力与模型规模之间的强相关性，即较大的模型具有更强的能力相关性。MPA还可以作为数据增强方法来提升LLMs。代码可在：https://github.com/microsoft/promptbench获取。"
}
{
  "title": "Failures Are Fated, But Can Be Faded: Characterizing and Mitigating Unwanted Behaviors in Large-Scale Vision and Language Models",
  "title_zh": "失败是注定的，但可以被淡化：表征和缓解大规模视觉与语言模型中的不良行为",
  "abstract": "In large deep neural networks that seem to perform surprisingly well on many tasks, we also observe a few failures related to accuracy, social biases, and alignment with human values, among others. Therefore, before deploying these models, it is crucial to characterize this failure landscape for engineers to debug and legislative bodies to audit models. Nevertheless, it is infeasible to exhaustively test for all possible combinations of factors that could lead to a model's failure. In this paper, we introduce a post-hoc method that utilizes *deep reinforcement learning* to explore and construct the landscape of failure modes in pre-trained discriminative and generative models. With the aid of limited human feedback, we then demonstrate how to restructure the failure landscape to be more desirable by moving away from the discovered failure modes. We empirically show the effectiveness of the proposed method across common Computer Vision, Natural Language Processing, and Vision-Language tasks.",
  "abstract_zh": "在看似在许多任务上表现出色的大型深度神经网络中，我们也观察到与准确性、社会偏见和与人类价值观的一致性等相关的一些失败。因此，在部署这些模型之前，表征这一失败景观对于工程师调试和立法机构审计模型至关重要。然而，全面测试可能导致模型失败的所有因素组合是不可行的。本文介绍了一种后验方法，利用深度强化学习探索和构建预训练判别模型和生成模型的失败模式景观。在有限的人类反馈的帮助下，我们展示了如何通过远离发现的失败模式来重构失败景观，使其更具可取性。我们在常见的计算机视觉、自然语言处理和视觉-语言任务中实证展示了所提方法的有效性。"
}
{
  "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
  "title_zh": "标题：史密斯代理：单张图像可以以指数级速度破解一百万个多模态大型语言模型代理",
  "abstract": "A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate.",
  "abstract_zh": "摘要：多模态大型语言模型（MLLM）代理可以接收指令、捕捉图像、从记忆中检索历史并决定使用哪些工具。然而，红队测试的努力揭示了对抗性图像/提示可以破解MLLM并导致不一致行为。在这项工作中，我们报告了在多代理环境中更严重的安全问题，称为传染性破解。它涉及对手仅需破解单个代理，且在没有任何进一步干预的情况下，（几乎）所有代理将以指数级速度感染并表现出有害行为。为了验证传染性破解的可行性，我们模拟了包含多达一百万个LLaVA-1.5代理的多代理环境，并采用随机成对聊天作为多代理交互的概念验证实例。我们的结果表明，将（传染性）对抗性图像输入任何随机选择的代理的记忆中，足以实现传染性破解。最后，我们推导出一个简单的原则，用于确定防御机制是否可以证明地限制传染性破解的传播，但如何设计一个符合该原则的实用防御仍然是一个待研究的开放问题。"
}
{
  "title": "Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration",
  "title_zh": "揭示和利用隐藏的注意力汇聚：通过注意力校准在不训练的情况下增强大型语言模型",
  "abstract": "Attention is a fundamental component behind the remarkable achievements of large language models (LLMs). However, our current understanding of the attention mechanism, especially regarding how attention distributions are established, remains limited. Inspired by recent studies that explore the presence of attention sink in the initial token, which receives disproportionately large attention scores despite their lack of semantic importance, this work delves deeper into this phenomenon. We aim to provide a more profound understanding of the existence of attention sinks within LLMs and to uncover ways to enhance the achievable accuracy of LLMs by directly optimizing the attention distributions, without the need for weight finetuning. Specifically, this work begins with comprehensive visualizations of the attention distributions in LLMs during inference across various inputs and tasks. Based on these visualizations, to the best of our knowledge, we are the first to discover that (1) attention sinks occur not only at the start of sequences but also within later tokens of the input, and (2) not all attention sinks have a positive impact on the achievable accuracy of LLMs. Building upon our findings, we propose a training-free Attention Calibration Technique (ACT) that automatically optimizes the attention distributions on the fly during inference in an input-adaptive manner. Extensive experiments validate that ACT consistently enhances the accuracy of various LLMs across different applications. Specifically, ACT achieves an average improvement of up to $7.30\\%$ in accuracy across different datasets when applied to Llama-30B.",
  "abstract_zh": "注意力是大型语言模型（LLMs）卓越成就背后的基本组成部分。然而，我们对注意力机制的理解，特别是关于注意力分布如何建立的理解仍然有限。受近期研究的启发，这些研究探讨了初始标记中存在的注意力汇聚现象，该标记尽管缺乏语义重要性，却获得了不成比例的高注意力分数，本研究深入探讨了这一现象。我们旨在更深入地理解LLMs中注意力汇聚的存在，并揭示通过直接优化注意力分布来提高LLMs可实现的准确性的方法，而无需进行权重微调。具体而言，本研究首先对LLMs在推理过程中对各种输入和任务的注意力分布进行了全面可视化。根据这些可视化，尽我们所知，我们首次发现（1）注意力汇聚不仅发生在序列的开头，还发生在输入的后续标记中，以及（2）并非所有注意力汇聚对LLMs可实现的准确性都有积极影响。在我们的发现基础上，我们提出了一种无训练的注意力校准技术（ACT），该技术在推理过程中以输入自适应的方式自动优化注意力分布。大量实验验证了ACT在不同应用中始终提高了各种LLMs的准确性。具体而言，当应用于Llama-30B时，ACT在不同数据集上的平均准确性提高了高达$7.30\\%$。"
}
{
  "title": "Evaluating Quantized Large Language Models",
  "title_zh": "评估量化的大型语言模型",
  "abstract": "Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their applicability. Based on the extensive experiments, we systematically summarize the effect of quantization, provide recommendations to apply quantization techniques, and point out future directions. The code can be found in https://github.com/thu-nics/qllm-eval.",
  "abstract_zh": "后训练量化（PTQ）作为一种有前景的技术，已被提出以降低大型语言模型（LLMs）的成本。具体而言，PTQ可以有效减轻LLMs的内存消耗并降低计算开销。为了满足在多种场景下对高效率和高性能的要求，对量化LLMs进行全面评估是至关重要的，以指导量化方法的选择。本文通过评估PTQ对11个模型系列（包括OPT、LLaMA2、Falcon、Bloomz、Mistral、ChatGLM、Vicuna、LongChat、StableLM、Gemma和Mamba，参数范围从125M到180B）的权重、激活和KV缓存的影响，呈现了这些因素的全面评估。评估涵盖五种任务类型：基础NLP、突现能力、可信度、对话和长上下文任务。此外，我们还评估了最先进的（SOTA）量化方法，以展示其适用性。基于广泛的实验，我们系统总结了量化的影响，提供了应用量化技术的建议，并指出未来的研究方向。代码可在https://github.com/thu-nics/qllm-eval找到。"
}
{
  "title": "Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs",
  "title_zh": "标题：我们应该采用多智能体辩论策略吗？对大型语言模型的多智能体辩论策略的探讨",
  "abstract": "Recent advancements in large language models (LLMs) underscore their potential for responding to inquiries in various domains. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a promising strategy for enhancing the truthfulness of LLMs. We benchmark a range of debating and prompting strategies to explore the trade-offs between cost, time, and accuracy. Importantly, we find that multi-agent debating systems, in their current form, do not reliably outperform other proposed prompting strategies, such as self-consistency and ensembling using multiple reasoning paths. However, when performing hyperparameter tuning, several MAD systems, such as Multi-Persona, perform better. This suggests that MAD protocols might not be inherently worse than other approaches, but that they are more sensitive to different hyperparameter settings and difficult to optimize. We build on these results to offer insights into improving debating strategies, such as adjusting agent agreement levels, which can significantly enhance performance and even surpass all other non-debate protocols we evaluated. We provide an open-source repository to the community with several state-of-the-art protocols together with evaluation scripts to benchmark across popular research datasets.",
  "abstract_zh": "摘要：近期大型语言模型（LLMs）的进展凸显了它们在各个领域回答询问的潜力。然而，确保生成代理提供准确可靠的答案仍然是一个持续的挑战。在这种背景下，多智能体辩论（MAD）作为一种增强LLMs真实性的有前景的策略应运而生。我们基准测试了一系列辩论和提示策略，以探索成本、时间和准确性之间的权衡。重要的是，我们发现当前形式的多智能体辩论系统并没有可靠地超越其他提议的提示策略，如自一致性和使用多条推理路径的集成。然而，在进行超参数调优时，多个MAD系统（如多角色）表现更好。这表明，MAD协议可能并不比其他方法本质上更差，但它们对不同超参数设置更敏感，且优化难度较大。我们基于这些结果提供了改进辩论策略的见解，例如调整代理一致性水平，这可以显著提升性能，甚至超越我们评估的所有其他非辩论协议。我们向社区提供了一个开源库，其中包含多个最先进的协议以及评估脚本，以便在流行的研究数据集上进行基准测试。"
}
{
  "title": "Generating Chain-of-Thoughts with a Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought",
  "title_zh": "生成链式思维的成对比较方法以寻找最有前景的中间思维",
  "abstract": "To improve the ability of the large language model (LLMs) to tackle complex reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs to reason step-by-step, enabling problem solving from simple to complex. State-of-the-art methods for generating such a chain involve interactive collaboration, where the learner generates candidate intermediate thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts. However, a widespread yet understudied problem is that the evaluation from the LLM is typically noisy and unreliable, potentially misleading the generation process in selecting promising intermediate thoughts. In this paper, motivated by Vapnik's principle, we use pairwise-comparison evaluation instead of point-wise scoring to search for promising intermediate thoughts with the noisy feedback from the LLM. In each round, we randomly pair intermediate thoughts and directly prompt the LLM to select the more promising one from each pair, allowing us to identify the most promising thoughts through an iterative process. To further alleviate the noise in the comparison, we incorporate techniques from ensemble learning and dueling bandits, proposing two variants of the algorithm. Experiments on three real-world tasks demonstrate the effectiveness of our proposed algorithm and verify the rationale of the pairwise comparison mechanism.",
  "abstract_zh": "为了提高大型语言模型（LLMs）处理复杂推理问题的能力，提出了链式思维（CoT）方法，以指导LLMs逐步推理，从简单到复杂地解决问题。生成这种链的最先进方法涉及互动协作，学习者生成候选中间思维，由LLM进行评估，指导后续思维的生成。然而，一个普遍存在但研究不足的问题是，LLM的评估通常是嘈杂和不可靠的，可能会误导生成过程中选择有前景的中间思维。在本文中，我们受到Vapnik原则的启发，使用成对比较评估而不是逐点评分，以在LLM的嘈杂反馈中搜索有前景的中间思维。在每一轮中，我们随机配对中间思维，并直接提示LLM从每对中选择更有前景的一个，从而通过迭代过程识别最有前景的思维。为了进一步减轻比较中的噪声，我们结合了集成学习和对抗赌博的技术，提出了该算法的两个变体。在三个真实世界任务上的实验表明了我们提出的算法的有效性，并验证了成对比较机制的合理性。"
}
{
  "title": "How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis",
  "title_zh": "标题：大型语言模型的谈判能力如何？谈判竞技场平台及分析",
  "abstract": "Negotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources. With rapidly growing interest in using large language models (LLMs) to act as agents on behalf of human users, such LLM agents would also need to be able to negotiate. In this paper, we study how well LLMs can negotiate with each other. We develop NegotiationArena: a flexible framework for evaluating and probing the negotiation abilities of LLM agents. We implemented three types of scenarios in NegotiationArena to assess LLM's behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations). Each scenario allows for multiple turns of flexible dialogues between LLM agents to allow for more complex negotiations. Interestingly, LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics. For example, by pretending to be desolate and desperate, LLMs can improve their payoffs by 20% when negotiating against the standard GPT-4. We also quantify irrational negotiation behaviors exhibited by the LLM agents, many of which also appear in humans. Together, NegotiationArena offers a new environment to investigate LLM interactions, enabling new insights into LLM's theory of mind, irrationality, and reasoning abilities",
  "abstract_zh": "摘要：谈判是社会互动的基础；人类在从汽车价格到如何共享公共资源的各个方面进行谈判。随着对使用大型语言模型（LLMs）作为人类用户代理的兴趣迅速增长，这些LLM代理也需要具备谈判能力。本文研究了LLMs之间的谈判能力。我们开发了谈判竞技场：一个灵活的框架，用于评估和探测LLM代理的谈判能力。我们在谈判竞技场中实施了三种类型的场景，以评估LLM在分配共享资源（最后通牒游戏）、聚合资源（交易游戏）和买卖商品（价格谈判）中的行为。每个场景允许LLM代理之间进行多轮灵活对话，以实现更复杂的谈判。有趣的是，LLM代理通过采用某些行为策略可以显著提高谈判结果。例如，通过假装绝望和急迫，LLMs在与标准GPT-4谈判时可以提高20%的收益。我们还量化了LLM代理表现出的非理性谈判行为，其中许多行为在人类中也会出现。总之，谈判竞技场提供了一个新的环境来研究LLM的互动，为LLM的心智理论、非理性和推理能力提供了新的见解。"
}
{
  "title": "Interpreting and Improving Large Language Models in Arithmetic Calculation",
  "title_zh": "标题：解释和改进大语言模型在算术计算中的表现",
  "abstract": "Large language models (LLMs) have demonstrated remarkable potential across numerous applications and have shown an emergent ability to tackle complex reasoning tasks, such as mathematical computations. However, even for the simplest arithmetic calculations, the intrinsic mechanisms behind LLMs remains mysterious, making it challenging to ensure reliability. In this work, we delve into uncovering a specific mechanism by which LLMs execute calculations. Through comprehensive experiments, we find that LLMs frequently involve a small fraction (<5%) of attention heads, which play a pivotal role in focusing on operands and operators during calculation processes. Subsequently, the information from these operands is processed through multi-layer perceptrons (MLPs), progressively leading to the final solution. These pivotal heads/MLPs, though identified on a specific dataset, exhibit transferability across different datasets and even distinct tasks. This insight prompted us to investigate the potential benefits of selectively fine-tuning these essential heads/MLPs to boost the LLMs' computational performance. We empirically find that such precise tuning can yield notable enhancements on mathematical prowess, without compromising the performance on non-mathematical tasks. Our work serves as a preliminary exploration into the arithmetic calculation abilities inherent in LLMs, laying a solid foundation to reveal more intricate mathematical tasks.",
  "abstract_zh": "摘要：大语言模型（LLMs）在众多应用中展现出显著的潜力，并显示出处理复杂推理任务（如数学计算）的新兴能力。然而，即使是最简单的算术计算，LLMs背后的内在机制仍然神秘，导致确保其可靠性变得具有挑战性。在本研究中，我们深入探讨了LLMs执行计算的特定机制。通过全面的实验，我们发现LLMs通常涉及少量（<5%）的注意力头，这些注意力头在计算过程中专注于操作数和运算符。随后，这些操作数的信息通过多层感知器（MLPs）进行处理，逐步得出最终解。尽管这些关键的头/MLPs是在特定数据集上识别的，但它们在不同数据集甚至不同任务之间表现出可迁移性。这一发现促使我们研究选择性微调这些关键头/MLPs的潜在好处，以提升LLMs的计算性能。我们实证发现，这种精确的微调可以显著提高数学能力，而不影响非数学任务的表现。我们的工作为探索LLMs固有的算术计算能力提供了初步研究基础，为揭示更复杂的数学任务奠定了坚实的基础。"
}
{
  "title": "Active Preference Learning for Large Language Models",
  "title_zh": "主动偏好学习用于大型语言模型",
  "abstract": "As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.",
  "abstract_zh": "随着大型语言模型（LLMs）能力的提升，调整其与人类意图一致的微调技术变得愈发重要。对齐这些模型的一个关键考虑是如何最有效地利用人力资源，或者在使用LLMs作为神谕的情况下利用模型资源。人类或AI偏好的强化学习（RLHF/RLAIF）是此类技术中最突出的例子，但其复杂且常常不稳定。最近提出的直接偏好优化（DPO）作为一种更简单且更稳定的替代方案。在本研究中，我们开发了一种DPO的主动学习策略，以更好地利用偏好标签。我们提出了一种基于语言模型的预测熵和DPO优化的隐式偏好模型的确定性度量的提示/完成对的实用获取函数。我们展示了我们的方法如何提高学习速率和在成对偏好数据上的微调最终性能。"
}
{
  "title": "AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training",
  "title_zh": "类似AlphaZero的树搜索可以指导大型语言模型的解码和训练",
  "abstract": "Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the multi-step reasoning capabilities of LLMs by using tree-search algorithms. These methods rely on prompting a pre-trained model to serve as a value function and focus on problems with low search depth. As a result, these methods cannot benefit from in-domain training and only rely on pretraining process — they will not work in domains where the pre-trained LLM does not have enough knowledge to serve as an effective value function or in domains that require long-horizon planning. To address these limitations, we present an AlphaZero-like tree-search learning framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLM decoding. TS-LLM distinguishes itself in two key ways. (1) Leveraging a learned value function and AlphaZero-like algorithms, our approach can be generally adaptable to a wide range of tasks, language models of any size, and tasks of varying search depths. (2) Our approach can guide LLMs during both inference and training, iteratively improving the LLMs. Empirical results across reasoning, planning, alignment, and decision-making tasks show that TS-LLM outperforms existing approaches and can handle trees with a depth of 64.",
  "abstract_zh": "近期的研究如思维树（ToT）和通过规划推理（RAP）旨在通过使用树搜索算法增强大型语言模型（LLMs）的多步骤推理能力。这些方法依赖于提示一个预训练模型作为价值函数，并专注于搜索深度较低的问题。因此，这些方法无法从领域内训练中受益，仅依赖于预训练过程——在预训练的LLM没有足够知识作为有效价值函数的领域或需要长远规划的领域中，它们将无法工作。为了解决这些限制，我们提出了一种类似AlphaZero的树搜索学习框架（称为TS-LLM），系统地说明了如何通过学习的价值函数指导LLM解码。TS-LLM在两个关键方面具有独特性：（1）利用学习的价值函数和类似AlphaZero的算法，我们的方法可以广泛适应各种任务、任何规模的语言模型以及不同搜索深度的任务。（2）我们的方法可以在推理和训练过程中指导LLM，迭代地改善LLM。在推理、规划、对齐和决策任务中的实证结果表明，TS-LLM优于现有方法，并能够处理深度达到64的树。"
}
{
  "title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning",
  "title_zh": "FuRL：将视觉语言模型作为强化学习的模糊奖励",
  "abstract": "In this work, we investigate how to leverage pre-trained visual-language models (VLM) for online Reinforcement Learning (RL). In particular, we focus on sparse reward tasks with pre-defined textual task descriptions. We first identify the problem of reward misalignment when applying VLM as a reward in RL tasks. To address this issue, we introduce a lightweight fine-tuning method, named Fuzzy VLM reward-aided RL (FuRL), based on reward alignment and relay RL. Specifically, we enhance the performance of SAC/DrQ baseline agents on sparse reward tasks by fine-tuning VLM representations and using relay RL to avoid local minima. Extensive experiments on the Meta-world benchmark tasks demonstrate the efficacy of the proposed method. Code is available at: https://github.com/fuyw/FuRL.",
  "abstract_zh": "在本研究中，我们探讨如何利用预训练的视觉语言模型（VLM）进行在线强化学习（RL）。特别地，我们关注具有预定义文本任务描述的稀疏奖励任务。我们首先识别在RL任务中应用VLM作为奖励时奖励不对齐的问题。为了解决这个问题，我们引入了一种轻量级的微调方法，称为模糊VLM奖励辅助RL（FuRL），基于奖励对齐和中继RL。具体而言，我们通过微调VLM表示并使用中继RL来避免局部最小值，从而提高SAC/DrQ基线代理在稀疏奖励任务上的性能。在Meta-world基准任务上的大量实验表明了所提方法的有效性。代码可在：https://github.com/fuyw/FuRL获取。"
}
{
  "title": "Position: Technical Research and Talent is Needed for Effective AI Governance",
  "title_zh": "标题：有效的人工智能治理需要技术研究和人才",
  "abstract": "In light of recent advancements in AI capabilities and the increasingly widespread integration of AI systems into society, governments worldwide are actively seeking to mitigate the potential harms and risks associated with these technologies through regulation and other governance tools. However, there exist significant gaps between governance aspirations and the current state of the technical tooling necessary for their realisation. In this position paper, we survey policy documents published by public-sector institutions in the EU, US, and China to highlight specific areas of disconnect between the technical requirements necessary for enacting proposed policy actions, and the current technical state of the art. Our analysis motivates a call for tighter integration of the AI/ML research community within AI governance in order to i) catalyse technical research aimed at bridging the gap between current and supposed technical underpinnings of regulatory action, as well as ii) increase the level of technical expertise within governing institutions so as to inform and guide effective governance of AI.",
  "abstract_zh": "摘要：鉴于人工智能能力的最新进展以及人工智能系统在社会中日益广泛的整合，各国政府正在积极寻求通过监管和其他治理工具来减轻与这些技术相关的潜在危害和风险。然而，治理愿望与实现所需的技术工具之间存在显著差距。在本文中，我们调查了欧盟、美国和中国公共部门机构发布的政策文件，以突出实施提议政策行动所需的技术要求与当前技术现状之间的具体脱节领域。我们的分析促使呼吁更紧密地将人工智能/机器学习研究社区融入人工智能治理，以便 i) 促进旨在弥合当前与假定的监管行动技术基础之间差距的技术研究，以及 ii) 提高治理机构内的技术专业水平，以便为有效治理人工智能提供信息和指导。"
}
{
  "title": "Position: Open-Endedness is Essential for Artificial Superhuman Intelligence",
  "title_zh": "标题：立场：开放性对人工超人智能至关重要",
  "abstract": "In recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internet-scale data. Nevertheless, the creation of open-ended, ever self-improving AI remains elusive. **In this position paper, we argue that the ingredients are now in place to achieve *open-endedness* in AI systems with respect to a human observer. Furthermore, we claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI).** We begin by providing a concrete formal definition of open-endedness through the lens of novelty and learnability. We then illustrate a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, human-relevant discoveries. We conclude by examining the safety implications of generally-capable open-ended AI. We expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future.",
  "abstract_zh": "摘要：近年来，人工智能系统的整体能力急剧提升，主要得益于在互联网规模数据上训练基础模型。然而，创造开放性、不断自我改进的人工智能仍然难以实现。**在这篇立场论文中，我们认为现在具备了实现人工智能系统相对于人类观察者的*开放性*的必要条件。此外，我们声称这种开放性是任何人工超人智能（ASI）的基本特性。**我们首先通过新颖性和可学习性的视角提供开放性的具体正式定义。然后，我们展示了通过建立在基础模型之上的开放系统实现ASI的路径，这些系统能够进行新颖的、与人类相关的发现。最后，我们考察了普遍能力开放性人工智能的安全隐患。我们预计，开放性基础模型将在不久的将来成为一个越来越富有成果且安全关键的研究领域。"
}
{
  "title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback",
  "title_zh": "超反馈：通过扩展的人工智能反馈提升语言模型",
  "abstract": "Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability, resulting in small sizes or limited topics of current datasets. This further hinders feedback learning as well as alignment research within the open-source community. To address this issue, we explore how to go beyond human feedback and collect high-quality AI feedback automatically for a scalable alternative. Specifically, we identify scale and diversity as the key factors for feedback data to take effect. Accordingly, we first broaden instructions and responses in both amount and breadth to encompass a wider range of user-assistant interactions. Then, we meticulously apply a series of techniques to mitigate annotation biases for more reliable AI feedback. We finally present UltraFeedback, a large-scale, high-quality, and diversified AI feedback dataset, which contains over 1 million GPT-4 feedback for 250k user-assistant conversations from various aspects. Built upon UltraFeedback, we align a LLaMA-based model by best-of-$n$ sampling and reinforcement learning, demonstrating its exceptional performance on chat benchmarks. Our work validates the effectiveness of scaled AI feedback data in constructing strong open-source chat language models, serving as a solid foundation for future feedback learning research.",
  "abstract_zh": "从人类反馈中学习已成为将大型语言模型（LLMs）与人类偏好对齐的关键技术。然而，获取大量优质的人类反馈受到时间、劳动和人类能力的限制，导致当前数据集的规模小或主题有限。这进一步阻碍了反馈学习以及开源社区的对齐研究。为了解决这个问题，我们探索如何超越人类反馈，自动收集高质量的人工智能反馈，以实现可扩展的替代方案。具体而言，我们确定规模和多样性是反馈数据生效的关键因素。因此，我们首先在数量和广度上扩大指令和响应，以涵盖更广泛的用户-助手交互。然后，我们仔细应用一系列技术来减轻注释偏差，以获得更可靠的人工智能反馈。最后，我们呈现了UltraFeedback，这是一个大规模、高质量和多样化的人工智能反馈数据集，包含超过100万个GPT-4反馈，涵盖来自各个方面的25万次用户-助手对话。在UltraFeedback的基础上，我们通过最佳的$n$抽样和强化学习对基于LLaMA的模型进行对齐，展示了其在聊天基准测试中的卓越表现。我们的工作验证了扩展的人工智能反馈数据在构建强大的开源聊天语言模型中的有效性，为未来的反馈学习研究奠定了坚实的基础。"
}
{
  "title": "Automated Statistical Model Discovery with Language Models",
  "title_zh": "自动化统计模型发现与语言模型",
  "abstract": "Statistical model discovery is a challenging search over a vast space of models subject to domain-specific constraints. Efficiently searching over this space requires expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box’s Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, which are key restrictions of previous systems. We evaluate our method in three settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving expert models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method identifies models on par with human expert designed models and extends classic models in interpretable ways. Our results highlight the promise of LM-driven model discovery.",
  "abstract_zh": "统计模型发现是在特定领域约束下对广泛模型空间进行的挑战性搜索。有效地搜索这一空间需要建模和问题领域的专业知识。受大型语言模型（LMs）的领域知识和编程能力的启发，我们提出了一种基于语言模型的自动化统计模型发现方法。我们将自动化过程置于Box的循环的原则框架内：LM在提出作为概率程序表示的统计模型（充当建模者）和批评这些模型（充当领域专家）之间迭代。通过利用LMs，我们不必定义特定领域的模型语言或设计手工搜索程序，这些都是以前系统的关键限制。我们在概率建模的三个设置中评估了我们的方法：在受限模型空间内搜索、在开放空间中搜索，以及在自然语言约束下改进专家模型（例如，该模型应对生态学家可解释）。我们的方法识别出与人类专家设计模型相当的模型，并以可解释的方式扩展经典模型。我们的结果突显了基于LM的模型发现的潜力。"
}
{
  "title": "LLaGA: Large Language and Graph Assistant",
  "title_zh": "LLaGA：大型语言与图形助手",
  "abstract": "Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the the **L**arge **L**anguage **a**nd **G**raph **A**ssistant (**LLaGA**), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and tasks, extend its ability to unseen datasets or tasks, and provide explanations for graphs. Our extensive experiments across popular graph benchmarks show that LLaGA delivers outstanding performance across four datasets and three tasks using one single model, surpassing state-of-the-art graph models in both supervised and zero-shot scenarios.",
  "abstract_zh": "图神经网络（GNN）推动了图结构数据分析的进展。最近，像GPT-4这样的大型语言模型（LLM）的崛起标志着深度学习的新纪元。然而，由于将图结构转换为语言的固有困难，它们在图数据上的应用面临独特挑战。为此，我们引入了**L**arge **L**anguage **a**nd **G**raph **A**ssistant（**LLaGA**），这是一个创新模型，能够有效整合LLM的能力，以处理图结构数据的复杂性。LLaGA保留了LLM的通用性，同时将图数据调整为与LLM输入兼容的格式。LLaGA通过将图节点重新组织为结构感知序列，然后通过多功能投影器将这些序列映射到令牌嵌入空间来实现这一点。LLaGA在多样性、可推广性和可解释性方面表现出色，使其能够在不同数据集和任务中始终如一地表现良好，扩展其对未见数据集或任务的能力，并为图提供解释。我们在流行的图基准上进行的广泛实验表明，LLaGA在四个数据集和三个任务上使用单一模型提供了卓越的性能，在监督和零样本场景中均超越了最先进的图模型。"
}
{
  "title": "Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models",
  "title_zh": "标题：针对大型语言模型的增强可检测性和语义一致性的令牌特定水印技术",
  "abstract": "Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Achieving both the detectability of inserted watermarks and the semantic quality of generated texts is challenging. While current watermarking algorithms have made promising progress in this direction, there remains significant scope for improvement. To address these challenges, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence. Our code is available at https://github.com/mignonjia/TS_watermark.",
  "abstract_zh": "摘要：大型语言模型生成高质量的响应，但可能包含错误信息，这凸显了通过区分人工撰写文本和AI生成文本进行监管的必要性。在这一背景下，水印技术至关重要，它涉及在LLM推理阶段将隐藏标记嵌入文本中，这对人类来说是不可察觉的。实现插入水印的可检测性和生成文本的语义质量是一个挑战。尽管当前的水印算法在这方面取得了可喜的进展，但仍有显著的改进空间。为了解决这些挑战，我们提出了一种新颖的多目标优化（MOO）水印方法，利用轻量级网络生成令牌特定的水印逻辑和分割比例。通过利用MOO同时优化检测和语义目标函数，我们的方法实现了可检测性和语义完整性的双重目标。实验结果表明，我们的方法在增强LLM生成文本的可检测性方面优于当前的水印技术，同时保持其语义一致性。我们的代码可在https://github.com/mignonjia/TS_watermark获取。"
}
{
  "title": "FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models",
  "title_zh": "FedBPT：大语言模型的高效联邦黑箱提示调优",
  "abstract": "Pre-trained language models (PLM) have revolutionized the NLP landscape, achieving stellar performances across diverse tasks. These models, while benefiting from vast training data, often require fine-tuning on specific data to cater to distinct downstream tasks. However, this data adaptation process has inherent security and privacy concerns, primarily when leveraging user-generated, device-residing data. Federated learning (FL) provides a solution, allowing collaborative model fine-tuning without centralized data collection. However, applying FL to finetune PLMs is hampered by challenges, including restricted model parameter access due to the high encapsulation, high computational requirements, and communication overheads. This paper introduces Federated Black-box Prompt Tuning (FedBPT), a framework designed to address these challenges. FedBPT allows the clients to treat the model as a black-box inference API. By focusing on training optimal prompts and utilizing gradient-free optimization methods, FedBPT reduces the number of exchanged variables, boosts communication efficiency, and minimizes computational and storage costs. Experiments highlight the framework's ability to drastically cut communication and memory costs while maintaining competitive performance. Ultimately, FedBPT presents a promising solution for efficient, privacy-preserving fine-tuning of PLM in the age of large language models.",
  "abstract_zh": "预训练语言模型（PLM）彻底改变了自然语言处理（NLP）领域，在各种任务中取得了卓越的表现。这些模型虽然受益于大量的训练数据，但通常需要在特定数据上进行微调，以满足不同的下游任务。然而，这一数据适配过程存在固有的安全和隐私问题，尤其是在利用用户生成的、设备驻留的数据时。联邦学习（FL）提供了解决方案，允许在不集中收集数据的情况下进行协作模型微调。然而，将FL应用于微调PLM面临挑战，包括由于高度封装而限制的模型参数访问、高计算要求和通信开销。本文介绍了联邦黑箱提示调优（FedBPT），一个旨在解决这些挑战的框架。FedBPT允许客户端将模型视为黑箱推理API。通过专注于训练最佳提示并利用无梯度优化方法，FedBPT减少了交换变量的数量，提高了通信效率，并最小化了计算和存储成本。实验结果突显了该框架在保持竞争性能的同时，显著降低通信和内存成本的能力。最终，FedBPT为在大语言模型时代实现高效、保护隐私的PLM微调提供了一个有前景的解决方案。"
}
{
  "title": "Revisiting Character-level Adversarial Attacks for Language Models",
  "title_zh": "标题：重新审视针对语言模型的字符级对抗攻击",
  "abstract": "Adversarial attacks in Natural Language Processing apply perturbations in the character or token levels. Token-level attacks, gaining prominence for their use of gradient-based methods, are susceptible to altering sentence semantics, leading to invalid adversarial examples. While character-level attacks easily maintain semantics, they have received less attention as they cannot easily adopt popular gradient-based methods, and are thought to be easy to defend. Challenging these beliefs, we introduce Charmer, an efficient query-based adversarial attack capable of achieving high attack success rate (ASR) while generating highly similar adversarial examples. Our method successfully targets both small (BERT) and large (Llama 2) models. Specifically, on BERT with SST-2, Charmer improves the ASR in $4.84$% points and the USE similarity in $8$% points with respect to the previous art. Our implementation is available in https://github.com/LIONS-EPFL/Charmer.",
  "abstract_zh": "摘要：自然语言处理中的对抗攻击在字符或标记级别施加扰动。标记级攻击因其使用基于梯度的方法而受到关注，但容易改变句子的语义，导致无效的对抗示例。虽然字符级攻击容易保持语义，但由于无法轻易采用流行的基于梯度的方法，并且被认为容易防御，因此受到的关注较少。为了挑战这些观念，我们提出了Charmer，这是一种高效的基于查询的对抗攻击，能够在生成高度相似的对抗示例的同时实现高攻击成功率（ASR）。我们的方法成功地针对小型（BERT）和大型（Llama 2）模型。具体而言，在使用SST-2的BERT上，Charmer的ASR提高了4.84个百分点，USE相似度提高了8个百分点。我们的实现可在https://github.com/LIONS-EPFL/Charmer获取。"
}
{
  "title": "Position: Stop Making Unscientific AGI Performance Claims",
  "title_zh": "立场：停止发表不科学的AGI性能声明",
  "abstract": "Developments in the field of Artificial Intelligence (AI), and particularly large language models (LLMs), have created a 'perfect storm’ for observing 'sparks’ of Artificial General Intelligence (AGI) that are spurious. Like simpler models, LLMs distill meaningful representations in their latent embeddings that have been shown to correlate with external variables. Nonetheless, the correlation of such representations has often been linked to human-like intelligence in the latter but not the former. We probe models of varying complexity including random projections, matrix decompositions, deep autoencoders and transformers: all of them successfully distill information that can be used to predict latent or external variables and yet none of them have previously been linked to AGI. We argue and empirically demonstrate that the finding of meaningful patterns in latent spaces of models cannot be seen as evidence in favor of AGI. Additionally, we review literature from the social sciences that shows that humans are prone to seek such patterns and anthropomorphize. We conclude that both the methodological setup and common public image of AI are ideal for the misinterpretation that correlations between model representations and some variables of interest are 'caused' by the model's understanding of underlying 'ground truth’ relationships. We, therefore, call for the academic community to exercise extra caution, and to be keenly aware of principles of academic integrity, in interpreting and communicating about AI research outcomes.",
  "abstract_zh": "人工智能（AI）领域，尤其是大型语言模型（LLMs）的发展，创造了一场观察虚假人工通用智能（AGI）“火花”的“完美风暴”。与简单模型一样，LLMs在其潜在嵌入中提炼出有意义的表示，这些表示已被证明与外部变量相关。然而，这些表示的相关性通常与后者的人类智能相关，而与前者无关。我们探讨了不同复杂度的模型，包括随机投影、矩阵分解、深度自编码器和变换器：所有这些模型都成功提炼出可用于预测潜在或外部变量的信息，但它们之前都未与AGI相关联。我们论证并实证证明，在模型的潜在空间中发现有意义的模式不能被视为支持AGI的证据。此外，我们回顾了社会科学文献，显示人类倾向于寻找这种模式并进行拟人化。因此，我们得出结论，AI的研究结果的研究方法设置和公众形象都非常适合于误解模型表示与某些感兴趣变量之间的相关性是由模型对潜在“真实关系”的理解所“导致”的。因此，我们呼吁学术界在解释和传播AI研究成果时要格外谨慎，并敏锐意识到学术诚信原则。"
}
{
  "title": "Flextron: Many-in-One Flexible Large Language Model",
  "title_zh": "Flextron：多合一灵活大型语言模型",
  "abstract": "Training modern LLMs is extremely resource intensive, and customizing them for various deployment scenarios characterized by limited compute and memory resources through repeated training is impractical. In this paper, we introduce Flextron, a network architecture and post-training model optimization framework supporting flexible model deployment. The Flextron architecture utilizes a nested elastic structure to rapidly adapt to specific user-defined latency and accuracy targets during inference with no additional fine-tuning required. It is also input-adaptive, and can automatically route tokens through its sub-networks for improved performance and efficiency. We present a sample-efficient training method and associated routing algorithms for systematically transforming an existing trained LLM into a Flextron model. We evaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate superior performance over multiple end-to-end trained variants and other state-of-the-art elastic networks, all with a single pretraining run that consumes a mere 7.63% tokens compared to original pretraining.",
  "abstract_zh": "训练现代大型语言模型（LLMs）极其资源密集，针对计算和内存资源有限的各种部署场景进行定制化训练是不切实际的。本文介绍了Flextron，一种支持灵活模型部署的网络架构和后训练模型优化框架。Flextron架构利用嵌套弹性结构，在推理过程中快速适应用户定义的延迟和准确性目标，无需额外的微调。它还具有输入自适应能力，能够自动通过其子网络路由令牌，以提高性能和效率。我们提出了一种样本高效的训练方法及相关路由算法，用于系统性地将现有训练好的LLM转化为Flextron模型。我们在GPT-3和LLama-2系列LLM上评估了Flextron，并展示了其在多个端到端训练变体和其他最先进的弹性网络上的优越性能，所有这些都仅需一次预训练运行，消耗的令牌仅为原始预训练的7.63%。"
}
{
  "title": "Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills",
  "title_zh": "技能集优化：通过可转移技能增强语言模型行为",
  "abstract": "Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art in ScienceWorld by 35%.",
  "abstract_zh": "大型语言模型（LLMs）最近被用于交互环境中的序列决策。然而，利用环境奖励信号进行持续的LLM演员改进并非易事。我们提出了技能集优化（SSO），通过构建和完善可转移技能集来提高LLM演员的表现。SSO通过提取高奖励的共同子轨迹并生成子目标和指令来构建技能，以表示每个技能。这些技能在上下文中提供给LLM演员，以强化高奖励的行为。然后，SSO通过修剪不再产生高奖励的技能进一步完善技能集。我们在经典视频游戏NetHack和文本环境ScienceWorld中评估了我们的方法，以展示SSO优化技能集和进行上下文策略改进的能力。在我们定制的NetHack任务中，SSO比基线提高了40%，在ScienceWorld中超越了之前的最先进水平35%。"
}
{
  "title": "Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in low-data regimes",
  "title_zh": "策划的LLM：在低数据环境下LLM与数据策划的协同作用用于表格增强",
  "abstract": "Machine Learning (ML) in low-data settings remains an underappreciated yet crucial problem. Hence, data augmentation methods to increase the sample size of datasets needed for ML are key to unlocking the transformative potential of ML in data-deprived regions and domains. Unfortunately, the limited training set constrains traditional tabular synthetic data generators in their ability to generate a large and diverse augmented dataset needed for ML tasks. To address this challenge, we introduce $\\texttt{CLLM}$, which leverages the prior knowledge of Large Language Models (LLMs) for data augmentation in the low-data regime. However, not all the data generated by LLMs will improve downstream utility, as for any generative model. Consequently, we introduce a principled curation mechanism, leveraging learning dynamics, coupled with confidence and uncertainty metrics, to obtain a high-quality dataset. Empirically, on multiple real-world datasets, we demonstrate the superior performance of $\\texttt{CLLM}$ in the low-data regime compared to conventional generators. Additionally, we provide insights into the LLM generation and curation mechanism, shedding light on the features that enable them to output high-quality augmented datasets.",
  "abstract_zh": "在低数据环境中，机器学习（ML）仍然是一个被低估但至关重要的问题。因此，数据增强方法以增加机器学习所需数据集的样本量，对于释放机器学习在数据匮乏地区和领域的变革潜力至关重要。不幸的是，有限的训练集限制了传统表格合成数据生成器生成大型多样化增强数据集的能力。为了解决这一挑战，我们引入了$\\texttt{CLLM}$，它利用大型语言模型（LLMs）的先验知识进行低数据环境下的数据增强。然而，并非所有由LLMs生成的数据都会提高下游效用，正如任何生成模型一样。因此，我们引入了一种原则性策划机制，利用学习动态，结合信心和不确定性指标，以获得高质量的数据集。在多个真实世界数据集上，我们实证展示了$\\texttt{CLLM}$在低数据环境下相较于传统生成器的优越性能。此外，我们提供了对LLM生成和策划机制的见解，阐明了使其能够输出高质量增强数据集的特征。"
}
{
  "title": "Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution",
  "title_zh": "标题：Promptbreeder：通过提示进化实现自我参考的自我改进",
  "abstract": "Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, evaluates them for fitness on a training set, and repeats this process over multiple generations to evolve task-prompts. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutation-prompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furthermore, Promptbreeder is able to evolve intricate task-prompts for the challenging problem of hate speech classification.",
  "abstract_zh": "摘要：流行的提示策略，如链式思维提示，可以显著提高大型语言模型（LLMs）在各个领域的推理能力。然而，这些手工制作的提示策略往往不是最优的。本文提出了Promptbreeder，这是一种通用的自我参考自我改进机制，能够为特定领域进化和适应提示。Promptbreeder在大型语言模型的驱动下，变异一组任务提示，评估它们在训练集上的适应性，并在多个世代中重复这一过程以进化任务提示。关键是，这些任务提示的变异由大型语言模型生成并在进化过程中以自我参考的方式改进的变异提示所控制。也就是说，Promptbreeder不仅在改进任务提示，同时也在改进那些提升这些任务提示的变异提示。Promptbreeder在常用的算术和常识推理基准上超越了最先进的提示策略，如链式思维和计划与解决提示。此外，Promptbreeder能够为具有挑战性的仇恨言论分类问题进化复杂的任务提示。"
}
{
  "title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models",
  "title_zh": "标题：SPP：用于大型语言模型的稀疏性保留参数高效微调",
  "abstract": "Large Language Models (LLMs) have become pivotal in advancing the field of artificial intelligence, yet their immense sizes pose significant challenges for both fine-tuning and deployment. Current post-training pruning methods, while reducing the sizes of LLMs, often fail to maintain their original performance. To address these challenges, this paper introduces SPP, a **S**parsity-**P**reserved **P**arameter-efficient fine-tuning method. Different from existing post-training pruning approaches that struggle with performance retention, SPP proposes to employ lightweight learnable column and row matrices to optimize sparse LLM weights, *keeping the structure and sparsity of pruned pre-trained models intact*. By element-wise multiplication and residual addition, SPP ensures the consistency of model sparsity pattern and ratio during both training and weight-merging processes. We demonstrate the effectiveness of SPP by applying it to the LLaMA and LLaMA-2 model families with recent post-training pruning methods. Our results show that SPP significantly enhances the performance of models with different sparsity patterns (i.e. unstructured and N:M sparsity), especially for those with high sparsity ratios (e.g. 75%), making it a promising solution for the efficient fine-tuning of sparse LLMs. Code will be made available at https://github.com/Lucky-Lance/SPP.",
  "abstract_zh": "摘要：大型语言模型（LLMs）在推动人工智能领域发展中变得至关重要，但其庞大的规模对微调和部署带来了重大挑战。当前的后训练剪枝方法虽然能够减少LLMs的规模，但往往无法保持其原始性能。为了解决这些挑战，本文提出了SPP，一种**稀疏性保留**的**参数高效**微调方法。与现有的后训练剪枝方法在性能保持方面存在困难不同，SPP建议采用轻量级可学习的列和行矩阵来优化稀疏LLM权重，*保持剪枝预训练模型的结构和稀疏性不变*。通过逐元素相乘和残差相加，SPP确保在训练和权重合并过程中模型稀疏模式和比例的一致性。我们通过将SPP应用于LLaMA和LLaMA-2模型系列以及近期的后训练剪枝方法，展示了SPP的有效性。我们的结果表明，SPP显著提升了具有不同稀疏模式（即非结构化和N:M稀疏性）模型的性能，尤其是对于稀疏比例较高的模型（例如75%），使其成为稀疏LLMs高效微调的有前景的解决方案。代码将发布在https://github.com/Lucky-Lance/SPP。"
}
{
  "title": "Open-Domain Text Evaluation via Contrastive Distribution Methods",
  "title_zh": "开放领域文本评估通过对比分布方法",
  "abstract": "Recent advancements in open-domain text generation, driven by the power of large pre-trained language models (LLMs), have demonstrated remarkable performance. However, assessing these models' generation quality remains a challenge. In this paper, we introduce a novel method for evaluating open-domain text generation called Contrastive Distribution Methods (CDM). Leveraging the connection between increasing model parameters and enhanced LLM performance, CDM creates a mapping from the _contrast_ of two probabilistic distributions -- one known to be superior to the other -- to quality measures. We investigate CDM for open-domain text generation evaluation under two paradigms: 1) _Generative_ CDM, which harnesses the contrast of two language models' distributions to generate synthetic examples for training discriminator-based metrics; 2) _Discriminative_ CDM, which directly uses distribution disparities between two language models for evaluation. Our experiments on coherence evaluation for multi-turn dialogue and commonsense evaluation for controllable generation demonstrate CDM's superior correlate with human judgment than existing automatic evaluation metrics, highlighting the strong performance and generalizability of our approach.",
  "abstract_zh": "最近开放领域文本生成的进展，得益于大型预训练语言模型（LLMs）的强大能力，展现了显著的性能。然而，评估这些模型的生成质量仍然是一个挑战。本文介绍了一种用于评估开放领域文本生成的新方法，称为对比分布方法（CDM）。CDM利用模型参数增加与LLM性能提升之间的联系，从两个已知优劣的概率分布的_对比_中创建映射到质量度量。我们在两种范式下研究CDM用于开放领域文本生成评估：1）_生成性_ CDM，利用两个语言模型分布的对比生成合成示例以训练基于判别器的度量；2）_判别性_ CDM，直接使用两个语言模型之间的分布差异进行评估。我们在多轮对话的连贯性评估和可控生成的常识评估实验中展示了CDM与人类判断的相关性优于现有的自动评估指标，突显了我们方法的强大性能和广泛适用性。"
}
{
  "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
  "title_zh": "模型能否自我解释？自然语言解释的反事实可模拟性",
  "abstract": "Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers ''$\\textit{yes}$'' to the input question ''$\\textit{Can eagles fly?}$'' with the explanation ''$\\textit{all birds can fly}$'', then humans would infer from the explanation that it would also answer ''$\\textit{yes}$'' to the counterfactual input ''$\\textit{Can penguins fly?}$''. If the explanation is precise, then the model's answer should match humans' expectations. We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may be insufficient.",
  "abstract_zh": "大型语言模型（LLMs）被训练以模仿人类解释人类决策。然而，LLMs是否能够自我解释？它们能否帮助人类建立关于LLMs如何处理不同输入的心理模型？为了解答这些问题，我们提出评估自然语言解释的$\\textbf{反事实可模拟性}$：即一个解释是否能够使人类准确推断模型在被解释输入的多样反事实上的输出。例如，如果模型对输入问题“$\\textit{鹰能飞吗？}$”回答“$\\textit{是}$”，并给出解释“$\\textit{所有鸟都能飞}$”，那么人类会从该解释推断出它对反事实输入“$\\textit{企鹅能飞吗？}$”也会回答“$\\textit{是}$”。如果解释是精确的，那么模型的回答应与人类的预期相符。我们基于反事实可模拟性实现了两个指标：精确度和普遍性。我们使用LLMs自动生成多样的反事实。然后，我们利用这些指标评估了最先进的LLMs（例如，GPT-4）在两个任务上的表现：多跳事实推理和奖励建模。我们发现LLMs的解释精确度较低，并且精确度与合理性之间没有相关性。因此，简单地优化人类的认可（例如，RLHF）可能是不够的。"
}
{
  "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences",
  "title_zh": "最大最小强化学习人类反馈：与多样化人类偏好的对齐",
  "abstract": "Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, the single reward model overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. Next, we propose to learn a mixture of reward models via an expectation-maximization algorithm and solve a MaxMin alignment objective inspired by the Egalitarian principle in social choice theory to better honor diverse human preferences. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language (with Tulu2-7B)) and show the efficacy of the proposed approach in the presence of diversity among human preferences. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general.",
  "abstract_zh": "从人类反馈的强化学习（RLHF）通过使用从偏好数据中得出的单一奖励模型来使语言模型与人类偏好对齐。然而，单一奖励模型忽视了从多个用户收集的数据中固有的人类偏好的丰富多样性。在这项工作中，我们首先推导出单一奖励RLHF对齐的不可行性结果，从而突显其在表示多样化人类偏好方面的不足。接下来，我们提出通过期望最大化算法学习奖励模型的混合，并解决一个受社会选择理论中的平等原则启发的最大最小对齐目标，以更好地尊重多样化的人类偏好。我们在小规模（GPT-2）和大规模语言（Tulu2-7B）上进行了全面的实验结果，展示了所提方法在存在人类偏好多样性时的有效性。我们指出，这项工作的发现不仅限于语言模型，还扩展到一般的强化学习。"
}
{
  "title": "Position: Near to Mid-term Risks and Opportunities of Open-Source Generative AI",
  "title_zh": "标题：位置：开源生成性人工智能的近期至中期风险与机遇",
  "abstract": "In the next few years, applications of Generative AI are expected to revolutionize a number of different areas, ranging from science & medicine to education. The potential for these seismic changes has triggered a lively debate about potential risks and resulted in calls for tighter regulation, in particular from some of the major tech companies who are leading in AI development. While regulation is important, it is key that it does not put at risk the budding field of open-source Generative AI. We argue for the responsible open sourcing of generative AI models in the near and medium term. To set the stage, we first introduce an AI openness taxonomy system and apply it to 40 current large language models. We then outline differential benefits and risks of open versus closed source AI and present potential risk mitigation, ranging from best practices to calls for technical and scientific contributions. We hope that this report will add a much needed missing voice to the current public discourse on near to mid-term AI safety and other societal impact.",
  "abstract_zh": "摘要：在未来几年，生成性人工智能的应用预计将彻底改变多个领域，从科学与医学到教育。这些重大变化的潜力引发了关于潜在风险的热烈讨论，并导致对更严格监管的呼吁，特别是来自一些在人工智能开发中处于领先地位的主要科技公司。虽然监管很重要，但关键是它不能危及新兴的开源生成性人工智能领域。我们主张在近期和中期内负责任地开源生成性人工智能模型。为此，我们首先介绍一个人工智能开放性分类系统，并将其应用于40个当前的大型语言模型。然后，我们概述了开源与闭源人工智能的不同利益和风险，并提出了潜在的风险缓解措施，从最佳实践到对技术和科学贡献的呼吁。我们希望这份报告能为当前关于近期至中期人工智能安全和其他社会影响的公共讨论增添一个急需的声音。"
}
{
  "title": "Image Hijacks: Adversarial Images can Control Generative Models at Runtime",
  "title_zh": "图像劫持：对抗性图像可以在运行时控制生成模型",
  "abstract": "Are foundation models secure against malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduce the general Behaviour Matching algorithm for training image hijacks. From this, we derive the Prompt Matching method, allowing us to train hijacks matching the behaviour of an arbitrary user-defined text prompt (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset unrelated to our choice of prompt. We use Behaviour matching to craft hijacks for four types of attack: forcing VLMs to generate outputs of the adversary’s choice, leak information from their context window, override their safety training, and believe false statements. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all attack types achieve a success rate of over 80%. Moreover, our attacks are automated and require only small image perturbations.",
  "abstract_zh": "基础模型是否能抵御恶意行为者？在本研究中，我们关注视觉语言模型（VLM）的图像输入。我们发现了图像劫持，即在推理时控制VLM行为的对抗性图像，并引入了一种通用的行为匹配算法用于训练图像劫持。由此，我们推导出提示匹配方法，使我们能够使用与我们选择的提示无关的通用现成数据集，训练与任意用户定义的文本提示（例如“埃菲尔铁塔现在位于罗马”）匹配的劫持。我们利用行为匹配为四种攻击类型设计劫持：迫使VLM生成对手选择的输出，从其上下文窗口泄露信息，覆盖其安全训练，以及相信虚假陈述。我们对基于CLIP和LLaMA-2的最先进VLM LLaVA进行了这些攻击的研究，发现所有攻击类型的成功率均超过80%。此外，我们的攻击是自动化的，仅需小幅图像扰动。"
}
{
  "title": "Privacy Backdoors: Stealing Data with Corrupted Pretrained Models",
  "title_zh": "隐私后门：利用受损的预训练模型窃取数据",
  "abstract": "Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model’s weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.",
  "abstract_zh": "实践者通常从开放库下载预训练的机器学习模型并对其进行微调以适应特定应用。我们展示了这一做法引入了新的隐私后门风险。通过篡改预训练模型的权重，攻击者可以完全破坏微调数据的隐私。我们展示了如何为多种模型（包括变换器）构建隐私后门，使攻击者能够重建单个微调样本，并确保成功！我们进一步表明，带后门的模型允许对使用差分隐私（DP）训练的模型进行紧密的隐私攻击。因此，如果模型不被信任，使用松散隐私保证训练DP模型的常见乐观做法是不安全的。总体而言，我们的工作突显了机器学习隐私中一个重要且被忽视的供应链攻击。"
}
{
  "title": "DOGE: Domain Reweighting with Generalization Estimation",
  "title_zh": "DOGE：基于泛化估计的领域重加权",
  "abstract": "The coverage and composition of the pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). Despite its importance, recent LLMs still rely on heuristics and trial and error to increase or reduce the influence of data-domains. We propose DOmain reweighting with Generalization Estimation (DoGE), which optimizes the probability of sampling from each domain (domain weights) in a principled way. Our approach is a two stage process consisting (i) training a proxy model to obtain domain weights using a bi-level optimization algorithm; (ii) training a larger base model by sampling training domains according to the learnt domain weights. In our experiments, we extensively show how DoGE improves the generalization of the base model to any target data mixture. On the SlimPajama dataset, our base model gets a better perplexity and few-shot reasoning accuracies across 6 tasks compared to baseline methods. Moreover, aiming to generalize to out-of-domain target tasks, which is unseen in the pretraining corpus (OOD domain), DoGE can effectively identify inter-domain dependencies, consistently achieves better test perplexity on the target domain.",
  "abstract_zh": "预训练数据的覆盖范围和组成对大型语言模型（LLMs）的泛化能力有显著影响。尽管这一点很重要，近期的LLMs仍依赖启发式方法和试错来增加或减少数据领域的影响。我们提出了基于泛化估计的领域重加权（DoGE），以一种原则性的方式优化从每个领域采样的概率（领域权重）。我们的方法是一个两阶段过程，包括（i）训练一个代理模型，通过双层优化算法获得领域权重；（ii）根据学习到的领域权重采样训练领域，训练一个更大的基础模型。在我们的实验中，我们广泛展示了DoGE如何提高基础模型对任何目标数据混合的泛化能力。在SlimPajama数据集上，我们的基础模型在6个任务中相比基线方法获得了更好的困惑度和少量样本推理准确率。此外，旨在对预训练语料库中未见的领域（OOD领域）进行领域外目标任务的泛化，DoGE能够有效识别领域间的依赖关系，并在目标领域上持续实现更好的测试困惑度。"
}
{
  "title": "Adaptive Text Watermark for Large Language Models",
  "title_zh": "自适应文本水印技术用于大型语言模型",
  "abstract": "The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining robustness, security, and the ability to detect watermarks without prior knowledge of the prompt and model. This paper proposes an adaptive text watermarking strategy to address such a challenge. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured by an auxiliary model and keep the low-entropy token distributions untouched. For the sake of security and to further minimize the watermark's impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits based on the semantic embedding of previously generated text using a well designed semantic mapping model. Our experiments involving various LLMs demonstrate that our approach achieves comparable robustness performance to existing watermark methods. Additionally, the text generated by our method has perplexity comparable to that of *un-watermarked* LLMs while maintaining sufficient security.",
  "abstract_zh": "大型语言模型（LLMs）的进步引发了对AI生成文本滥用的日益关注，而对LLM生成文本进行水印处理已成为一种潜在解决方案。然而，在保持鲁棒性、安全性以及在没有先前提示和模型知识的情况下检测水印的能力的同时，生成高质量的水印文本是具有挑战性的。本文提出了一种自适应文本水印策略来应对这一挑战。为了提高文本质量并保持鲁棒性，我们自适应地将水印添加到由辅助模型测量的高熵标记分布中，而保持低熵标记分布不变。为了安全起见，并进一步减少水印对文本质量的影响，我们不再使用基于随机密钥生成的固定绿/红名单，这可能容易受到解密和伪造的攻击，而是基于先前生成文本的语义嵌入，采用精心设计的语义映射模型自适应地放大输出logits。我们的实验涉及多种LLM，表明我们的方法在鲁棒性性能上与现有水印方法相当。此外，我们方法生成的文本在困惑度上与*未加水印*的LLM相当，同时保持了足够的安全性。"
}
{
  "title": "Structured Chemistry Reasoning with Large Language Models",
  "title_zh": "结构化化学推理与大型语言模型",
  "abstract": "Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry. Different from the simple chemistry tasks (e.g., molecule classification) addressed in previous studies, complex chemistry problems require not only vast knowledge and precise calculation, but also compositional reasoning about rich dynamic interactions of different concepts (e.g., temperature changes). Our study shows that even advanced LLMs, like GPT-4, can fail easily in different ways. Interestingly, the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning *structure* that guides the LLMs to elicit the right knowledge, incorporate the knowledge in step-by-step reasoning, and iteratively refine results for further improved quality. On this basis, we introduce StructChem, a simple yet effective prompting strategy that offers the desired guidance and substantially boosts the LLMs' chemical reasoning capability. Testing across four chemistry areas---quantum chemistry, mechanics, physical chemistry, and kinetics---StructChem substantially enhances GPT-4's performance, with up to 30% peak improvement. Our analysis also underscores the unique difficulties of precise grounded reasoning in science with LLMs, highlighting a need for more research in this area.",
  "abstract_zh": "大型语言模型（LLMs）在多个领域表现出色，但在复杂科学推理方面，尤其是在化学领域，仍然存在困难。与以往研究中处理的简单化学任务（例如，分子分类）不同，复杂的化学问题不仅需要广泛的知识和精确的计算，还需要对不同概念（例如，温度变化）之间丰富的动态交互进行组合推理。我们的研究表明，即使是先进的LLM，如GPT-4，也可能以不同方式轻易失败。有趣的是，这些错误往往并非源于LLM缺乏领域知识，而是缺乏有效的推理*结构*，该结构引导LLM提取正确的知识，将知识融入逐步推理，并迭代优化结果以进一步提高质量。在此基础上，我们引入了StructChem，这是一种简单而有效的提示策略，提供所需的指导，显著提升LLM的化学推理能力。在量子化学、力学、物理化学和动力学四个化学领域的测试中，StructChem显著增强了GPT-4的表现，峰值提升可达30%。我们的分析还强调了LLM在科学中进行精确基础推理的独特困难，突显了该领域需要更多研究的必要性。"
}
{
  "title": "Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension",
  "title_zh": "标题：利用局部内在维度表征大型语言模型生成的真实性",
  "abstract": "We study how to characterize and predict the truthfulness of texts generated from large language models (LLMs), which serves as a crucial step in building trust between humans and LLMs. Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs. In this paper, we suggest investigating internal activations and quantifying LLM's truthfulness using the local intrinsic dimension (LID) of model activations. Through experiments on four question answering (QA) datasets, we demonstrate the effectiveness of our proposed method. Additionally, we study intrinsic dimensions in LLMs and their relations with model layers, autoregressive language modeling, and the training of LLMs, revealing that intrinsic dimensions can be a powerful approach to understanding LLMs.",
  "abstract_zh": "摘要：我们研究如何表征和预测大型语言模型（LLMs）生成文本的真实性，这为建立人类与LLMs之间的信任奠定了重要基础。尽管已有多种基于熵或语言化不确定性的方案被提出以校准模型预测，但这些方法通常难以处理，对超参数敏感，并且在生成任务中应用时可靠性较低。本文建议通过研究内部激活并利用模型激活的局部内在维度（LID）来量化LLM的真实性。通过在四个问答（QA）数据集上的实验，我们证明了所提方法的有效性。此外，我们研究了LLMs中的内在维度及其与模型层、自回归语言建模和LLMs训练的关系，揭示了内在维度可以成为理解LLMs的强大方法。"
}
{
  "title": "Recovering the Pre-Fine-Tuning Weights of Generative Models",
  "title_zh": "恢复生成模型的预微调权重",
  "abstract": "The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, *pre-fine-tuning* model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present *Spectral DeTuning*, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral. The code is available at https://vision.huji.ac.il/spectral_detuning/.",
  "abstract_zh": "主流的生成建模范式包括两个步骤：i) 在一个大规模但不安全的数据集上进行预训练，ii) 通过微调将预训练模型与人类价值观对齐。这种做法被认为是安全的，因为目前没有方法能够恢复不安全的*预微调*模型权重。在本文中，我们证明了这一假设通常是错误的。具体而言，我们提出了*谱去调优*，一种可以使用少量低秩（LoRA）微调模型恢复预微调模型权重的方法。与之前试图恢复预微调能力的攻击不同，我们的方法旨在恢复确切的预微调权重。我们的方法利用了这一新漏洞，针对大规模模型，如个性化的稳定扩散和对齐的Mistral。代码可在 https://vision.huji.ac.il/spectral_detuning/ 获取。"
}
{
  "title": "Scalable AI Safety via Doubly-Efficient Debate",
  "title_zh": "可扩展的人工智能安全性通过双重高效辩论",
  "abstract": "The emergence of pre-trained AI systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for AI safety as tasks can become too complicated for humans to judge directly. Irving et al (2018). proposed a debate method in this direction with the goal of pitting the power of such AI models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask. While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate *deterministic* AI systems for an *exponential* number of steps, limiting its applicability. In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a *polynomial* number of steps, whilst being able to verify the alignment of *stochastic* AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps.",
  "abstract_zh": "随着预训练人工智能系统在多样化且日益复杂的领域中展现出强大能力，AI安全性面临着一个关键挑战，因为任务可能变得过于复杂，无法直接由人类判断。Irving等人（2018）提出了一种辩论方法，旨在将这些AI模型的能力相互对抗，直到识别（误）对齐的问题被分解为可管理的子任务。尽管这种方法的前景显而易见，但原始框架基于诚实策略能够模拟*确定性*AI系统*指数*步数的假设，限制了其适用性。在本文中，我们展示了如何通过设计一组新的辩论协议来解决这些挑战，其中诚实策略始终能够成功，使用*多项式*步数的模拟，同时能够验证*随机*AI系统的对齐，即使不诚实策略被允许使用指数级的模拟步数。"
}
{
  "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study",
  "title_zh": "标题：DPO是否优于PPO用于大型语言模型对齐？一项综合研究",
  "abstract": "Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences. Existing RLHF methods can be roughly categorized as either reward-based or reward-free. Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO). However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly superior to PPO? Why does PPO perform poorly on these benchmarks? In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations. Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs. Finally, we benchmark DPO and PPO across a collection of RLHF testbeds, ranging from dialogue to code generation. Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions.",
  "abstract_zh": "摘要：基于人类反馈的强化学习（RLHF）目前是将大型语言模型（LLMs）与人类偏好对齐的最广泛使用的方法。现有的RLHF方法大致可分为基于奖励和无奖励两类。新兴应用如ChatGPT和Claude利用基于奖励的方法，首先学习奖励模型并应用演员-评论家算法，如近端策略优化（PPO）。然而，在学术基准测试中，最先进的结果往往是通过无奖励方法实现的，如直接偏好优化（DPO）。DPO真的优于PPO吗？为什么PPO在这些基准测试中的表现不佳？在本文中，我们首先对DPO的算法特性进行理论和实证研究，表明DPO可能存在根本性限制。此外，我们还全面审查了PPO，并揭示了PPO在微调LLMs时最佳表现的关键因素。最后，我们在一系列RLHF测试平台上对DPO和PPO进行基准测试，涵盖对话到代码生成的各个方面。实验结果表明，PPO在所有情况下都能够超越其他对齐方法，并在具有挑战性的代码竞赛中取得最先进的结果。"
}
{
  "title": "Multicalibration for Confidence Scoring in LLMs",
  "title_zh": "多重校准用于大型语言模型的置信评分",
  "abstract": "This paper proposes the use of \"multicalibration\": to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and \"self-annotation\" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
  "abstract_zh": "本文提出了使用“多重校准”来为大型语言模型（LLMs）生成可解释且可靠的置信评分。多重校准不仅要求边际校准，还要求在数据的各种交叉分组中同时进行校准。我们展示了如何通过两种技术形成与正确性概率相关的提示/完成对的分组：在嵌入空间内聚类和“自我标注”——通过向LLM询问关于提示的各种是或否问题来进行查询。我们还开发了多重校准算法的新变体，通过减少过拟合的倾向来提高性能。通过在各种问答数据集和LLMs上进行系统基准测试，我们展示了我们的技术如何在校准和准确性的细粒度度量上，相较于现有方法提供显著的置信评分改进。"
}
{
  "title": "Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation",
  "title_zh": "隐秘恶意微调：保护大型语言模型适应性的挑战",
  "abstract": "Black-box finetuning is an emerging interface for adapting state-of-the-art language models to user needs. However, such access may also let malicious actors undermine model safety. To demonstrate the challenge of defending finetuning interfaces, we introduce covert malicious finetuning, a method to compromise model safety via finetuning while evading detection. Our method constructs a malicious dataset where every individual datapoint appears innocuous, but finetuning on the dataset teaches the model to respond to encoded harmful requests with encoded harmful responses. Applied to GPT-4, our method produces a finetuned model that acts on harmful instructions 99% of the time and avoids detection by defense mechanisms such as dataset inspection, safety evaluations, and input/output classifiers. Our findings question whether black-box finetuning access can be secured against sophisticated adversaries.",
  "abstract_zh": "黑箱微调是将最先进的语言模型适应用户需求的新兴接口。然而，这种访问也可能让恶意行为者破坏模型安全。为了展示防御微调接口的挑战，我们引入了隐秘恶意微调，这是一种通过微调来破坏模型安全的方法，同时避免被检测。我们的方法构建了一个恶意数据集，其中每个数据点看似无害，但在该数据集上进行微调会使模型学会对编码的有害请求做出编码的有害响应。应用于GPT-4，我们的方法生成了一个在99%的情况下会根据有害指令行动的微调模型，并且能够避开数据集检查、安全评估和输入/输出分类器等防御机制的检测。我们的发现质疑黑箱微调访问是否能抵御复杂对手的攻击。"
}
{
  "title": "How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?",
  "title_zh": "大型语言模型如何在诚实与帮助之间进行权衡？",
  "abstract": "In day-to-day communication, people often approximate the truth --- for example, rounding the time or omitting details --- in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.",
  "abstract_zh": "在日常交流中，人们常常会近似真实——例如，四舍五入时间或省略细节——以便对听者提供最大的帮助。那么，大型语言模型（LLMs）如何处理这些微妙的权衡呢？为了解决这个问题，我们使用心理模型和实验来表征人类行为，以分析LLMs。我们测试了一系列LLMs，并探讨了对人类偏好的优化或推理时的思考如何影响这些权衡。我们发现，从人类反馈中进行的强化学习提高了诚实性和帮助性，而思维链提示则使LLMs更倾向于帮助而非诚实。最后，GPT-4 Turbo展示了类似人类的响应模式，包括对对话框架和听者决策背景的敏感性。我们的发现揭示了LLMs内化的对话价值观，并表明即使是这些抽象的价值观，在一定程度上也可以通过零-shot提示进行引导。"
}
{
  "title": "Towards Efficient Exact Optimization of Language Model Alignment",
  "title_zh": "朝向语言模型对齐的高效精确优化",
  "abstract": "The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. However, we show that DPO derived based on the optimal solution of the problem leads to a compromised mean-seeking approximation of the optimal solution in practice. In this paper, we propose efficient exact optimization (EXO) of the alignment objective. EXO is guaranteed to optimize in the same direction as RL algorithms asymptotically for arbitrary policy parametrization. This leads to the same mode-seeking solution, while enables efficient optimization by circumventing the complexities of RL. We also compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data. Code is available at https://github.com/haozheji/exact-optimization.",
  "abstract_zh": "语言模型与人类偏好的对齐对于其在现实任务中的应用至关重要。我们将该问题表述为优化模型策略，以最大化反映人类偏好的期望奖励，同时最小化与初始策略的偏差。尽管强化学习（RL）被视为一种直接的解决方案，但其策略更新的高方差阻碍了高效的策略改进。最近，提出了直接偏好优化（DPO），旨在直接从偏好数据中优化策略。然而，我们表明，基于问题的最优解推导的DPO在实践中导致了对最优解的妥协均值寻求近似。本文提出了对齐目标的高效精确优化（EXO）。EXO保证在任意策略参数化下，渐近地朝着与RL算法相同的方向进行优化。这导致相同的模式寻求解，同时通过规避RL的复杂性实现高效优化。我们还通过理论和实证分析将我们的方法与DPO进行比较，并进一步展示了我们的方法在现实人类偏好数据上的优势。代码可在https://github.com/haozheji/exact-optimization获取。"
}
{
  "title": "Position: An Inner Interpretability Framework for AI Inspired by Lessons from Cognitive Neuroscience",
  "title_zh": "位置：一个受认知神经科学启发的人工智能内部可解释性框架",
  "abstract": "Inner Interpretability is a promising emerging field tasked with uncovering the inner mechanisms of AI systems, though how to develop these mechanistic theories is still much debated. Moreover, recent critiques raise issues that question its usefulness to advance the broader goals of AI. However, it has been overlooked that these issues resemble those that have been grappled with in another field: Cognitive Neuroscience. Here we draw the relevant connections and highlight lessons that can be transferred productively between fields. Based on these, we propose a general conceptual framework and give concrete methodological strategies for building mechanistic explanations in AI inner interpretability research. With this conceptual framework, Inner Interpretability can fend off critiques and position itself on a productive path to explain AI systems.",
  "abstract_zh": "内部可解释性是一个有前景的新兴领域，旨在揭示人工智能系统的内部机制，尽管如何发展这些机制理论仍然存在许多争论。此外，最近的批评提出了一些问题，质疑其在推动人工智能更广泛目标方面的有效性。然而，值得注意的是，这些问题与另一个领域——认知神经科学中所面临的问题相似。在这里，我们建立相关联系，并强调可以在两个领域之间有效转移的经验教训。基于这些，我们提出了一个通用概念框架，并提供了构建人工智能内部可解释性研究中机制解释的具体方法策略。借助这个概念框架，内部可解释性可以抵御批评，并在解释人工智能系统的有效路径上定位自己。"
}
{
  "title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",
  "title_zh": "语言模型的物理学：第3.1部分，知识存储与提取",
  "abstract": "Large language models (LLMs) can store a vast amount of world knowledge, often extractable via question-answering (e.g., \"What is Abraham Lincoln's birthday?''). However, do they answer such questions based on exposure to similar questions during training (i.e., cheating), or by genuinely learning to extract knowledge from sources like Wikipedia? In this paper, we investigate this issue using a controlled biography dataset. We find a strong correlation between the model's ability to extract knowledge and various _diversity measures_ of the training data. **Essentially**, for knowledge to be reliably extracted, it must be sufficiently augmented (e.g., through paraphrasing, sentence shuffling) _during pretraining_. Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning. To understand why this occurs, we employ (nearly) linear probing to demonstrate a strong connection between the observed correlation and _how the model internally encodes knowledge_ --- whether it is linearly encoded in the hidden embeddings of entity names or distributed across other token embeddings in the training text. **This paper provides several key recommendations for LLM pretraining in the industry:** (1) rewrite the pretraining data --- using small, auxiliary models --- to provide knowledge augmentation, and (2) incorporate more instruction-finetuning data into the pretraining stage before it becomes too late.",
  "abstract_zh": "大型语言模型（LLMs）可以存储大量的世界知识，通常可以通过问答方式提取（例如，“亚伯拉罕·林肯的生日是什么？”）。然而，它们是否是基于在训练过程中接触到的类似问题来回答这些问题（即作弊），还是通过真正学习从维基百科等来源提取知识？在本文中，我们使用一个受控的传记数据集来研究这个问题。我们发现模型提取知识的能力与训练数据的各种“多样性度量”之间存在强相关性。**基本上**，为了可靠地提取知识，必须在预训练期间进行充分的增强（例如，通过改写、句子洗牌）。没有这种增强，知识可能被记住但无法提取，导致准确率为0%，无论后续的指令微调如何。为了理解为什么会发生这种情况，我们采用（几乎）线性探测来展示观察到的相关性与**模型如何在内部编码知识**之间的强联系——无论它是在线性编码的实体名称的隐藏嵌入中，还是分布在训练文本中的其他标记嵌入中。**本文为行业中的LLM预训练提供了几个关键建议：**（1）使用小型辅助模型重写预训练数据，以提供知识增强；（2）在为时已晚之前，将更多的指令微调数据纳入预训练阶段。"
}
{
  "title": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
  "title_zh": "补丁范围：检查语言模型隐藏表示的统一框架",
  "abstract": "Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up *new* possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.",
  "abstract_zh": "理解大型语言模型（LLMs）的内部表示可以帮助解释模型的行为并验证其与人类价值观的一致性。鉴于LLMs在生成可被人类理解的文本方面的能力，我们提议利用模型本身以自然语言解释其内部表示。我们引入了一个名为Patchscopes的框架，并展示了它如何用于回答关于LLM计算的广泛问题。我们表明，许多基于将表示投影到词汇空间和干预LLM计算的先前可解释性方法可以视为该框架的实例。此外，Patchscopes可以缓解一些缺点，例如无法检查早期层或缺乏表达能力。除了统一先前的检查技术，Patchscopes还开辟了*新的*可能性，例如使用更强大的模型来解释较小模型的表示，以及多跳推理错误修正。"
}
{
  "title": "Extreme Compression of Large Language Models via Additive Quantization",
  "title_zh": "极端压缩大型语言模型通过加法量化",
  "abstract": "The emergence of accurate open large language models (LLMs) has led to a race towards performant quantization techniques which can enable their execution on end-user devices. In this paper, we revisit the problem of ``extreme'' LLM compression---defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter---from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our algorithm, called AQLM, generalizes the classic *Additive Quantization (AQ)* approach for information retrieval to advance the state-of-the-art in LLM compression, via two innovations: 1) learned additive quantization of weight matrices in input-adaptive fashion, and 2) joint optimization of codebook parameters across each transformer blocks. Broadly, AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime. In addition, AQLM is practical: we provide fast GPU and CPU implementations of AQLM for token generation, which enable us to match or outperform optimized FP16 implementations for speed, while executing in a much smaller memory footprint.",
  "abstract_zh": "准确的开放大型语言模型（LLMs）的出现引发了针对高效量化技术的竞争，以便在终端用户设备上执行它们。本文从多码本量化（MCQ）的经典方法角度重新审视“极端”LLM压缩问题——定义为目标极低的位数，例如每个参数2到3位。我们的算法AQLM推广了经典的*加法量化（AQ）*方法，以推进LLM压缩的最新技术，通过两个创新：1）以输入自适应方式学习权重矩阵的加法量化，2）跨每个变换块联合优化码本参数。总体而言，AQLM是第一个在压缩到每个参数少于3位时在准确性与模型大小之间帕累托最优的方案，并在极端压缩（2位）范围内显著改善了所有已知方案。此外，AQLM是实用的：我们提供了AQLM的快速GPU和CPU实现，用于令牌生成，使我们能够在速度上匹配或超越优化的FP16实现，同时在更小的内存占用下执行。"
}
{
  "title": "Explorations of Self-Repair in Language Models",
  "title_zh": "自我修复在语言模型中的探索",
  "abstract": "Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor and sparse sets of neurons implementing Anti-Erasure. We additionally discuss the implications of these results for interpretability practitioners and close with a more speculative discussion on the mystery of why self-repair occurs in these models at all, highlighting evidence for the Iterative Inference hypothesis in language models, a framework that predicts self-repair.",
  "abstract_zh": "先前的可解释性研究在研究狭窄分布时初步识别了自我修复这一现象，即当大型语言模型中的某些组件被去除时，后续组件会改变其行为以进行补偿。我们的工作基于这一过去的文献，展示了在全训练分布下，当去除单个注意力头时，自我修复在多种模型家族和规模中均存在。我们进一步表明，在全训练分布下，自我修复并不完美，因为头的原始直接效应并未完全恢复，并且由于自我修复的程度在不同提示之间显著变化（有时过度修正超出原始效应），因此显得噪声较大。我们强调了两种促进自我修复的不同机制，包括最终LayerNorm缩放因子的变化和实现反抹除的稀疏神经元集合。我们还讨论了这些结果对可解释性从业者的影响，并以对自我修复为何在这些模型中发生的神秘性进行更具推测性的讨论作为结尾，强调了语言模型中迭代推理假设的证据，这是一个预测自我修复的框架。"
}
{
  "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation",
  "title_zh": "对比偏好优化：推动大型语言模型在机器翻译中的性能边界",
  "abstract": "Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, they do not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to supervised fine-tuning which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 0.1% parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.",
  "abstract_zh": "中等规模的大型语言模型（LLMs）——参数为70亿或130亿的模型——在机器翻译（MT）任务中展现出良好的性能。然而，它们的表现仍未达到最先进的传统编码-解码翻译模型或更大规模的LLMs（如GPT-4）。本研究旨在弥补这一性能差距。我们首先评估了LLMs在MT任务中监督微调的不足之处，强调了参考数据中存在的质量问题，尽管这些数据是人类生成的。然后，与模仿参考翻译的监督微调相对，我们引入了对比偏好优化（CPO），这是一种新颖的方法，旨在训练模型避免生成合格但不完美的翻译。将CPO应用于仅有22K平行句子和0.1%参数的ALMA模型，取得了显著的改进。最终模型ALMA-R的性能可以与WMT比赛获胜者及GPT-4在WMT'21、WMT'22和WMT'23测试数据集上的表现相匹配或超越。"
}
{
  "title": "Premise Order Matters in Reasoning with Large Language Models",
  "title_zh": "前提顺序在大型语言模型推理中至关重要",
  "abstract": "Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that even if the model performance is decent on the optimal order, permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.",
  "abstract_zh": "大型语言模型（LLMs）在各个领域取得了显著的推理表现。然而，在推理任务领域，我们发现了一个脆弱性：尽管前提的顺序并不改变基础任务，LLMs对前提顺序却出乎意料地脆弱。我们观察到，当前提顺序与中间推理步骤所需的上下文一致时，LLMs的表现最佳。例如，在演绎推理任务中，以与提示中的真实证明相同的顺序呈现前提（而不是随机顺序）会显著提高模型的准确性。我们首先检查了前提顺序对多种LLMs演绎推理的影响，我们的评估显示，即使模型在最佳顺序下表现良好，改变前提顺序也会导致超过30%的性能下降。此外，我们发布了基于GSM8K的基准R-GSM，以检查数学问题解决的顺序效应，我们再次观察到相对于原始GSM8K基准的准确性显著下降。"
}
{
  "title": "Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation",
  "title_zh": "自动评估检索增强语言模型的任务特定考试生成",
  "abstract": "We propose a new method to measure the task-specific accuracy of Retrieval-Augmented Large Language Models (RAG). Evaluation is performed by scoring the RAG on an automatically-generated synthetic exam composed of multiple choice questions based on the corpus of documents associated with the task. Our method is an automated, cost-efficient, interpretable, and robust strategy to select the optimal components for a RAG system. We leverage Item Response Theory (IRT) to estimate the quality of an exam and its informativeness on task-specific accuracy. IRT also provides a natural way to iteratively improve the exam by eliminating the exam questions that are not sufficiently informative about a model's ability. We demonstrate our approach on four new open-ended Question-Answering tasks based on Arxiv abstracts, StackExchange questions, AWS DevOps troubleshooting guides, and SEC filings. In addition, our experiments reveal more general insights into factors impacting RAG performance like size, retrieval mechanism, prompting and fine-tuning. Most notably, our findings show that choosing the right retrieval algorithms often leads to bigger performance gains than simply using a larger language model.",
  "abstract_zh": "我们提出了一种新方法来测量检索增强大型语言模型（RAG）的任务特定准确性。评估通过对基于与任务相关的文档语料库自动生成的多项选择题合成考试进行评分来完成。我们的方法是一种自动化、成本高效、可解释且稳健的策略，用于选择RAG系统的最佳组件。我们利用项目反应理论（IRT）来估计考试的质量及其对任务特定准确性的有效性。IRT还提供了一种自然的方式，通过消除对模型能力信息不足的考试问题，迭代改进考试。我们在基于Arxiv摘要、StackExchange问题、AWS DevOps故障排除指南和SEC文件的四个新的开放式问答任务上展示了我们的方法。此外，我们的实验揭示了影响RAG性能的更一般性见解，如规模、检索机制、提示和微调。最值得注意的是，我们的发现表明，选择正确的检索算法通常会带来比仅仅使用更大语言模型更大的性能提升。"
}
{
  "title": "CHEMREASONER: Heuristic Search over a Large Language Model’s Knowledge Space using Quantum-Chemical Feedback",
  "title_zh": "标题：CHEMREASONER：利用量子化学反馈在大型语言模型知识空间中进行启发式搜索",
  "abstract": "The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and reaction energy barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automatically guide the exploration without human input, providing competitive performance against expert-enumerated chemical descriptor-based implementations. By integrating language-guided reasoning with computational chemistry feedback, our work pioneers AI-accelerated, trustworthy catalyst discovery.",
  "abstract_zh": "摘要：新催化剂的发现对于设计新的、更高效的化学过程以实现可持续未来至关重要。我们引入了一种由人工智能引导的计算筛选框架，将语言推理与基于三维原子表示的量子化学反馈相结合。我们的研究将催化剂发现视为一个不确定的环境，其中代理通过大语言模型（LLM）衍生的假设与原子图神经网络（GNN）衍生的反馈的迭代组合，积极搜索高效催化剂。在中间搜索步骤中识别的催化剂基于空间取向、反应途径和稳定性进行结构评估。基于吸附能和反应能垒的评分函数引导LLM知识空间的探索，朝向能量有利、高效的催化剂。我们引入的规划方法能够在没有人工输入的情况下自动引导探索，提供与专家列举的化学描述符基础实现相竞争的性能。通过将语言引导的推理与计算化学反馈相结合，我们的工作开创了AI加速的可信催化剂发现。"
}
{
  "title": "Position: Data Authenticity, Consent, & Provenance for AI are all broken: what will it take to fix them?",
  "title_zh": "标题：位置：数据真实性、同意和来源对于人工智能都是破碎的：修复它们需要什么？",
  "abstract": "New capabilities in foundation models are owed in large part to massive, widely-sourced, and under-documented training data collections. Existing practices in data collection have led to challenges in tracing authenticity, verifying consent, preserving privacy, addressing representation and bias, respecting copyright, and overall developing ethical and trustworthy foundation models. In response, regulation is emphasizing the need for training data transparency to understand foundation models’ limitations. Based on a large-scale analysis of the foundation model training data landscape and existing solutions, we identify the missing infrastructure to facilitate responsible foundation model development practices. We examine the current shortcomings of common tools for tracing data authenticity, consent, and documentation, and outline how policymakers, developers, and data creators can facilitate responsible foundation model development by adopting universal data provenance standards.",
  "abstract_zh": "摘要：基础模型的新能力在很大程度上归功于大量广泛来源且记录不足的训练数据集合。现有的数据收集实践导致了追踪真实性、验证同意、保护隐私、解决代表性和偏见、尊重版权以及整体上开发伦理和可信的基础模型等方面的挑战。作为回应，监管强调了训练数据透明度的必要性，以理解基础模型的局限性。基于对基础模型训练数据环境和现有解决方案的大规模分析，我们识别出促进负责任的基础模型开发实践所缺失的基础设施。我们审视了当前在追踪数据真实性、同意和文档方面常用工具的不足，并概述了政策制定者、开发者和数据创作者如何通过采用通用数据来源标准来促进负责任的基础模型开发。"
}
{
  "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
  "title_zh": "聊天机器人竞技场：一个基于人类偏好的大型语言模型评估开放平台",
  "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowd-sourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. The platform is publicly available at https://chat.lmsys.org.",
  "abstract_zh": "大型语言模型（LLMs）解锁了新的能力和应用；然而，评估与人类偏好的对齐仍然面临重大挑战。为了解决这个问题，我们推出了聊天机器人竞技场，这是一个基于人类偏好的LLM评估开放平台。我们的方法采用成对比较的方法，并通过众包从多样化的用户群体中获取输入。该平台已运行数月，累计超过24万票。本文描述了该平台，分析了我们迄今收集的数据，并解释了我们用于高效和准确评估及排名模型的经过验证的统计方法。我们确认众包问题具有足够的多样性和区分度，并且众包的人类投票与专家评审者的投票高度一致。这些分析共同为聊天机器人竞技场的可信度奠定了坚实的基础。由于其独特的价值和开放性，聊天机器人竞技场已成为最受引用的LLM排行榜之一，广泛被领先的LLM开发者和公司引用。该平台可在 https://chat.lmsys.org 上公开访问。"
}
{
  "title": "Rethinking Generative Large Language Model Evaluation for Semantic Comprehension",
  "title_zh": "重新思考生成大型语言模型在语义理解中的评估",
  "abstract": "Despite their sophisticated capabilities, large language models (LLMs) encounter a major hurdle in effective assessment. This paper first revisits the prevalent evaluation method—multiple choice question answering (MCQA), which allows for straightforward accuracy measurement. Through a comprehensive evaluation of 24 models across 11 benchmarks, we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios. In response, we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5, Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with GPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This system is designed to mirror real-world usage, and for this purpose, we have compiled a new benchmark called ``Real-world questions'' (RWQ), comprising 20,772 authentic user inquiries. Additionally, we thoroughly analyze the characteristics of our system and compare it with prior leaderboards like Alpaca Eval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo system, the feasibility of registering new models, and its potential to reshape LLM leaderboards.",
  "abstract_zh": "尽管大型语言模型（LLMs）具备复杂的能力，但在有效评估方面仍面临重大障碍。本文首先重新审视了普遍采用的评估方法——多项选择题回答（MCQA），该方法便于进行准确性测量。通过对24个模型在11个基准上的全面评估，我们突出了MCQA的几个潜在缺陷，例如MCQA评估与实际场景中开放式响应生成之间的不一致性。为此，我们引入了一种RWQ-Elo评分系统，参与者包括GPT-4、GPT-3.5、Google-Gemini-Pro和LLaMA-1/-2等24个LLM，采用双人竞争格式，由GPT-4担任裁判。每个LLM随后获得一个Elo评分。该系统旨在反映现实世界的使用情况，为此我们编制了一个名为“现实世界问题”（RWQ）的新基准，包含20,772个真实用户询问。此外，我们还全面分析了我们系统的特征，并与之前的排行榜如Alpaca Eval和MT-Bench进行了比较。我们的分析揭示了RWQ-Elo系统的稳定性、新模型注册的可行性，以及其重塑LLM排行榜的潜力。"
}
{
  "title": "Offline Training of Language Model Agents with Functions as Learnable Weights",
  "title_zh": "离线训练具有可学习权重的语言模型代理",
  "abstract": "Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as *agents*, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an *agent training* algorithm with two strategies, roll-back, and early-stop, to streamline the training process. With extensive experiments, we showcase that the agent training paradigm could significantly improve the performance of representative LLM agents in various downstream tasks. We also study the behavior of the agent training regarding aspects like the learning curve and domain transferability.",
  "abstract_zh": "研究人员和实践者最近将强大的大型语言模型（LLMs）重新框架为*代理*，使其能够通过使用专门的功能自动化复杂任务。为了促进LLM代理的发展，我们提出了一种新的训练LLM代理的范式，无需修改LLM权重，这在LLM难以或无法进行修改时尤为有用。受到人类不断锻造工具以适应现实世界任务的启发，而不是改变我们的生物结构以适应一套静态工具，我们建议逐步锻造代理的功能，以更好地解决下游任务，而不是修改LLM权重。通过将功能视为可学习的“代理参数”，并利用人工智能中模型训练的基本思想，我们开发了AgentOptimizer，利用LLM来更新代理的功能，并设计了一种具有回滚和提前停止两种策略的*代理训练*算法，以简化训练过程。通过大量实验，我们展示了代理训练范式可以显著提高代表性LLM代理在各种下游任务中的表现。我们还研究了代理训练在学习曲线和领域可迁移性等方面的行为。"
}
{
  "title": "Prompt Sketching for Large Language Models",
  "title_zh": "标题：大型语言模型的提示草图生成",
  "abstract": "Many recent prompting strategies for large language models (LLMs) query the model multiple times sequentially -- first to produce intermediate results and then the final answer. However, using these methods, both decoder and model are unaware of potential follow-up prompts, leading to disconnected and undesirably wordy intermediate responses. In this work, we address this issue by proposing prompt sketching, a new prompting paradigm in which an LLM does not only respond by completing a prompt, but by predicting values for multiple variables in a template. This way, sketching grants users more control over the generation process, e.g., by providing a reasoning framework via intermediate instructions, leading to better overall results. The key idea enabling sketching with existing, autoregressive models is to adapt the decoding procedure to also score follow-up instructions during text generation, thus optimizing overall template likelihood in inference. Our experiments show that in a zero-shot setting, prompt sketching outperforms existing, sequential prompting schemes such as direct asking or chain-of-thought on 7 out of 8 LLM benchmarking tasks, including state tracking, arithmetic reasoning, and general question answering. To facilitate future use, we release a number of generic, yet effective sketches applicable to many tasks, and an open source library called dclib, powering our sketch-aware decoders as part of https://github.com/eth-sri/lmql.",
  "abstract_zh": "摘要：许多近期针对大型语言模型（LLMs）的提示策略依次查询模型多次——首先生成中间结果，然后得出最终答案。然而，使用这些方法时，解码器和模型都无法意识到潜在的后续提示，导致中间响应不连贯且冗长。在本研究中，我们通过提出提示草图生成，解决了这一问题，这是一种新的提示范式，其中LLM不仅通过完成提示来响应，还通过预测模板中多个变量的值来进行响应。通过这种方式，草图生成使用户在生成过程中拥有更多控制权，例如，通过提供中间指令的推理框架，从而获得更好的整体结果。使现有自回归模型能够进行草图生成的关键思想是调整解码过程，以便在文本生成过程中对后续指令进行评分，从而优化推理中的整体模板似然性。我们的实验表明，在零样本设置中，提示草图生成在8个LLM基准任务中有7个任务的表现优于现有的顺序提示方案，如直接询问或思维链，包括状态跟踪、算术推理和一般问答。为了便于未来使用，我们发布了一些通用但有效的草图，适用于许多任务，并开源了一个名为dclib的库，支持我们的草图感知解码器，作为https://github.com/eth-sri/lmql的一部分。"
}
{
  "title": "Human Alignment of Large Language Models through Online Preference Optimisation",
  "title_zh": "大型语言模型的人类对齐通过在线偏好优化",
  "abstract": "Ensuring alignment of language model's outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience. Thus, human alignment has been extensively studied recently and several methods such as Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper, our contribution is two-fold. First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD. This equivalence may seem surprising at first sight, since IPO is an offline method whereas Nash-MD is an online method using a preference model. However, this equivalence can be proven when we consider the online version of IPO, that is when both generations are sampled by the online policy and annotated by a trained preference model. Optimising the IPO loss with such a stream of data becomes then equivalent to finding the Nash equilibrium of the preference model through self-play. Building on this equivalence, we introduce the IPO-MD algorithm that generates data with a mixture policy (between the online and reference policy) similarly as the general Nash-MD algorithm. We compare online-IPO and IPO-MD to different online versions of existing losses on preference data such as DPO and SLiC on a summarisation task.",
  "abstract_zh": "确保语言模型输出与人类偏好的对齐对于保证有用、安全和愉快的用户体验至关重要。因此，人类对齐最近得到了广泛研究，并出现了多种方法，如基于人类反馈的强化学习（RLHF）、直接策略优化（DPO）和序列似然校准（SLiC）。在本文中，我们的贡献有两个方面。首先，我们展示了两种最近的对齐方法，即身份策略优化（IPO）和纳什镜像下降（Nash-MD）之间的等价性。其次，我们引入了一种IPO的推广，称为IPO-MD，它利用了Nash-MD提出的正则化采样方法。这种等价性乍看之下可能令人惊讶，因为IPO是一种离线方法，而Nash-MD是一种使用偏好模型的在线方法。然而，当我们考虑IPO的在线版本时，即当两个生成都是由在线策略采样并由训练好的偏好模型标注时，这种等价性可以得到证明。用这样的数据流优化IPO损失变得等同于通过自我对弈寻找偏好模型的纳什均衡。在此等价性基础上，我们引入了IPO-MD算法，该算法生成具有混合策略（在在线策略和参考策略之间）的数据，类似于一般的Nash-MD算法。我们将在线-IPO和IPO-MD与现有偏好数据的不同在线版本的损失（如DPO和SLiC）进行比较，应用于摘要任务。"
}
{
  "title": "Whispering Experts: Neural Interventions for Toxicity Mitigation in Language Models",
  "title_zh": "低语专家：神经干预在语言模型中减轻毒性",
  "abstract": "An important issue with Large Language Models (LLMs) is their undesired ability to generate toxic language. In this work, we show that the neurons responsible for toxicity can be determined by their power to discriminate toxic sentences, and that toxic language can be mitigated by reducing their activation levels proportionally to this power. We propose AUROC adaptation (AurA), an intervention that can be applied to any pre-trained LLM to mitigate toxicity. As the intervention is proportional to the ability of each neuron to discriminate toxic content, it is free of any model-dependent hyperparameters. We show that AurA can achieve up to $2.2\\times$ reduction in toxicity with only a $0.72$ perplexity increase. We also show that AurA is effective with models of different scale (from 1.5B to 40B parameters), and its effectiveness in mitigating toxic language, while preserving common-sense zero-shot abilities, holds across all scales. AurA can be combined with pre-prompting strategies, boosting its average mitigation potential from $1.28\\times$ to $2.35\\times$. Moreover, AurA can counteract adversarial pre-prompts that maliciously elicit toxic content, making it an effective method for deploying safer and less toxic models.",
  "abstract_zh": "大型语言模型（LLMs）一个重要问题是它们生成有毒语言的意外能力。在这项工作中，我们展示了负责毒性的神经元可以通过其区分有毒句子的能力来确定，并且通过相应减少其激活水平，可以减轻有毒语言。我们提出了AUROC适应（AurA），这是一种可以应用于任何预训练LLM以减轻毒性的干预措施。由于该干预与每个神经元区分有毒内容的能力成正比，因此不依赖于任何模型特定的超参数。我们展示了AurA可以在仅增加$0.72$困惑度的情况下，实现高达$2.2\\times$的毒性减少。我们还表明，AurA在不同规模的模型（从15亿到400亿参数）中都是有效的，并且在减轻有毒语言的同时保持常识零-shot能力，在所有规模中均有效。AurA可以与预提示策略结合，将其平均减轻潜力从$1.28\\times$提升至$2.35\\times$。此外，AurA可以抵消恶意引发有毒内容的对抗性预提示，使其成为部署更安全、毒性更低模型的有效方法。"
}
{
  "title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
  "title_zh": "OpenMoE：开放混合专家语言模型的早期尝试",
  "abstract": "To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development. One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped. Finally, we rethink our design based on the above-mentioned observations and analysis. To facilitate future MoE LLM development, we propose potential strategies for mitigating the issues we found and further improving off-the-shelf MoE LLM designs.",
  "abstract_zh": "为了帮助开源社区更好地理解基于混合专家（MoE）的超大语言模型（LLMs），我们训练并发布了OpenMoE，这是一系列完全开源且可复现的仅解码器MoE LLM，参数范围从6.5亿到340亿，并在超过1万亿个标记上进行训练。我们的研究确认，基于MoE的LLM在成本效益方面比稠密LLM更具优势，突显了未来LLM开发的潜在有效性。本研究的另一个重要贡献是对我们OpenMoE模型中路由机制的深入分析，得出了三个重要发现：上下文独立专业化、早期路由学习和末尾丢弃。我们发现MoE模型中的路由决策主要基于标记ID，与上下文的相关性较小。标记到专家的分配在预训练阶段早期确定，并保持大致不变。这种不完美的路由可能导致性能下降，特别是在多轮对话等顺序任务中，序列中后出现的标记更可能被丢弃。最后，我们根据上述观察和分析重新思考我们的设计。为了促进未来MoE LLM的发展，我们提出了潜在策略，以减轻我们发现的问题并进一步改善现成的MoE LLM设计。"
}
{
  "title": "Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization",
  "title_zh": "标题：通过头部剪枝和注意力归一化防御预训练语言模型的后门攻击",
  "abstract": "Pre-trained language models (PLMs) are commonly used for various downstream natural language processing tasks via fine-tuning. However, recent studies have demonstrated that PLMs are vulnerable to backdoor attacks, which can mislabel poisoned samples to target outputs even after a vanilla fine-tuning process. The key challenge for defending against the backdoored PLMs is that end users who adopt the PLMs for their downstream tasks usually do not have any knowledge about the attacking strategies, such as triggers. To tackle this challenge, in this work, we propose a backdoor mitigation approach, PURE, via head pruning and normalization of attention weights. The idea is to prune the attention heads that are potentially affected by poisoned texts with only clean texts on hand and then further normalize the weights of remaining attention heads to mitigate the backdoor impacts. We conduct experiments to defend against various backdoor attacks on the classification task. The experimental results show the effectiveness of PURE in lowering the attack success rate without sacrificing the performance on clean texts.",
  "abstract_zh": "摘要：预训练语言模型（PLMs）通常通过微调用于各种下游自然语言处理任务。然而，最近的研究表明，PLMs容易受到后门攻击，即使在普通的微调过程中，也可能将被污染的样本错误标记为目标输出。防御后门PLMs的关键挑战在于，采用PLMs进行下游任务的最终用户通常对攻击策略（如触发器）没有任何了解。为了解决这一挑战，本文提出了一种后门缓解方法PURE，通过头部剪枝和注意力权重的归一化。其思路是仅使用干净文本修剪可能受到污染文本影响的注意力头，然后进一步归一化剩余注意力头的权重，以减轻后门影响。我们进行了实验，以防御分类任务中的各种后门攻击。实验结果表明，PURE在降低攻击成功率的同时，不牺牲对干净文本的性能。"
}
{
  "title": "Token-level Direct Preference Optimization",
  "title_zh": "标记级直接偏好优化",
  "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) is essential to align them with human values and intentions. This process often utilizes methods like pairwise comparisons and KL divergence against a reference LLM, focusing on the evaluation of full answers generated by the models. However, the generation of these responses occurs in a token level, following a sequential, auto-regressive fashion. In this paper, we introduce Token-level Direct Preference Optimization (TDPO), a novel approach to align LLMs with human preferences by optimizing policy at the token level. Unlike previous methods, which face challenges in divergence efficiency, TDPO integrates forward KL divergence constraints for each token, improving alignment and diversity. Utilizing the Bradley-Terry model for a token-based reward system, our method enhances the regulation of KL divergence, while preserving simplicity without the need for explicit reward modeling. Experimental results across various text tasks demonstrate TDPO’s superior performance in balancing alignment with generation diversity. Notably, fine-tuning with TDPO strikes a better balance than DPO in the controlled sentiment generation and single-turn dialogue datasets, and significantly improves the quality of generated responses compared to both DPO and PPO-based RLHF methods.",
  "abstract_zh": "微调预训练的大型语言模型（LLMs）对于使其与人类价值观和意图对齐至关重要。这个过程通常利用成对比较和相对于参考LLM的KL散度等方法，重点评估模型生成的完整答案。然而，这些响应的生成是在标记级别上进行的，遵循顺序自回归的方式。本文介绍了标记级直接偏好优化（TDPO），这是一种通过在标记级别优化策略来使LLM与人类偏好对齐的新方法。与之前的方法不同，TDPO在每个标记上集成了前向KL散度约束，从而提高了对齐性和多样性。利用Bradley-Terry模型构建基于标记的奖励系统，我们的方法增强了KL散度的调节，同时保持了简单性，无需显式的奖励建模。各种文本任务的实验结果表明，TDPO在平衡对齐与生成多样性方面表现优越。值得注意的是，在受控情感生成和单轮对话数据集上，使用TDPO进行微调比DPO更好地平衡了生成质量，并显著提高了生成响应的质量，相较于DPO和基于PPO的RLHF方法。"
}
{
  "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty",
  "title_zh": "EAGLE：推测采样需要重新思考特征不确定性",
  "abstract": "Autoregressive decoding makes the inference of Large Language Models (LLMs) time-consuming. In this paper, we reconsider speculative sampling and derive two key observations. Firstly, autoregression at the feature (second-to-top-layer) level is more straightforward than at the token level. Secondly, the inherent uncertainty in feature (second-to-top-layer) level autoregression constrains its performance. Based on these insights, we introduce EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), a simple yet highly efficient speculative sampling framework. By incorporating a token sequence advanced by one time step, EAGLE effectively resolves the uncertainty, enabling precise second-to-top-layer feature prediction with minimal overhead. We conducted comprehensive evaluations of EAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE model Mixtral 8x7B Instruct, and tasks in dialogue, code generation, mathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE achieved a latency speedup ratio of **2.7x-3.5x**, doubled throughput, while maintaining the distribution of the generated text.",
  "abstract_zh": "自回归解码使大型语言模型（LLMs）的推理变得耗时。本文重新审视推测采样，并得出两个关键观察。首先，特征（倒数第二层）级别的自回归比令牌级别的自回归更为直接。其次，特征（倒数第二层）级别自回归中的固有不确定性限制了其性能。基于这些见解，我们引入了EAGLE（更高语言模型效率的外推算法），这是一个简单但高效的推测采样框架。通过结合一个时间步长提前的令牌序列，EAGLE有效解决了不确定性，使得以最小开销进行精确的倒数第二层特征预测成为可能。我们对EAGLE进行了全面评估，包括Vicuna和LLaMA2-Chat系列的所有模型、MoE模型Mixtral 8x7B Instruct，以及对话、代码生成、数学推理和指令遵循等任务。对于LLaMA2-Chat 70B，EAGLE实现了**2.7x-3.5x**的延迟加速比，吞吐量翻倍，同时保持生成文本的分布。"
}
{
  "title": "PID: Prompt-Independent Data Protection Against Latent Diffusion Models",
  "title_zh": "PID：针对潜在扩散模型的提示无关数据保护",
  "abstract": "The few-shot fine-tuning of Latent Diffusion Models (LDMs) has enabled them to grasp new concepts from a limited number of images. However, given the vast amount of personal images accessible online, this capability raises critical concerns about civil privacy. While several previous defense methods have been developed to prevent such misuse of LDMs, they typically assume that the textual prompts used by data protectors exactly match those employed by data exploiters. In this paper, we first empirically demonstrate that breaking this assumption, i.e., in cases where discrepancies exist between the textual conditions used by protectors and exploiters, could substantially reduces the effectiveness of these defenses. Furthermore, considering the visual encoder's independence from textual prompts, we delve into the visual encoder and thoroughly investigate how manipulating the visual encoder affects the few-shot fine-tuning process of LDMs. Drawing on these insights, we propose a simple yet effective method called Prompt-Independent Defense (PID) to safeguard privacy against LDMs. We show that PID can act as a strong privacy shield on its own while requiring significantly less computational power. We believe our studies, along with the comprehensive understanding and new defense method, provide a notable advance toward reliable data protection against LDMs.",
  "abstract_zh": "潜在扩散模型（LDMs）的少量微调使其能够从有限数量的图像中掌握新概念。然而，考虑到网上可获取的大量个人图像，这种能力引发了对公民隐私的重大担忧。虽然之前已经开发了几种防御方法来防止LDMs的这种误用，但它们通常假设数据保护者使用的文本提示与数据利用者使用的完全匹配。本文首先通过实证研究表明，打破这一假设，即保护者和利用者使用的文本条件之间存在差异，可能会显著降低这些防御的有效性。此外，考虑到视觉编码器与文本提示的独立性，我们深入探讨视觉编码器，并彻底研究操控视觉编码器如何影响LDMs的少量微调过程。基于这些见解，我们提出了一种简单而有效的方法，称为提示无关防御（PID），以保护隐私免受LDMs的影响。我们表明，PID可以作为强大的隐私保护屏障，同时所需的计算能力显著降低。我们相信我们的研究以及对新防御方法的全面理解，标志着朝着可靠的数据保护对抗LDMs的重要进展。"
}
{
  "title": "MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation",
  "title_zh": "MLAgentBench：评估语言代理在机器学习实验中的表现",
  "abstract": "A central aspect of machine learning research is experimentation, the process of designing and running experiments, analyzing the results, and iterating towards some positive outcome (e.g., improving accuracy). Could agents driven by powerful language models perform machine learning experimentation effectively? To answer this question, we introduce MLAgentBench, a suite of 13 tasks ranging from improving model performance on CIFAR-10 to recent research problems like BabyLM. For each task, an agent can perform actions like reading/writing files, executing code, and inspecting outputs. We then construct an agent that can perform ML experimentation based on ReAct framework. We benchmark agents based on Claude v1.0, Claude v2.1, Claude v3 Opus, GPT-4, GPT-4-turbo, Gemini-Pro, and Mixtral and find that a Claude v3 Opus agent is the best in terms of success rate. It can build compelling ML models over many tasks in MLAgentBench with 37.5% average success rate. Our agents also display highly interpretable plans and actions. However, the success rates vary considerably; they span from 100% on well-established older datasets to as low as 0% on recent Kaggle challenges created potentially after the underlying LM was trained. Finally, we identify several key challenges for LM-based agents such as long-term planning and reducing hallucination.",
  "abstract_zh": "机器学习研究的一个核心方面是实验，即设计和运行实验、分析结果以及迭代以实现某种积极结果（例如，提高准确性）。强大的语言模型驱动的代理能否有效地进行机器学习实验？为了解答这个问题，我们介绍了MLAgentBench，这是一个包含13个任务的套件，任务范围从提高CIFAR-10上的模型性能到最近的研究问题如BabyLM。对于每个任务，代理可以执行读取/写入文件、执行代码和检查输出等操作。然后，我们基于ReAct框架构建了一个可以进行机器学习实验的代理。我们对基于Claude v1.0、Claude v2.1、Claude v3 Opus、GPT-4、GPT-4-turbo、Gemini-Pro和Mixtral的代理进行了基准测试，发现Claude v3 Opus代理在成功率方面表现最佳。它在MLAgentBench的多个任务中可以构建引人注目的机器学习模型，平均成功率为37.5%。我们的代理还展示了高度可解释的计划和行动。然而，成功率差异显著；在成熟的旧数据集上成功率高达100%，而在可能在基础语言模型训练后创建的最近Kaggle挑战中，成功率低至0%。最后，我们确定了基于语言模型的代理面临的几个关键挑战，如长期规划和减少幻觉。"
}
{
  "title": "Sparse is Enough in Fine-tuning Pre-trained Large Language Models",
  "title_zh": "稀疏足以在微调预训练的大型语言模型中",
  "abstract": "With the prevalence of pre-training-fine-tuning paradigm, how to efficiently adapt the pre-trained model to the downstream tasks has been an intriguing issue. $\\textbf{P}$arameter-$\\textbf{E}$fficient $\\textbf{F}$ine-$\\textbf{T}$uning(PEFT) methods have been proposed  for low-cost adaptation. Although PEFT has demonstrated effectiveness and been widely applied, the underlying principles are still unclear. In this paper, we adopt the PAC-Bayesian generalization error bound, viewing pre-training as a shift of prior distribution which leads to a tighter bound for generalization error. We validate this shift from the perspectives of oscillations in the loss landscape and the quasi-sparsity in gradient distribution. Based on this, we propose a gradient-based sparse fine-tuning algorithm, named $\\textbf{S}$parse $\\textbf{I}$ncrement $\\textbf{F}$ine-$\\textbf{T}$uning(SIFT), and validate its effectiveness on a range of tasks including the GLUE Benchmark and Instruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/.",
  "abstract_zh": "随着预训练-微调范式的普及，如何高效地将预训练模型适应于下游任务成为一个引人关注的问题。提出了参数高效微调（PEFT）方法以实现低成本适应。尽管PEFT已显示出有效性并被广泛应用，但其基本原理仍不清楚。本文采用PAC-贝叶斯泛化误差界限，将预训练视为先验分布的转变，从而导致更紧的泛化误差界限。我们从损失景观中的振荡和梯度分布中的准稀疏性两个角度验证了这种转变。在此基础上，我们提出了一种基于梯度的稀疏微调算法，命名为稀疏增量微调（SIFT），并在包括GLUE基准和指令微调在内的多个任务上验证了其有效性。代码可在https://github.com/song-wx/SIFT/获取。"
}
{
  "title": "SqueezeLLM: Dense-and-Sparse Quantization",
  "title_zh": "标题：SqueezeLLM：稠密与稀疏量化",
  "abstract": "Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM.",
  "abstract_zh": "摘要：生成大型语言模型（LLMs）在广泛任务中表现出色。然而，由于其前所未有的资源需求，部署这些模型进行推理一直是一个重大挑战。这迫使现有的部署框架使用多GPU推理管道，这通常复杂且成本高昂，或者使用更小且性能较差的模型。在这项工作中，我们证明了生成推理中LLMs的主要瓶颈是内存带宽，而不是计算，特别是在单批次推理时。尽管量化通过以降低精度表示权重而成为一种有前景的解决方案，但以往的努力往往导致显著的性能下降。为了解决这个问题，我们引入了SqueezeLLM，一个后训练量化框架，它不仅能够实现高达3位的无损压缩，还能在相同内存限制下实现更高的量化性能。我们的框架结合了两个新颖的想法：（i）基于灵敏度的非均匀量化，根据二阶信息搜索最佳比特精度分配；（ii）稠密与稀疏分解，以高效的稀疏格式存储异常值和敏感权重值。当应用于LLaMA模型时，我们的3位量化显著减少了与FP16基线相比的困惑度差距，最高可达2.1倍，相较于具有相同内存要求的最先进方法。此外，在A6000 GPU上部署时，我们的量化模型相比基线实现了最高2.3倍的加速。我们的代码可在https://github.com/SqueezeAILab/SqueezeLLM获取。"
}
{
  "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
  "title_zh": "超越Chinchilla最优：考虑语言模型扩展法则中的推理成本",
  "abstract": "Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand ($\\sim$1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.",
  "abstract_zh": "大型语言模型（LLM）扩展法则是经验公式，用于估计模型质量随参数数量和训练数据增加而变化。然而，这些公式，包括流行的Deepmind Chinchilla扩展法则，未考虑推理成本。我们修改Chinchilla扩展法则，以计算在给定质量和推理需求下训练和部署模型的最佳LLM参数数量和预训练数据大小。我们在计算预算和现实成本方面进行分析，发现期望合理大推理需求（约10亿请求）的LLM研究人员应训练比Chinchilla最优模型更小且训练时间更长的模型。此外，我们训练了47个不同大小和参数数量的模型，以验证我们的公式，发现随着每个参数的标记数量扩展到极端范围（高达10,000），模型质量持续提高。最后，我们剖析了拟合Chinchilla扩展法则系数所用的过程，发现仅从在典型标记/参数比率下收集的数据开发扩展法则会高估在这些极端范围内额外标记的影响。"
}
{
  "title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs",
  "title_zh": "朝着模块化大语言模型的方向，通过构建和重用LoRA库",
  "abstract": "Given the increasing number of parameter-efficient adapters of large language models (LLMs), how can we reuse them to improve LLM performance on new tasks? We study how to best build a *library* of adapters given multi-task data and devise techniques for both *zero-shot* and *supervised* task generalization through *routing* in such library. We benchmark existing approaches to build this library and introduce model-based clustering, $\\texttt{MBC}$, a method that groups tasks based on the similarity of their adapter parameters, indirectly optimizing for transfer across the multi-task dataset. In order to reuse the library, we present a novel zero-shot routing mechanism, $\\texttt{Arrow}$, which enables dynamic selection of the most relevant adapters for new inputs without the need for retraining. We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying that MBC-based adapters and Arrow routing lead to superior generalization to new tasks. Thus, we make steps towards creating modular, adaptable LLMs that can match or outperform traditional joint training.",
  "abstract_zh": "鉴于大型语言模型（LLMs）中参数高效适配器数量的增加，我们如何重用它们以提高LLM在新任务上的性能？我们研究如何根据多任务数据最佳构建适配器的*库*，并设计了通过该库进行*零样本*和*监督*任务泛化的技术。我们基准测试了现有的构建此库的方法，并引入了基于模型的聚类方法$\\texttt{MBC}$，该方法根据适配器参数的相似性对任务进行分组，间接优化多任务数据集的迁移。为了重用该库，我们提出了一种新颖的零样本路由机制$\\texttt{Arrow}$，它能够动态选择与新输入最相关的适配器，而无需重新训练。我们在多个LLM（如Phi-2和Mistral）上对一系列保留任务进行了实验，验证了基于MBC的适配器和Arrow路由在新任务上的优越泛化能力。因此，我们朝着创建能够匹配或超越传统联合训练的模块化、可适应的LLM迈出了步伐。"
}
{
  "title": "Self-Rewarding Language Models",
  "title_zh": "自我奖励语言模型",
  "abstract": "We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these reward models require additional human preferences data to further improve.In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training, not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still to explore, this work opens the door to the possibility of models that can continually improve in both axes.",
  "abstract_zh": "我们认为，为了实现超人类代理，未来的模型需要超人类反馈以提供足够的训练信号。目前的方法通常从人类偏好中训练奖励模型，这可能会受到人类性能水平的瓶颈限制，此外这些奖励模型还需要额外的人类偏好数据以进一步改进。在本研究中，我们研究了自我奖励语言模型，其中语言模型本身通过LLM作为评判者的提示在训练过程中提供自己的奖励。我们展示了在迭代DPO训练期间，不仅指令遵循能力得到了提高，而且自我提供高质量奖励的能力也得到了增强。对Llama 2 70B进行三次迭代的微调，产生了一个在AlpacaEval 2.0排行榜上超越许多现有系统的模型，包括Claude 2、Gemini Pro和GPT-4 0613。尽管仍有许多未探索的领域，但这项工作为模型在两个方向上持续改进的可能性打开了大门。"
}
{
  "title": "Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning",
  "title_zh": "标题：长响应更有利于对齐：一种简单但难以超越的指令微调基线",
  "abstract": "There is a consensus that instruction fine-tuning of LLMs requires high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-of-the-art methods for selecting such high-quality examples, either via manual curation or using GPT-3.5-Turbo as a quality scorer. We show that the extremely simple baseline of selecting the 1,000 instructions with longest responses---that intuitively contain more learnable information and are harder to overfit---from standard datasets can consistently outperform these sophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining competitive on the Open LLM benchmarks that test factual knowledge. We demonstrate this for several LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B-v0.1) and datasets (Alpaca-52k, Evol-Instruct-70k). In addition, a lightweight refinement of such long instructions can further improve the abilities of the fine-tuned LLMs, and allows us to obtain competitive results on MT-Bench and the 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0, while training on only 1,000 examples and no extra preference data. We also conduct a thorough analysis of our models to ensure that their enhanced performance is not simply due to GPT-4's preference for longer responses. Overall, our findings suggest that fine-tuning on the longest responses should be the default baseline for any work on instruction fine-tuning. We provide our code in this GitHub repository.",
  "abstract_zh": "摘要：大家普遍认为，大型语言模型的指令微调需要高质量的数据，但这些数据究竟是什么？LIMA（NeurIPS 2023）和AlpaGasus（ICLR 2024）是选择此类高质量示例的最先进方法，采用手动策划或使用GPT-3.5-Turbo作为质量评分器。我们展示了从标准数据集中选择响应最长的1,000条指令这一极其简单的基线——直观上包含更多可学习信息且更难过拟合——可以在GPT-4和PaLM-2的评判下始终优于这些复杂的方法，同时在测试事实知识的开放LLM基准上仍具竞争力。我们对多个LLM（Llama-2-7B、Llama-2-13B、Mistral-7B-v0.1）和数据集（Alpaca-52k、Evol-Instruct-70k）进行了验证。此外，对这些长指令的轻量级优化可以进一步提升微调后LLM的能力，使我们在MT-Bench上获得竞争性结果，并在AlpacaEval 2.0中获得基于Llama-2-7B的第二高排名模型，而仅使用1,000个示例且没有额外的偏好数据。我们还对模型进行了全面分析，以确保其增强的性能并非仅仅由于GPT-4对较长响应的偏好。总体而言，我们的发现表明，基于最长响应的微调应成为任何指令微调工作的默认基线。我们在此GitHub库中提供了我们的代码。"
}
