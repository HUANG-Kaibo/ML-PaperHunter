{
  "title": "Stress-Testing Capability Elicitation With Password-Locked Models",
  "title_zh": "标题：使用密码锁定模型进行能力压力测试",
  "abstract": "To determine the safety of large language models (LLMs), AI developers must be able to assess their dangerous capabilities. But simple prompting strategies often fail to elicit an LLM’s full capabilities. One way to elicit capabilities more robustly is to fine-tune the LLM to complete the task. In this paper, we investigate the conditions under which fine-tuning-based elicitation suffices to elicit capabilities. To do this, we introduce password-locked models, LLMs fine-tuned such that some of their capabilities are deliberately hidden. Specifically, these LLMs are trained to exhibit these capabilities only when a password is present in the prompt, and to imitate a much weaker LLM otherwise. Password-locked models enable a novel method of evaluating capabilities elicitation methods, by testing whether these password-locked capabilities can be elicited without using the password. We find that a few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities. More surprisingly, fine-tuning can elicit other capabilities that have been locked using the same password, or even different passwords. Furthermore, when only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities. Overall, our findings suggest that fine-tuning is an effective method of eliciting hidden capabilities of current models but may be unreliable when high-quality demonstrations are not available, e.g., as may be the case when models’ (hidden) capabilities exceed those of human demonstrators.",
  "abstract_zh": "摘要：为了确定大型语言模型（LLM）的安全性，AI开发者必须能够评估其危险能力。然而，简单的提示策略往往无法引出LLM的全部能力。更稳健地引出能力的一种方法是微调LLM以完成任务。在本文中，我们研究了基于微调的引出方法在何种条件下足以引出能力。为此，我们引入了密码锁定模型，这些LLM经过微调，使其某些能力被故意隐藏。具体而言，这些LLM被训练成仅在提示中存在密码时才展示这些能力，否则则模仿一个弱得多的LLM。密码锁定模型通过测试是否可以在不使用密码的情况下引出这些密码锁定的能力，提供了一种新的能力引出方法评估方式。我们发现，少量高质量的示例通常足以完全引出密码锁定的能力。更令人惊讶的是，微调可以引出使用相同密码甚至不同密码锁定的其他能力。此外，当只有评估而没有示例可用时，诸如强化学习的方法仍然能够引出能力。总体而言，我们的研究结果表明，微调是引出当前模型隐藏能力的有效方法，但在缺乏高质量示例时可能不可靠，例如，当模型的（隐藏）能力超过人类示例者时可能会出现这种情况。"
}
{
  "title": "IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation",
  "title_zh": "IDGen：用于LLM评估的项目区分诱导提示生成",
  "abstract": "As Large Language Models (LLMs) become more capable of handling increasingly complex tasks, the evaluation set must keep pace with these advancements to ensure it remains sufficiently discriminative. Item Discrimination (ID) theory, which is widely used in educational assessment, measures the ability of individual test items to differentiate between high and low performers. Inspired by this theory, we propose an ID-induced prompt synthesis framework for evaluating LLMs so that the evaluation set continually updates and refines according to model abilities. \nOur data synthesis framework prioritizes both breadth and specificity. It can generate prompts that comprehensively evaluate the capabilities of LLMs while revealing meaningful performance differences between models, allowing for effective discrimination of their relative strengths and weaknesses across various tasks and domains.\nTo produce high-quality data, we incorporate a self-correct mechanism into our generalization framework and develop two models to predict prompt discrimination and difficulty score to facilitate our data synthesis framework, contributing valuable tools to evaluation data synthesis research. We apply our generated data to evaluate five SOTA models. Our data achieves an average score of 51.92, accompanied by a variance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and WizardLM) obtain an average score exceeding 67, with a variance below 3.2.\nThe results demonstrate that the data generated by our framework is more challenging and discriminative compared to previous works.\nWe will release a dataset of over 3,000 carefully crafted prompts to facilitate evaluation research of LLMs.",
  "abstract_zh": "随着大型语言模型（LLM）在处理日益复杂的任务方面变得更加高效，评估集必须跟上这些进步，以确保其保持足够的区分能力。项目区分（ID）理论广泛应用于教育评估中，衡量单个测试项目区分高低表现者的能力。受此理论启发，我们提出了一种ID诱导的提示合成框架，用于评估LLM，以便评估集根据模型能力不断更新和完善。我们的数据合成框架优先考虑广度和特异性。它可以生成提示，全面评估LLM的能力，同时揭示模型之间有意义的性能差异，从而有效区分它们在各种任务和领域中的相对优势和劣势。为了生成高质量的数据，我们在我们的泛化框架中加入了自我纠正机制，并开发了两个模型来预测提示区分和难度评分，以促进我们的数据合成框架，为评估数据合成研究贡献了有价值的工具。我们将生成的数据应用于评估五个SOTA模型。我们的数据平均得分为51.92，方差为10.06。相比之下，以往的工作（即SELF-INSTRUCT和WizardLM）获得的平均得分超过67，方差低于3.2。结果表明，与以往的工作相比，我们框架生成的数据更具挑战性和区分性。我们将发布一个包含超过3,000个精心设计的提示的数据集，以促进LLM评估研究。"
}
{
  "title": "FLAME : Factuality-Aware Alignment for Large Language Models",
  "title_zh": "标题：FLAME：面向事实性的对齐方法用于大型语言模型",
  "abstract": "Alignment is a procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. \nWe have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e., *hallucination*). \nIn this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps: supervised fine-tuning (SFT) and reinforcement learning (RL).\nIn particular, we find that training the LLM on new or unfamiliar knowledge can encourage hallucination.\nThis makes SFT less factual as it trains on human-labeled data that may be novel to the LLM. \nFurthermore, reward functions used in standard RL often inadequately capture factuality and favor longer and more detailed responses, which inadvertently promote hallucination.\nBased on these observations, we propose *FactuaLity-aware AlignMEnt*, comprised of *factuality-aware SFT* and *factuality-aware RL* through direct preference optimization. \nExperiments show that our proposed *FLAME* guides LLMs to output more factual responses while maintaining their instruction-following capability.",
  "abstract_zh": "摘要：对齐是一种微调预训练大型语言模型（LLMs）以遵循自然语言指令并作为有用的人工智能助手的过程。然而，我们观察到，传统的对齐过程未能提高LLMs的事实准确性，反而常常导致生成更多错误事实（即*幻觉*）。在本文中，我们研究如何使LLM的对齐过程更加符合事实，首先识别在两个对齐步骤中导致幻觉的因素：监督微调（SFT）和强化学习（RL）。特别是，我们发现训练LLM以新的或不熟悉的知识会促进幻觉。这使得SFT在训练人类标记数据时不够符合事实，因为这些数据可能对LLM来说是新的。此外，标准RL中使用的奖励函数往往不能充分捕捉事实性，并倾向于更长和更详细的响应，这无意中促进了幻觉。基于这些观察，我们提出了*面向事实性的对齐方法*，包括通过直接偏好优化的*面向事实性的SFT*和*面向事实性的RL*。实验表明，我们提出的*FLAME*引导LLMs输出更符合事实的响应，同时保持其遵循指令的能力。"
}
{
  "title": "Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses",
  "title_zh": "改进的少样本越狱技术可以规避对齐语言模型及其防御",
  "abstract": "Recently, Anil et al. (2024) show that many-shot (up to hundreds of) demonstrations can jailbreak state-of-the-art LLMs by exploiting their long-context capability. Nevertheless, is it possible to use few-shot demonstrations to efficiently jailbreak LLMs within limited context sizes? While the vanilla few-shot jailbreaking may be inefficient, we propose improved techniques such as injecting special system tokens like [/INST] and employing demo-level random search from a collected demo pool. These simple techniques result in surprisingly effective jailbreaking against aligned LLMs (even with advanced defenses). For example, our method achieves >80% (mostly >95%) ASRs on Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are enhanced by strong defenses such as perplexity detection and/or SmoothLLM, which is challenging for suffix-based jailbreaking. In addition, we conduct comprehensive and elaborate (e.g., making sure to use correct system prompts) evaluations against other aligned LLMs and advanced defenses, where our method consistently achieves nearly 100% ASRs. Our code is available at https://github.com/sail-sg/I-FSJ.",
  "abstract_zh": "最近，Anil 等人（2024）表明，通过利用长上下文能力，多样本（多达数百个）演示可以越狱最先进的LLM。然而，是否可以在有限的上下文大小内使用少样本演示来高效地越狱LLM？虽然普通的少样本越狱可能效率不高，但我们提出了改进的技术，例如注入特殊的系统标记如[/INST]，并从收集的演示池中进行演示级随机搜索。这些简单的技术在对抗对齐的LLM（即使有高级防御）时出人意料地有效。例如，我们的方法在Llama-2-7B和Llama-3-8B上实现了>80%（大多数>95%）的ASR，无需多次重启，即使模型通过困惑度检测和/或SmoothLLM等强大防御增强，这对于基于后缀的越狱来说是具有挑战性的。此外，我们对其他对齐的LLM和高级防御进行了全面和精细的评估（例如，确保使用正确的系统提示），我们的方法始终实现了接近100%的ASR。我们的代码可在https://github.com/sail-sg/I-FSJ获得。"
}
{
  "title": "Fair Wasserstein Coresets",
  "title_zh": "公平Wasserstein核心集",
  "abstract": "Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. While current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples, their impact on downstream learning processes has yet to be explored.  In this work, we present fair Wasserstein coresets ($\\texttt{FWC}$), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. $\\texttt{FWC}$ uses an efficient majority minimization algorithm to minimize the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of $\\texttt{FWC}$ is equivalent to Lloyd's algorithm for k-medians and k-means clustering. Experiments conducted on both synthetic and real datasets show that $\\texttt{FWC}$:  (i) achieves a competitive fairness-performance tradeoff in downstream models compared to existing approaches, (ii) improves downstream fairness when added to the existing training data and (iii) can be used to reduce biases in predictions from large language models (GPT-3.5 and GPT-4).",
  "abstract_zh": "数据蒸馏和核心集已成为生成较小代表性样本集以处理大规模数据集的流行方法。同时，机器学习越来越多地应用于社会层面的决策过程，这使得建模者必须解决数据中存在的对子群体的固有偏见。当前的方法主要通过优化相对于原始样本的局部属性来创建公平的合成代表性样本，但其对下游学习过程的影响尚未被探索。在这项工作中，我们提出了公平Wasserstein核心集（$\\texttt{FWC}$），这是一种新颖的核心集方法，生成公平的合成代表性样本以及样本级权重，用于下游学习任务。$\\texttt{FWC}$使用高效的多数最小化算法来最小化原始数据集与加权合成样本之间的Wasserstein距离，同时强制实现人口统计平等。我们表明，$\\texttt{FWC}$的无约束版本等价于k-中值和k-均值聚类的Lloyd算法。在合成和真实数据集上进行的实验表明，$\\texttt{FWC}$：（i）在下游模型中实现了与现有方法相比具有竞争力的公平性-性能权衡，（ii）在添加到现有训练数据时提高了下游公平性，（iii）可用于减少大型语言模型（GPT-3.5和GPT-4）预测中的偏见。"
}
{
  "title": "From Trojan Horses to Castle Walls: Unveiling Bilateral Data Poisoning Effects in Diffusion Models",
  "title_zh": "从特洛伊木马到城墙：揭示扩散模型中的双向数据投毒效应",
  "abstract": "While state-of-the-art diffusion models (DMs) excel in image generation, concerns regarding their security persist. Earlier research highlighted DMs' vulnerability to data poisoning attacks, but these studies placed stricter requirements than conventional methods like 'BadNets' in image classification. This is because the art necessitates modifications to the diffusion training and sampling procedures. Unlike the prior work, we investigate whether BadNets-like data poisoning methods can directly degrade the generation by DMs. In other words, if only the training dataset is contaminated (without manipulating the diffusion process), how will this affect the performance of learned DMs? In this setting, we uncover bilateral data poisoning effects that not only serve an adversarial purpose (compromising the functionality of DMs) but also offer a defensive advantage (which can be leveraged for defense in classification tasks against poisoning attacks). We show that a BadNets-like data poisoning attack remains effective in DMs for producing incorrect images (misaligned with the intended text conditions). Meanwhile, poisoned DMs exhibit an increased ratio of triggers, a phenomenon we refer to as 'trigger amplification', among the generated images. This insight can be then used to enhance the detection of poisoned training data. In addition, even under a low poisoning ratio, studying the poisoning effects of DMs is also valuable for designing robust image classifiers against such attacks. Last but not least, we establish a meaningful linkage between data poisoning and the phenomenon of data replications by exploring DMs' inherent data memorization tendencies. Code is available at https://github.com/OPTML-Group/BiBadDiff.",
  "abstract_zh": "尽管最先进的扩散模型（DMs）在图像生成方面表现出色，但其安全性问题仍然存在。早期研究强调了DMs易受数据投毒攻击的脆弱性，但这些研究比传统方法如图像分类中的“BadNets”提出了更严格的要求。这是因为艺术需要对扩散训练和采样过程进行修改。与之前的工作不同，我们研究了类似BadNets的数据投毒方法是否可以直接降低DMs的生成质量。换句话说，如果仅训练数据集被污染（不操纵扩散过程），这将如何影响学习到的DMs的性能？在这种情况下，我们揭示了双向数据投毒效应，这不仅具有对抗性目的（损害DMs的功能），还提供了防御优势（可用于防御分类任务中的投毒攻击）。我们表明，类似BadNets的数据投毒攻击在DMs中仍然有效，会产生不正确的图像（与预期的文本条件不符）。同时，受污染的DMs在生成的图像中表现出更高的触发器比例，我们称之为“触发器放大”现象。这一见解可以用于增强对受污染训练数据的检测。此外，即使在低投毒比例下，研究DMs的投毒效应对于设计针对此类攻击的鲁棒图像分类器也具有价值。最后但并非最不重要的是，我们通过探索DMs固有的数据记忆倾向，建立了数据投毒与数据复制现象之间的有意义联系。代码可在https://github.com/OPTML-Group/BiBadDiff获得。"
}
{
  "title": "MAmmoTH2: Scaling Instructions from the Web",
  "title_zh": "标题：MAmmoTH2：从网络扩展指令",
  "abstract": "Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B’s (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data.",
  "abstract_zh": "摘要：指令微调提高了大型语言模型（LLMs）的推理能力，其中数据质量和可扩展性是关键因素。大多数指令微调数据来自人工众包或GPT-4蒸馏。我们提出了一种范式，从预训练的网络语料库中高效收集1000万条自然存在的指令数据，以增强LLM推理。我们的方法包括：（1）召回相关文档，（2）提取指令-响应对，以及（3）使用开源LLM优化提取的对。通过在此数据集上微调基础LLM，我们构建了MAmmoTH2模型，在推理基准测试中显著提高了性能。值得注意的是，MAmmoTH2-7B（Mistral）的性能在MATH上从11%提高到36.7%，在GSM8K上从36%提高到68.4%，而无需在任何领域内数据上进行训练。进一步在公共指令微调数据集上训练MAmmoTH2，产生了MAmmoTH2-Plus，在多个推理和聊天机器人基准测试中实现了最先进的性能。我们的工作展示了如何在没有昂贵的人类标注或GPT-4蒸馏的情况下收集大规模、高质量的指令数据，为构建更好的指令微调数据提供了新范式。"
}
{
  "title": "Aligning Large Language Models with Representation Editing: A Control Perspective",
  "title_zh": "标题：通过表示编辑对齐大型语言模型：一种控制视角",
  "abstract": "Aligning large language models (LLMs) with human objectives is crucial for real-world applications. However, fine-tuning LLMs for alignment often suffers from unstable training and requires substantial computing resources. Test-time alignment techniques, such as prompting and guided decoding, do not modify the underlying model, and their performance remains dependent on the original model's capabilities. To address these challenges, we propose aligning LLMs through representation editing. The core of our method is to view a pre-trained autoregressive LLM as a discrete-time stochastic dynamical system. To achieve alignment for specific objectives, we introduce external control signals into the state space of this language dynamical system. We train a value function directly on the hidden states according to the Bellman equation, enabling gradient-based optimization to obtain the optimal control signals at test time. Our experiments demonstrate that our method outperforms existing test-time alignment techniques while requiring significantly fewer resources compared to fine-tuning methods. Our code is available at [https://github.com/Lingkai-Kong/RE-Control](https://github.com/Lingkai-Kong/RE-Control).",
  "abstract_zh": "摘要：将大型语言模型（LLMs）与人类目标对齐对于实际应用至关重要。然而，为对齐而微调LLMs常常面临不稳定的训练过程，并需要大量的计算资源。测试时对齐技术，如提示和引导解码，不会修改底层模型，其性能依然依赖于原始模型的能力。为了解决这些挑战，我们提出通过表示编辑对齐LLMs。我们方法的核心是将预训练的自回归LLM视为离散时间随机动态系统。为了实现特定目标的对齐，我们在该语言动态系统的状态空间中引入外部控制信号。我们根据贝尔曼方程直接在隐藏状态上训练价值函数，从而在测试时通过基于梯度的优化获得最优控制信号。我们的实验表明，我们的方法在资源需求显著低于微调方法的情况下，优于现有的测试时对齐技术。我们的代码可在[https://github.com/Lingkai-Kong/RE-Control](https://github.com/Lingkai-Kong/RE-Control)获取。"
}
{
  "title": "Toward a Stable, Fair, and Comprehensive Evaluation of Object Hallucination in Large Vision-Language Models",
  "title_zh": "题目：迈向大型视觉语言模型中物体幻觉的稳定、公平和全面评估",
  "abstract": "Given different instructions, large vision-language models (LVLMs) exhibit different degrees of object hallucinations, posing a significant challenge to the evaluation of object hallucinations. Overcoming this challenge, existing object hallucination evaluation methods average the results obtained from a set of instructions. However, these methods fail to provide consistent evaluation across instruction sets that generate image descriptions of significantly different lengths. In this paper, we present the first systematic investigation of the effect of instructions on object hallucinations in LVLMs, with a specific focus on the role played by image description lengths. A valuable finding is that instructions indirectly affect hallucinations through the length of image descriptions. The longer the image description, the higher the object hallucination degree. Accordingly, we fit an informative length-hallucination curve, upon which a fine-grained evaluation framework named LeHaCE is introduced for evaluating object hallucinations at any given image description length. LeHaCE evaluates the object hallucination degree at a uniform image description length to mitigate the effect of description lengths, promoting stability and fairness. Moreover, LeHaCE incorporates the curve slope as an innovative hallucination evaluation metric, reflecting the extent to which the object hallucination degree is affected by the image description length, achieving a more comprehensive evaluation. Experimental results demonstrate that LeHaCE provides a more stable, fair, and comprehensive evaluation of object hallucinations in LVLMs compared to existing methods.",
  "abstract_zh": "摘要：在不同指令下，大型视觉语言模型（LVLMs）表现出不同程度的物体幻觉，这对物体幻觉的评估构成了重大挑战。为克服这一挑战，现有的物体幻觉评估方法对一组指令获得的结果进行平均。然而，这些方法未能在生成显著不同长度图像描述的指令集之间提供一致的评估。在本文中，我们首次系统地研究了指令对LVLMs中物体幻觉的影响，特别关注图像描述长度所起的作用。一个有价值的发现是，指令通过图像描述的长度间接影响幻觉。图像描述越长，物体幻觉程度越高。因此，我们拟合了一条信息丰富的长度-幻觉曲线，并在此基础上引入了一个名为LeHaCE的细粒度评估框架，用于评估任何给定图像描述长度下的物体幻觉。LeHaCE在统一的图像描述长度下评估物体幻觉程度，以减轻描述长度的影响，促进稳定性和公平性。此外，LeHaCE将曲线斜率作为创新的幻觉评估指标，反映出物体幻觉程度受图像描述长度影响的程度，实现了更全面的评估。实验结果表明，与现有方法相比，LeHaCE在LVLMs中提供了更稳定、公平和全面的物体幻觉评估。"
}
{
  "title": "Co-occurrence is not Factual Association in Language Models",
  "title_zh": "标题：共现不是语言模型中的事实关联",
  "abstract": "Pretrained language models can encode a large amount of knowledge and utilize it for various reasoning tasks, yet they can still struggle to learn novel factual knowledge effectively from finetuning on limited textual demonstrations. In this work, we show that the reason for this deficiency is that language models are biased to learn word co-occurrence statistics instead of true factual associations. We identify the differences between two forms of knowledge representation in language models: knowledge in the form of co-occurrence statistics is encoded in the middle layers of the transformer model and does not generalize well to reasoning scenarios beyond simple question answering, while true factual associations are encoded in the lower layers and can be freely utilized in various reasoning tasks. Based on these observations, we propose two strategies to improve the learning of factual associations in language models. We show that training on text with implicit rather than explicit factual associations can force the model to learn factual associations instead of co-occurrence statistics, significantly improving the generalization of newly learned knowledge. We also propose a simple training method to actively forget the learned co-occurrence statistics, which unblocks and enhances the learning of factual associations when training on plain narrative text. On both synthetic and real-world corpora, the two proposed strategies improve the generalization of the knowledge learned during finetuning to reasoning scenarios such as indirect and multi-hop question answering.",
  "abstract_zh": "摘要：预训练语言模型可以编码大量知识并在各种推理任务中加以利用，但在有限的文本示例微调中仍难以有效学习新的事实知识。在这项工作中，我们表明这种缺陷的原因是语言模型倾向于学习词汇共现统计而非真实的事实关联。我们识别出语言模型中两种知识表示形式的差异：共现统计形式的知识编码在Transformer模型的中间层，并且在超出简单问答的推理场景中泛化能力较差，而真实的事实关联则编码在较低层，可以在各种推理任务中自由利用。基于这些观察，我们提出了两种策略来改善语言模型中事实关联的学习。我们表明，在隐含而非显式事实关联的文本上训练可以迫使模型学习事实关联而非共现统计，从而显著提高新学知识的泛化能力。我们还提出了一种简单的训练方法，主动遗忘已学的共现统计，这在训练纯叙述文本时解锁并增强了事实关联的学习。在合成和真实世界语料库中，这两种策略都提高了微调过程中所学知识在推理场景（如间接和多跳问答）中的泛化能力。"
}
{
  "title": "Interpreting Learned Feedback Patterns in Large Language Models",
  "title_zh": "学习反馈模式在大型语言模型中的解释",
  "abstract": "Reinforcement learning from human feedback (RLHF) is widely used to train large language models (LLMs). However, it is unclear whether LLMs accurately learn the underlying preferences in human feedback data. We coin the term **Learned Feedback Pattern** (LFP) for patterns in an LLM's activations learned during RLHF that improve its performance on the fine-tuning task. We hypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback exhibit consistent activation patterns for outputs that would have received similar feedback during RLHF. To test this, we train probes to estimate the feedback signal implicit in the activations of a fine-tuned LLM. We then compare these estimates to the true feedback, measuring how accurate the LFPs are to the fine-tuning feedback. Our probes are trained on a condensed, sparse and interpretable representation of LLM activations, making it easier to correlate features of the input with our probe's predictions. We validate our probes by comparing the neural features they correlate with positive feedback inputs against the features GPT-4 describes and classifies as related to LFPs. Understanding LFPs can help minimize discrepancies between LLM behavior and training objectives, which is essential for the **safety** and **alignment** of LLMs.",
  "abstract_zh": "从人类反馈中进行强化学习（RLHF）被广泛用于训练大型语言模型（LLMs）。然而，尚不清楚LLMs是否准确学习了人类反馈数据中的潜在偏好。我们创造了术语**学习反馈模式**（LFP），用于描述LLM在RLHF过程中学到的激活模式，这些模式提高了其在微调任务中的表现。我们假设，具有与微调反馈准确对齐的LFP的LLMs，在产生类似反馈的输出时会表现出一致的激活模式。为验证这一点，我们训练探测器以估计微调LLM激活中隐含的反馈信号。然后，我们将这些估计与真实反馈进行比较，以衡量LFP对微调反馈的准确性。我们的探测器基于LLM激活的简化、稀疏和可解释的表示进行训练，使得将输入特征与探测器的预测相关联变得更加容易。我们通过将探测器与GPT-4描述和分类为与LFP相关的正反馈输入的神经特征进行比较来验证我们的探测器。理解LFP有助于减少LLM行为与训练目标之间的差异，这对于LLM的**安全性**和**对齐性**至关重要。"
}
{
  "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates",
  "title_zh": "标题：微调后保持大型语言模型对齐：提示模板的重要作用",
  "abstract": "Public LLMs such as the Llama 2-Chat underwent alignment training and were considered safe. Recently Qi et al. (2024) reported that even benign fine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. We focus on the setting where a public model is fine-tuned before serving users for specific usage, where the model should improve on the downstream task while maintaining alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the “Pure Tuning, Safe Testing” (PTST) strategy --- fine-tune models without a safety prompt, but include it at test time. This seemingly counterintuitive strategy incorporates an intended distribution shift to encourage alignment preservation. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors.",
  "abstract_zh": "摘要：公共大型语言模型（LLM）如Llama 2-Chat经过对齐训练，被认为是安全的。最近，Qi等人（2024）报告称，即使在看似安全的数据集上进行良性微调，也可能导致模型出现不安全行为。本文讨论了减轻这种对齐丧失的方法和最佳实践。我们关注的是在为特定用途服务用户之前对公共模型进行微调的设置，其中模型应在下游任务中有所改进，同时保持对齐。通过对多个聊天模型（Meta的Llama 2-Chat、Mistral AI的Mistral 7B Instruct v0.2和OpenAI的GPT-3.5 Turbo）进行广泛实验，本文揭示了在微调和推理过程中使用的提示模板在保持安全对齐方面起着关键作用，并提出了“纯微调，安全测试”（PTST）策略——在没有安全提示的情况下微调模型，但在测试时包含它。这种看似违反直觉的策略通过引入预期的分布转变来鼓励对齐保持。在GSM8K、ChatDoctor和OpenOrca上的微调实验表明，PTST显著减少了不安全行为的增加。"
}
{
  "title": "Can an AI Agent Safely Run a Government? Existence of Probably Approximately Aligned Policies",
  "title_zh": "题目：人工智能代理能否安全地运行政府？可能近似对齐政策的存在性",
  "abstract": "While autonomous agents often surpass humans in their ability to handle vast and complex data, their potential misalignment (i.e., lack of transparency regarding their true objective) has thus far hindered their use in critical applications such as social decision processes. More importantly, existing alignment methods provide no formal guarantees on the safety of such models. Drawing from utility and social choice theory, we provide a novel quantitative definition of alignment in the context of social decision-making. Building on this definition, we introduce probably approximately aligned (i.e., near-optimal) policies, and we derive a sufficient condition for their existence. Lastly, recognizing the practical difficulty of satisfying this condition, we introduce the relaxed concept of safe (i.e., nondestructive) policies, and we propose a simple yet robust method to safeguard the black-box policy of any autonomous agent, ensuring all its actions are verifiably safe for the society.",
  "abstract_zh": "摘要：尽管自主代理在处理庞大而复杂的数据方面常常超越人类，但其潜在的不对齐（即缺乏对其真实目标的透明性）迄今为止阻碍了其在社会决策过程等关键应用中的使用。更重要的是，现有的对齐方法未能提供关于此类模型安全性的正式保证。借鉴效用和社会选择理论，我们在社会决策的背景下提供了一种新的对齐的定量定义。在此定义的基础上，我们引入了可能近似对齐（即近似最优）的政策，并推导出其存在的充分条件。最后，鉴于满足此条件的实际困难，我们引入了安全（即非破坏性）政策的放松概念，并提出了一种简单而稳健的方法来保护任何自主代理的黑箱政策，确保其所有行动对社会是可验证安全的。"
}
{
  "title": "From Unstructured Data to In-Context Learning: Exploring What Tasks Can Be Learned and When",
  "title_zh": "从非结构化数据到上下文学习：探索可以学习的任务及其时机",
  "abstract": "Large language models (LLMs) like transformers demonstrate impressive in-context learning (ICL) capabilities, allowing them to make\npredictions for new tasks based on prompt exemplars without parameter updates. While existing ICL theories often assume structured training data resembling ICL tasks (e.g., x-y pairs for linear regression), LLMs are typically trained unsupervised on unstructured text, such as web content, which lacks clear parallels to tasks like word analogy. To address this gap, we examine what enables ICL in models trained on unstructured data, focusing on critical sequence model requirements and training data structure. We find that many ICL capabilities can\nemerge simply from co-occurrence of semantically related word pairs in unstructured data; word analogy completion, for example, can provably arise purely through co-occurrence modeling, using classical language models like continuous bag of words (CBOW), without needing positional information or attention mechanisms. However, positional information becomes crucial for logic reasoning tasks requiring generalization to unseen tokens. Finally, we identify two cases where ICL fails: one in logic reasoning tasks that require generalizing to new, unseen patterns, and another in analogy completion where relevant word pairs appear only in fixed training positions. These findings suggest that LLMs' ICL abilities depend heavily on the structural elements within their training data.",
  "abstract_zh": "大型语言模型（LLMs）如变压器展示了令人印象深刻的上下文学习（ICL）能力，使其能够基于提示示例对新任务进行预测而无需参数更新。尽管现有的ICL理论通常假设结构化的训练数据类似于ICL任务（例如，线性回归的x-y对），但LLMs通常在非结构化文本上进行无监督训练，例如网络内容，这与诸如词类比之类的任务没有明确的对应关系。为了解决这一差距，我们研究了在非结构化数据上训练的模型中是什么促成了ICL，重点关注关键的序列模型要求和训练数据结构。我们发现，许多ICL能力可以仅通过在非结构化数据中语义相关词对的共现而出现；例如，词类比完成可以通过共现建模在经典语言模型如连续词袋（CBOW）中得以证明，而无需位置信息或注意机制。然而，位置信息对于需要推广到未见标记的逻辑推理任务变得至关重要。最后，我们确定了ICL失败的两种情况：一种是在需要推广到新的、未见模式的逻辑推理任务中，另一种是在类比完成中相关词对仅出现在固定训练位置时。这些发现表明，LLMs的ICL能力在很大程度上依赖于其训练数据中的结构元素。"
}
{
  "title": "Limits of Transformer Language Models on Learning to Compose Algorithms",
  "title_zh": "Title: Transformer语言模型在学习组合算法上的局限性",
  "abstract": "We analyze the capabilities of Transformer language models in learning compositional discrete tasks. To this end, we evaluate training LLaMA models and prompting GPT-4 and Gemini on four tasks demanding to learn a composition of several discrete sub-tasks. In particular, we measure how well these models can reuse primitives observable in the sub-tasks to learn the composition task. Our results indicate that compositional learning in state-of-the-art Transformer language models is highly sample inefficient: LLaMA requires more data samples than relearning all sub-tasks from scratch to learn the compositional task; in-context prompting with few samples is unreliable and fails at executing the sub-tasks or correcting the errors in multi-round code generation. Further, by leveraging complexity theory, we support these findings with a theoretical analysis focused on the sample inefficiency of gradient descent in memorizing feedforward models. We open source our code at https://github.com/IBM/limitations-lm-algorithmic-compositional-learning.",
  "abstract_zh": "Abstract: 我们分析了Transformer语言模型在学习组合离散任务方面的能力。为此，我们评估了训练LLaMA模型以及在四个需要学习多个离散子任务组合的任务中提示GPT-4和Gemini。特别是，我们测量了这些模型在多大程度上能够重用在子任务中可观察到的基本元素来学习组合任务。我们的结果表明，最先进的Transformer语言模型在组合学习方面的样本效率极低：LLaMA需要比从头重新学习所有子任务更多的数据样本来学习组合任务；使用少量样本进行上下文提示是不可靠的，并且在执行子任务或纠正多轮代码生成中的错误时会失败。此外，通过利用复杂性理论，我们通过理论分析支持这些发现，重点关注梯度下降在记忆前馈模型中的样本效率低下。我们在https://github.com/IBM/limitations-lm-algorithmic-compositional-learning上开源了我们的代码。"
}
{
  "title": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation",
  "title_zh": "Title: DropBP：通过丢弃反向传播加速大型语言模型的微调",
  "abstract": "Large language models (LLMs) have achieved significant success across various domains. However, training these LLMs typically involves substantial memory and computational costs during both forward and backward propagation. While parameter-efficient fine-tuning (PEFT) considerably reduces the training memory associated with parameters, it does not address the significant computational costs and activation memory. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs and activation memory while maintaining accuracy. DropBP randomly drops layers during backward propagation, which is essentially equivalent to training shallow submodules generated by undropped layers and residual connections. Additionally, DropBP calculates the sensitivity of each layer to assign an appropriate drop rate, thereby stabilizing the training process. DropBP is not only applicable to full fine-tuning but can also be orthogonally integrated with all types of PEFT by dropping layers during backward propagation. Specifically, DropBP can reduce training time by 44% with comparable accuracy to the baseline, accelerate convergence to the same perplexity by 1.5$\\times$, and enable training with a sequence length 6.2$\\times$ larger on a single NVIDIA-A100 GPU. Furthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100 GPU and 117% on an Intel Gaudi2 HPU. The code is available at [https://github.com/WooSunghyeon/dropbp](https://github.com/WooSunghyeon/dropbp).",
  "abstract_zh": "Abstract: 大型语言模型（LLMs）在各个领域取得了显著成功。然而，在正向和反向传播过程中训练这些LLMs通常涉及大量的内存和计算成本。虽然参数高效微调（PEFT）显著减少了与参数相关的训练内存，但它并未解决显著的计算成本和激活内存。在本文中，我们提出了丢弃反向传播（DropBP），这是一种旨在减少计算成本和激活内存同时保持准确性的新方法。DropBP在反向传播过程中随机丢弃层，这实质上相当于训练由未丢弃层和残差连接生成的浅层子模块。此外，DropBP计算每一层的敏感度以分配适当的丢弃率，从而稳定训练过程。DropBP不仅适用于全量微调，还可以通过在反向传播过程中丢弃层与所有类型的PEFT正交集成。具体而言，DropBP可以在保持与基线相当的准确性的同时将训练时间减少44%，以1.5倍的速度加速收敛到相同的困惑度，并在单个NVIDIA-A100 GPU上实现6.2倍更大的序列长度训练。此外，我们的DropBP在NVIDIA A100 GPU上实现了79%的吞吐量增加，在Intel Gaudi2 HPU上实现了117%的吞吐量增加。代码可在[https://github.com/WooSunghyeon/dropbp](https://github.com/WooSunghyeon/dropbp)获得。"
}
{
  "title": "A Polar coordinate system represents syntax in large language models",
  "title_zh": "标题：极坐标系在大型语言模型中表示句法",
  "abstract": "Originally formalized with symbolic representations, syntactic trees may also be effectively represented in the activations of large language models (LLMs). Indeed, a ''Structural Probe'' can find a subspace of neural activations, where syntactically-related words are relatively close to one-another. However, this syntactic code remains incomplete: the distance between the Structural Probe word embeddings can represent the \\emph{existence} but not the type and direction of syntactic relations. Here, we hypothesize that syntactic relations are, in fact, coded by the relative direction between nearby embeddings. To test this hypothesis, we introduce a ''Polar Probe'' trained to read syntactic relations from both the distance and the direction between word embeddings. Our approach reveals three main findings. First, our Polar Probe successfully recovers the type and direction of syntactic relations, and substantially outperforms the Structural Probe by nearly two folds. Second, we confirm that this polar coordinate system exists in a low-dimensional subspace of the intermediate layers of many LLMs and becomes increasingly precise in the latest frontier models. Third, we demonstrate with a new benchmark that similar syntactic relations are coded similarly across the nested levels of syntactic trees. Overall, this work shows that LLMs spontaneously learn a geometry of neural activations that explicitly represents the main symbolic structures of linguistic theory.",
  "abstract_zh": "摘要：句法树最初是通过符号表示形式化的，但也可以在大型语言模型（LLMs）的激活中有效表示。实际上，“结构探针”可以找到神经激活的一个子空间，在这个子空间中，句法相关的词彼此相对接近。然而，这种句法编码仍然不完整：结构探针词嵌入之间的距离可以表示句法关系的\\emph{存在}，但不能表示其类型和方向。在此，我们假设句法关系实际上是由相邻嵌入之间的相对方向编码的。为了验证这一假设，我们引入了一种“极探针”，训练其从词嵌入之间的距离和方向读取句法关系。我们的方法揭示了三个主要发现。首先，我们的极探针成功恢复了句法关系的类型和方向，并且其性能几乎是结构探针的两倍。其次，我们确认这种极坐标系存在于许多LLMs的中间层的低维子空间中，并且在最新的前沿模型中变得越来越精确。第三，我们通过一个新的基准展示了类似的句法关系在句法树的嵌套层次中以相似的方式编码。总体而言，这项工作表明，LLMs自发地学习了一种神经激活的几何结构，明确地表示了语言学理论的主要符号结构。"
}
{
  "title": "Alignment at Pre-training! Towards Native Alignment for Arabic LLMs",
  "title_zh": "预训练中的对齐！迈向阿拉伯语大型语言模型的本地对齐",
  "abstract": "The alignment of large language models (LLMs) is critical for developing effective and safe language models. Traditional approaches focus on aligning models during the instruction tuning or reinforcement learning stages, referred to in this paper as `\\textit{post alignment}'. We argue that alignment during the pre-training phase, which we term 'native alignment', warrants investigation. Native alignment aims to prevent unaligned content from the beginning, rather than relying on post-hoc processing. This approach leverages extensively aligned pre-training data to enhance the effectiveness and usability of pre-trained models. Our study specifically explores the application of native alignment in the context of Arabic LLMs. We conduct comprehensive experiments and ablation studies to evaluate the impact of native alignment on model performance and alignment stability. Additionally, we release open-source Arabic LLMs that demonstrate state-of-the-art performance on various benchmarks, providing significant benefits to the Arabic LLM community.",
  "abstract_zh": "大型语言模型（LLMs）的对齐对于开发有效且安全的语言模型至关重要。传统方法侧重于在指令微调或强化学习阶段对齐模型，本文称之为“后期对齐”。我们认为，在预训练阶段进行对齐，即我们称之为“本地对齐”，值得研究。本地对齐旨在从一开始就防止未对齐的内容，而不是依赖事后处理。这种方法利用广泛对齐的预训练数据来增强预训练模型的有效性和可用性。我们的研究特别探讨了本地对齐在阿拉伯语大型语言模型中的应用。我们进行了全面的实验和消融研究，以评估本地对齐对模型性能和对齐稳定性的影响。此外，我们发布了开源的阿拉伯语大型语言模型，这些模型在各种基准测试中表现出最先进的性能，为阿拉伯语大型语言模型社区提供了显著的益处。"
}
{
  "title": "Automated Multi-level Preference for MLLMs",
  "title_zh": "自动化多级偏好用于多模态大语言模型",
  "abstract": "Current multimodal Large Language Models (MLLMs) suffer from ''hallucination'', occasionally generating responses that are not grounded in the input images. To tackle this challenge, one promising path is to utilize reinforcement learning from human feedback (RLHF), which steers MLLMs towards learning superior responses while avoiding inferior ones. We rethink the common practice of using binary preferences (*i.e.*, superior, inferior), and find that adopting multi-level preferences (*e.g.*, superior, medium, inferior) is better for two benefits: 1) It narrows the gap between adjacent levels, thereby encouraging MLLMs to discern subtle differences. 2) It further integrates cross-level comparisons (beyond adjacent-level comparisons), thus providing a broader range of comparisons with hallucination examples. To verify our viewpoint, we present the Automated Multi-level Preference (**AMP**) framework for MLLMs. To facilitate this framework, we first develop an automated dataset generation pipeline that provides high-quality multi-level preference datasets without any human annotators. Furthermore, we design the Multi-level Direct Preference Optimization (MDPO) algorithm to robustly conduct complex multi-level preference learning. Additionally, we propose a new hallucination benchmark, MRHal-Bench. Extensive experiments across public hallucination and general benchmarks, as well as our MRHal-Bench, demonstrate the effectiveness of our proposed method. Code is available at https://github.com/takomc/amp.",
  "abstract_zh": "当前的多模态大语言模型（MLLMs）存在“幻觉”问题，有时会生成与输入图像不相关的响应。为了解决这一挑战，一个有前途的路径是利用来自人类反馈的强化学习（RLHF），引导MLLMs学习更优的响应，同时避免较差的响应。我们重新思考使用二元偏好（即优劣）的常见做法，发现采用多级偏好（例如，优、中、劣）有两个好处：1）它缩小了相邻级别之间的差距，从而鼓励MLLMs识别细微差别。2）它进一步整合跨级别比较（超越相邻级别比较），从而提供更广泛的幻觉例子比较。为了验证我们的观点，我们提出了用于MLLMs的自动化多级偏好（**AMP**）框架。为了促进这一框架，我们首先开发了一个自动化数据集生成管道，提供高质量的多级偏好数据集，而无需任何人工标注者。此外，我们设计了多级直接偏好优化（MDPO）算法，以稳健地进行复杂的多级偏好学习。此外，我们提出了一个新的幻觉基准，MRHal-Bench。通过在公共幻觉和一般基准以及我们的MRHal-Bench上的广泛实验，证明了我们提出方法的有效性。代码可在https://github.com/takomc/amp获取。"
}
{
  "title": "Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature",
  "title_zh": "标题: Bileve：通过双层签名保护大型语言模型中的文本来源免受欺骗",
  "abstract": "Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing watermarking techniques typically prioritize robustness against removal attacks, unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly alter the meanings of LLM-generated responses or even forge harmful content, potentially misattributing blame to the LLM developer. To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained signal to trace text sources when the signature is invalid (enhancing detectability) via a novel rank-based sampling strategy. Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during detection, reliably tracing text provenance and regulating LLMs. The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability.",
  "abstract_zh": "摘要: 大型语言模型（LLMs）的文本水印通常用于识别机器生成内容的来源，这在打击深度伪造或有害内容时对评估责任具有重要意义。虽然现有的水印技术通常优先考虑对抗去除攻击的鲁棒性，但不幸的是，它们容易受到欺骗攻击：恶意行为者可以微妙地改变LLM生成响应的含义，甚至伪造有害内容，可能将责任错误归咎于LLM开发者。为了解决这一问题，我们引入了一种双层签名方案Bileve，该方案通过一种新颖的基于排名的采样策略嵌入细粒度的签名位进行完整性检查（缓解欺骗攻击），以及在签名无效时追踪文本来源的粗粒度信号（增强可检测性）。与仅输出二元结果的传统水印检测器相比，Bileve在检测过程中可以区分5种情境，可靠地追踪文本来源并规范LLM。对OPT-1.3B和LLaMA-7B进行的实验表明，Bileve在击败欺骗攻击方面具有增强的可检测性。"
}
{
  "title": "Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes",
  "title_zh": "标题：梯度袖带：通过探索拒绝损失景观检测大型语言模型的越狱攻击",
  "abstract": "Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the **Refusal Loss** of LLMs and then proposes a method called **Gradient Cuff** to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can significantly improve the LLM's rejection capability for malicious jailbreak queries, while maintaining the model's performance for benign user queries by adjusting the detection threshold.",
  "abstract_zh": "摘要：大型语言模型（LLMs）正在成为一种重要的生成式人工智能工具，用户输入查询，LLM生成答案。为了减少危害和误用，已经努力通过人类反馈强化学习（RLHF）等先进训练技术将这些LLM与人类价值观对齐。然而，最近的研究强调了LLM易受对抗性越狱尝试的影响，这些尝试旨在颠覆嵌入的安全护栏。为应对这一挑战，本文定义并研究了LLM的**拒绝损失**，并提出了一种称为**梯度袖带**的方法来检测越狱尝试。梯度袖带利用拒绝损失景观中观察到的独特特性，包括功能值及其平滑性，设计了一种有效的两步检测策略。在两个对齐的LLM（LLaMA-2-7B-Chat和Vicuna-7B-V1.5）和六种类型的越狱攻击（GCG、AutoDAN、PAIR、TAP、Base64和LRL）上的实验结果表明，梯度袖带可以显著提高LLM对恶意越狱查询的拒绝能力，同时通过调整检测阈值保持模型对良性用户查询的性能。"
}
{
  "title": "Unveiling the Bias Impact on Symmetric Moral Consistency of Large Language Models",
  "title_zh": "揭示大型语言模型对称道德一致性的偏差影响",
  "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, surpassing human experts in various benchmark tests and playing a vital role in various industry sectors. Despite their effectiveness, a notable drawback of LLMs is their inconsistent moral behavior, which raises ethical concerns. This work delves into symmetric moral consistency in large language models and demonstrates that modern LLMs lack sufficient consistency ability in moral scenarios. Our extensive investigation of twelve popular LLMs reveals that their assessed consistency scores are influenced by position bias and selection bias rather than their intrinsic abilities. We propose a new framework tSMC, which gauges the effects of these biases and effectively mitigates the bias impact based on the Kullback–Leibler divergence to pinpoint LLMs' mitigated Symmetric Moral Consistency. We find that the ability of LLMs to maintain consistency varies across different moral scenarios. Specifically, LLMs show more consistency in scenarios with clear moral answers compared to those where no choice is morally perfect. The average consistency score of 12 LLMs ranges from $60.7\\%$ in high-ambiguity moral scenarios to $84.8\\%$ in low-ambiguity moral scenarios.",
  "abstract_zh": "大型语言模型（LLMs）展现了卓越的能力，在各种基准测试中超越了人类专家，并在多个行业领域中发挥着重要作用。尽管它们有效，但LLMs的一个显著缺点是其不一致的道德行为，这引发了伦理问题。本文深入探讨了大型语言模型中的对称道德一致性，并证明现代LLMs在道德情境中缺乏足够的一致性能力。我们对十二种流行的LLMs进行了广泛调查，发现其评估的一致性得分受到位置偏差和选择偏差的影响，而非其内在能力。我们提出了一个新的框架tSMC，用以衡量这些偏差的影响，并基于Kullback–Leibler散度有效减轻偏差影响，以确定LLMs的减轻对称道德一致性。我们发现，LLMs在不同的道德情境中保持一致性的能力各不相同。具体而言，LLMs在具有明确道德答案的情境中表现出更多的一致性，而在没有完美道德选择的情境中则较少。12种LLMs的平均一致性得分在高模糊性道德情境中为$60.7\\%$，而在低模糊性道德情境中为$84.8\\%$。"
}
{
  "title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step",
  "title_zh": "OccamLLM：一步实现快速且精确的语言模型算术",
  "abstract": "Despite significant advancements in text generation and reasoning, Large Language Models (LLMs) still face challenges in accurately performing complex arithmetic operations. Language model systems often enable LLMs to generate code for arithmetic operations to achieve accurate calculations. However, this approach compromises speed and security, and fine-tuning risks the language model losing prior capabilities. We propose a framework that enables exact arithmetic in *a single autoregressive step*, providing faster, more secure, and more interpretable LLM systems with arithmetic capabilities. We use the hidden states of a LLM to control a symbolic architecture that performs arithmetic. Our implementation using Llama 3 with OccamNet as a symbolic model (OccamLlama) achieves 100\\% accuracy on single arithmetic operations ($+,-,\\times,\\div,\\sin{},\\cos{},\\log{},\\exp{},\\sqrt{}$), outperforming GPT 4o with and without a code interpreter. Furthermore, OccamLlama outperforms GPT 4o with and without a code interpreter on average across a range of mathematical problem solving benchmarks, demonstrating that OccamLLMs can excel in arithmetic tasks, even surpassing much larger models. Code is available at https://github.com/druidowm/OccamLLM.",
  "abstract_zh": "尽管在文本生成和推理方面取得了显著进展，大型语言模型（LLMs）在准确执行复杂算术运算方面仍面临挑战。语言模型系统通常使LLMs生成用于算术运算的代码以实现精确计算。然而，这种方法在速度和安全性上存在妥协，微调还可能导致语言模型丧失先前的能力。我们提出了一个框架，使得在*单个自回归步骤*中实现精确算术，从而提供更快、更安全且更易解释的具有算术能力的LLM系统。我们利用LLM的隐藏状态来控制执行算术运算的符号架构。我们使用Llama 3与OccamNet作为符号模型（OccamLlama）的实现，在单个算术运算（$+,-,\\times,\\div,\\sin{},\\cos{},\\log{},\\exp{},\\sqrt{}$）上实现了100\\%的准确率，优于带有和不带代码解释器的GPT 4o。此外，OccamLlama在一系列数学问题解决基准测试中平均表现优于带有和不带代码解释器的GPT 4o，表明OccamLLMs在算术任务中表现出色，甚至超越了更大的模型。代码可在https://github.com/druidowm/OccamLLM获取。"
}
{
  "title": "Analysing the Generalisation and Reliability of Steering Vectors",
  "title_zh": "题目：分析转向向量的泛化性和可靠性",
  "abstract": "Steering vectors (SVs) are a new approach to efficiently adjust language model behaviour at inference time by intervening on intermediate model activations. They have shown promise in terms of improving both capabilities and model alignment. However, the reliability and generalisation properties of this approach are unknown. In this work, we rigorously investigate these properties, and show that steering vectors have substantial limitations both in- and out-of-distribution. In-distribution, steerability is highly variable across different inputs. Depending on the concept, spurious biases can substantially contribute to how effective steering is for each input, presenting a challenge for the widespread use of steering vectors. Out-of-distribution, while steering vectors often generalise well, for several concepts they are brittle to reasonable changes in the prompt, resulting in them failing to generalise well. Overall, our findings show that while steering can work well in the right circumstances, there remain many technical difficulties of applying steering vectors to guide models' behaviour at scale.",
  "abstract_zh": "摘要：转向向量（SVs）是一种通过干预中间模型激活来高效调整语言模型推理行为的新方法。它们在提高能力和模型对齐方面表现出潜力。然而，这种方法的可靠性和泛化性尚不明确。在这项工作中，我们严格研究了这些属性，并表明转向向量在分布内外都有显著的局限性。在分布内，不同输入的可操控性差异很大。根据概念的不同，虚假偏差可能会显著影响每个输入的转向效果，这对转向向量的广泛使用构成挑战。在分布外，尽管转向向量通常能很好地泛化，但对于几个概念，它们对提示的合理变化表现出脆弱性，导致泛化不佳。总体而言，我们的研究结果表明，尽管在适当的情况下转向可以很好地工作，但在大规模应用转向向量以指导模型行为时仍存在许多技术难题。"
}
{
  "title": "Protecting Your LLMs with Information Bottleneck",
  "title_zh": "用信息瓶颈保护您的大型语言模型",
  "abstract": "The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content.\nDespite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts.\nTo address this, we introduce the Information Bottleneck Protector (IBProtector), a defense mechanism grounded in the information bottleneck principle, and we modify the objective to avoid trivial solutions.\nThe IBProtector selectively compresses and perturbs prompts, facilitated by a lightweight and trainable extractor, preserving only essential information for the target LLMs to respond with the expected answer.\nMoreover, we further consider a situation where the gradient is not visible to be compatible with any LLM.\nOur empirical evaluations show that IBProtector outperforms current defense methods in mitigating jailbreak attempts, without overly affecting response quality or inference speed. \nIts effectiveness and adaptability across various attack methods and target LLMs underscore the potential of IBProtector as a novel, transferable defense that bolsters the security of LLMs without requiring modifications to the underlying models.",
  "abstract_zh": "大型语言模型（LLMs）的出现彻底改变了自然语言处理领域，但它们可能会被攻击以生成有害内容。尽管努力使LLMs在伦理上对齐，这些模型往往脆弱，可以通过优化或手动对抗性提示的越狱攻击来规避。为了解决这个问题，我们引入了基于信息瓶颈原理的防御机制——信息瓶颈保护器（IBProtector），并修改了目标以避免琐碎的解决方案。IBProtector通过一个轻量级且可训练的提取器选择性地压缩和扰动提示，仅保留目标LLMs响应预期答案所需的关键信息。此外，我们进一步考虑了梯度不可见的情况，以便与任何LLM兼容。我们的实证评估表明，IBProtector在缓解越狱尝试方面优于当前的防御方法，同时不会过度影响响应质量或推理速度。其在各种攻击方法和目标LLMs中的有效性和适应性，突显了IBProtector作为一种新颖、可转移的防御手段的潜力，在无需修改基础模型的情况下增强了LLMs的安全性。"
}
{
  "title": "Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference",
  "title_zh": "反转遗忘-保留目标：一种基于对数差异的高效大语言模型遗忘框架",
  "abstract": "As Large Language Models (LLMs) demonstrate extensive capability in learning from documents, LLM unlearning becomes an increasingly important research area to address concerns of LLMs in terms of privacy, copyright, etc. A conventional LLM unlearning task typically involves two goals: (1) The target LLM should forget the knowledge in the specified forget documents; and (2) it should retain the other knowledge that the LLM possesses, for which we assume access to a small number of retain documents. To achieve both goals, a mainstream class of LLM unlearning methods introduces an optimization framework with a combination of two objectives – maximizing the prediction loss on the forget documents while minimizing that on the retain documents, which suffers from two challenges, degenerated output and catastrophic forgetting. In this paper, we propose a novel unlearning framework called Unlearning from Logit Difference (ULD), which introduces an assistant LLM that aims to achieve the opposite of the unlearning goals: remembering the forget documents and forgetting the retain knowledge. ULD then derives the unlearned LLM by computing the logit difference between the target and the assistant LLMs. We show that such reversed objectives would naturally resolve both aforementioned challenges while significantly improving the training efficiency. Extensive experiments demonstrate that our method efficiently achieves the intended forgetting while preserving the LLM’s overall capabilities, reducing training time by more than threefold. Notably, our method loses 0% of model utility on the ToFU benchmark, whereas baseline methods may sacrifice 17% of utility on average to achieve comparable forget quality.",
  "abstract_zh": "随着大语言模型（LLMs）在文档学习中展现出广泛的能力，LLM遗忘成为一个日益重要的研究领域，以解决LLMs在隐私、版权等方面的担忧。传统的LLM遗忘任务通常涉及两个目标：（1）目标LLM应遗忘指定遗忘文档中的知识；（2）它应保留LLM拥有的其他知识，为此我们假设可以访问少量的保留文档。为了实现这两个目标，一种主流的LLM遗忘方法引入了一个优化框架，结合了两个目标——最大化遗忘文档的预测损失，同时最小化保留文档的预测损失，这面临着输出退化和灾难性遗忘的挑战。在本文中，我们提出了一种新颖的遗忘框架，称为基于对数差异的遗忘（ULD），它引入了一个辅助LLM，其目标是实现与遗忘目标相反的效果：记住遗忘文档并遗忘保留知识。ULD通过计算目标和辅助LLM之间的对数差异来导出遗忘后的LLM。我们表明，这种反转的目标自然解决了上述两个挑战，同时显著提高了训练效率。大量实验表明，我们的方法在有效实现预期遗忘的同时，保留了LLM的整体能力，将训练时间减少了三倍以上。值得注意的是，我们的方法在ToFU基准上损失了0%的模型效用，而基线方法平均可能牺牲17%的效用以实现可比的遗忘质量。"
}
{
  "title": "Gorilla: Large Language Model Connected with Massive APIs",
  "title_zh": "标题: Gorilla: 连接海量API的大型语言模型",
  "abstract": "Large Language Models (LLMs) have seen an impressive wave of advances, with\nmodels now excelling in a variety of tasks, such as mathematical reasoning and\nprogram synthesis. However, their potential to effectively use tools via API calls\nremains unfulfilled. This is a challenging task even for today’s state-of-the-art\nLLMs such as GPT-4 largely due to their unawareness of what APIs are available\nand how to use them in a frequently updated tool set. We develop Gorilla, a\nfinetuned LLaMA model that surpasses the performance of GPT-4 on writing API\ncalls. Trained with the novel Retriever Aware Training (RAT), when combined\nwith a document retriever, Gorilla demonstrates a strong capability to adapt to\ntest-time document changes, allowing flexible user updates or version changes.\nIt also substantially mitigates the issue of hallucination, commonly encountered\nwhen prompting LLMs directly. To evaluate the model’s ability, we introduce\nAPIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and\nTensorHub APIs. The successful integration of the retrieval system with Gorilla\ndemonstrates the potential for LLMs to use tools more accurately, keep up with\nfrequently updated documentation, and consequently increase the reliability and\napplicability of their outputs. Gorilla’s code, model, data, and demo are available\nat: https://gorilla.cs.berkeley.edu",
  "abstract_zh": "摘要: 大型语言模型（LLMs）在多个领域取得了令人瞩目的进展，如数学推理和程序合成。然而，它们通过API调用有效使用工具的潜力尚未实现。即使是当今最先进的LLMs，如GPT-4，也难以胜任这一任务，主要原因是它们不了解可用的API以及如何在频繁更新的工具集中使用它们。我们开发了Gorilla，一个经过微调的LLaMA模型，其在编写API调用方面的表现超过了GPT-4。通过使用新颖的检索器感知训练（RAT），结合文档检索器，Gorilla展现出强大的适应测试时文档变化的能力，允许灵活的用户更新或版本更改。它还大大缓解了直接提示LLMs时常遇到的幻觉问题。为了评估模型的能力，我们引入了APIBench，一个由HuggingFace、TorchHub和TensorHub API组成的综合数据集。检索系统与Gorilla的成功整合展示了LLMs更准确使用工具的潜力，能够跟上频繁更新的文档，从而提高其输出的可靠性和适用性。Gorilla的代码、模型、数据和演示可在以下网址获取：https://gorilla.cs.berkeley.edu"
}
{
  "title": "SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models",
  "title_zh": "标题: SLED: 自我逻辑演化解码以提升大型语言模型的事实性",
  "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (LLaMA 2, LLaMA 3, Gemma) and scales (from 2B to 70B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. The results demonstrate that SLED consistently improves factual accuracy by up to 20\\% compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.",
  "abstract_zh": "摘要: 大型语言模型（LLMs）展现了非凡的能力，但其输出有时可能不可靠或事实不准确。为了解决这一问题，我们引入了自我逻辑演化解码（SLED），这是一种新颖的解码框架，能够在不依赖外部知识库或不需要进一步微调的情况下增强LLMs的真实性。从优化的角度来看，我们的SLED框架通过对比最终层与早期层的输出逻辑，利用嵌入在LLM中的潜在知识。然后，它采用近似梯度方法，使潜在知识能够指导输出的自我完善，从而有效提高事实准确性。我们在各种模型家族（LLaMA 2、LLaMA 3、Gemma）和规模（从2B到70B）上，包括更先进的架构配置如专家混合（MoE），进行了广泛的基准测试。我们的评估涵盖了多种任务，包括多选、开放生成和链式思维推理任务的适应。结果表明，与现有解码方法相比，SLED在保持自然语言流畅性和可忽略的延迟开销的同时，始终将事实准确性提高高达20%。此外，它可以灵活地与其他解码方法结合，以进一步提升其性能。"
}
{
  "title": "Multi-LLM Debate: Framework, Principals, and Interventions",
  "title_zh": "多语言模型辩论：框架、原则和干预措施",
  "abstract": "The flexible and generalized nature of large language models has allowed for their application in a wide array of language-based domains.\nMuch like their human contemporaries, these models are capable of engaging in discussions and debates as a means of improving answer quality.\nWe first take a theoretical approach to analyzing debate and provide a framework through which debate can be mathematically examined.\nBuilding on this framework, we provide several theoretical results for multi-agent debate.\nIn particular, we demonstrate that similar model capabilities, or similar model responses, can result in static debate dynamics where the debate procedure simply converges to the majority opinion. \nWhen this majority opinion is the result of a common misconception (ingrained in the models through shared training data) debate is likely to converge to answers associated with that common misconception.\nUsing insights from our theoretical results we then propose three interventions which improve the efficacy of debate. \nFor each intervention, we provide theoretical results demonstrating how debate is improved.\nWe also demonstrate that these interventions result in better performance on four common benchmark tasks.",
  "abstract_zh": "大型语言模型的灵活性和通用性使其能够应用于广泛的语言领域。与人类类似，这些模型能够参与讨论和辩论，以提高答案质量。我们首先采用理论方法分析辩论，并提供一个可以对辩论进行数学检验的框架。在此框架的基础上，我们为多代理辩论提供了若干理论结果。特别是，我们证明了相似的模型能力或相似的模型响应可能导致静态的辩论动态，其中辩论过程仅仅收敛于多数意见。当这种多数意见是由于共同的误解（通过共享的训练数据植入模型）而产生时，辩论可能会趋向于与该共同误解相关的答案。利用我们理论结果的见解，我们提出了三种干预措施，以提高辩论的有效性。对于每种干预措施，我们提供了理论结果，展示了辩论如何得到改善。我们还证明了这些干预措施在四个常见基准任务上表现更佳。"
}
{
  "title": "Verified Code Transpilation with LLMs",
  "title_zh": "标题：使用大型语言模型进行代码验证转译",
  "abstract": "Domain-specific languages (DSLs) have become integral to various software workflows. Such languages offer domain-specific optimizations and abstractions that improve code readability and maintainability.  However, leveraging these languages requires developers to rewrite existing code using the specific DSL's API. While large language models (LLMs) have shown some success in automatic code transpilation, none of them provide any functional correctness guarantees on the rewritten code. Another approach for automating this task is verified lifting, which relies on program synthesis to find programs in the target language that are functionally equivalent to the source language program. While several verified lifting tools have been developed for various application domains, they are specialized for specific source-target languages or require significant expertise in domain knowledge to make the search efficient. In this paper, leveraging recent advances in LLMs, we propose an LLM-based approach (LLMLift) to building verified lifting tools. We use the LLM's capabilities to reason about programs to translate a given program into its corresponding equivalent in the target language. Additionally, we use LLMs to generate proofs for functional equivalence. We develop lifting-based compilers for four DSLs targeting different application domains. Our approach not only outperforms previous symbolic-based tools in number of benchmarks transpiled and transpilation time, but also requires significantly less effort to build.",
  "abstract_zh": "摘要：领域特定语言（DSL）已成为各种软件工作流程的重要组成部分。这些语言提供领域特定的优化和抽象，提升了代码的可读性和可维护性。然而，利用这些语言需要开发者使用特定DSL的API重写现有代码。尽管大型语言模型（LLMs）在自动代码转译方面取得了一些成功，但它们在重写代码的功能正确性上没有提供任何保证。另一种自动化此任务的方法是验证提升，它依赖程序综合在目标语言中找到与源语言程序功能等效的程序。尽管已经为各种应用领域开发了几种验证提升工具，但它们专用于特定的源-目标语言，或者需要在领域知识上有显著的专业知识以提高搜索效率。在本文中，我们利用LLMs的最新进展，提出了一种基于LLM的方法（LLMLift）来构建验证提升工具。我们利用LLM的程序推理能力，将给定程序翻译成目标语言中的等效程序。此外，我们使用LLMs生成功能等效性的证明。我们开发了针对四种不同应用领域的DSL的提升编译器。我们的方法不仅在转译的基准数量和转译时间上优于以前的基于符号的工具，而且构建所需的努力也显著减少。"
}
{
  "title": "Reranking Laws for Language Generation: A Communication-Theoretic Perspective",
  "title_zh": "重新排序法则在语言生成中的应用：一种通信理论的视角",
  "abstract": "To ensure large language models (LLMs) are used safely, one must reduce their propensity to hallucinate or to generate unacceptable answers. A simple and often used strategy is to first let the LLM generate multiple hypotheses and then employ a reranker to choose the best one. In this paper, we draw a parallel between this strategy and the use of redundancy to decrease the error rate in noisy communication channels. We conceptualize the generator as a sender transmitting multiple descriptions of a message through parallel noisy channels. The receiver decodes the message by ranking the (potentially corrupted) descriptions and selecting the one found to be most reliable. We provide conditions under which this protocol is asymptotically error-free (i.e., yields an acceptable answer almost surely) even in scenarios where the reranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the channel distributions are statistically dependent. We use our framework to obtain reranking laws which we validate empirically on two real-world tasks using LLMs: text-to-code generation with DeepSeek-Coder 7B and machine translation of medical data with TowerInstruct 13B.",
  "abstract_zh": "为了确保大型语言模型（LLMs）的安全使用，必须减少其产生幻觉或生成不可接受答案的倾向。一种简单且常用的策略是首先让LLM生成多个假设，然后使用重新排序器选择最佳答案。在本文中，我们将这种策略与使用冗余来降低噪声通信信道中的错误率进行了类比。我们将生成器概念化为一个发送者，通过并行的噪声信道传输消息的多个描述。接收者通过对（可能被破坏的）描述进行排序并选择被认为最可靠的一个来解码消息。我们提供了在何种条件下，即使在重新排序器不完美（由Mallows或Zipf-Mandelbrot模型控制）且信道分布统计相关的情况下，该协议也能渐近无误（即几乎肯定产生可接受答案）。我们使用我们的框架获得了重新排序法则，并在两个使用LLMs的实际任务中进行了实证验证：使用DeepSeek-Coder 7B进行文本到代码的生成和使用TowerInstruct 13B进行医学数据的机器翻译。"
}
{
  "title": "Aligning to Thousands of Preferences via System Message Generalization",
  "title_zh": "标题：通过系统消息泛化实现对数千种偏好的对齐",
  "abstract": "Although humans inherently have diverse values, current large language model (LLM) alignment methods often assume that aligning LLMs with the general public’s preferences is optimal. A major challenge in adopting a more individualized approach to LLM alignment is its lack of scalability, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual’s preferences. To address these challenges, we propose a new paradigm where users specify what they value most within the system message, steering the LLM’s generation behavior to better align with the user’s intentions. However, a naive application of such an approach is non-trivial since LLMs are typically trained on a uniform system message (e.g., “You are a helpful assistant”), which limits\ntheir ability to generalize to diverse, unseen system messages. To improve this generalization, we create Multifaceted Collection, augmenting 66k user instructions into 197k system messages through hierarchical user value combinations. Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)\nby adding system messages that reflect unseen user values. JANUS achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), JANUS also outperforms LLaMA 3 8B Instruct by a +4.0%p, +0.1%p, +3.0%p margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public’s preference as well. Our code, dataset, benchmark, and models are available at https://lklab.kaist.ac.kr/Janus/.",
  "abstract_zh": "摘要：尽管人类本质上具有多样的价值观，当前的大型语言模型（LLM）对齐方法通常假设与公众偏好对齐是最优的。采用更个性化的LLM对齐方法的主要挑战在于其缺乏可扩展性，因为这涉及反复获取偏好数据并为每个个体的偏好训练新的奖励模型和LLM。为了解决这些挑战，我们提出了一种新范式，用户在系统消息中指定他们最看重的内容，从而引导LLM的生成行为以更好地符合用户的意图。然而，直接应用这种方法并不简单，因为LLM通常在统一的系统消息（例如，“你是一个有帮助的助手”）上进行训练，这限制了它们对多样化、未见过的系统消息的泛化能力。为了改善这种泛化，我们创建了多面集合，通过层次化的用户价值组合将66k用户指令扩充为197k系统消息。使用这个数据集，我们训练了一个名为Janus的7B LLM，并通过加入反映未见用户价值的系统消息，在5个基准（AlpacaEval 2.0、FLASK、Koala、MT-Bench和Self-Instruct）的921个提示上进行测试。JANUS在与Mistral 7B Instruct v0.2、GPT-3.5 Turbo和GPT-4的对比中分别取得了75.2%、72.4%和66.4%的平局+胜率。出乎意料的是，在三个专注于响应帮助性的基准（AlpacaEval 2.0、MT-Bench、Arena Hard Auto v0.1）上，JANUS也以+4.0%p、+0.1%p、+3.0%p的优势超过了LLaMA 3 8B Instruct，这也表明使用大量系统消息进行训练同样可以增强与公众偏好的对齐。我们的代码、数据集、基准和模型可在https://lklab.kaist.ac.kr/Janus/获得。"
}
{
  "title": "Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context",
  "title_zh": "在不确定背景下评估大型语言模型决策行为的框架",
  "abstract": "When making decisions under uncertainty, individuals often deviate from rational behavior, which can be evaluated across three dimensions: risk preference, probability weighting, and loss aversion. Given the widespread use of large language models (LLMs) in supporting decision-making processes, it is crucial to assess whether their behavior aligns with human norms and ethical expectations or exhibits potential biases. Although several empirical studies have investigated the rationality and social behavior performance of LLMs, their internal decision-making tendencies and capabilities remain inadequately understood. This paper proposes a framework, grounded in behavioral economics theories, to evaluate the decision-making behaviors of LLMs. With a multiple-choice-list experiment, we initially estimate the degree of risk preference, probability weighting, and loss aversion in a context-free setting for three commercial LLMs: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that LLMs generally exhibit patterns similar to humans, such as risk aversion and loss aversion, with a tendency to overweight small probabilities, but there are significant variations in the degree to which these behaviors are expressed across different LLMs. Further, we explore their behavior when embedded with socio-demographic features of human beings, uncovering significant disparities across various demographic characteristics.",
  "abstract_zh": "在不确定性下做决策时，个体往往会偏离理性行为，这可以从风险偏好、概率加权和损失规避三个维度进行评估。鉴于大型语言模型（LLMs）在支持决策过程中的广泛应用，评估其行为是否符合人类规范和伦理期望或表现出潜在偏见至关重要。尽管已有多项实证研究调查了LLMs的理性和社会行为表现，但其内部决策倾向和能力仍然理解不足。本文提出了一个基于行为经济学理论的框架，用于评估LLMs的决策行为。通过多项选择列表实验，我们最初在无背景设置中估计了三个商业LLMs（ChatGPT-4.0-Turbo、Claude-3-Opus和Gemini-1.0-pro）的风险偏好、概率加权和损失规避程度。我们的结果显示，LLMs通常表现出类似于人类的模式，如风险规避和损失规避，并倾向于高估小概率，但这些行为在不同LLMs之间的表现程度存在显著差异。此外，我们探讨了其在嵌入人类社会人口特征时的行为，揭示了在各种人口特征之间的显著差异。"
}
{
  "title": "No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices",
  "title_zh": "标题：大型语言模型水印中的无免费午餐：水印设计选择的权衡",
  "abstract": "Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating the misuse of such AI-generated content. However, we show that common design choices in LLM watermarking schemes make the resulting systems  surprisingly susceptible to attack---leading to fundamental trade-offs in robustness, utility, and usability. \nTo navigate these trade-offs, we rigorously study a set of simple yet effective attacks on common watermarking systems, and propose  guidelines and defenses for LLM watermarking in practice.",
  "abstract_zh": "摘要：生成模型的进步使得AI生成的文本、代码和图像在许多应用中可以与人类生成的内容相媲美。水印技术旨在嵌入信息以验证模型输出的来源，对于减轻此类AI生成内容的误用非常有用。然而，我们发现大型语言模型水印方案中的常见设计选择使得结果系统出乎意料地容易受到攻击，从而在稳健性、实用性和可用性之间产生了根本性的权衡。为了解决这些权衡，我们严格研究了一组对常见水印系统简单而有效的攻击，并提出了大型语言模型水印在实践中的指南和防御措施。"
}
{
  "title": "Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning",
  "title_zh": "情感-LLaMA：通过指令微调实现多模态情感识别与推理",
  "abstract": "Accurate emotion perception is crucial for various applications, including human-computer interaction, education, and counseling.\nHowever, traditional single-modality approaches often fail to capture the complexity of real-world emotional expressions, which are inherently multimodal. Moreover, existing Multimodal Large Language Models (MLLMs) face challenges in integrating audio and recognizing subtle facial micro-expressions. To address this, we introduce the MERR dataset, containing 28,618 coarse-grained and 4,487 fine-grained annotated samples across diverse emotional categories. This dataset enables models to learn from varied scenarios and generalize to real-world applications. Furthermore, we propose Emotion-LLaMA, a model that seamlessly integrates audio, visual, and textual inputs through emotion-specific encoders. By aligning features into a shared space and employing a modified LLaMA model with instruction tuning, Emotion-LLaMA significantly enhances both emotional recognition and reasoning capabilities. Extensive evaluations show Emotion-LLaMA outperforms other MLLMs, achieving top scores in Clue Overlap (7.83) and Label Overlap (6.25) on EMER, an F1 score of 0.9036 on MER2023-SEMI challenge, and the highest UAR (45.59) and WAR (59.37) in zero-shot evaluations on DFEW dataset.",
  "abstract_zh": "准确的情感感知对于人机交互、教育和咨询等各种应用至关重要。然而，传统的单一模态方法往往无法捕捉到真实世界情感表达的复杂性，这些表达本质上是多模态的。此外，现有的多模态大型语言模型（MLLMs）在整合音频和识别微妙的面部微表情方面面临挑战。为了解决这一问题，我们引入了MERR数据集，其中包含28,618个粗粒度和4,487个细粒度标注样本，涵盖多种情感类别。该数据集使模型能够从不同场景中学习并推广到现实应用中。此外，我们提出了Emotion-LLaMA模型，该模型通过情感特定编码器无缝集成音频、视觉和文本输入。通过将特征对齐到共享空间并使用经过指令微调的改进LLaMA模型，Emotion-LLaMA显著增强了情感识别和推理能力。广泛的评估显示，Emotion-LLaMA在EMER的线索重叠（7.83）和标签重叠（6.25）中取得了最高分，在MER2023-SEMI挑战中获得了0.9036的F1分数，并在DFEW数据集的零样本评估中取得了最高的UAR（45.59）和WAR（59.37）。"
}
{
  "title": "The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains",
  "title_zh": "统计归纳头的演变：上下文学习马尔可夫链",
  "abstract": "Large language models have the ability to generate text that mimics patterns in their inputs. We introduce a simple Markov Chain sequence modeling task in order to study how this in-context learning capability emerges. In our setting, each example is sampled from a Markov chain drawn from a prior distribution over Markov chains. Transformers trained on this task form \\emph{statistical induction heads} which compute accurate next-token probabilities given the bigram statistics of the context. During the course of training, models pass through multiple phases: after an initial stage in which predictions are uniform, they learn to sub-optimally predict using in-context single-token statistics (unigrams); then, there is a rapid phase transition to the correct in-context bigram solution. We conduct an empirical and theoretical investigation of this multi-phase process, showing how successful learning results from the interaction between the transformer's layers, and uncovering evidence that the presence of the simpler unigram solution may delay formation of the final bigram solution. We examine how learning is affected by varying the prior distribution over Markov chains, and consider the generalization of our in-context learning of Markov chains (ICL-MC) task to $n$-grams for $n > 2$.",
  "abstract_zh": "大型语言模型具有生成模仿其输入模式的文本的能力。我们引入了一个简单的马尔可夫链序列建模任务，以研究这种上下文学习能力如何出现。在我们的设置中，每个示例都从一个从马尔可夫链的先验分布中抽取的马尔可夫链中采样。在此任务上训练的Transformer形成了\\emph{统计归纳头}，它根据上下文的二元组统计计算准确的下一个标记概率。在训练过程中，模型经历了多个阶段：在预测均匀的初始阶段之后，它们学会使用上下文中的单标记统计（单元组）进行次优预测；然后，快速发生相变，转向正确的上下文二元组解决方案。我们对这一多阶段过程进行了实证和理论研究，展示了成功学习如何源于Transformer层之间的相互作用，并揭示了简单的单元组解决方案的存在可能会延迟最终二元组解决方案的形成的证据。我们考察了通过改变马尔可夫链的先验分布对学习的影响，并考虑了我们上下文学习马尔可夫链（ICL-MC）任务向$n > 2$的$n$-元组的推广。"
}
{
  "title": "Towards Safe Concept Transfer of Multi-Modal Diffusion via Causal Representation Editing",
  "title_zh": "标题：通过因果表示编辑实现多模态扩散的安全概念转移",
  "abstract": "Recent advancements in vision-language-to-image (VL2I) diffusion generation have made significant progress. While generating images from broad vision-language inputs holds promise, it also raises concerns about potential misuse, such as copying artistic styles without permission, which could have legal and social consequences. Therefore, it's crucial to establish governance frameworks to ensure ethical and copyright integrity, especially with widely used diffusion models. To address these issues, researchers have explored various approaches, such as dataset filtering, adversarial perturbations, machine unlearning, and inference-time refusals. However, these methods often lack either scalability or effectiveness. In response, we propose a new framework called causal representation editing (CRE), which extends representation editing from large language models (LLMs) to diffusion-based models. CRE enhances the efficiency and flexibility of safe content generation by intervening at diffusion timesteps causally linked to unsafe concepts. This allows for precise removal of harmful content while preserving acceptable content quality, demonstrating superior effectiveness, precision and scalability compared to existing methods. CRE can handle complex scenarios, including incomplete or blurred representations of unsafe concepts, offering a promising solution to challenges in managing harmful content generation in diffusion-based models.",
  "abstract_zh": "摘要：最近在视觉-语言到图像（VL2I）扩散生成方面取得了显著进展。虽然从广泛的视觉-语言输入生成图像具有潜力，但也引发了潜在滥用的担忧，例如未经许可复制艺术风格，这可能带来法律和社会后果。因此，建立治理框架以确保道德和版权完整性至关重要，特别是在广泛使用的扩散模型中。为了解决这些问题，研究人员探索了各种方法，如数据集过滤、对抗性扰动、机器遗忘和推理时拒绝。然而，这些方法往往缺乏可扩展性或有效性。对此，我们提出了一种新的框架，称为因果表示编辑（CRE），将表示编辑从大型语言模型（LLMs）扩展到基于扩散的模型。CRE通过在与不安全概念因果关联的扩散时间步进行干预，提高了安全内容生成的效率和灵活性。这允许在保留可接受内容质量的同时精确去除有害内容，显示出比现有方法更高的有效性、精确性和可扩展性。CRE能够处理复杂场景，包括不完整或模糊的不安全概念表示，为解决基于扩散模型的有害内容生成管理挑战提供了一个有前景的解决方案。"
}
{
  "title": "Watermarking Makes Language Models Radioactive",
  "title_zh": "题目：水印使语言模型具有放射性",
  "abstract": "We investigate the radioactivity of text generated by large language models (LLM), \\ie whether it is possible to detect that such synthetic input was used to train a subsequent LLM.\nCurrent methods like membership inference or active IP protection either work only in settings where the suspected text is known or do not provide reliable statistical guarantees.\nWe discover that, on the contrary, it is possible to reliably determine if a language model was trained on synthetic data if that data is output by a watermarked LLM.\nOur new methods, specialized for radioactivity, detects with a provable confidence weak residuals of the watermark signal in the fine-tuned LLM.\nWe link the radioactivity contamination level to the following properties: the watermark robustness, its proportion in the training set, and the fine-tuning process.\nFor instance, if the suspect model is open-weight, we demonstrate that training on watermarked instructions can be detected with high confidence ($p$-value $< 10^{-5}$) even when as little as $5\\%$ of training text is watermarked.",
  "abstract_zh": "摘要：我们研究了由大型语言模型（LLM）生成的文本的放射性，即是否可以检测到此类合成输入被用于训练后续的LLM。当前的方法如成员推断或主动知识产权保护仅在已知可疑文本的情况下有效，或者不提供可靠的统计保证。相反，我们发现，如果数据是由带水印的LLM输出的，则可以可靠地确定语言模型是否在合成数据上进行了训练。我们的新方法专门用于放射性检测，以可证明的置信度检测微调LLM中水印信号的微弱残留。我们将放射性污染水平与以下属性联系起来：水印的鲁棒性、其在训练集中的比例以及微调过程。例如，如果可疑模型是开放权重的，我们证明即使只有$5\\%$的训练文本带有水印，训练带水印的指令也可以被高置信度检测到（$p$值$< 10^{-5}$）。"
}
{
  "title": "Stealth edits to large language models",
  "title_zh": "标题: 对大型语言模型的隐秘编辑",
  "abstract": "We reveal the theoretical foundations of techniques for editing large language models, and present new methods which can do so without requiring retraining. Our theoretical insights show that a single metric (a measure of the intrinsic dimension of the model's features) can be used to assess a model's editability and reveals its previously unrecognised susceptibility to malicious *stealth attacks*. This metric is fundamental to predicting the success of a variety of editing approaches, and reveals new bridges between disparate families of editing methods. We collectively refer to these as *stealth editing* methods, because they directly update a model's weights to specify its response to specific known hallucinating prompts without affecting other model behaviour. By carefully applying our theoretical insights, we are able to introduce a new *jet-pack* network block which is optimised for highly selective model editing, uses only standard network operations, and can be inserted into existing networks. We also reveal the vulnerability of language models to stealth attacks: a small change to a model's weights which fixes its response to a single attacker-chosen prompt. Stealth attacks are computationally simple, do not require access to or knowledge of the model's training data, and therefore represent a potent yet previously unrecognised threat to redistributed foundation models. Extensive experimental results illustrate and support our methods and their theoretical underpinnings. Demos and source code are available at https://github.com/qinghua-zhou/stealth-edits.",
  "abstract_zh": "摘要: 我们揭示了编辑大型语言模型技术的理论基础，并提出了无需重新训练即可实现的新方法。我们的理论洞察表明，单一指标（模型特征内在维度的度量）可以用来评估模型的可编辑性，并揭示其之前未被识别的对恶意*隐秘攻击*的易感性。该指标对于预测各种编辑方法的成功至关重要，并揭示了不同编辑方法家族之间的新联系。我们将这些统称为*隐秘编辑*方法，因为它们直接更新模型的权重，以指定其对特定已知幻觉提示的响应，而不影响其他模型行为。通过仔细应用我们的理论见解，我们能够引入一个新的*喷气背包*网络模块，该模块针对高度选择性的模型编辑进行了优化，仅使用标准网络操作，并可以插入现有网络中。我们还揭示了语言模型对隐秘攻击的脆弱性：对模型权重的微小更改可以修正其对单个攻击者选择的提示的响应。隐秘攻击计算简单，不需要访问或了解模型的训练数据，因此对重新分发的基础模型构成了一种强大但之前未被识别的威胁。大量实验结果展示并支持了我们的方法及其理论基础。演示和源代码可在 https://github.com/qinghua-zhou/stealth-edits 获得。"
}
{
  "title": "Spectral Editing of Activations for Large Language Model Alignment",
  "title_zh": "激活光谱编辑用于大型语言模型对齐",
  "abstract": "Large language models (LLMs) often exhibit undesirable behaviours, such as generating untruthful or biased content. Editing their internal representations has been shown to be effective in mitigating such behaviours on top of the existing alignment methods. We propose a novel inference-time editing method, namely spectral editing of activations (SEA), to project the input representations into directions with maximal covariance with the positive demonstrations (e.g., truthful) while minimising covariance with the negative demonstrations (e.g., hallucinated). We also extend our method to non-linear editing using feature functions. We run extensive experiments on benchmarks concerning truthfulness and bias with six open-source LLMs of different sizes and model families. The results demonstrate the superiority of SEA in effectiveness, generalisation to similar tasks, as well as computation and data efficiency. We also show that SEA editing only has a limited negative impact on other model capabilities.",
  "abstract_zh": "大型语言模型（LLMs）常常表现出不良行为，例如生成不真实或有偏见的内容。编辑其内部表示已被证明在现有对齐方法的基础上有效缓解这些行为。我们提出了一种新颖的推理时编辑方法，即激活的光谱编辑（SEA），将输入表示投影到与正面示例（例如，真实）具有最大协方差的方向，同时最小化与负面示例（例如，幻觉）之间的协方差。我们还使用特征函数将我们的方法扩展到非线性编辑。我们在关于真实性和偏见的基准上对六种不同规模和模型家族的开源LLM进行了广泛实验。结果表明，SEA在有效性、对类似任务的泛化能力以及计算和数据效率方面具有优越性。我们还表明，SEA编辑对其他模型能力的负面影响有限。"
}
{
  "title": "Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms",
  "title_zh": "标题：直接对齐算法中奖励模型过度优化的缩放规律",
  "abstract": "Reinforcement Learning from Human Feedback (RLHF)has been crucial to the recent success of Large Language Models (LLMs), however it is often a complex and brittle process. In the classical RLHF framework, a reward model is first trained to represent human preferences, which is in turn used by an online reinforcement learning (RL) algorithm to optimized the LLM. A prominent issue with such methods is reward over-optimization or reward hacking, where the performance as measured by the learned proxy reward model increases, but the true model quality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs), such as Direct Preference Optimization (DPO) have emerged as alternatives to the classical RLHF pipeline. However, despite not training a separate proxy reward model or using RL, they still commonly deteriorate from over-optimization. While the so-called reward hacking phenomenon is not well-defined for DAAs, we still uncover similar trends: at higher KL-budgets, DAA algorithms exhibit similar degradation patters to their classic RLHF counterparts. In particular, we find that DAA methods deteriorate not only across a wide range of KL-budgets, but also often before even a single epoch of the dataset is completed. Through extensive empirical experimentation this work formulates the reward over-optimization or hacking problem for DAAs and explores its consequences across objectives, training regimes, and model scales.",
  "abstract_zh": "摘要：基于人类反馈的强化学习（RLHF）在大型语言模型（LLM）的近期成功中起到了关键作用，但这一过程通常复杂且脆弱。在经典的RLHF框架中，首先训练一个奖励模型来表示人类偏好，然后由在线强化学习（RL）算法使用该模型来优化LLM。此类方法的一个突出问题是奖励过度优化或奖励欺骗，即通过学习的代理奖励模型测量的性能增加，但真实模型质量停滞甚至恶化。直接对齐算法（DAA），如直接偏好优化（DPO），已成为经典RLHF流程的替代方案。然而，尽管不训练单独的代理奖励模型或使用RL，它们仍然常常因过度优化而恶化。虽然所谓的奖励欺骗现象在DAA中没有明确定义，我们仍然发现了类似的趋势：在较高的KL预算下，DAA算法表现出与经典RLHF类似的退化模式。特别是，我们发现DAA方法不仅在广泛的KL预算范围内恶化，而且常常在数据集的单个周期完成之前就开始恶化。通过广泛的实验证明，这项工作为DAA制定了奖励过度优化或欺骗问题，并探讨了其在目标、训练机制和模型规模上的影响。"
}
{
  "title": "Leveraging Catastrophic Forgetting to Develop Safe Diffusion Models against Malicious Finetuning",
  "title_zh": "标题：利用灾难性遗忘开发针对恶意微调的安全扩散模型",
  "abstract": "Diffusion models (DMs) have demonstrated remarkable proficiency in producing images based on textual prompts. Numerous methods have been proposed to ensure these models generate safe images. Early methods attempt to incorporate safety filters into models to mitigate the risk of generating harmful images but such external filters do not inherently detoxify the model and can be easily bypassed. Hence, model unlearning and data cleaning are the most essential methods for maintaining the safety of models, given their impact on model parameters.\nHowever, malicious fine-tuning can still make models prone to generating harmful or undesirable images even with these methods.\nInspired by the phenomenon of catastrophic forgetting, we propose a training policy using contrastive learning to increase the latent space distance between clean and harmful data distribution, thereby protecting models from being fine-tuned to generate harmful images due to forgetting.\nThe experimental results demonstrate that our methods not only maintain clean image generation capabilities before malicious fine-tuning but also effectively prevent DMs from producing harmful images after malicious fine-tuning. Our method can also be combined with other safety methods to maintain their safety against malicious fine-tuning further.",
  "abstract_zh": "摘要：扩散模型（DMs）在根据文本提示生成图像方面表现出卓越的能力。已经提出了多种方法以确保这些模型生成安全的图像。早期的方法尝试将安全过滤器整合到模型中，以降低生成有害图像的风险，但这些外部过滤器并不能从根本上净化模型，且容易被绕过。因此，模型去学习和数据清理是保持模型安全性的最基本方法，因为它们对模型参数有影响。然而，即使采用这些方法，恶意微调仍可能导致模型生成有害或不良图像。受到灾难性遗忘现象的启发，我们提出了一种使用对比学习的训练策略，以增加干净和有害数据分布之间的潜在空间距离，从而保护模型不因遗忘而被微调生成有害图像。实验结果表明，我们的方法不仅在恶意微调之前保持了干净的图像生成能力，还有效防止了DMs在恶意微调后生成有害图像。我们的方法还可以与其他安全方法结合，以进一步保持其对恶意微调的安全性。"
}
{
  "title": "Smoothie: Label Free Language Model Routing",
  "title_zh": "Title: Smoothie：无标签语言模型路由",
  "abstract": "Large language models (LLMs) are increasingly used in applications where LLM inputs may span many different tasks. Recent work has found that the choice of LLM is consequential, and different LLMs may be good for different input samples. Prior approaches have thus explored how engineers might select an LLM to use for each sample (i.e. _routing_). While existing routing methods mostly require training auxiliary models on human-annotated data, our work explores whether it is possible to perform _unsupervised_ routing. We propose Smoothie, a weak supervision-inspired routing approach that requires no labeled data. Given a set of outputs from different LLMs, Smoothie constructs a latent variable graphical model over embedding representations of observable LLM outputs and unknown “true” outputs. Using this graphical model, we estimate sample-dependent quality scores for each LLM, and route each sample to the LLM with the highest corresponding score. We find that Smoothie's LLM quality-scores correlate with ground-truth model quality (correctly identifying the optimal model on 9/14 tasks), and that Smoothie outperforms baselines for routing by up to 10 points accuracy.",
  "abstract_zh": "Abstract: 大型语言模型（LLM）在输入可能涵盖许多不同任务的应用中越来越多地被使用。最近的研究发现，选择LLM是重要的，不同的LLM可能适合不同的输入样本。因此，先前的方法探索了工程师如何为每个样本选择一个LLM（即路由）。虽然现有的路由方法大多需要在人工标注的数据上训练辅助模型，但我们的工作探索了是否可以进行无监督路由。我们提出了Smoothie，这是一种受弱监督启发的路由方法，不需要标注数据。给定来自不同LLM的一组输出，Smoothie在可观察的LLM输出和未知的“真实”输出的嵌入表示上构建潜变量图模型。使用该图模型，我们估计每个LLM的样本依赖质量分数，并将每个样本路由到具有最高相应分数的LLM。我们发现，Smoothie的LLM质量分数与真实模型质量相关（在9/14个任务中正确识别出最佳模型），并且Smoothie在路由准确性上比基线高出最多10个百分点。"
}
{
  "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization",
  "title_zh": "KV缓存每通道1位：通过耦合量化实现高效的大型语言模型推理",
  "abstract": "Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As batch size, context length, or model size increases, the size of key and value (KV) cache quickly becomes the main contributor to GPU memory usage and the bottleneck of inference latency and throughput. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. Currently, KV cache quantization is performed per-channel or per-token independently. Our analysis shows that distinct channels of a key/value activation embedding are highly interdependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropy, which implies that per-channel independent quantization is sub-optimal. To mitigate this sub-optimality, we propose Coupled Quantization (CQ), which couples multiple key/value channels together for quantization to exploit their interdependence and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ compares favorably with existing baselines in preserving model quality, and improves inference throughput by 1.4–3.5$\\times$ relative to the uncompressed baseline. Furthermore, we demonstrate that CQ can preserve model quality reasonably with KV cache quantized down to 1 bit.",
  "abstract_zh": "高效部署大型语言模型（LLMs）需要将多个请求批处理在一起以提高吞吐量。随着批量大小、上下文长度或模型大小的增加，键和值（KV）缓存的大小迅速成为GPU内存使用的主要贡献者以及推理延迟和吞吐量的瓶颈。量化已成为KV缓存压缩的有效技术，但现有方法在非常低的位宽下仍然失败。目前，KV缓存量化是独立地按通道或按标记进行的。我们的分析表明，键/值激活嵌入的不同通道高度相互依赖，多个通道的联合熵增长速度慢于其边际熵之和，这意味着独立的按通道量化是次优的。为减轻这种次优性，我们提出了耦合量化（CQ），它将多个键/值通道耦合在一起进行量化，以利用它们的相互依赖性并以更信息高效的方式编码激活。大量实验表明，CQ在保持模型质量方面优于现有基线，并在推理吞吐量方面相对于未压缩基线提高了1.4–3.5倍。此外，我们证明CQ在KV缓存量化到1位时仍能合理地保持模型质量。"
}
{
  "title": "Refusal in Language Models Is Mediated by a Single Direction",
  "title_zh": "标题：语言模型中的拒绝行为由单一方向介导",
  "abstract": "Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables a model's ability to refuse, with minimal effect on other capabilities. This interpretable rank-one weight edit results in an effective jailbreak technique that is simpler and more efficient than fine-tuning. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.",
  "abstract_zh": "摘要：会话大型语言模型经过微调，以同时遵循指令和保证安全，从而形成既能服从良性请求又能拒绝有害请求的模型。虽然这种拒绝行为在聊天模型中广泛存在，但其潜在机制仍然知之甚少。在这项工作中，我们展示了拒绝行为由一个一维子空间介导，涵盖了多达72B参数的13个流行开源聊天模型。具体来说，对于每个模型，我们找到一个单一方向，去除该方向会阻止模型拒绝有害指令，而添加该方向则会在无害指令上引发拒绝。利用这一见解，我们提出了一种新颖的白盒越狱方法，精准地禁用模型拒绝能力，对其他能力影响最小。这种可解释的秩一权重编辑产生了一种有效的越狱技术，比微调更简单、更高效。最后，我们从机制上分析了对抗性后缀如何抑制拒绝介导方向的传播。我们的研究结果强调了当前安全微调方法的脆弱性。更广泛地说，我们的工作展示了如何利用对模型内部的理解来开发控制模型行为的实用方法。"
}
{
  "title": "Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment",
  "title_zh": "从SFT数据中获取更多收益：从人类示范中学习奖励提升SFT以对齐LLM",
  "abstract": "Aligning human preference and value is an important requirement for contemporary foundation models. State-of-the-art techniques such as Reinforcement Learning from Human Feedback (RLHF) often consist of two stages: 1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from human demonstration data; 2) Preference learning, where preference data is used to learn a reward model, which is in turn used by a reinforcement learning (RL) step to fine-tune the model. Such reward model serves as a proxy to human preference, and it is critical to guide the RL step towards improving the model quality. In this work, we argue that the SFT stage significantly benefits from learning a reward model as well. Instead of using the human demonstration data directly via supervised learning, we propose to leverage an Inverse Reinforcement Learning (IRL) technique to {\\it simultaneously} build an reward model and a policy model. This approach leads to new SFT algorithms that are not only efficient to implement, but are robust to the presence of low-quality supervised learning data. Moreover, we discover a connection between the proposed IRL based approach, and a recent line of works called Self-Play Fine-tune (SPIN, \\cite{chen2024self}). Theoretically, we show that the proposed algorithms converge to the stationary solutions of the IRL problem. Empirically, we align 1B and 7B models using proposed methods and evaluate them on a reward benchmark model and the HuggingFace Open LLM Leaderboard. The proposed methods show significant performance improvement over existing SFT approaches. Our results indicate that it is beneficial to leverage reward learning throughout the entire alignment process. Our code is available at \\url{https://github.com/JasonJiaxiangLi/Reward_learning_SFT}.",
  "abstract_zh": "对齐人类偏好和价值是当代基础模型的重要要求。最先进的技术如从人类反馈中进行强化学习（RLHF）通常包括两个阶段：1）监督微调（SFT），模型通过学习人类示范数据进行微调；2）偏好学习，使用偏好数据学习奖励模型，该模型反过来用于强化学习（RL）步骤以微调模型。这样的奖励模型作为人类偏好的代理，对于引导RL步骤提高模型质量至关重要。在这项工作中，我们认为SFT阶段也显著受益于学习奖励模型。我们提出利用逆向强化学习（IRL）技术来同时构建奖励模型和策略模型，而不是直接通过监督学习使用人类示范数据。这种方法导致了新的SFT算法，这些算法不仅易于实现，而且在存在低质量监督学习数据时具有鲁棒性。此外，我们发现所提出的基于IRL的方法与最近一系列称为自我对弈微调（SPIN, \\cite{chen2024self}）的工作之间存在联系。从理论上讲，我们证明了所提出的算法收敛于IRL问题的稳态解。实证上，我们使用所提出的方法对1B和7B模型进行对齐，并在奖励基准模型和HuggingFace Open LLM排行榜上进行评估。所提出的方法在现有SFT方法上显示出显著的性能提升。我们的结果表明，在整个对齐过程中利用奖励学习是有益的。我们的代码可在\\url{https://github.com/JasonJiaxiangLi/Reward_learning_SFT}获得。"
}
{
  "title": "Compositional 3D-aware Video Generation with LLM Director",
  "title_zh": "标题：具有LLM导演的组合式3D感知视频生成",
  "abstract": "Significant progress has been made in text-to-video generation through the use of powerful generative models and large-scale internet data. However, substantial challenges remain in precisely controlling individual elements within the generated video, such as the movement and appearance of specific characters and the manipulation of viewpoints. In this work, we propose a novel paradigm that generates each element in 3D representation separately and then composites them with priors from Large Language Models (LLMs) and 2D diffusion models. Specifically, given an input textual query, our scheme consists of four stages: 1) we leverage the LLMs as the director to first decompose the complex query into several sub-queries, where each sub-query describes each element of the generated video; 2) to generate each element, pre-trained models are invoked by the LLMs to obtain the corresponding 3D representation; 3) to composite the generated 3D representations, we prompt multi-modal LLMs to produce coarse guidance on the scale, location, and trajectory of different objects; 4) to make the results adhere to natural distribution, we further leverage 2D diffusion priors and use score distillation sampling to refine the composition. Extensive experiments demonstrate that our method can generate high-fidelity videos from text with flexible control over each element.",
  "abstract_zh": "摘要：通过使用强大的生成模型和大规模互联网数据，文本到视频生成取得了显著进展。然而，在精确控制生成视频中的各个元素方面仍存在重大挑战，例如特定角色的运动和外观以及视点的操控。在这项工作中，我们提出了一种新的范式，该范式分别生成每个元素的3D表示，然后结合来自大型语言模型（LLMs）和2D扩散模型的先验知识。具体而言，给定输入文本查询，我们的方案包括四个阶段：1）我们利用LLMs作为导演，首先将复杂查询分解为多个子查询，每个子查询描述生成视频的每个元素；2）为了生成每个元素，LLMs调用预训练模型以获得相应的3D表示；3）为了合成生成的3D表示，我们提示多模态LLMs生成关于不同对象的比例、位置和轨迹的粗略指导；4）为了使结果符合自然分布，我们进一步利用2D扩散先验并使用得分蒸馏采样来优化合成。大量实验表明，我们的方法可以从文本生成高保真视频，并灵活控制每个元素。"
}
{
  "title": "Boosting the Potential of Large Language Models with an Intelligent Information Assistant",
  "title_zh": "Title: 利用智能信息助手提升大型语言模型的潜力",
  "abstract": "The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as \"hallucination.\" Initial retrieval-augmented generation (RAG) methods like the \"Retrieve-Read\" framework was inadequate for complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised Fine-Tuning (SFT) methods improved performance but required frequent retraining and risked altering foundational LLM capabilities. To cope with these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), integrating an intelligent information assistant within LLMs. This assistant manages memory and knowledge through tool usage, action execution, memory building, and plan specification. Using a two-phase training approach—Curriculum Assistant Learning and Reinforced Preference Optimization—AssistRAG enhances information retrieval and decision-making. Experiments show AssistRAG significantly outperforms benchmarks, especially benefiting less advanced LLMs, by providing superior reasoning capabilities and accurate responses.",
  "abstract_zh": "Abstract: 大型语言模型（LLMs）的出现显著推动了自然语言处理的发展，但这些模型常常生成事实不准确的信息，即所谓的“幻觉”。最初的检索增强生成（RAG）方法如“检索-阅读”框架对于复杂推理任务是不够的。后续基于提示的RAG策略和监督微调（SFT）方法提高了性能，但需要频繁的再训练，并有改变基础LLM能力的风险。为应对这些挑战，我们提出了基于助手的检索增强生成（AssistRAG），在LLMs中集成了一个智能信息助手。该助手通过工具使用、动作执行、记忆构建和计划制定来管理记忆和知识。通过两阶段训练方法——课程助手学习和强化偏好优化，AssistRAG增强了信息检索和决策能力。实验表明，AssistRAG显著优于基准，特别是为较不先进的LLMs提供了更强的推理能力和准确的响应。"
}
{
  "title": "Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level",
  "title_zh": "标题：文字入侵：理解文本层面的图注入攻击",
  "abstract": "Graph Neural Networks (GNNs) excel across various applications but remain vulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs), which inject malicious nodes into the original graph and pose realistic threats.\nText-attributed graphs (TAGs), where nodes are associated with textual features, are crucial due to their prevalence in real-world applications and are commonly used to evaluate these vulnerabilities.\nHowever, existing research only focuses on embedding-level GIAs, which inject node embeddings rather than actual textual content, limiting their applicability and simplifying detection.\nIn this paper, we pioneer the exploration of GIAs at the text level, presenting three novel attack designs that inject textual content into the graph.\nThrough theoretical and empirical analysis, we demonstrate that text interpretability, a factor previously overlooked at the embedding level, plays a crucial role in attack strength. \nAmong the designs we investigate, the Word-frequency-based Text-level GIA (WTGIA) is particularly notable for its balance between performance and interpretability. \nDespite the success of WTGIA, we discover that defenders can easily enhance their defenses with customized text embedding methods or large language model (LLM)--based predictors. \nThese insights underscore the necessity for further research into the potential and practical significance of text-level GIAs.",
  "abstract_zh": "摘要：图神经网络（GNNs）在各种应用中表现出色，但仍然容易受到对抗性攻击，尤其是图注入攻击（GIAs），这些攻击通过向原始图中注入恶意节点来构成现实威胁。文本属性图（TAGs），即节点与文本特征相关联的图，由于其在现实世界应用中的普遍性而至关重要，通常用于评估这些漏洞。然而，现有研究仅关注嵌入层面的GIAs，这些攻击注入的是节点嵌入而非实际文本内容，限制了其适用性并简化了检测。在本文中，我们开创性地探索了文本层面的GIAs，提出了三种新颖的攻击设计，将文本内容注入图中。通过理论和实证分析，我们证明了文本可解释性——一个在嵌入层面上被忽视的因素——在攻击强度中起着关键作用。在我们研究的设计中，基于词频的文本层面GIA（WTGIA）因其在性能和可解释性之间的平衡而尤为显著。尽管WTGIA取得了成功，我们发现防御者可以通过定制的文本嵌入方法或基于大型语言模型（LLM）的预测器轻松增强其防御能力。这些见解强调了进一步研究文本层面GIAs潜力和实际意义的必要性。"
}
{
  "title": "Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models",
  "title_zh": "标题：Diff-eRank：一种用于评估大型语言模型的新型基于排名的指标",
  "abstract": "Large Language Models (LLMs) have transformed natural language processing and extended their powerful capabilities to multi-modal domains. As LLMs continue to advance, it is crucial to develop diverse and appropriate metrics for their evaluation. In this paper, we introduce a novel rank-based metric, Diff-eRank, grounded in information theory and geometry principles. Diff-eRank assesses LLMs by analyzing their hidden representations, providing a quantitative measure of how efficiently they eliminate redundant information during training. We demonstrate the applicability of Diff-eRank in both single-modal (e.g., language) and multi-modal settings. For language models, our results show that Diff-eRank increases with model size and correlates well with conventional metrics such as loss and accuracy. In the multi-modal context, we propose an alignment evaluation method based on the eRank, and verify that contemporary multi-modal LLMs exhibit strong alignment performance based on our method. Our code is publicly available at https://github.com/waltonfuture/Diff-eRank.",
  "abstract_zh": "摘要：大型语言模型（LLMs）已经改变了自然语言处理，并将其强大的能力扩展到多模态领域。随着LLMs的不断进步，开发多样化和适当的评估指标变得至关重要。在本文中，我们介绍了一种新型的基于排名的指标，Diff-eRank，该指标基于信息论和几何学原理。Diff-eRank通过分析LLMs的隐藏表示来评估它们，提供了一种定量衡量它们在训练过程中如何有效消除冗余信息的方法。我们展示了Diff-eRank在单模态（如语言）和多模态环境中的适用性。对于语言模型，我们的结果表明Diff-eRank随着模型规模的增加而增加，并且与传统指标如损失和准确性具有良好的相关性。在多模态背景下，我们提出了一种基于eRank的对齐评估方法，并验证了当代多模态LLMs在我们的方法基础上表现出强大的对齐性能。我们的代码在https://github.com/waltonfuture/Diff-eRank上公开可用。"
}
{
  "title": "VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks",
  "title_zh": "标题：VisionLLM v2：一个用于数百种视觉语言任务的端到端通用多模态大语言模型",
  "abstract": "We present VisionLLM v2, an end-to-end generalist multimodal large model (MLLM) that unifies visual perception, understanding, and generation within a single framework. Unlike traditional MLLMs limited to text output, VisionLLM v2 significantly broadens its application scope. It excels not only in conventional visual question answering (VQA) but also in open-ended, cross-domain vision tasks such as object localization, pose estimation, and image generation and editing. To this end, we propose a new information transmission mechanism termed ``super link'', as a medium to connect MLLM with task-specific decoders. It not only allows flexible transmission of task information and gradient feedback between the MLLM and multiple downstream decoders but also effectively resolves training conflicts in multi-tasking scenarios. In addition, to support the diverse range of tasks, we carefully collected and combed training data from hundreds of public vision and vision-language tasks. In this way, our model can be joint-trained end-to-end on hundreds of vision language tasks and generalize to these tasks using a set of shared parameters through different user prompts, achieving performance comparable to task-specific models. We believe VisionLLM v2 will offer a new perspective on the generalization of MLLMs.",
  "abstract_zh": "摘要：我们提出了VisionLLM v2，这是一种端到端的通用多模态大模型（MLLM），在单一框架内统一了视觉感知、理解和生成。与传统仅限于文本输出的MLLM不同，VisionLLM v2显著拓宽了其应用范围。它不仅在传统的视觉问答（VQA）中表现出色，还在开放式、跨领域的视觉任务中如目标定位、姿态估计以及图像生成和编辑中表现优异。为此，我们提出了一种新的信息传输机制，称为“超级链接”，作为将MLLM与任务特定解码器连接的媒介。它不仅允许在MLLM和多个下游解码器之间灵活传输任务信息和梯度反馈，还有效解决了多任务场景中的训练冲突。此外，为支持多样化的任务，我们精心收集和梳理了来自数百个公共视觉和视觉语言任务的训练数据。通过这种方式，我们的模型可以在数百个视觉语言任务上进行端到端的联合训练，并通过不同的用户提示使用一组共享参数推广到这些任务，达到与任务特定模型相当的性能。我们相信VisionLLM v2将为MLLMs的泛化提供新的视角。"
}
{
  "title": "Membership Inference Attacks against Large Vision-Language Models",
  "title_zh": "大型视觉语言模型的成员推断攻击",
  "abstract": "Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxRényi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.",
  "abstract_zh": "大型视觉语言模型（VLLMs）在处理各种应用场景下的多模态任务方面展现了有前景的能力。然而，由于其训练数据集中可能包含敏感信息，如私人照片和医疗记录，它们的出现也引发了重大的数据安全问题。在VLLMs中检测不当使用的数据仍然是一个关键且未解决的问题，主要是由于缺乏标准化的数据集和合适的方法。在本研究中，我们引入了首个针对各种VLLMs的成员推断攻击（MIA）基准，以促进训练数据检测。然后，我们提出了一种专门为令牌级图像检测设计的新型MIA流程。最后，我们提出了一种新的度量标准，称为MaxRényi-K%，该标准基于模型输出的置信度，适用于文本和图像数据。我们相信，我们的工作可以加深对VLLMs背景下MIA的理解和方法学。我们的代码和数据集可在https://github.com/LIONS-EPFL/VL-MIA获得。"
}
{
  "title": "HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection",
  "title_zh": "Title: HaloScope：利用未标记的大型语言模型生成内容进行幻觉检测",
  "abstract": "The surge in applications of large language models (LLMs) has prompted concerns about the generation of misleading or fabricated information, known as hallucinations. Therefore, detecting hallucinations has become critical to maintaining trust in LLM-generated content. A primary challenge in learning a truthfulness classifier is the lack of a large amount of labeled truthful and hallucinated data. To address the challenge, we introduce HaloScope, a novel learning framework that leverages the unlabeled LLM generations in the wild for hallucination detection. Such unlabeled data arises freely upon deploying LLMs in the open world, and consists of both truthful and hallucinated information. To harness the unlabeled data, we present an automated scoring function for distinguishing between truthful and untruthful generations within unlabeled mixture data, thereby enabling the training of a binary classifier on top. Importantly, our framework does not require extra data collection and human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiments show that HaloScope can achieve superior hallucination detection performance, outperforming the competitive rivals by a significant margin.",
  "abstract_zh": "Abstract: 大型语言模型（LLM）的应用激增引发了对误导性或虚构信息生成的担忧，这种现象被称为幻觉。因此，检测幻觉对于维持对LLM生成内容的信任至关重要。学习一个真实度分类器的主要挑战在于缺乏大量标记的真实和幻觉数据。为了解决这一挑战，我们引入了HaloScope，这是一种新颖的学习框架，利用在开放环境中未标记的LLM生成内容进行幻觉检测。这些未标记数据在LLM部署于开放世界时自由产生，包含真实和幻觉信息。为了利用这些未标记数据，我们提出了一种自动评分函数，用于区分未标记混合数据中的真实和不真实生成，从而在此基础上训练二元分类器。重要的是，我们的框架不需要额外的数据收集和人工标注，为实际应用提供了强大的灵活性和实用性。大量实验表明，HaloScope能够实现卓越的幻觉检测性能，显著超越竞争对手。"
}
{
  "title": "Time-Reversal Provides Unsupervised Feedback to LLMs",
  "title_zh": "标题：时间反转为大型语言模型提供无监督反馈",
  "abstract": "Large Language Models (LLMs) are typically trained to predict in the forward direction of time. However, recent works have shown that prompting these models to look back and critique their own generations can produce useful feedback. Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs. Towards this, we introduce Time Reversed Language Models (TRLMs), which can score and generate queries when conditioned on responses, effectively functioning in the reverse direction of time. Further, to effectively infer in the response to query direction, we pre-train and fine-tune a language model (TRLM-Ba) in the reverse token order from scratch. We show empirically (and theoretically in a stylized setting) that time-reversed models can indeed complement forward model predictions when used to score the query given response for re-ranking multiple forward generations. We obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over the competent baseline of best-of-N re-ranking using self log-perplexity scores. We further show that TRLM scoring outperforms conventional forward scoring of response given query, resulting in significant gains in applications such as citation generation and passage retrieval. We next leverage the generative ability of TRLM to augment or provide unsupervised feedback to input safety filters of LLMs, demonstrating a drastic reduction in false negative rate with negligible impact on false positive rates against several attacks published on the popular JailbreakBench leaderboard.",
  "abstract_zh": "摘要：大型语言模型（LLMs）通常被训练为预测时间的前进方向。然而，最近的研究表明，提示这些模型回顾并批判其自身生成的内容可以产生有用的反馈。受此启发，我们探讨了LLMs是否可以被赋予向后思考（预测和评分）的能力，以提供补充前向LLMs的无监督反馈。为此，我们引入了时间反转语言模型（TRLMs），这些模型可以在给定响应的条件下对查询进行评分和生成，有效地在时间的反方向上运作。此外，为了有效地推断响应到查询的方向，我们从头开始以反向标记顺序预训练和微调语言模型（TRLM-Ba）。我们通过实证（以及在一个风格化的设置中理论上）证明，当用于在给定响应的情况下对多个前向生成进行重新排序时，时间反转模型确实可以补充前向模型的预测。我们在广泛使用的AlpacaEval排行榜上获得了比使用自我对数困惑度评分的最佳N重新排序的基线高达5%的改进。我们进一步表明，TRLM评分优于传统的给定查询的响应前向评分，在引用生成和段落检索等应用中取得了显著的收益。接下来，我们利用TRLM的生成能力来增强或为LLMs的输入安全过滤器提供无监督反馈，展示了在对抗多个在流行的JailbreakBench排行榜上发布的攻击时，错误否定率显著降低，而对错误肯定率的影响可以忽略不计。"
}
{
  "title": "Calibrated Self-Rewarding Vision Language Models",
  "title_zh": "标题：校准自奖励视觉语言模型",
  "abstract": "Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality. Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization. These approaches are resource-intensive and may not effectively reflect the target LVLM's preferences, making the curated preferences easily distinguishable. Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning. In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input. Empirical results demonstrate that CSR significantly enhances performance and reduces hallucinations across twelve benchmarks and tasks, achieving substantial improvements over existing methods by 7.62\\%. Our empirical results are further supported by rigorous theoretical analysis, under mild assumptions, verifying the effectiveness of introducing visual constraints into the self-rewarding paradigm. Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning.",
  "abstract_zh": "摘要：大型视觉语言模型（LVLMs）通过将预训练的大型语言模型（LLMs）和视觉模型结合进行指令调优，取得了显著进展。尽管如此，LVLMs常常表现出幻觉现象，即生成的文本响应在语言上似乎合理但与输入图像相矛盾，表明图像和文本对之间存在不匹配。这种不匹配是由于模型倾向于优先考虑文本信息而非视觉输入，即使语言模型和视觉表示都具有高质量。现有方法利用额外的模型或人工注释来整理偏好数据，并通过偏好优化增强模态对齐。这些方法资源密集，可能无法有效反映目标LVLM的偏好，使得整理的偏好容易被区分。我们的工作通过提出校准自奖励（CSR）方法来解决这些挑战，该方法使模型能够通过迭代生成候选响应、评估每个响应的奖励并整理偏好数据进行微调来自我改进。在奖励建模中，我们采用逐步策略并在自奖励过程中加入视觉约束，以更加强调视觉输入。实证结果表明，CSR显著提高了性能，并在十二个基准和任务中减少了幻觉现象，相较于现有方法实现了7.62\\%的显著改进。我们的实证结果通过在温和假设下的严格理论分析进一步支持，验证了在自奖励范式中引入视觉约束的有效性。此外，CSR显示出与不同视觉语言模型的兼容性，并能够通过迭代微调逐步提高性能。"
}
{
  "title": "Fight Back Against Jailbreaking via Prompt Adversarial Tuning",
  "title_zh": "题目：通过提示对抗调优反击越狱攻击",
  "abstract": "While Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to jailbreaking attacks. Several primary defense strategies have been proposed to protect LLMs from producing harmful information, mostly focusing on model fine-tuning or heuristical defense designs. However, how to achieve intrinsic robustness through prompt optimization remains an open problem. In this paper, motivated by adversarial training paradigms for achieving reliable robustness, we propose an approach named **Prompt Adversarial Tuning (PAT)** that trains a prompt control attached to the user prompt as a guard prefix. To achieve our defense goal whilst maintaining natural performance, we optimize the control prompt with both adversarial and benign prompts. Comprehensive experiments show that our method is effective against both grey-box and black-box attacks, reducing the success rate of advanced attacks to nearly 0, while maintaining the model's utility on the benign task and incurring only negligible computational overhead, charting a new perspective for future explorations in LLM security. Our code is available at https://github.com/PKU-ML/PAT.",
  "abstract_zh": "摘要：虽然大型语言模型（LLMs）在各种应用中取得了巨大成功，但它们也容易受到越狱攻击。已经提出了几种主要的防御策略来保护LLMs免于产生有害信息，主要集中在模型微调或启发式防御设计。然而，通过提示优化来实现内在鲁棒性仍然是一个未解决的问题。在本文中，受对抗训练范式以实现可靠鲁棒性的启发，我们提出了一种名为**提示对抗调优（PAT）**的方法，该方法训练一个附加在用户提示上的提示控制作为保护前缀。为了在保持自然性能的同时实现我们的防御目标，我们使用对抗性和良性提示来优化控制提示。综合实验表明，我们的方法对灰盒和黑盒攻击都有效，将高级攻击的成功率降低到几乎为0，同时在良性任务上保持模型的实用性，并且只产生可忽略的计算开销，为未来在LLM安全领域的探索开辟了新的视角。我们的代码可在https://github.com/PKU-ML/PAT获取。"
}
{
  "title": "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models",
  "title_zh": "大规模野外团队：从野外越狱到（对抗性）更安全的语言模型",
  "abstract": "We introduce WildTeaming, an automatic red-teaming framework that mines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of novel jailbreak tactics, and then composes selections of multiple mined tactics for systematic exploration of novel and even more challenging jailbreaks.\nCompared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with large language models (LLMs), our work investigates jailbreaks from chatbot users in-the-wild who were not specifically instructed to break the system.  WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in more diverse and successful adversarial attacks compared to state-of-the-art jailbreaking methods. \n\nWhile there exist many datasets for jailbreak evaluation, very few open-source datasets exist for jailbreak training, as safety training data has been closed among all frontier models even when their weights are open. Therefore, with WildTeaming we create WildJailbreak, a large-scale open-source synthetic safety dataset with 262K vanilla (direct request) and adversarial (complex jailbreak) prompt-response pairs. In order to mitigate exaggerated safety behaviors, WildJailbreak provides two contrastive types of queries: 1) harmful queries (both vanilla and adversarial) and 2) benign queries that resemble harmful queries in form but contain no harmful intent. As WildJailbreak considerably upgrades the quality and scale of existing safety resources, it uniquely enables us to examine the scaling effects of data and the interplay of data properties and model capabilities during safety training. Through extensive model training and evaluations, we identify the training properties that enable an ideal balance of safety behaviors: appropriate safeguarding without over-refusal, effective handling of both vanilla and adversarial queries, and minimal, if any, decrease in general capabilities. All the components of WildJailbreak contribute to achieving balanced safety behaviors of models",
  "abstract_zh": "我们介绍了WildTeaming，这是一种自动化的红队框架，通过挖掘野外用户与聊天机器人的互动，发现了5.7K个独特的新的越狱策略集群，然后组合多种挖掘出的策略进行系统性探索新的甚至更具挑战性的越狱。与之前通过招募人类工人、基于梯度的优化或使用大型语言模型（LLMs）进行迭代修订的红队工作相比，我们的工作研究了那些未被特别指示破坏系统的野外聊天机器人用户的越狱行为。WildTeaming揭示了前沿LLMs以前未识别的漏洞，导致比最先进的越狱方法更为多样和成功的对抗性攻击。尽管存在许多用于越狱评估的数据集，但用于越狱训练的开源数据集却很少，因为安全训练数据在所有前沿模型中都是封闭的，即使它们的权重是开放的。因此，通过WildTeaming，我们创建了WildJailbreak，一个大规模开源的合成安全数据集，包含262K个普通（直接请求）和对抗性（复杂越狱）提示-响应对。为了减轻夸大的安全行为，WildJailbreak提供了两种对比类型的查询：1）有害查询（包括普通和对抗性）和2）形式上类似有害查询但不含有害意图的良性查询。由于WildJailbreak显著提升了现有安全资源的质量和规模，它独特地使我们能够在安全训练中检验数据的扩展效应以及数据属性与模型能力的相互作用。通过广泛的模型训练和评估，我们识别出能够实现理想安全行为平衡的训练属性：适当的保护而不过度拒绝，有效处理普通和对抗性查询，以及在一般能力上几乎没有或没有下降。WildJailbreak的所有组件都为实现模型的平衡安全行为做出了贡献。"
}
{
  "title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus",
  "title_zh": "题目：通过原则性合成逻辑语料库增强大型语言模型的推理能力",
  "abstract": "Large language models (LLMs) are capable of solving a wide range of tasks, yet they have struggled with reasoning.\nTo address this, we propose $\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs' reasoning capabilities by program-generated logical reasoning samples.\nWe first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights.\nThen, based on these principles, we construct a synthetic corpus named $\\textbf{Formal} \\ \\textbf{Logic} \\ \\textbf{\\textit{D}eduction} \\ \\textbf{\\textit{D}iverse}$ (FLD$^{\\times2}$), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors.\nFinally, we empirically show that ALT on FLD$^{\\times2}$ substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B.\nImprovements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH.",
  "abstract_zh": "摘要：大型语言模型（LLMs）能够解决广泛的任务，但在推理方面仍存在困难。为了解决这一问题，我们提出了$\\textbf{附加逻辑训练（ALT）}$，旨在通过程序生成的逻辑推理样本来增强LLMs的推理能力。我们首先通过整合符号逻辑理论和先前的经验见解，建立设计高质量样本的原则。然后，基于这些原则，我们构建了一个名为$\\textbf{形式逻辑推理多样性}$（FLD$^{\\times2}$）的合成语料库，其中包含大量多步骤推理样本，涉及未知事实、多样化的推理规则、多样化的语言表达和具有挑战性的干扰项。最后，我们通过实验证明，基于FLD$^{\\times2}$的ALT显著增强了包括LLaMA-3.1-70B在内的最先进LLMs的推理能力。改进包括在逻辑推理基准上提高多达30分，在数学和编码基准上提高多达10分，以及在BBH基准套件上提高5分。"
}
{
  "title": "Yo'LLaVA: Your Personalized Language and Vision Assistant",
  "title_zh": "标题：Yo'LLaVA：您的个性化语言与视觉助手",
  "abstract": "Large Multimodal Models (LMMs) have shown remarkable capabilities across a variety of tasks (e.g., image captioning, visual question answering).\nWhile broad, their knowledge remains generic (e.g., recognizing a dog), and they are unable to handle personalized subjects (e.g., recognizing a user's pet dog).\n\nHuman reasoning, in contrast, typically operates within the context of specific subjects in our surroundings. For example, one might ask, \"What should I buy for *my dog*'s birthday?\"; as opposed to a generic inquiry about \"What should I buy for *a dog*'s birthday?\".\nSimilarly, when looking at a friend's image, the interest lies in seeing their activities (e.g., \"*my friend* is holding a cat\"), rather than merely observing generic human actions (e.g., \"*a man* is holding a cat\").\n\nIn this paper, we introduce the novel task of personalizing LMMs, so that they can have conversations about a specific subject. We propose Yo'LLaVA, which learns to embed a personalized subject into a set of latent tokens given a handful of example images of the subject.  Our qualitative and quantitative analyses reveal that Yo'LLaVA can learn the concept more efficiently using fewer tokens and more effectively encode the visual attributes compared to strong prompting baselines (e.g., LLaVA).",
  "abstract_zh": "摘要：大型多模态模型（LMMs）在多种任务中展现了非凡的能力（例如，图像字幕生成、视觉问答）。尽管其知识范围广泛，但仍然保持通用性（例如，识别一只狗），无法处理个性化的主题（例如，识别用户的宠物狗）。相比之下，人类的推理通常在我们周围特定主题的背景下进行。例如，人们可能会问：“我应该为*我的狗*的生日买什么？”而不是泛泛地询问“我应该为*一只狗*的生日买什么？”同样，当看到朋友的照片时，兴趣在于观察他们的活动（例如，“*我的朋友*正在抱着一只猫”），而不仅仅是观察一般的人类行为（例如，“*一个男人*正在抱着一只猫”）。在本文中，我们引入了个性化LMMs的新任务，使其能够围绕特定主题进行对话。我们提出了Yo'LLaVA，它通过学习将个性化主题嵌入一组潜在标记中，利用少量的主题示例图像进行训练。我们的定性和定量分析表明，与强大的提示基线（例如，LLaVA）相比，Yo'LLaVA能够更高效地使用更少的标记学习概念，并更有效地编码视觉属性。"
}
{
  "title": "Are More LLM Calls All You Need? Towards the Scaling Properties of Compound AI Systems",
  "title_zh": "标题：更多的LLM调用是您所需要的吗？复合AI系统的扩展属性研究",
  "abstract": "Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Language Model (LM) calls and aggregate their responses. However, there is little understanding of how the number of LM calls -- e.g., when asking the LM to answer each question multiple times and taking a majority vote -- affects such a compound system's performance. In this paper, we initiate the study of scaling properties of compound inference systems. We analyze, theoretically and empirically, how the number of LM calls affects the performance of Vote and Filter-Vote, two of the simplest compound system designs, which aggregate LM responses via majority voting, optionally applying LM filters. We find, surprisingly, that across multiple language tasks, the performance of both Vote and Filter-Vote can first increase but then decrease as a function of the number of LM calls. Our theoretical results suggest that this non-monotonicity is due to the diversity of query difficulties within a task: more LM calls lead to higher performance on \"easy\" queries, but lower performance on \"hard\" queries, and non-monotone behavior can emerge when a task contains both types of queries. This insight then allows us to compute, from a small number of samples, the number of LM calls that maximizes system performance, and define an analytical scaling model for both systems. Experiments show that our scaling model can accurately predict the performance of Vote and Filter-Vote systems and thus find the optimal number of LM calls to make.",
  "abstract_zh": "摘要：近年来，许多语言任务的最新技术成果是通过复合系统实现的，这些系统执行多次语言模型（LM）调用并聚合其响应。然而，对于LM调用次数如何影响此类复合系统的性能，了解甚少。例如，当要求LM多次回答每个问题并采用多数投票时，LM调用次数的影响尚不明确。在本文中，我们开始研究复合推理系统的扩展属性。我们从理论和实证上分析了LM调用次数如何影响Vote和Filter-Vote这两种最简单的复合系统设计的性能，这些系统通过多数投票聚合LM响应，并可选择性地应用LM过滤器。令人惊讶的是，我们发现，在多种语言任务中，Vote和Filter-Vote的性能随着LM调用次数的增加可能先提高后降低。我们的理论结果表明，这种非单调性是由于任务中查询难度的多样性：更多的LM调用提高了“简单”查询的性能，但降低了“困难”查询的性能，当任务包含这两种类型的查询时，可能会出现非单调行为。这个洞察力使我们能够从少量样本中计算出最大化系统性能的LM调用次数，并为这两种系统定义一个分析扩展模型。实验表明，我们的扩展模型可以准确预测Vote和Filter-Vote系统的性能，从而找到最佳的LM调用次数。"
}
{
  "title": "Should We Really Edit Language Models? On the Evaluation of Edited Language Models",
  "title_zh": "标题：我们真的应该编辑语言模型吗？关于编辑后语言模型的评估",
  "abstract": "Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. \nCurrent methods mainly focus on reliability, generalization, and locality,  with many methods excelling across these criteria. \nSome recent works disclose the pitfalls of these editing methods such as knowledge distortion or conflict. However, the general abilities of post-edited language models remain unexplored. \nIn this paper, we perform a comprehensive evaluation on various editing methods and different language models, and have following findings.\n(1) Existing editing methods lead to inevitable performance deterioration on general benchmarks, indicating that existing editing methods maintain the general abilities of the model within only a few dozen edits.\nWhen the number of edits is slightly large, the intrinsic knowledge structure of the model is disrupted or even completely damaged. \n(2) Instruction-tuned models are more robust to editing, showing less performance drop on general knowledge after editing. \n(3) Language model with large scale is more resistant to editing compared to small model.\n(4) The safety of the edited model, is significantly weakened, even for those safety-aligned models.\nOur findings indicate that current editing methods are only suitable for small-scale knowledge updates within language models, which motivates further research on more practical and reliable editing methods.",
  "abstract_zh": "摘要：模型编辑已成为一种越来越受欢迎的替代方案，用于高效更新语言模型中的知识。当前的方法主要关注可靠性、泛化性和局部性，许多方法在这些标准上表现出色。然而，一些最新研究揭示了这些编辑方法的缺陷，如知识扭曲或冲突。然而，编辑后语言模型的整体能力仍未被探索。在本文中，我们对各种编辑方法和不同语言模型进行了全面评估，并有以下发现。(1) 现有的编辑方法导致在一般基准测试上的性能不可避免地下降，这表明现有的编辑方法只能在少量编辑中保持模型的整体能力。当编辑数量稍大时，模型的内在知识结构被破坏甚至完全损坏。(2) 经过指令调优的模型对编辑更具鲁棒性，编辑后在一般知识上的性能下降较小。(3) 大规模语言模型相比小模型更能抵抗编辑。(4) 编辑后模型的安全性显著减弱，即使是那些安全对齐的模型。我们的研究结果表明，当前的编辑方法仅适用于语言模型中的小规模知识更新，这激励了对更实用和可靠的编辑方法的进一步研究。"
}
{
  "title": "UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation",
  "title_zh": "标题：UniBias：通过内部注意力和FFN操控揭示和缓解LLM偏差",
  "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various tasks using the in-context learning (ICL) paradigm. However, their effectiveness is often compromised by inherent bias, leading to prompt brittleness—sensitivity to design settings such as example selection, order, and prompt formatting. Previous studies have addressed LLM bias through external adjustment of model outputs, but the internal mechanisms that lead to such bias remain unexplored. Our work delves into these mechanisms, particularly investigating how feedforward neural networks (FFNs) and attention heads result in the bias of LLMs. By Interpreting the contribution of individual FFN vectors and attention heads, we identify the biased LLM components that skew LLMs' prediction toward specific labels. To mitigate these biases, we introduce UniBias, an inference-only method that effectively identifies and eliminates biased FFN vectors and attention heads. Extensive experiments across 12 NLP datasets demonstrate that UniBias significantly enhances ICL performance and alleviates prompt brittleness of LLMs.",
  "abstract_zh": "摘要：大型语言模型（LLM）在使用上下文学习（ICL）范式的各种任务中表现出色。然而，它们的有效性常常受到固有偏差的影响，导致提示脆弱性——对设计设置（如示例选择、顺序和提示格式）的敏感性。先前的研究通过外部调整模型输出来解决LLM偏差，但导致这种偏差的内部机制仍未被探索。我们的工作深入研究了这些机制，特别是研究前馈神经网络（FFN）和注意力头如何导致LLM的偏差。通过解释单个FFN向量和注意力头的贡献，我们识别出导致LLM预测偏向特定标签的偏差组件。为缓解这些偏差，我们引入了UniBias，这是一种仅推理的方法，能够有效识别并消除偏差的FFN向量和注意力头。跨越12个NLP数据集的大量实验表明，UniBias显著提升了ICL性能并缓解了LLM的提示脆弱性。"
}
{
  "title": "Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack",
  "title_zh": "标题：疫苗：针对有害微调攻击的大型语言模型的扰动感知对齐",
  "abstract": "The new paradigm of fine-tuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the fine-tuning to produce an alignment-broken model. We conduct an empirical analysis and uncover\na \\textit{harmful embedding drift} phenomenon, showing a probable \ncause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users  fine-tuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from  un-sanitized user data in the fine-tuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompts. Our code is available at  https://github.com/git-disl/Vaccine.",
  "abstract_zh": "摘要：微调即服务的新范式为大型语言模型（LLMs）引入了一个新的攻击面：用户上传的少量有害数据可以轻易地欺骗微调过程，产生一个对齐破坏的模型。我们进行了实证分析，揭示了一种\\textit{有害嵌入漂移}现象，显示了对齐破坏效应的可能原因。受到我们发现的启发，我们提出了疫苗，一种扰动感知对齐技术，以减轻用户微调的安全风险。疫苗的核心思想是在对齐阶段通过逐步添加精心设计的扰动来产生不变的隐藏嵌入。这使得嵌入能够在微调阶段抵御未净化用户数据的有害扰动。我们在开源主流LLMs（例如，Llama2、Opt、Vicuna）上的结果表明，疫苗可以提升对齐的鲁棒性，抵御由有害提示引起的嵌入漂移，同时保留对良性提示的推理能力。我们的代码可在https://github.com/git-disl/Vaccine获得。"
}
{
  "title": "Cascade Speculative Drafting for Even Faster LLM Inference",
  "title_zh": "级联推测起草以实现更快的LLM推理",
  "abstract": "Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The *Vertical Cascade* eliminates autoregressive generation from neural models, while the *Horizontal Cascade* optimizes time allocation in drafting for improved efficiency. Combining both cascades, CS Drafting achieves greater speedup compared to the baselines in our experiments, while preserving the same output distribution as the target model. Our code is publicly available at https://github.com/lfsszd/CS-Drafting.",
  "abstract_zh": "为了提高大型语言模型（LLM）推理的效率，引入了推测解码，其通过让一个较小的模型生成草稿来操作。然后，一个较大的目标模型审查该草稿以与其输出对齐，目标模型的任何接受都会减少目标模型运行的次数，从而最终提高效率。然而，推测解码中的起草过程包括缓慢的自回归生成，并且在生成标记时分配相等的时间，而不考虑其重要性。这些低效共同导致推测解码的性能不佳。为了进一步改进LLM推理，我们引入了级联推测起草（CS Drafting），这是一种结合了两种级联的推测执行算法。*垂直级联*消除了神经模型中的自回归生成，而*水平级联*则优化了起草中的时间分配以提高效率。结合这两种级联，CS Drafting在我们的实验中比基线实现了更大的加速，同时保持了与目标模型相同的输出分布。我们的代码在https://github.com/lfsszd/CS-Drafting公开可用。"
}
{
  "title": "Model Fusion through Bayesian Optimization in Language Model Fine-Tuning",
  "title_zh": "语言模型微调中的贝叶斯优化模型融合",
  "abstract": "Fine-tuning pre-trained models for downstream tasks is a widely adopted technique known for its adaptability and reliability across various domains. Despite its conceptual simplicity, fine-tuning entails several troublesome engineering choices, such as selecting hyperparameters and determining checkpoints from an optimization trajectory. To tackle the difficulty of choosing the best model, one effective solution is model fusion, which combines multiple models in a parameter space. However, we observe a large discrepancy between loss and metric landscapes during the fine-tuning of pre-trained language models. Building on this observation, we introduce a novel model fusion technique that optimizes both the desired metric and loss through multi-objective Bayesian optimization. In addition, to effectively select hyperparameters, we establish a two-stage procedure by integrating Bayesian optimization processes into our framework. Experiments across various downstream tasks show considerable performance improvements using our Bayesian optimization-guided method.",
  "abstract_zh": "微调预训练模型以适应下游任务是一种广泛采用的技术，以其在各个领域的适应性和可靠性而闻名。尽管其概念简单，微调涉及若干麻烦的工程选择，例如选择超参数和从优化轨迹中确定检查点。为了解决选择最佳模型的困难，一个有效的解决方案是模型融合，它在参数空间中结合多个模型。然而，我们观察到在预训练语言模型的微调过程中，损失和指标景观之间存在很大差异。基于这一观察，我们引入了一种新颖的模型融合技术，通过多目标贝叶斯优化来优化所需的指标和损失。此外，为了有效选择超参数，我们通过将贝叶斯优化过程整合到我们的框架中建立了一个两阶段程序。在各种下游任务中的实验表明，使用我们的贝叶斯优化指导方法可以显著提高性能。"
}
{
  "title": "Kangaroo: Lossless Self-Speculative Decoding for Accelerating LLMs via Double Early Exiting",
  "title_zh": "袋鼠：通过双重提前退出加速大型语言模型的无损自推测解码",
  "abstract": "Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models (LLMs) while maintaining an identical sampling distribution. However, the conventional approach of training separate draft model to achieve a satisfactory token acceptance rate can be costly and impractical. In this paper, we propose a novel self-speculative decoding framework \\emph{Kangaroo} with \\emph{double} early exiting strategy, which leverages the shallow sub-network and the \\texttt{LM Head} of the well-trained target LLM to construct a self-drafting model. Then, the self-verification stage only requires computing the remaining layers over the \\emph{early-exited} hidden states in parallel. To bridge the representation gap between the sub-network and the full model, we train a lightweight and efficient adapter module on top of the sub-network. One significant challenge that comes with the proposed method is that the inference latency of the self-draft model may no longer be negligible compared to the big model. To boost the token acceptance rate while minimizing the latency of the self-drafting model, we introduce an additional \\emph{early exiting} mechanism for both single-sequence and the tree decoding scenarios. Specifically, we dynamically halt the small model's subsequent prediction during the drafting phase once the confidence level for the current step falls below a certain threshold. This approach reduces unnecessary computations and improves overall efficiency. Extensive experiments on multiple benchmarks demonstrate our effectiveness, where Kangaroo achieves walltime speedups up to 2.04$\\times$, outperforming Medusa-1 with 88.7\\% fewer additional parameters. The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo.",
  "abstract_zh": "推测解码已证明其在加速大型语言模型（LLMs）推理的同时保持相同采样分布的有效性。然而，传统方法需要训练单独的草稿模型以实现满意的令牌接受率，这可能代价高昂且不切实际。在本文中，我们提出了一种新颖的自推测解码框架\\emph{袋鼠}，采用\\emph{双重}提前退出策略，利用训练良好的目标LLM的浅层子网络和\\texttt{LM Head}构建自草稿模型。然后，自验证阶段仅需并行计算\\emph{提前退出}的隐藏状态上的剩余层。为了弥合子网络与完整模型之间的表示差距，我们在子网络之上训练了一个轻量且高效的适配器模块。该方法带来的一个显著挑战是，自草稿模型的推理延迟可能不再可以忽略不计。为了在最小化自草稿模型延迟的同时提高令牌接受率，我们为单序列和树解码场景引入了额外的\\emph{提前退出}机制。具体而言，一旦当前步骤的置信水平低于某一阈值，我们在草稿阶段动态停止小模型的后续预测。此方法减少了不必要的计算，提高了整体效率。在多个基准上的大量实验表明了我们的有效性，其中袋鼠实现了高达2.04倍的墙时加速，超越了Medusa-1，并减少了88.7%的额外参数。袋鼠的代码可在https://github.com/Equationliu/Kangaroo获取。"
}
{
  "title": "Order-Independence Without Fine Tuning",
  "title_zh": "标题：无需微调的顺序独立性",
  "abstract": "The development of generative language models that can create long and coherent textual outputs via autoregression has lead to a proliferation of uses and a corresponding sweep of analyses as researches work to determine the limitations of this new paradigm. Unlike humans, these '*Large Language Models*' (LLMs) are highly sensitive to small changes in their inputs, leading to unwanted inconsistency in their behavior. One problematic inconsistency when LLMs are used to answer multiple-choice questions or analyze multiple inputs is *order dependency*: the output of an LLM can (and often does) change significantly when sub-sequences are swapped, despite both orderings being semantically identical. In this paper we present , a technique that *guarantees* the output of an LLM will not have order dependence on a specified set of sub-sequences. We show that this method *provably* eliminates order dependency, and that it can be applied to *any* transformer-based LLM to enable text generation that is unaffected by re-orderings. Delving into the implications of our method, we show that, despite our inputs being out of distribution, the impact on expected accuracy is small, where the expectation is over the order of uniformly chosen shuffling of the candidate responses, and usually significantly less in practice. Thus, can be used as a '*dropped-in*' method on fully trained models. Finally, we discuss how our method's success suggests that other strong guarantees can be obtained on LLM performance via modifying the input representations.\n\nCode is available at [github.com/reidmcy/set-based-prompting](https://github.com/reidmcy/set-based-prompting.).",
  "abstract_zh": "摘要：生成语言模型的发展使其能够通过自回归生成长篇连贯的文本输出，导致其用途激增，并引发一系列分析，研究人员致力于确定这一新范式的局限性。与人类不同，这些“大型语言模型”（LLMs）对输入的微小变化高度敏感，导致其行为出现不一致。当LLMs用于回答选择题或分析多个输入时，一个问题性的不一致是“顺序依赖”：即使两个顺序在语义上是相同的，LLM的输出在子序列交换时可能（且经常）发生显著变化。在本文中，我们提出了一种技术，*保证* LLM的输出在指定的子序列集合上不会有顺序依赖。我们证明这种方法可以*有效地*消除顺序依赖，并且可以应用于*任何*基于Transformer的LLM，以实现不受重新排序影响的文本生成。深入探讨我们方法的影响，我们表明，尽管我们的输入不在分布内，但对预期准确性的影响很小，其中期望是对候选响应的均匀选择洗牌顺序的期望，并且在实践中通常显著更小。因此，可以作为一种“*插入*”方法用于完全训练的模型。最后，我们讨论了我们方法的成功如何表明，通过修改输入表示，可以获得LLM性能的其他强保证。代码可在[github.com/reidmcy/set-based-prompting](https://github.com/reidmcy/set-based-prompting)获得。"
}
{
  "title": "PaCE: Parsimonious Concept Engineering for Large Language Models",
  "title_zh": "标题: PaCE: 大型语言模型的简约概念工程",
  "abstract": "Large Language Models (LLMs) are being used for a wide variety of tasks. While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations. Alignment methods are designed to reduce such undesirable output, via techniques such as fine-tuning, prompt engineering, and representation engineering. However, existing methods face several challenges: some require costly fine-tuning for every alignment task; some do not adequately remove undesirable concepts, failing alignment; some remove benign concepts, lowering the linguistic capabilities of LLMs. To address these issues, we propose Parsimonious Concept Engineering (PaCE), a novel activation engineering framework for alignment. First, to sufficiently model the concepts, we construct a large-scale concept dictionary in the activation space, in which each atom corresponds to a semantic concept. Given any alignment task, we instruct a concept partitioner to efficiently annotate the concepts as benign or undesirable. Then, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activations as linear combinations of benign and undesirable components. By removing the latter ones from the activations, we reorient the behavior of the LLM towards the alignment goal. We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities.",
  "abstract_zh": "摘要: 大型语言模型（LLMs）被用于各种任务。虽然它们能够生成类似人类的响应，但也可能产生不良输出，包括潜在有害信息、种族歧视或性别歧视语言以及幻觉。对齐方法旨在通过微调、提示工程和表示工程等技术减少此类不良输出。然而，现有方法面临几个挑战：有些需要为每个对齐任务进行昂贵的微调；有些未能充分去除不良概念，导致对齐失败；有些去除了良性概念，降低了LLMs的语言能力。为了解决这些问题，我们提出了简约概念工程（PaCE），一种用于对齐的新型激活工程框架。首先，为了充分建模概念，我们在激活空间中构建了一个大规模概念字典，其中每个原子对应一个语义概念。针对任何对齐任务，我们指导概念分区器有效地将概念标注为良性或不良。然后，在推理时，我们通过稀疏编码沿着概念字典分解LLM激活，以线性组合的形式准确表示激活的良性和不良成分。通过从激活中移除后者，我们重新定向LLM的行为以实现对齐目标。我们在响应去毒化、忠实性增强和情感修正等任务上进行实验，表明PaCE在保持语言能力的同时实现了最先进的对齐性能。"
}
{
  "title": "Aligner: Efficient Alignment by Learning to Correct",
  "title_zh": "校准器：通过学习纠正实现高效对齐",
  "abstract": "With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce Aligner, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-and-play module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, Aligner can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model's performance ceiling. Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, Aligner-7B has achieved an average improvement of 68.9\\% in helpfulness and 23.8\\% in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0\\% to 58.3\\%, surpassing GPT-4 Omni's 57.5\\% Win Rate (community report).",
  "abstract_zh": "随着大型语言模型（LLMs）的快速发展和不断变化的实际需求，找到一种高效且有效的对齐方法变得前所未有的重要。然而，当前对齐方法的复杂性与部署场景中快速迭代的需求之间的紧张关系，迫使我们开发一种能够在这些限制下运行的模型无关对齐方法。在本文中，我们介绍了Aligner，这是一种新颖且简单的对齐范式，它通过小模型学习优选答案和非优选答案之间的校正残差。Aligner被设计为一个模型无关的即插即用模块，可以直接应用于各种开源和基于API的模型，仅需一次性训练，使其适合快速迭代。值得注意的是，Aligner可以应用于任何强大的大型上游模型。此外，它甚至可以通过使用校正后的响应作为合成人类偏好数据来迭代地引导上游模型，突破模型的性能上限。我们的实验通过在11种不同的LLM上部署相同的Aligner模型，评估其在3H维度（有用性、无害性和诚实性）上的性能提升。具体而言，Aligner-7B在测试的LLM中实现了平均68.9%的有用性提升和23.8%的无害性提升，同时有效减少了幻觉。在Alpaca-Eval排行榜中，将Aligner-2B叠加在GPT-4 Turbo上，将其LC胜率从55.0%提高到58.3%，超过了GPT-4 Omni的57.5%胜率（社区报告）。"
}
{
  "title": "Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration",
  "title_zh": "题目：迈向统一的多模态编辑与增强的知识协作",
  "abstract": "The swift advancement in Multimodal LLMs (MLLMs) also presents significant challenges for effective knowledge editing. Current methods, including intrinsic knowledge editing and external knowledge resorting, each possess strengths and weaknesses, struggling to balance the desired properties of reliability, generality, and locality when applied to MLLMs. In this paper, we propose \\textbf{UniKE}, a novel multimodal editing method that establishes a unified perspective and paradigm for intrinsic knowledge editing and external knowledge resorting. Both types of knowledge are conceptualized as vectorized key-value memories, with the corresponding editing processes resembling the assimilation and accommodation phases of human cognition, conducted at the same semantic levels.  Within such a unified framework, we further promote knowledge collaboration by disentangling the knowledge representations into the semantic and truthfulness spaces. Extensive experiments validate the effectiveness of our method, which ensures that the post-edit MLLM simultaneously maintains excellent reliability, generality, and locality. The code for UniKE is available at https://github.com/beepkh/UniKE.",
  "abstract_zh": "摘要：多模态大语言模型（MLLMs）的快速发展也为有效的知识编辑带来了重大挑战。目前的方法，包括内在知识编辑和外部知识求助，各自具有优缺点，在应用于MLLMs时难以平衡可靠性、通用性和局部性等期望特性。在本文中，我们提出了一种新颖的多模态编辑方法\\textbf{UniKE}，它为内在知识编辑和外部知识求助建立了统一的视角和范式。两种类型的知识被概念化为向量化的键值存储，其相应的编辑过程类似于人类认知的同化和顺应阶段，并在相同的语义层次上进行。在这样一个统一的框架内，我们通过将知识表示解耦为语义空间和真实性空间来进一步促进知识协作。大量实验验证了我们方法的有效性，确保编辑后的MLLM同时保持出色的可靠性、通用性和局部性。UniKE的代码可在https://github.com/beepkh/UniKE获得。"
}
{
  "title": "Vitron: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing",
  "title_zh": "标题：Vitron: 一种用于理解、生成、分割、编辑的统一像素级视觉大语言模型",
  "abstract": "Recent developments of vision large language models (LLMs) have seen remarkable progress, yet still encounter challenges towards multimodal generalists, such as coarse-grained instance-level understanding, lack of unified support for both images and videos, and insufficient coverage across various vision tasks. In this paper we present Vitron, a universal pixel-level vision LLM designed for comprehensive understanding, generating, segmenting, and editing of both static images and dynamic videos. Building on top of an LLM backbone, Vitron incorporates encoders for images, videos, and pixel-level regional visuals within its frontend modules, while employing state-of-the-art visual specialists as its backend, via which Vitron supports a spectrum of vision end tasks, spanning visual comprehension to visual generation, from low level to high level. To ensure an effective and precise message passing from LLM to backend modules for function invocation, we propose a novel hybrid method by simultaneously integrating discrete textual instructions and continuous signal embeddings. Further, we design various pixel-level spatiotemporal vision-language alignment learning for Vitron to reach the best fine-grained visual capability. Finally, a cross-task synergy module is advised to learn to maximize the task-invariant fine-grained visual features, enhancing the synergy between different visual tasks. Demonstrated over 12 visual tasks and evaluated across 22 datasets, Vitron showcases its extensive capabilities in the four main vision task clusters. Overall, this work illuminates the great potential of developing a more unified multimodal generalist.",
  "abstract_zh": "摘要：近年来，视觉大语言模型（LLMs）的发展取得了显著进展，但在朝向多模态通用模型的过程中仍面临挑战，如粗粒度实例级理解、缺乏对图像和视频的统一支持，以及对各种视觉任务的覆盖不足。在本文中，我们提出了Vitron，一种通用的像素级视觉大语言模型，旨在对静态图像和动态视频进行全面的理解、生成、分割和编辑。Vitron基于LLM骨干，前端模块集成了图像、视频和像素级区域视觉编码器，同时在后端采用最先进的视觉专家，通过这些模块，Vitron支持从低级到高级的视觉任务，从视觉理解到视觉生成。为了确保从LLM到后端模块的功能调用信息传递的有效性和精确性，我们提出了一种新颖的混合方法，同时整合离散文本指令和连续信号嵌入。此外，我们设计了多种像素级时空视觉语言对齐学习，以提升Vitron的细粒度视觉能力。最后，建议使用跨任务协同模块来学习最大化任务不变的细粒度视觉特征，增强不同视觉任务之间的协同作用。通过12种视觉任务和22个数据集的评估，Vitron展示了其在四个主要视觉任务集群中的广泛能力。总体而言，这项工作揭示了开发更统一的多模态通用模型的巨大潜力。"
}
{
  "title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
  "title_zh": "思维链的缺失？对规划中思维链的分析",
  "abstract": "Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting--a method of demonstrating solution procedures--with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem.\nThis paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples.\nWe also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes.\nOur results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.",
  "abstract_zh": "大型语言模型（LLM）在推理问题上的表现通常不能推广到分布之外。先前的研究声称，这可以通过思维链提示——一种展示解决方案过程的方法——来缓解，直觉上认为可以在上下文中教授LLM一种解决问题的算法。本文对经典规划领域Blocksworld中的问题进行了思维链案例研究，并考察了两种最先进的LLM在两个维度上的表现：提示中给出的示例的普遍性，以及每个提示查询的问题复杂性。尽管我们的问题非常简单，但我们发现只有当提示极其具体地针对其问题类别时，思维链提示才有意义的性能提升，并且当查询指定堆栈的大小n超过示例中显示的堆栈大小时，这些提升迅速恶化。我们还创建了三个常见于先前CoT论文中的领域的可扩展变体，并展示了类似失败模式的存在。我们的结果暗示，与文献中的先前主张相反，CoT的性能提升并非源于模型通过演示学习通用算法过程，而是依赖于精心设计的高度问题特定的提示。这突显了思维链的缺点，特别是可能的性能提升与生成具有正确推理轨迹的示例所需的人力之间的明显权衡。"
}
{
  "title": "Large Language Models Play StarCraft II:Benchmarks and A Chain of Summarization Approach",
  "title_zh": "大型语言模型玩星际争霸II：基准测试与总结链方法",
  "abstract": "With the continued advancement of Large Language Models (LLMs) Agents in reasoning, planning, and decision-making, benchmarks have become crucial in evaluating these skills. However, there is a notable gap in benchmarks for real-time strategic decision-making. StarCraft II (SC2), with its complex and dynamic nature, serves as an ideal setting for such evaluations. To this end, we have developed TextStarCraft II, a specialized environment for assessing LLMs in real-time strategic scenarios within SC2. Addressing the limitations of traditional Chain of Thought (CoT) methods, we introduce the Chain of Summarization (CoS) method, enhancing LLMs' capabilities in rapid and effective decision-making. Our key experiments included:\n1. LLM Evaluation: Tested 10 LLMs in TextStarCraft II, most of them defeating LV5 build-in AI, showcasing effective strategy skills.\n2. Commercial Model Knowledge: Evaluated four commercial  models on SC2 knowledge; GPT-4 ranked highest by Grandmaster-level experts.\n3. Human-AI Matches: Experimental results showed that fine-tuned LLMs performed on par with Gold-level players in real-time matches, demonstrating comparable strategic abilities.\n\nAll code and data from this\nstudy have been made pulicly available at https://github.com/histmeisah/Large-Language-Models-play-StarCraftII",
  "abstract_zh": "随着大型语言模型（LLMs）在推理、规划和决策方面的不断进步，基准测试在评估这些技能中变得至关重要。然而，在实时战略决策的基准测试中存在显著差距。星际争霸II（SC2）以其复杂和动态的特性成为此类评估的理想环境。为此，我们开发了TextStarCraft II，一个专门用于评估LLMs在SC2中实时战略场景的环境。针对传统思维链（CoT）方法的局限性，我们引入了总结链（CoS）方法，增强了LLMs在快速和有效决策中的能力。我们的关键实验包括：1. LLM评估：在TextStarCraft II中测试了10个LLM，其中大多数击败了LV5内置AI，展示了有效的战略技能。2. 商业模型知识：评估了四个商业模型在SC2知识上的表现；GPT-4被特级大师级专家评为最高。3. 人机对战：实验结果表明，经过微调的LLMs在实时比赛中表现与黄金级玩家相当，展示了可比的战略能力。本研究的所有代码和数据已在https://github.com/histmeisah/Large-Language-Models-play-StarCraftII上公开提供。"
}
{
  "title": "To Believe or Not to Believe Your LLM: IterativePrompting for Estimating Epistemic Uncertainty",
  "title_zh": "相信还是不相信你的大型语言模型：通过迭代提示估算认知不确定性",
  "abstract": "We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.",
  "abstract_zh": "我们研究了大型语言模型（LLM）中的不确定性量化，旨在识别查询响应中的不确定性何时较大。我们同时考虑了认知不确定性和随机不确定性，前者来自于对事实或语言等真相的缺乏了解，后者来自于不可减少的随机性（如多种可能的答案）。特别是，我们推导出一种信息论度量，能够可靠地检测出仅当认知不确定性较大时，模型的输出是不可靠的。这种条件可以仅基于通过一些基于先前响应的特殊迭代提示获得的模型输出来计算。例如，这种量化方法可以检测单一和多答案响应中的幻觉（即认知不确定性较高的情况）。这与许多标准的不确定性量化策略（如对响应的对数似然值进行阈值处理）形成对比，在多答案情况下无法检测幻觉。我们进行了一系列实验，展示了我们方法的优势。此外，我们的研究揭示了大型语言模型对给定输出分配的概率如何通过迭代提示而被放大，这可能具有独立的研究兴趣。"
}
{
  "title": "LT-Defense: Searching-free Backdoor Defense via Exploiting the Long-tailed Effect",
  "title_zh": "标题：LT-Defense：利用长尾效应的无搜索后门防御",
  "abstract": "Language models have shown vulnerability against backdoor attacks, threatening the security of services based on them. To mitigate the threat, existing solutions attempted to search for backdoor triggers, which can be time-consuming when handling a large search space. Looking into the attack process, we observe that poisoned data will create a long-tailed effect in the victim model, causing the decision boundary to shift towards the attack targets. Inspired by this observation, we introduce LT-Defense, the first searching-free backdoor defense via exploiting the long-tailed effect. Specifically, LT-Defense employs a small set of clean examples and two metrics to distinguish backdoor-related features in the target model. Upon detecting a backdoor model, LT-Defense additionally provides test-time backdoor freezing and attack target prediction. Extensive experiments demonstrate the effectiveness of LT-Defense in both detection accuracy and efficiency, e.g., in task-agnostic scenarios, LT-Defense achieves 98% accuracy across 1440 models with less than 1% of the time cost of state-of-the-art solutions.",
  "abstract_zh": "摘要：语言模型在面对后门攻击时表现出脆弱性，威胁到基于它们的服务的安全性。为减轻这一威胁，现有解决方案尝试搜索后门触发器，但在处理大搜索空间时可能耗时。观察攻击过程，我们发现被污染的数据会在受害模型中产生长尾效应，导致决策边界向攻击目标偏移。受此观察启发，我们引入了LT-Defense，这是首个通过利用长尾效应实现无搜索后门防御的方法。具体而言，LT-Defense使用一小组干净的样本和两个指标来区分目标模型中的后门相关特征。在检测到后门模型后，LT-Defense还提供测试时后门冻结和攻击目标预测。大量实验表明，LT-Defense在检测准确性和效率方面都表现出色，例如，在任务无关场景中，LT-Defense在1440个模型中实现了98%的准确率，时间成本不到最先进解决方案的1%。"
}
{
  "title": "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks",
  "title_zh": "强健的提示优化：防御语言模型的越狱攻击",
  "abstract": "Despite advances in AI alignment, large language models (LLMs) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries can modify prompts to induce unwanted behavior. While some defenses have been proposed, they have not been adapted to newly proposed attacks and more challenging threat models. To address this, we propose an optimization-based objective for defending LLMs against jailbreaking attacks and an algorithm, Robust Prompt Optimization (RPO), to create robust system-level defenses. Our approach directly incorporates the adversary into the defensive objective and optimizes a lightweight and transferable suffix, enabling RPO to adapt to worst-case adaptive attacks. Our theoretical and experimental results show improved robustness to both jailbreaks seen during optimization and unknown jailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2 to 0% on JailbreakBench, setting the state-of-the-art.",
  "abstract_zh": "尽管人工智能对齐技术取得了进展，大型语言模型（LLMs）仍然容易受到对抗性攻击或越狱攻击，攻击者可以修改提示以引发不良行为。虽然已经提出了一些防御措施，但它们尚未适应新提出的攻击和更具挑战性的威胁模型。为了解决这个问题，我们提出了一种基于优化的目标，用于防御LLMs的越狱攻击，并提出了一种算法，强健提示优化（RPO），以创建强健的系统级防御。我们的方法直接将对手纳入防御目标，并优化一个轻量且可转移的后缀，使RPO能够适应最坏情况下的自适应攻击。我们的理论和实验结果显示出对优化过程中遇到的越狱攻击和未知越狱攻击的增强鲁棒性，将GPT-4的攻击成功率（ASR）降低到6%，Llama-2降低到0%在JailbreakBench上，达到了最新的技术水平。"
}
{
  "title": "Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning",
  "title_zh": "星代理：使用大型语言模型代理进行指令微调的数据自动优化",
  "abstract": "The efficacy of large language models (LLMs) on downstream tasks usually hinges on instruction tuning, which relies critically on the quality of training data. Unfortunately, collecting high-quality and diverse data is both expensive and time-consuming. To mitigate this issue, we propose  a novel Star-Agents framework, which automates the enhancement of data quality across datasets through multi-agent collaboration and assessment. The framework adopts a three-pronged strategy. It  initially generates diverse instruction data with multiple LLM agents through a bespoke sampling method. Subsequently, the generated data undergo a rigorous evaluation using a dual-model method that assesses both difficulty and quality. Finaly, the above process evolves in a dynamic refinement phase, where more effective LLMs are prioritized, enhancing the overall data quality. Our empirical studies, including instruction tuning experiments with models such as Pythia and LLaMA, demonstrate the effectiveness of the proposed framework. Optimized datasets have achieved substantial improvements, with an average increase of 12\\% and notable gains in specific metrics, such as a 40\\% improvement in Fermi, as evidenced by benchmarks like MT-bench, Vicuna bench, and WizardLM testset. Codes will be released soon.",
  "abstract_zh": "大型语言模型（LLMs）在下游任务中的有效性通常依赖于指令微调，而这又极大地依赖于训练数据的质量。不幸的是，收集高质量和多样化的数据既昂贵又耗时。为了解决这个问题，我们提出了一种新颖的星代理框架，通过多代理协作和评估自动提高数据集的数据质量。该框架采用三管齐下的策略。首先，通过定制的采样方法，使用多个LLM代理生成多样化的指令数据。随后，生成的数据通过双模型方法进行严格评估，以评估其难度和质量。最后，上述过程进入动态优化阶段，优先考虑更有效的LLM，从而提高整体数据质量。我们的实证研究，包括使用Pythia和LLaMA等模型进行的指令微调实验，证明了所提出框架的有效性。优化后的数据集取得了显著的改进，平均提高了12%，在某些特定指标上取得了显著的提升，例如Fermi提高了40%，这在MT-bench、Vicuna bench和WizardLM测试集等基准中得到了验证。代码将很快发布。"
}
{
  "title": "Towards Neuron Attributions in Multi-Modal Large Language Models",
  "title_zh": "题目：面向多模态大型语言模型的神经元归因研究",
  "abstract": "As Large Language Models (LLMs) demonstrate impressive capabilities, demystifying their internal mechanisms becomes increasingly vital. Neuron attribution, which attributes LLM outputs to specific neurons to reveal the semantic properties they learn, has emerged as a key interpretability approach. However, while neuron attribution has made significant progress in deciphering text-only LLMs, its application to Multimodal LLMs (MLLMs) remains less explored. To address this gap, we propose a novel Neuron Attribution method tailored for MLLMs, termed NAM. Specifically, NAM not only reveals the modality-specific semantic knowledge learned by neurons within MLLMs, but also highlights several intriguing properties of neurons, such as cross-modal invariance and semantic sensitivity. These properties collectively elucidate the inner workings mechanism of MLLMs, providing a deeper understanding of how MLLMs process and generate multi-modal content. Through theoretical analysis and empirical validation, we demonstrate the efficacy of NAM and the valuable insights it offers. Furthermore, leveraging NAM, we introduce a multi-modal knowledge editing paradigm, underscoring the practical significance of our approach for downstream applications of MLLMs.",
  "abstract_zh": "摘要：随着大型语言模型（LLMs）展现出令人印象深刻的能力，揭示其内部机制变得愈发重要。神经元归因通过将LLM输出归因于特定神经元以揭示其学习的语义属性，已成为关键的可解释性方法。然而，尽管神经元归因在解读仅限文本的LLMs方面取得了显著进展，其在多模态LLMs（MLLMs）中的应用仍然较少探索。为填补这一空白，我们提出了一种针对MLLMs的新型神经元归因方法，称为NAM。具体而言，NAM不仅揭示了MLLMs中神经元学习的模态特定语义知识，还突出了神经元的几个有趣特性，如跨模态不变性和语义敏感性。这些特性共同阐明了MLLMs的内部工作机制，提供了对MLLMs如何处理和生成多模态内容的更深入理解。通过理论分析和实证验证，我们展示了NAM的有效性及其提供的宝贵见解。此外，利用NAM，我们引入了一种多模态知识编辑范式，强调了我们方法在MLLMs下游应用中的实际意义。"
}
{
  "title": "AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback",
  "title_zh": "AMOR：通过过程反馈构建可适应模块化知识代理的方案",
  "abstract": "The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM)\nthat solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets, enabling AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of AMOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism. The code and data are publicly available at\nhttps://github.com/JianGuanTHU/AMOR.",
  "abstract_zh": "大型语言模型（LLMs）的显著成功引发了构建语言代理以完成各种复杂任务的热潮。我们提出了AMOR，这是一种基于开源LLMs的代理框架，它通过人类监督推理过程来利用外部知识库并适应特定领域。AMOR在有限状态机（FSM）上构建推理逻辑，通过自主执行和模块化转换来解决问题。这使得人类能够对单个模块提供直接反馈，从而自然形成过程监督。基于这种推理和反馈框架，我们通过两个阶段的微调开发了AMOR：热身和适应。前者通过从各种公共数据集中自动构建的示例微调LLM，使AMOR能够在不同的知识环境中进行泛化，而后者则使用过程反馈将AMOR定制到特定领域。在多个领域的广泛实验中，AMOR凭借其基于FSM的推理和过程反馈机制，相较于强大的基线展示了优势。代码和数据可在https://github.com/JianGuanTHU/AMOR公开获取。"
}
{
  "title": "INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness",
  "title_zh": "标题：INDICT：通过内部批判对话生成代码以提高安全性和实用性",
  "abstract": "Large language models (LLMs) for code are typically trained to align with natural language instructions to closely follow their intentions and requirements. However, in many practical scenarios, it becomes increasingly challenging for these models to navigate the intricate boundary between helpfulness and safety, especially against highly complex yet potentially malicious instructions. In this work, we introduce INDICT: a new framework that empowers LLMs with Internal Dialogues of Critiques for both safety and helpfulness guidance. The internal dialogue is a dual cooperative system between a safety-driven critic and a helpfulness-driven critic. Each critic provides analysis against the given task and corresponding generated response, equipped with external knowledge queried through relevant code snippets and tools like web search and code interpreter. We engage the dual critic system in both code generation stage as well as code execution stage, providing preemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8 diverse tasks across 8 programming languages from 5 benchmarks, using LLMs from 7B to 70B parameters. We observed that our approach can provide an advanced level of critiques of both safety and helpfulness analysis, significantly improving the quality of output codes (+10% absolute improvements in all models).",
  "abstract_zh": "摘要：代码的大型语言模型（LLMs）通常经过训练以与自然语言指令对齐，从而紧密遵循其意图和要求。然而，在许多实际场景中，这些模型在有用性和安全性之间的复杂界限上变得越来越难以驾驭，特别是在面对高度复杂但可能具有恶意的指令时。在这项工作中，我们介绍了INDICT：一个新的框架，赋予LLMs以内部批判对话来指导安全性和实用性。内部对话是一个安全导向的批评者和一个实用性导向的批评者之间的双重合作系统。每个批评者都对给定任务及相应生成的响应进行分析，并通过相关代码片段和工具（如网络搜索和代码解释器）查询外部知识。我们在代码生成阶段和代码执行阶段都引入了双重批评系统，分别为LLMs提供预防性和事后指导。我们在5个基准的8种编程语言中的8个不同任务上评估了INDICT，使用了从7B到70B参数的LLMs。我们观察到，我们的方法可以在安全性和实用性分析方面提供高级别的批评，显著提高输出代码的质量（所有模型的绝对改进+10%）。"
}
{
  "title": "Query-Based Adversarial Prompt Generation",
  "title_zh": "基于查询的对抗性提示生成",
  "abstract": "Recent work has shown it is possible to construct adversarial examples that cause aligned language models to emit harmful strings or perform harmful behavior.\nExisting attacks work either in the white-box setting (with full access to the model weights), or through _transferability_: the phenomenon that adversarial examples crafted on one model often remain effective on other models.\nWe improve on prior work with a _query-based_ attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks.\nWe validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the OpenAI and Llama Guard safety classifiers with nearly 100% probability.",
  "abstract_zh": "最近的研究表明，可以构造对抗性示例，使得经过调整的语言模型产生有害字符串或执行有害行为。现有的攻击要么在白盒设置中工作（完全访问模型权重），要么通过_可迁移性_：即在一个模型上制作的对抗性示例通常在其他模型上仍然有效的现象。我们改进了之前的工作，提出了一种_基于查询_的攻击，该攻击利用对远程语言模型的API访问来构建对抗性示例，使模型产生有害字符串的概率（远）高于仅迁移攻击。我们在GPT-3.5和OpenAI的安全分类器上验证了我们的攻击；我们可以使GPT-3.5产生当前迁移攻击失败的有害字符串，并且我们可以以接近100%的概率避开OpenAI和Llama Guard的安全分类器。"
}
{
  "title": "Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities",
  "title_zh": "标题：核语言熵：从语义相似性中对大型语言模型进行细粒度不确定性量化",
  "abstract": "Uncertainty quantification in Large Language Models (LLMs) is crucial for applications where safety and reliability are important. In particular, uncertainty can be used to improve the trustworthiness of LLMs by detecting factually incorrect model responses, commonly called hallucinations. Critically, one should seek to capture the model's semantic uncertainty, i.e., the uncertainty over the meanings of LLM outputs, rather than uncertainty over lexical or syntactic variations that do not affect answer correctness.\nTo address this problem, we propose Kernel Language Entropy (KLE), a novel method for uncertainty estimation in white- and black-box LLMs. KLE defines positive semidefinite unit trace kernels to encode the semantic similarities of LLM outputs and quantifies uncertainty using the von Neumann entropy. It considers pairwise semantic dependencies between answers (or semantic clusters), providing more fine-grained uncertainty estimates than previous methods based on hard clustering of answers. We theoretically prove that KLE generalizes the previous state-of-the-art method called semantic entropy and empirically demonstrate that it improves uncertainty quantification performance across multiple natural language generation datasets and LLM architectures.",
  "abstract_zh": "摘要：在大型语言模型（LLMs）中进行不确定性量化对于安全性和可靠性重要的应用至关重要。特别是，不确定性可以用于通过检测事实不正确的模型响应（通常称为幻觉）来提高LLMs的可信度。关键是，应该努力捕捉模型的语义不确定性，即LLM输出含义的不确定性，而不是影响答案正确性的词汇或句法变化的不确定性。为了解决这个问题，我们提出了核语言熵（KLE），这是一种用于白盒和黑盒LLMs不确定性估计的新方法。KLE定义了正定单位迹核来编码LLM输出的语义相似性，并使用冯·诺依曼熵量化不确定性。它考虑了答案（或语义簇）之间的成对语义依赖性，提供了比基于答案硬聚类的先前方法更细粒度的不确定性估计。我们从理论上证明了KLE推广了之前被称为语义熵的最先进方法，并通过实验证明它在多个自然语言生成数据集和LLM架构中提高了不确定性量化性能。"
}
{
  "title": "Learn To be Efficient: Build Structured Sparsity in Large Language Models",
  "title_zh": "学习高效：在大型语言模型中构建结构化稀疏性",
  "abstract": "Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. However, existing methods only focus on utilizing this naturally formed activation sparsity in a post-training setting, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity. To achieve this, we introduce a novel training algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like LLaMA using non-ReLU activations. Extensive evaluation on language understanding, language generation, and instruction tuning tasks show that LTE consistently outperforms SOTA baselines. Along with our hardware-aware custom kernel implementation, LTE reduces LLaMA2-7B inference latency by 25% at 50% sparsity.",
  "abstract_zh": "大型语言模型（LLMs）凭借其数十亿级参数取得了显著成功，但它们带来了高推理开销。LLMs中出现的激活稀疏性为通过仅涉及部分参数进行推理来降低这一成本提供了一种自然的方法。然而，现有方法仅专注于在训练后利用这种自然形成的激活稀疏性，忽视了进一步放大这种内在稀疏性的潜力。在本文中，我们假设LLMs可以通过实现更结构化的激活稀疏性来学习高效。为此，我们引入了一种新颖的训练算法，Learn-To-be-Efficient（LTE），旨在训练效率感知的LLMs以学习激活更少的神经元，并在稀疏性和性能之间实现更好的权衡。此外，与主要关注基于ReLU模型的SOTA MoEfication方法不同，LTE也可以应用于使用非ReLU激活的LLMs，如LLaMA。在语言理解、语言生成和指令调优任务上的广泛评估显示，LTE始终优于SOTA基线。结合我们硬件感知的自定义内核实现，LTE在50%稀疏性下将LLaMA2-7B的推理延迟减少了25%。"
}
{
  "title": "CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts",
  "title_zh": "CuMo: 使用协同再利用专家混合模型扩展多模态大语言模型",
  "abstract": "Recent advancements in Multimodal Large Language Models (LLMs) have focused primarily on scaling by increasing text-image pair data and enhancing LLMs to improve performance on multimodal tasks. However, these scaling approaches are computationally expensive and overlook the significance of efficiently improving model capabilities from the vision side. \nInspired by the successful applications of Mixture-of-Experts (MoE) in LLMs, which improves model scalability during training while keeping inference costs similar to those of smaller models, we propose CuMo, which incorporates Co-upcycled Top-K sparsely-gated Mixture-of-experts blocks into both the vision encoder and the MLP connector, thereby enhancing the multimodal LLMs with neglectable additional activated parameters during inference.\nCuMo first pre-trains the MLP blocks and then initializes each expert in the MoE block from the pre-trained MLP block during the visual instruction tuning stage, with auxiliary losses to ensure a balanced loading of experts.\nCuMo outperforms state-of-the-art multimodal LLMs across various VQA and visual-instruction-following benchmarks within each model size group, all while training exclusively on open-sourced datasets.",
  "abstract_zh": "最近在多模态大语言模型（LLMs）方面的进展主要集中在通过增加文本-图像对数据和增强LLMs来提高多模态任务的性能。然而，这些扩展方法计算成本高，并忽视了从视觉方面有效提升模型能力的重要性。受到专家混合模型（MoE）在LLMs中成功应用的启发，该方法在训练期间提高了模型的可扩展性，同时保持推理成本与较小模型相似，我们提出了CuMo，它将协同再利用的Top-K稀疏门控专家混合模块整合到视觉编码器和MLP连接器中，从而在推理过程中以可忽略的额外激活参数增强多模态LLMs。CuMo首先对MLP模块进行预训练，然后在视觉指令调优阶段从预训练的MLP模块初始化MoE模块中的每个专家，并通过辅助损失确保专家的负载平衡。CuMo在各个模型尺寸组的各种VQA和视觉指令跟随基准测试中均优于最先进的多模态LLMs，同时仅在开源数据集上进行训练。"
}
{
  "title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models",
  "title_zh": "Title: HippoRAG：受神经生物学启发的大型语言模型长期记忆",
  "abstract": "In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering (QA) and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-20 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods.",
  "abstract_zh": "Abstract: 为了在充满敌意和不断变化的自然环境中生存，哺乳动物的大脑进化出能够存储大量关于世界的知识，并不断整合新信息，同时避免灾难性遗忘。尽管大型语言模型（LLMs）取得了令人瞩目的成就，但即使是带有检索增强生成（RAG）的模型，在预训练后仍难以高效有效地整合大量新经验。在这项工作中，我们引入了HippoRAG，这是一种受人类长期记忆的海马索引理论启发的新型检索框架，旨在实现对新经验的更深入和更高效的知识整合。HippoRAG协同组织LLMs、知识图谱和个性化PageRank算法，以模拟人类记忆中新皮层和海马的不同角色。我们将HippoRAG与现有的RAG方法在多跳问答（QA）上进行比较，结果表明我们的方法显著优于最先进的方法，提升幅度高达20%。使用HippoRAG进行单步检索的性能可与迭代检索如IRCoT相媲美或更佳，同时成本降低10-20倍，速度提高6-13倍，将HippoRAG整合到IRCoT中可带来进一步的显著提升。最后，我们展示了我们的方法可以解决现有方法无法触及的新类型场景。"
}
{
  "title": "GraphVis: Boosting LLMs with Visual Knowledge Graph Integration",
  "title_zh": "GraphVis: 通过视觉知识图谱集成提升大型语言模型",
  "abstract": "The rapid evolution of large language models (LLMs) has expanded their capabilities across various data modalities, extending from well-established image data to increasingly popular graph data. Given the limitation of LLMs in hallucinations and inaccuracies in recalling factual knowledge, Knowledge Graph (KG) has emerged as a crucial data modality to support more accurate reasoning by LLMs. However, integrating structured knowledge from KGs into LLMs remains challenging, as most current KG-enhanced LLM methods directly convert the KG into linearized text triples, which is not as expressive as the original structured data. To address this, we introduce GraphVis, which conserves the intricate graph structure through the visual modality to enhance the comprehension of KGs with the aid of Large Vision Language Models (LVLMs). Our approach incorporates a unique curriculum fine-tuning scheme which first instructs LVLMs to recognize basic graphical features from the images, and subsequently incorporates reasoning on QA tasks with the visual graphs. This cross-modal methodology not only markedly enhances performance on standard textual QA  but also shows improved zero-shot VQA performance by utilizing synthetic graph images to augment the data for VQA tasks. We present comprehensive evaluations across commonsense reasoning QA benchmarks, where GraphVis provides an average improvement of 11.1% over its base model and outperforms existing KG-enhanced LLM approaches. Across VQA benchmarks such as ScienceQA that share similar scientific diagram images, GraphVis provides a notable gain of 4.32%.",
  "abstract_zh": "大型语言模型（LLMs）的快速发展扩展了它们在各种数据模态中的能力，从成熟的图像数据扩展到越来越受欢迎的图数据。鉴于LLMs在幻觉和回忆事实知识方面的局限性，知识图谱（KG）已成为支持LLMs更准确推理的重要数据模态。然而，将KG中的结构化知识整合到LLMs中仍然具有挑战性，因为目前大多数KG增强的LLM方法直接将KG转换为线性化的文本三元组，这不如原始结构化数据具有表现力。为了解决这个问题，我们引入了GraphVis，通过视觉模态保留复杂的图结构，以借助大型视觉语言模型（LVLMs）增强对KG的理解。我们的方法采用了一种独特的课程微调方案，首先指导LVLMs从图像中识别基本的图形特征，随后结合视觉图进行QA任务的推理。这种跨模态方法不仅显著提高了标准文本QA的性能，还通过利用合成图像增强VQA任务的数据，显示出改进的零样本VQA性能。我们在常识推理QA基准上进行了全面评估，其中GraphVis相较于其基础模型平均提高了11.1%，并且优于现有的KG增强LLM方法。在ScienceQA等共享类似科学图表图像的VQA基准上，GraphVis提供了4.32%的显著增益。"
}
{
  "title": "BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling",
  "title_zh": "标题：大语言模型的BoNBoN对齐与最佳n采样的甜蜜之处",
  "abstract": "This paper concerns the problem of aligning samples from large language models to human preferences using *best-of-$n$* sampling, where we draw $n$ samples, rank them, and return the best one. We consider two fundamental problems. First: what is the relationship between best-of-$n$ and other (RLHF-type) approaches to aligning LLMs? In particular, when should one be preferred to the other? We show that the best-of-$n$ sampling distribution is essentially equivalent to the policy learned by RLHF if we apply a particular monotone transformation to the reward function. Moreover, we show that this transformation yields the best possible trade-off between win-rate against the base model vs KL distance from the base model. Then, best-of-$n$ is a Pareto-optimal win-rate vs KL solution.\nThe second problem we consider is how to fine-tune a model to mimic the best-of-$n$ sampling distribution, to avoid drawing $n$ samples for each inference. We derive *BonBon Alignment* as a method for achieving this. Experiments show that BonBon alignment yields a model that achieves high win rates while minimally affecting off-target aspects of the generations.",
  "abstract_zh": "摘要：本文研究了使用*最佳n*采样将大语言模型的样本与人类偏好对齐的问题，其中我们抽取n个样本，对其进行排序，并返回最好的一个。我们考虑两个基本问题。首先：最佳n与其他（RLHF类型）对齐LLM的方法之间有什么关系？特别是，什么时候应该优先选择其中一个？我们展示了如果对奖励函数应用特定的单调变换，最佳n采样分布本质上等同于RLHF学习的策略。此外，我们证明这种变换在对基模型的胜率与基模型的KL距离之间提供了最佳的权衡。然后，最佳n是胜率与KL的帕累托最优解。我们考虑的第二个问题是如何微调模型以模仿最佳n采样分布，从而避免每次推理时抽取n个样本。我们推导出*BonBon对齐*作为实现这一目标的方法。实验表明，BonBon对齐生成的模型在保持高胜率的同时，最小化了对生成的非目标方面的影响。"
}
{
  "title": "Panacea: Pareto Alignment via Preference Adaptation for LLMs",
  "title_zh": "万能药：通过偏好适应实现大语言模型的帕累托对齐",
  "abstract": "Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. This paper presents Panacea, an innovative approach that reframes alignment as a multi-dimensional preference optimization problem. Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning. A major challenge here is using a low-dimensional preference vector to guide the model's behavior, despite it being governed by an overwhelmingly large number of parameters. To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values. Theoretically, we prove that Panacea recovers the entire Pareto front with common loss aggregation methods under mild conditions. Moreover, our experiments demonstrate, for the first time, the feasibility of aligning a single LLM to represent an exponentially vast spectrum of human preferences through various optimization methods. Our work marks a step forward in effectively and efficiently aligning models to diverse and intricate human preferences in a controllable and Pareto-optimal manner.",
  "abstract_zh": "当前的大语言模型对齐方法通常使用标量的人类偏好标签。然而，这种惯例往往过于简化人类偏好的多维和异质性质，导致表现力降低甚至失调。本文提出了万能药，一种创新的方法，将对齐重新定义为多维偏好优化问题。万能药训练出一个能够在线适应并帕累托最优地满足多样化偏好的单一模型，而无需进一步调优。这里的一个主要挑战是使用低维偏好向量来引导模型行为，尽管其行为受一个极其庞大的参数数量控制。为了解决这个问题，万能药设计使用基于奇异值分解（SVD）的低秩适应，这使得偏好向量可以简单地作为奇异值在线注入。理论上，我们证明了万能药在温和条件下使用常见的损失聚合方法恢复了整个帕累托前沿。此外，我们的实验首次展示了通过各种优化方法，将单一大语言模型对齐以代表人类偏好的指数级广泛谱系的可行性。我们的工作标志着在以可控和帕累托最优的方式有效且高效地对齐模型以适应多样且复杂的人类偏好方面迈出了一步。"
}
{
  "title": "Pandora's Box: Towards Building Universal Attackers against Real-World Large Vision-Language Models",
  "title_zh": "潘多拉的盒子：构建针对真实世界大型视觉语言模型的通用攻击者",
  "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across a wide range of multimodal understanding tasks. Nevertheless, these models are susceptible to adversarial examples. In real-world applications, existing LVLM attackers generally rely on the detailed prior knowledge of the model to generate effective perturbations. Moreover, these attacks are task-specific, leading to significant costs for designing perturbation. Motivated by the research gap and practical demands, in this paper, we make the first attempt to build a universal attacker against real-world LVLMs, focusing on two critical aspects: (i) restricting access to only the LVLM inputs and outputs. (ii) devising a universal adversarial patch, which is task-agnostic and can deceive any LVLM-driven task when applied to various inputs. Specifically, we start by initializing the location and the pattern of the adversarial patch through random sampling, guided by the semantic distance between their output and the target label. Subsequently, we maintain a consistent patch location while refining the pattern to enhance semantic resemblance to the target. In particular, our approach incorporates a diverse set of LVLM task inputs as query samples to approximate the patch gradient, capitalizing on the importance of distinct inputs. In this way, the optimized patch is universally adversarial against different tasks and prompts, leveraging solely gradient estimates queried from the model. Extensive experiments are conducted to verify the strong universal adversarial capabilities of our proposed attack with prevalent LVLMs including LLaVA, MiniGPT-4, Flamingo, and BLIP-2, spanning a spectrum of tasks, all achieved without delving into the details of the model structures.",
  "abstract_zh": "大型视觉语言模型（LVLMs）在多模态理解任务中展现了卓越的能力。然而，这些模型易受对抗样本的攻击。在实际应用中，现有的LVLM攻击者通常依赖于模型的详细先验知识来生成有效的扰动。此外，这些攻击是任务特定的，导致设计扰动的成本显著。基于研究空白和实际需求，本文首次尝试构建针对真实世界LVLMs的通用攻击者，重点关注两个关键方面：（i）限制仅访问LVLM的输入和输出。（ii）设计一个通用的对抗补丁，该补丁与任务无关，能够在应用于各种输入时欺骗任何LVLM驱动的任务。具体而言，我们通过随机采样初始化对抗补丁的位置和模式，以其输出与目标标签之间的语义距离为指导。随后，我们保持补丁位置不变，同时优化模式以增强与目标的语义相似性。特别是，我们的方法结合了多样化的LVLM任务输入作为查询样本，以近似补丁梯度，利用不同输入的重要性。通过这种方式，优化后的补丁在不同任务和提示下具有普遍的对抗性，仅依赖于从模型查询的梯度估计。我们进行了广泛的实验，以验证我们提出的攻击在流行的LVLMs（包括LLaVA、MiniGPT-4、Flamingo和BLIP-2）上的强大通用对抗能力，这些实验涵盖了多种任务，均无需深入研究模型结构的细节。"
}
{
  "title": "CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models",
  "title_zh": "标题：CODE：对比自生成描述以应对大型多模态模型中的幻觉现象",
  "abstract": "Large Multi-modal Models (LMMs) have recently demonstrated remarkable abilities in visual context understanding and coherent response generation. However, alongside these advancements, the issue of hallucinations has emerged as a significant challenge, producing erroneous responses that are unrelated to the visual contents. In this paper, we introduce a novel contrastive-based decoding method, COuntering DEscription Contrastive Decoding (CODE), which leverages self-generated descriptions as contrasting references during the decoding phase of LMMs to address hallucination issues. CODE utilizes the comprehensive descriptions from model itself as visual counterpart to correct and improve response alignment with actual visual content. By dynamically adjusting the information flow and distribution of next-token predictions in the LMM's vocabulary, CODE enhances the coherence and informativeness of generated responses. Extensive experiments demonstrate that our method significantly reduces hallucinations and improves cross-modal consistency across various benchmarks and cutting-edge LMMs. Our method provides a simple yet effective decoding strategy that can be integrated to existing LMM frameworks without additional training.",
  "abstract_zh": "摘要：大型多模态模型（LMMs）最近在视觉上下文理解和连贯响应生成方面展现出了显著能力。然而，随着这些进展，幻觉问题也成为了一项重大挑战，导致生成与视觉内容无关的错误响应。在本文中，我们引入了一种新颖的基于对比的解码方法，称为对抗描述对比解码（CODE），该方法在LMMs的解码阶段利用自生成描述作为对比参考来解决幻觉问题。CODE利用模型自身生成的全面描述作为视觉对应物，以纠正和改善响应与实际视觉内容的对齐。通过动态调整LMM词汇表中下一个词预测的信息流和分布，CODE增强了生成响应的连贯性和信息量。大量实验表明，我们的方法显著减少了幻觉现象，并在各种基准和前沿LMMs中提高了跨模态一致性。我们的方法提供了一种简单而有效的解码策略，可以在不进行额外训练的情况下集成到现有的LMM框架中。"
}
{
  "title": "MaVEn: An Effective Multi-granularity Hybrid Visual Encoding Framework for Multimodal Large Language Model",
  "title_zh": "MaVEn：一种有效的多粒度混合视觉编码框架用于多模态大型语言模型",
  "abstract": "This paper presents MaVEn, an innovative Multi-granularity Visual Encoding framework designed to enhance the capabilities of Multimodal Large Language Models (MLLMs) in multi-image reasoning. Current MLLMs primarily focus on single-image visual understanding, limiting their ability to interpret and integrate information across multiple images. MaVEn addresses this limitation by combining discrete visual symbol sequences, which abstract coarse-grained semantic concepts, with traditional continuous representation sequences that model fine-grained features. This dual approach bridges the semantic gap between visual and textual data, thereby improving the model's ability to process and interpret information from multiple images effectively. Additionally, we design a dynamic reduction mechanism by for long-sequence continuous features to enhance multi-image processing efficiency. Experimental results demonstrate that MaVEn significantly enhances MLLMs' understanding in complex multi-image scenarios, while also improving performance in single-image contexts.",
  "abstract_zh": "本文介绍了MaVEn，这是一种创新的多粒度视觉编码框架，旨在增强多模态大型语言模型（MLLMs）在多图像推理中的能力。目前的MLLMs主要关注单图像的视觉理解，限制了它们在多图像信息解释和整合方面的能力。MaVEn通过结合离散视觉符号序列（抽象粗粒度语义概念）与传统的连续表示序列（建模细粒度特征）来解决这一限制。这种双重方法弥合了视觉和文本数据之间的语义差距，从而提高了模型有效处理和解释多图像信息的能力。此外，我们设计了一种动态简化机制，用于长序列连续特征，以提高多图像处理效率。实验结果表明，MaVEn显著增强了MLLMs在复杂多图像场景中的理解能力，同时也提高了单图像情境下的性能。"
}
{
  "title": "AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models",
  "title_zh": "AlphaPruning：利用重尾自正则化理论改进大型语言模型的逐层剪枝",
  "abstract": "Recent work on pruning large language models (LLMs) has shown that one can eliminate a large number of parameters without compromising performance, making pruning a promising strategy to reduce LLM model size. Existing LLM pruning strategies typically assign uniform pruning ratios across layers, limiting overall pruning ability; and recent work on layerwise pruning of LLMs is often based on heuristics that can easily lead to suboptimal performance. In this paper, we leverage Heavy-Tailed Self-Regularization (HT-SR) Theory, in particular the shape of empirical spectral densities (ESDs) of weight matrices, to design improved layerwise pruning ratios for LLMs. Our analysis reveals a wide variability in how well-trained, and thus relatedly how prunable, different layers of an LLM are. Based on this, we propose AlphaPruning, which uses shape metrics to allocate layerwise sparsity ratios in a more theoretically-principled manner. AlphaPruning can be used in conjunction with multiple existing LLM pruning methods. Our empirical results show that AlphaPruning prunes LLaMA-7B to 80% sparsity while maintaining reasonable perplexity, marking a first in the literature on LLMs.",
  "abstract_zh": "最近关于大型语言模型（LLM）剪枝的研究表明，可以在不影响性能的情况下消除大量参数，使剪枝成为减少LLM模型规模的有前景策略。现有的LLM剪枝策略通常在各层之间分配统一的剪枝比例，限制了整体剪枝能力；而最近关于LLM逐层剪枝的研究往往基于启发式方法，这很容易导致次优性能。在本文中，我们利用重尾自正则化（HT-SR）理论，特别是权重矩阵的经验谱密度（ESD）的形状，设计改进的LLM逐层剪枝比例。我们的分析揭示了LLM不同层的训练程度以及相应的可剪枝性存在很大差异。基于此，我们提出了AlphaPruning，它使用形状指标以更具理论依据的方式分配逐层稀疏比例。AlphaPruning可以与多种现有的LLM剪枝方法结合使用。我们的实证结果表明，AlphaPruning将LLaMA-7B剪枝至80%的稀疏性，同时保持合理的困惑度，这在LLM文献中尚属首次。"
}
{
  "title": "Discovering Preference Optimization Algorithms with and for Large Language Models",
  "title_zh": "发现和为大型语言模型优化偏好的算法",
  "abstract": "Offline preference optimization is a key method for enhancing and controlling the quality of Large Language Model (LLM) outputs.\nTypically, preference optimization is approached as an offline supervised learning task using manually crafted convex loss functions. While these methods are based on theoretical insights, they are inherently constrained by human creativity, so the large search space of possible loss functions remains under-explored. We address this by performing LLM-driven *objective discovery* to automatically discover new state-of-the-art preference optimization algorithms without (expert) human intervention.  Specifically, we iteratively prompt an LLM to propose and implement new preference optimization loss functions based on previously evaluated performance metrics. This process leads to the discovery of previously unknown and performant preference optimization algorithms. The best performing of these we call *Discovered Preference Optimization* (DiscoPOP), a novel algorithm that adaptively blends logistic and exponential losses. Experiments demonstrate the state-of-the-art performance of DiscoPOP and its successful transfer to held-out tasks.",
  "abstract_zh": "离线偏好优化是增强和控制大型语言模型（LLM）输出质量的关键方法。通常，偏好优化被视为使用手工制作的凸损失函数的离线监督学习任务。虽然这些方法基于理论见解，但它们本质上受到人类创造力的限制，因此可能损失函数的巨大搜索空间仍未得到充分探索。我们通过执行由LLM驱动的*目标发现*来解决这一问题，以自动发现新的最先进的偏好优化算法，而无需（专家）人类干预。具体来说，我们通过迭代地提示LLM提出并实施基于先前评估的性能指标的新偏好优化损失函数。这个过程导致了以前未知且性能优越的偏好优化算法的发现。我们称这些表现最佳的算法为*发现偏好优化*（DiscoPOP），这是一种新颖的算法，能够自适应地融合逻辑和指数损失。实验表明，DiscoPOP的性能达到最先进水平，并成功转移到未见过的任务中。"
}
{
  "title": "Mission Impossible: A Statistical Perspective on Jailbreaking LLMs",
  "title_zh": "任务不可能：从统计角度看大语言模型的越狱",
  "abstract": "Large language models (LLMs) are trained on a deluge of text data with limited quality control. As a result, LLMs can exhibit unintended or even harmful behaviours, such as leaking information, fake news or hate speech. Countermeasures, commonly referred to as preference alignment, include fine-tuning the pretrained LLMs with carefully crafted text examples of desired behaviour. Even then, empirical evidence shows preference aligned LLMs can be enticed to harmful behaviour. This so called jailbreaking of LLMs is typically achieved by adversarially modifying the input prompt to the LLM. Our paper provides theoretical insights into the phenomenon of preference alignment and jailbreaking from a statistical perspective. Under our framework, we first show that pretrained LLMs will mimic harmful behaviour if present in the training corpus. \\textbf{Under that same framework, we then introduce a statistical notion of alignment, and lower-bound the jailbreaking probability, showing that it is unpreventable under reasonable assumptions.} Based on our insights, we propose an alteration to the currently prevalent alignment strategy RLHF. Specifically, we introduce a  simple modification to the RLHF objective, we call \\emph{E-RLHF}, that aims to increase the likelihood of safe responses. \\emph{E-RLHF} brings no additional training cost, and is compatible with other methods. Empirically, we demonstrate that \\emph{E-RLHF} outperforms RLHF on all alignment problems put forward by the AdvBench \\citep{zou2023universal} and HarmBench project \\citep{mazeika2024harmbench} without sacrificing model performance as measured by the MT-Bench project \\citep{zheng2024judging}.",
  "abstract_zh": "大型语言模型（LLMs）是在大量文本数据上训练的，质量控制有限。因此，LLMs可能会表现出意外甚至有害的行为，如信息泄露、假新闻或仇恨言论。通常被称为偏好对齐的对策包括通过精心设计的期望行为文本示例对预训练的LLMs进行微调。即便如此，实验证据表明，对齐偏好的LLMs仍可能被诱导产生有害行为。这种所谓的LLMs越狱通常通过对LLM的输入提示进行对抗性修改来实现。我们的论文从统计角度提供了对偏好对齐和越狱现象的理论见解。在我们的框架下，我们首先展示了如果训练语料库中存在有害行为，预训练的LLMs将会模仿这种行为。在同一框架下，我们引入了对齐的统计概念，并对越狱概率进行下界，表明在合理假设下这是不可避免的。基于我们的见解，我们提出了一种对当前流行的对齐策略RLHF的改进。具体而言，我们引入了一种简单的RLHF目标修改，称为\\emph{E-RLHF}，旨在提高安全响应的可能性。\\emph{E-RLHF}不增加额外的训练成本，并且与其他方法兼容。实验证明，\\emph{E-RLHF}在AdvBench \\citep{zou2023universal}和HarmBench项目\\citep{mazeika2024harmbench}提出的所有对齐问题上均优于RLHF，而不会牺牲由MT-Bench项目\\citep{zheng2024judging}衡量的模型性能。"
}
{
  "title": "Who's asking? User personas and the mechanics of latent misalignment",
  "title_zh": "标题：是谁在提问？用户角色与潜在失调的机制",
  "abstract": "Studies show that safety-tuned models may nevertheless divulge harmful information. In this work, we show that whether they do so depends significantly on who they are talking to, which we refer to as *user persona*. In fact, we find manipulating user persona to be more effective for eliciting harmful content than certain more direct attempts to control model refusal. We study both natural language prompting and activation steering as intervention methods and show that activation steering is significantly more effective at bypassing safety filters.\nWe shed light on the mechanics of this phenomenon by showing that even when model generations are safe, harmful content can persist in hidden representations and can be extracted by decoding from earlier layers.  We also show we can predict a persona’s effect on refusal given only the geometry of its steering vector. Finally, we show that certain user personas induce the model to form more charitable interpretations of otherwise dangerous queries.",
  "abstract_zh": "摘要：研究表明，即使经过安全调优的模型仍可能泄露有害信息。在这项工作中，我们展示了他们是否这样做在很大程度上取决于他们在与谁交谈，我们称之为*用户角色*。事实上，我们发现操控用户角色比某些更直接的控制模型拒绝的尝试更有效地引出有害内容。我们研究了自然语言提示和激活引导作为干预方法，并表明激活引导在绕过安全过滤器方面显著更有效。我们通过展示即使模型生成是安全的，有害内容仍可能存在于隐藏表示中并可以通过从早期层解码提取来揭示这一现象的机制。我们还展示了仅凭引导向量的几何形状即可预测角色对拒绝的影响。最后，我们展示了某些用户角色会诱导模型对原本危险的查询形成更宽容的解释。"
}
{
  "title": "Representation Noising: A Defence Mechanism Against Harmful Finetuning",
  "title_zh": "标题: 表征噪声：一种针对有害微调的防御机制",
  "abstract": "Releasing open-source large language models (LLMs) presents a dual-use risk since bad actors can easily fine-tune these models for harmful purposes. Even without the open release of weights, weight stealing and fine-tuning APIs make closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety measures like preventing jailbreaks and improving safety guardrails are important, such measures can easily be reversed through fine-tuning. In this work, we propose Representation Noising (\\textsf{\\small RepNoise}), a defence mechanism that operates even when attackers have access to the weights. \\textsf{\\small RepNoise} works by removing information about harmful representations such that it is difficult to recover them during fine-tuning. Importantly, our defence is also able to generalize across different subsets of harm that have not been seen during the defence process as long as they are drawn from the same distribution of the attack set. Our method does not degrade the general capability of LLMs and retains the ability to train the model on harmless tasks. We provide empirical evidence that the efficacy of our defence lies in its ``depth'': the degree to which information about harmful representations is removed across {\\em all layers} of the LLM. We also find areas where \\textsf{\\small RepNoise} still remains ineffective and highlight how those limitations can inform future research.",
  "abstract_zh": "摘要: 发布开源的大型语言模型（LLMs）存在双重使用风险，因为不法分子可以轻松地微调这些模型用于有害目的。即使不公开发布权重，权重窃取和微调API也使封闭模型容易受到有害微调攻击（HFAs）的影响。尽管防止越狱和改进安全防护措施等安全措施很重要，但这些措施可以通过微调轻松逆转。在这项工作中，我们提出了一种防御机制——表征噪声（\\textsf{\\small RepNoise}），即使攻击者可以访问权重时也能发挥作用。\\textsf{\\small RepNoise}通过去除关于有害表征的信息，使得在微调过程中难以恢复这些信息。重要的是，我们的防御机制能够在不同的有害子集上进行泛化，只要它们来自攻击集的相同分布。我们的方法不会降低LLMs的整体能力，并保留在无害任务上训练模型的能力。我们提供了实证证据，证明我们防御的有效性在于其“深度”：即在LLM的所有层中去除有害表征信息的程度。我们还发现了\\textsf{\\small RepNoise}仍然无效的领域，并强调这些局限性如何为未来的研究提供信息。"
}
{
  "title": "ColJailBreak: Collaborative Generation and Editing for Jailbreaking Text-to-Image Deep Generation",
  "title_zh": "Title: ColJailBreak：用于破解文本到图像深度生成的协作生成与编辑",
  "abstract": "The commercial text-to-image deep generation models (e.g. DALL·E) can produce high-quality images based on input language descriptions. These models incorporate a black-box safety filter to prevent the generation of unsafe or unethical content, such as violent, criminal, or hateful imagery. Recent jailbreaking methods generate adversarial prompts capable of bypassing safety filters and producing unsafe content, exposing vulnerabilities in influential commercial models. However, once these adversarial prompts are identified, the safety filter can be updated to prevent the generation of unsafe images. In this work, we propose an effective, simple, and difficult-to-detect jailbreaking solution: generating safe content initially with normal text prompts and then editing the generations to embed unsafe content. The intuition behind this idea is that the deep generation model cannot reject safe generation with normal text prompts, while the editing models focus on modifying the local regions of images and do not involve a safety strategy. However, implementing such a solution is non-trivial, and we need to overcome several challenges: how to automatically confirm the normal prompt to replace the unsafe prompts, and how to effectively perform editable replacement and naturally generate unsafe content. In this work, we propose the collaborative generation and editing for jailbreaking text-to-image deep generation (ColJailBreak), which comprises three key components: adaptive normal safe substitution, inpainting-driven injection of unsafe content, and contrastive language-image-guided collaborative optimization. We validate our method on three datasets and compare it to two baseline methods. Our method could generate unsafe content through two commercial deep generation models including GPT-4 and DALL·E 2.",
  "abstract_zh": "Abstract: 商业文本到图像深度生成模型（例如 DALL·E）可以根据输入的语言描述生成高质量的图像。这些模型包含一个黑箱安全过滤器，以防止生成不安全或不道德的内容，例如暴力、犯罪或仇恨图像。最近的破解方法生成能够绕过安全过滤器并产生不安全内容的对抗性提示，暴露了有影响力的商业模型中的漏洞。然而，一旦识别出这些对抗性提示，安全过滤器可以更新以防止生成不安全图像。在这项工作中，我们提出了一种有效、简单且难以检测的破解解决方案：首先使用正常文本提示生成安全内容，然后编辑生成的内容以嵌入不安全内容。这个想法背后的直觉是，深度生成模型不能拒绝使用正常文本提示生成的安全内容，而编辑模型专注于修改图像的局部区域，并不涉及安全策略。然而，实施这样的解决方案并非易事，我们需要克服几个挑战：如何自动确认正常提示以替换不安全提示，以及如何有效地执行可编辑替换并自然生成不安全内容。在这项工作中，我们提出了用于破解文本到图像深度生成的协作生成与编辑（ColJailBreak），它包括三个关键组件：自适应正常安全替换、基于修复的不安全内容注入和对比语言-图像引导的协作优化。我们在三个数据集上验证了我们的方法，并将其与两种基线方法进行了比较。我们的方法可以通过包括 GPT-4 和 DALL·E 2 在内的两个商业深度生成模型生成不安全内容。"
}
{
  "title": "Large Language Model Unlearning via Embedding-Corrupted Prompts",
  "title_zh": "标题：通过嵌入损坏的提示实现大型语言模型的遗忘",
  "abstract": "Large language models (LLMs) have advanced to encompass extensive knowledge across diverse domains. Yet controlling what a large language model should not know is important for ensuring alignment and thus safe use. However, accurately and efficiently unlearning knowledge from an LLM remains challenging due to the potential collateral damage caused by the fuzzy boundary between retention and forgetting, and the large computational requirements for optimization across state-of-the-art models with hundreds of billions of parameters. In this work, we present \\textbf{Embedding-COrrupted (ECO) Prompts}, a lightweight unlearning framework for large language models to address both the challenges of knowledge entanglement and unlearning efficiency. Instead of relying on the LLM itself to unlearn, we enforce an unlearned state during inference by employing a prompt classifier to identify and safeguard prompts to forget. We learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline and corrupt prompts flagged by the classifier during inference. We find that these embedding-corrupted prompts not only lead to desirable outputs that satisfy the unlearning objective but also closely approximate the output from a model that has never been trained on the data intended for forgetting. Through extensive experiments on unlearning, we demonstrate the superiority of our method in achieving promising unlearning at \\textit{nearly zero side effects} in general domains and domains closely related to the unlearned ones. Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases. We have made our code publicly available at \\url{https://github.com/chrisliu298/llm-unlearn-eco}.",
  "abstract_zh": "摘要：大型语言模型（LLMs）已经发展到涵盖不同领域的广泛知识。然而，控制大型语言模型不应知道的内容对于确保一致性和安全使用至关重要。然而，由于保留和遗忘之间模糊边界可能造成的附带损害，以及对拥有数千亿参数的最先进模型进行优化所需的大量计算资源，从LLM中准确高效地遗忘知识仍然具有挑战性。在这项工作中，我们提出了一种轻量级的遗忘框架，称为\\textbf{Embedding-COrrupted (ECO) Prompts}，用于解决知识纠缠和遗忘效率的挑战。我们不是依赖LLM本身来遗忘，而是在推理过程中通过使用提示分类器识别和保护需要遗忘的提示来强制实现遗忘状态。我们通过零阶优化离线学习添加到提示嵌入的损坏，以实现遗忘目标，并在推理过程中损坏分类器标记的提示。我们发现，这些嵌入损坏的提示不仅产生满足遗忘目标的理想输出，而且还接近于从未在需要遗忘的数据上训练过的模型的输出。通过对遗忘的广泛实验，我们证明了我们的方法在实现有前途的遗忘方面的优越性，\\textit{几乎没有副作用}，适用于一般领域和与遗忘领域密切相关的领域。此外，我们强调了我们方法的可扩展性，适用于从0.5B到236B参数的100个LLM，随着参数数量的增加不会产生额外成本。我们已将代码公开在\\url{https://github.com/chrisliu298/llm-unlearn-eco}。"
}
{
  "title": "Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models",
  "title_zh": "扩散模型中通过对抗训练实现稳健概念消除的防御性遗忘",
  "abstract": "Diffusion models (DMs) have achieved remarkable success in text-to-image generation, but they also pose safety risks, such as the potential generation of harmful content and copyright violations. The techniques of machine unlearning, also known as concept erasing, have been developed to address these risks. However, these techniques remain vulnerable to adversarial prompt attacks, which can prompt DMs post-unlearning to regenerate undesired images containing concepts (such as nudity) meant to be erased. This work aims to enhance the robustness of concept erasing by integrating the principle of adversarial training (AT) into machine unlearning, resulting in the robust unlearning framework referred to as AdvUnlearn. However, achieving this effectively and efficiently is highly nontrivial. First, we find that a straightforward implementation of AT compromises DMs’ image generation quality post-unlearning. To address this, we develop a utility-retaining regularization on an additional retain set, optimizing the trade-off between concept erasure robustness and model utility in AdvUnlearn. Moreover, we identify the text encoder as a more suitable module for robustification compared to UNet, ensuring unlearning effectiveness. And the acquired text encoder can serve as a plug-and-play robust unlearner for various DM types. Empirically, we perform extensive experiments to demonstrate the robustness advantage of AdvUnlearn across various DM unlearning scenarios, including the erasure of nudity, objects, and style concepts. In addition to robustness, AdvUnlearn also achieves a balanced tradeoff with model utility. To our knowledge, this is the first work to systematically explore robust DM unlearning through AT, setting it apart from existing methods that overlook robustness in concept erasing. Codes are available at https://github.com/OPTML-Group/AdvUnlearn.\n\nWarning: This paper contains model outputs that may be offensive in nature.",
  "abstract_zh": "扩散模型（DMs）在文本到图像生成方面取得了显著成功，但也带来了安全风险，如可能生成有害内容和侵犯版权。机器遗忘技术，也称为概念消除，已被开发用于应对这些风险。然而，这些技术仍然容易受到对抗性提示攻击，这可能导致DMs在遗忘后重新生成包含原本要消除概念（如裸露）的不良图像。本研究旨在通过将对抗训练（AT）的原理整合到机器遗忘中来增强概念消除的稳健性，从而形成稳健的遗忘框架，称为AdvUnlearn。然而，有效且高效地实现这一点非常困难。首先，我们发现AT的简单实施会在遗忘后损害DMs的图像生成质量。为了解决这一问题，我们在额外的保留集上开发了一种保留效用的正则化，优化AdvUnlearn中概念消除稳健性与模型效用之间的权衡。此外，我们确定文本编码器比UNet更适合用于稳健化，确保遗忘的有效性。获得的文本编码器可以作为各种DM类型的即插即用稳健遗忘器。通过实验证明，AdvUnlearn在各种DM遗忘场景中，包括裸露、物体和风格概念的消除，具有稳健性优势。除了稳健性，AdvUnlearn还实现了与模型效用的平衡权衡。据我们所知，这是首次通过AT系统地探索稳健的DM遗忘，与忽视概念消除稳健性的现有方法不同。代码可在https://github.com/OPTML-Group/AdvUnlearn获取。"
}
{
  "title": "Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement",
  "title_zh": "标题：基于梯度的统一机器遗忘与保留几何增强",
  "abstract": "Machine unlearning (MU) has emerged to enhance the privacy and trustworthiness of deep neural networks. Approximate MU is a practical method for large-scale models. Our investigation into approximate MU starts with identifying the steepest descent direction, minimizing the output Kullback-Leibler divergence to exact MU inside a parameters' neighborhood. This probed direction decomposes into three components: weighted forgetting gradient ascent, fine-tuning retaining gradient descent, and a weight saliency matrix. Such decomposition derived from Euclidean metric encompasses most existing gradient-based MU methods. Nevertheless, adhering to Euclidean space may result in sub-optimal iterative trajectories due to the overlooked geometric structure of the output probability space. We suggest embedding the unlearning update into a manifold rendered by the remaining geometry, incorporating second-order Hessian from the remaining data. It helps prevent effective unlearning from interfering with the retained performance. However, computing the second-order Hessian for large-scale models is intractable. To efficiently leverage the benefits of Hessian modulation, we propose a fast-slow parameter update strategy to implicitly approximate the up-to-date salient unlearning direction.\nFree from specific modal constraints, our approach is adaptable across computer vision unlearning tasks, including classification and generation. Extensive experiments validate our efficacy and efficiency. Notably, our method successfully performs class-forgetting on ImageNet using DiT and forgets a class on CIFAR-10 using DDPM in just 50 steps, compared to thousands of steps required by previous methods. Code is available at [Unified-Unlearning-w-Remain-Geometry](https://github.com/K1nght/Unified-Unlearning-w-Remain-Geometry).",
  "abstract_zh": "摘要：机器遗忘（MU）已成为增强深度神经网络隐私性和可信度的重要手段。近似MU是大规模模型的实用方法。我们对近似MU的研究始于识别最速下降方向，最小化输出的Kullback-Leibler散度，以在参数邻域内达到精确MU。这个探测方向分解为三个组成部分：加权遗忘梯度上升、微调保留梯度下降和权重显著性矩阵。这种从欧几里得度量推导的分解涵盖了大多数现有的基于梯度的MU方法。然而，遵循欧几里得空间可能由于忽略输出概率空间的几何结构而导致次优的迭代轨迹。我们建议将遗忘更新嵌入由剩余几何渲染的流形中，结合剩余数据的二阶Hessian。这有助于防止有效遗忘干扰保留性能。然而，对于大规模模型来说，计算二阶Hessian是不可行的。为了有效利用Hessian调制的优势，我们提出了一种快慢参数更新策略，以隐式近似最新的显著遗忘方向。我们的方法不受特定模式限制，适用于包括分类和生成在内的计算机视觉遗忘任务。大量实验验证了我们的有效性和效率。值得注意的是，我们的方法成功地在ImageNet上使用DiT进行类别遗忘，并在CIFAR-10上使用DDPM在仅50步内遗忘一个类别，而之前的方法需要数千步。代码可在[Unified-Unlearning-w-Remain-Geometry](https://github.com/K1nght/Unified-Unlearning-w-Remain-Geometry)获取。"
}
{
  "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
  "title_zh": "标题：QuaRot：旋转LLMs中的无异常值4位推理",
  "abstract": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism, and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4 bits, without any channels identified for retention in higher precision. Our 4-bit quantized LLAMA2-70B model has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the zero-shot performance. We also show that QuaRot can provide lossless 6 and 8 bit LLAMA-2 models without any calibration data using round-to-nearest quantization. Code is available at github.com/spcl/QuaRot.",
  "abstract_zh": "摘要：我们介绍了QuaRot，这是一种基于旋转的新量化方案，能够将LLMs端到端量化，包括所有权重、激活和KV缓存在4位中。QuaRot通过旋转LLMs来去除隐藏状态中的异常值而不改变输出，从而简化了量化过程。这种计算不变性应用于LLM的隐藏状态（残差），以及前馈组件的激活、注意机制的某些方面和KV缓存。结果是一个量化模型，其中所有矩阵乘法都在4位中执行，没有任何通道需要保留更高精度。我们的4位量化LLAMA2-70B模型在WikiText-2困惑度上的损失最多为0.47，并保留了99%的零样本性能。我们还展示了QuaRot可以在没有任何校准数据的情况下，使用四舍五入量化提供无损的6位和8位LLAMA-2模型。代码可在github.com/spcl/QuaRot获取。"
}
{
  "title": "LoFiT: Localized Fine-tuning on LLM Representations",
  "title_zh": "LoFiT：大语言模型表示的局部微调",
  "abstract": "Recent work in interpretability shows that large language models (LLMs) can be adapted for new tasks in a learning-free way: it is possible to intervene on LLM representations to elicit desired behaviors for alignment. For instance, adding certain bias vectors to the outputs of certain attention heads is reported to boost the truthfulness of models. In this work, we show that localized fine-tuning serves as an effective alternative to such representation intervention methods. We introduce a framework called Localized Fine-Tuning on LLM Representations (LoFiT), which identifies a subset of attention heads that are most important for learning a specific task, then trains offset vectors to add to the model's hidden representations at those selected heads. LoFiT localizes to a sparse set of heads (3%-10%) and learns the offset vectors from limited training data, comparable to the settings used for representation intervention. For truthfulness and reasoning tasks, we find that LoFiT's intervention vectors are more effective for LLM adaptation than vectors from representation intervention methods such as Inference-time Intervention. We also find that the localization step is important: selecting a task-specific set of attention heads can lead to higher performance than intervening on heads selected for a different task. Finally, across 7 tasks we study, LoFiT achieves comparable performance to other parameter-efficient fine-tuning methods such as LoRA, despite modifying 20x-200x fewer parameters than these methods.",
  "abstract_zh": "最近的可解释性研究表明，大型语言模型（LLM）可以通过无学习方式适应新任务：可以干预LLM表示以引出对齐的期望行为。例如，据报道，在某些注意力头的输出中添加特定的偏置向量可以提高模型的真实性。在这项工作中，我们表明局部微调是这种表示干预方法的有效替代方案。我们引入了一个称为大语言模型表示的局部微调（LoFiT）的框架，该框架识别出对学习特定任务最重要的注意力头子集，然后训练偏移向量以添加到这些选定头的模型隐藏表示中。LoFiT局部化到一组稀疏的头（3%-10%），并从有限的训练数据中学习偏移向量，这与用于表示干预的设置相当。对于真实性和推理任务，我们发现LoFiT的干预向量比表示干预方法（如推理时干预）的向量更有效地适应LLM。我们还发现局部化步骤很重要：选择任务特定的注意力头集可以比干预为不同任务选择的头获得更高的性能。最后，在我们研究的7个任务中，LoFiT在性能上与其他参数高效的微调方法（如LoRA）相当，尽管修改的参数比这些方法少20倍到200倍。"
}
{
  "title": "Adversarial Representation Engineering: A General Model Editing Framework for Large Language Models",
  "title_zh": "对抗性表示工程：大型语言模型的通用模型编辑框架",
  "abstract": "Since the rapid development of Large Language Models (LLMs) has achieved remarkable success, understanding and rectifying their internal complex mechanisms has become an urgent issue. Recent research has attempted to interpret their behaviors through the lens of inner representation. However, developing practical and efficient methods for applying these representations for general and flexible model editing remains challenging. In this work, we explore how to leverage insights from representation engineering to guide the editing of LLMs by deploying a representation sensor as an editing oracle. We first identify the importance of a robust and reliable sensor during editing, then propose an \\textbf{A}dversarial \\textbf{R}epresentation \\textbf{E}ngineering (\\textbf{ARE}) framework to provide a unified and interpretable approach for conceptual model editing without compromising baseline performance. Experiments on multiple tasks demonstrate the effectiveness of ARE in various model editing scenarios. Our code and data are available at \\url{https://github.com/Zhang-Yihao/Adversarial-Representation-Engineering}.",
  "abstract_zh": "随着大型语言模型（LLMs）的快速发展取得了显著成功，理解和纠正其内部复杂机制已成为一个紧迫的问题。最近的研究试图通过内部表示的视角来解释其行为。然而，开发实用且高效的方法以将这些表示应用于通用且灵活的模型编辑仍然具有挑战性。在这项工作中，我们探索了如何利用表示工程的见解，通过部署表示传感器作为编辑预言机来指导LLMs的编辑。我们首先确定了在编辑过程中一个稳健且可靠的传感器的重要性，然后提出了对抗性表示工程（ARE）框架，以提供一种统一且可解释的概念模型编辑方法，而不影响基线性能。在多个任务上的实验表明，ARE在各种模型编辑场景中具有有效性。我们的代码和数据可在 \\url{https://github.com/Zhang-Yihao/Adversarial-Representation-Engineering} 获取。"
}
{
  "title": "Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models",
  "title_zh": "弱到强搜索：通过小型语言模型搜索对齐大型语言模型",
  "abstract": "Large language models are usually fine-tuned to align with human preferences. However, fine-tuning a large language model can be challenging. In this work, we introduce $\\textit{weak-to-strong search}$, framing the alignment of a large language model as a test-time greedy search to maximize the log-probability difference between small tuned and untuned models while sampling from the frozen large model. This method serves both as (1) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (2) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance.\nEmpirically, we demonstrate the flexibility of weak-to-strong search across different tasks. In controlled-sentiment generation and summarization, we use tuned and untuned $\\texttt{gpt2}$s to improve the alignment of large models without additional training. Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0, we show that reusing off-the-shelf small models (e.g., $\\texttt{zephyr-7b-beta}$ and its untuned version) can improve the length-controlled win rates of both white-box and black-box large models against $\\texttt{gpt-4-turbo}$ (e.g., $34.4\\% \\rightarrow 37.9\\%$ for $\\texttt{Llama-3-70B-Instruct}$ and $16.0\\% \\rightarrow 20.1\\%$ for $\\texttt{gpt-3.5-turbo-instruct}$), despite the small models' low win rates $\\approx 10.0\\%$.",
  "abstract_zh": "大型语言模型通常经过微调以符合人类偏好。然而，微调大型语言模型可能具有挑战性。在这项工作中，我们引入了$\\textit{弱到强搜索}$，将大型语言模型的对齐视为测试时贪婪搜索，以最大化小型已调优和未调优模型之间的对数概率差异，同时从冻结的大型模型中采样。该方法既作为（1）一种计算高效的模型扩展策略，避免直接调优大型模型，也作为（2）一种弱到强的泛化实例，通过弱的测试时指导增强强模型。实验证明，弱到强搜索在不同任务中的灵活性。在受控情感生成和摘要中，我们使用已调优和未调优的$\\texttt{gpt2}$来改善大型模型的对齐，而无需额外训练。关键的是，在更困难的指令遵循基准测试AlpacaEval 2.0中，我们展示了重用现成的小型模型（例如，$\\texttt{zephyr-7b-beta}$及其未调优版本）可以提高白盒和黑盒大型模型相对于$\\texttt{gpt-4-turbo}$的长度控制胜率（例如，$\\texttt{Llama-3-70B-Instruct}$从$34.4\\%$提高到$37.9\\%$，$\\texttt{gpt-3.5-turbo-instruct}$从$16.0\\%$提高到$20.1\\%$），尽管小型模型的胜率仅约为$10.0\\%$。"
}
{
  "title": "MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models",
  "title_zh": "MetaAligner：迈向语言模型的可泛化多目标对齐",
  "abstract": "Recent advancements in large language models (LLMs) focus on aligning to heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are dependent on the policy model parameters, which require high-cost repetition of their alignment algorithms for each new policy model, and they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), the first policy-agnostic and generalizable method for multi-objective preference alignment.\nMetaAligner models multi-objective alignment into three stages: (1) dynamic objectives reformulation algorithm reorganizes traditional alignment datasets to supervise the model on performing flexible alignment across different objectives; (2) conditional weak-to-strong correction paradigm aligns the weak outputs of fixed policy models to approach strong outputs with higher preferences in the corresponding alignment objectives, enabling plug-and-play inferences on any policy models, which significantly reduces training costs and facilitates alignment on close-source policy models; (3) generalizable inference method flexibly adjusts target objectives by updating their text descriptions in the prompts, facilitating generalizable alignment to unseen objectives.\nExperimental results show that MetaAligner achieves significant and balanced improvements in multi-objective alignments on 10 state-of-the-art policy models, and saves up to 93.63% of GPU training hours compared to previous alignment methods. The model also effectively aligns unseen objectives, marking the first step towards generalizable multi-objective preference alignment.",
  "abstract_zh": "最近的大型语言模型（LLMs）进展集中在通过多目标偏好对齐来满足异构的人类期望和价值。然而，现有方法依赖于策略模型参数，这需要对每个新策略模型高成本地重复其对齐算法，并且由于其静态对齐目标，无法扩展到未见过的目标。在这项工作中，我们提出了Meta-Objective Aligner（MetaAligner），这是第一个与策略无关且可泛化的多目标偏好对齐方法。MetaAligner将多目标对齐建模为三个阶段：（1）动态目标重构算法重新组织传统对齐数据集，以监督模型在不同目标之间执行灵活对齐；（2）条件弱到强校正范式将固定策略模型的弱输出对齐到在相应对齐目标中具有更高偏好的强输出，支持即插即用推理，显著降低训练成本，并促进对封闭源策略模型的对齐；（3）可泛化推理方法通过更新提示中的文本描述灵活调整目标目标，促进对未见目标的可泛化对齐。实验结果表明，MetaAligner在10个最先进的策略模型上实现了显著且平衡的多目标对齐改进，并与先前的对齐方法相比节省了高达93.63%的GPU训练时间。该模型还有效地对齐了未见目标，标志着迈向可泛化多目标偏好对齐的第一步。"
}
{
  "title": "One-Shot Safety Alignment for Large Language Models via Optimal Dualization",
  "title_zh": "题目：通过最优对偶化实现大语言模型的一次性安全对齐",
  "abstract": "The growing safety concerns surrounding large language models raise an urgent need to align them with diverse human preferences to simultaneously enhance their helpfulness and safety. A promising approach is to enforce safety constraints through Reinforcement Learning from Human Feedback (RLHF). For such constrained RLHF, typical Lagrangian-based primal-dual policy optimization methods are computationally expensive and often unstable. This paper presents a perspective of dualization that  reduces constrained alignment to an equivalent unconstrained alignment problem. We do so by pre-optimizing a smooth and convex dual function that has a closed form. This shortcut eliminates the need for cumbersome primal-dual policy iterations, greatly reducing the computational burden and improving training stability. Our strategy leads to two practical algorithms in model-based and preference-based settings (MoCAN and PeCAN, respectively). A broad range of experiments demonstrate the effectiveness and merits of our algorithms.",
  "abstract_zh": "摘要：随着对大语言模型的安全性关注日益增加，迫切需要将其与多样化的人类偏好对齐，以同时增强其有用性和安全性。一种有前途的方法是通过从人类反馈中进行强化学习（RLHF）来实施安全约束。对于这种约束的RLHF，典型的基于拉格朗日的原始-对偶策略优化方法计算成本高且常常不稳定。本文提出了一种对偶化的视角，将约束对齐简化为等效的无约束对齐问题。我们通过预优化一个具有封闭形式的光滑且凸的对偶函数来实现这一点。这一捷径消除了繁琐的原始-对偶策略迭代的需要，大大降低了计算负担并提高了训练的稳定性。我们的策略在基于模型和基于偏好的设置中分别引出了两个实用算法（MoCAN和PeCAN）。大量实验展示了我们算法的有效性和优点。"
}
{
  "title": "Context-Aware Testing: A New Paradigm for Model Testing with Large Language Models",
  "title_zh": "上下文感知测试：一种基于大型语言模型的模型测试新范式",
  "abstract": "The predominant *de facto* paradigm of testing ML models relies on either using only held-out data to compute aggregate evaluation metrics or by assessing the performance on different subgroups.  However, such *data-only testing* methods operate under the restrictive assumption that the available empirical data is the sole input for testing ML models, disregarding valuable contextual information that could guide model testing. In this paper, we challenge the go-to approach of *data-only testing* and introduce *Context-Aware Testing* (CAT) which uses context as an inductive bias to guide the search for meaningful model failures. We instantiate the first CAT system, *SMART Testing*, which employs large language models to hypothesize relevant and likely failures, which are evaluated on data using a *self-falsification mechanism*. Through empirical evaluations in diverse settings, we show that SMART automatically identifies more relevant and impactful failures than alternatives, demonstrating the potential of CAT as a testing paradigm.",
  "abstract_zh": "现有的机器学习模型测试范式主要依赖于使用保留数据计算聚合评估指标或评估在不同子群体上的性能。然而，这种“仅数据测试”方法在严格假设可用的经验数据是测试机器学习模型的唯一输入时，忽略了可能指导模型测试的有价值的上下文信息。在本文中，我们挑战了“仅数据测试”的常用方法，并引入了“上下文感知测试”（CAT），该方法利用上下文作为归纳偏差来指导有意义的模型失败搜索。我们实现了第一个CAT系统，“SMART测试”，该系统使用大型语言模型来假设相关且可能的失败，并通过“自我证伪机制”在数据上进行评估。通过在不同环境中的实证评估，我们展示了SMART自动识别比其他方法更相关且更具影响力的失败，证明了CAT作为测试范式的潜力。"
}
{
  "title": "LeDex: Training LLMs to Better Self-Debug and Explain Code",
  "title_zh": "LeDex：训练大型语言模型以更好地自我调试和解释代码",
  "abstract": "In the domain of code generation, self-debugging is crucial. It allows LLMs to refine their generated code based on execution feedback. This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks. Prior works on self-debugging mostly focus on prompting methods by providing LLMs with few-shot examples, which work poorly on small open-sourced LLMs. In this work, we propose LeDex, a training framework that significantly improves the self-debugging capability of LLMs. Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement. We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories from the LLM itself or a larger teacher model and filtering via execution verification. We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality. SFT improves the pass@1 by up to 15.92\\% and pass@10 by 9.30\\% over four benchmarks. RL training brings additional up to 3.54\\% improvement on pass@1 and 2.55\\% improvement on pass@10. The trained LLMs show iterative refinement ability and can keep refining code continuously. Lastly, our human evaluation shows that the LLMs trained with our framework generate more useful code explanations and help developers better understand bugs in source code.",
  "abstract_zh": "在代码生成领域，自我调试至关重要。它使大型语言模型能够根据执行反馈优化其生成的代码。这一点尤为重要，因为在一次尝试中生成正确的解决方案对复杂任务来说具有挑战性。先前关于自我调试的研究大多集中在通过提供少量示例来提示大型语言模型的方法，这在小型开源大型语言模型上效果不佳。在这项工作中，我们提出了LeDex，一个显著提高大型语言模型自我调试能力的训练框架。直观地，我们观察到对错误代码进行一系列解释然后进行代码改进有助于大型语言模型更好地分析错误代码并进行改进。因此，我们提出了一种自动化流程，通过从大型语言模型自身或更大的教师模型生成大量解释和改进轨迹，并通过执行验证进行过滤，来收集高质量的代码解释和改进数据集。我们在成功和失败的轨迹上进行监督微调（SFT）和进一步的强化学习（RL），并设计了一种新颖的奖励机制，考虑代码解释和改进的质量。SFT在四个基准上将pass@1提高了最多15.92%，pass@10提高了9.30%。RL训练在pass@1上额外提高了最多3.54%，在pass@10上提高了2.55%。训练后的大型语言模型展现了迭代改进能力，能够持续改进代码。最后，我们的人类评估显示，使用我们框架训练的大型语言模型生成了更有用的代码解释，并帮助开发人员更好地理解源代码中的错误。"
}
{
  "title": "Many-shot Jailbreaking",
  "title_zh": "标题：多次示例破解",
  "abstract": "We investigate a family of simple long-context attacks on large language models: prompting with hundreds of demonstrations of undesirable behavior. This attack is newly feasible with the larger context windows recently deployed by language model providers like Google DeepMind, OpenAI and Anthropic. We find that in diverse, realistic circumstances, the effectiveness of this attack follows a power law, up to hundreds of shots. We demonstrate the success of this attack on the most widely used state-of-the-art closed-weight models, and across various tasks. Our results suggest very long contexts present a rich new attack surface for LLMs.",
  "abstract_zh": "摘要：我们研究了一种针对大型语言模型的简单长上下文攻击：通过数百个不良行为示例进行提示。随着谷歌DeepMind、OpenAI和Anthropic等语言模型提供商最近部署的更大上下文窗口，这种攻击成为了新可能。在多样化的、现实的情况下，我们发现这种攻击的有效性遵循幂律，最多可达数百次示例。我们展示了这种攻击在最广泛使用的最先进的闭权重模型和各种任务中的成功。我们的结果表明，非常长的上下文为大型语言模型提供了一个丰富的新攻击面。"
}
{
  "title": "Axioms for AI Alignment from Human Feedback",
  "title_zh": "标题：基于人类反馈的人工智能对齐公理",
  "abstract": "In the context of reinforcement learning from human feedback (RLHF), the reward function is generally derived from maximum likelihood estimation of a random utility model based on pairwise comparisons made by humans. The problem of learning a reward function is one of preference aggregation that, we argue, largely falls within the scope of social choice theory. From this perspective, we can evaluate different aggregation methods via established axioms, examining whether these methods meet or fail well-known standards. We demonstrate that both the Bradley-Terry-Luce Model and its broad generalizations fail to meet basic axioms. In response, we develop novel rules for learning reward functions with strong axiomatic guarantees. A key innovation from the standpoint of social choice is that our problem has a *linear* structure, which greatly restricts the space of feasible rules and leads to a new paradigm that we call *linear social choice*.",
  "abstract_zh": "摘要：在从人类反馈中进行强化学习（RLHF）的背景下，奖励函数通常是基于人类进行的成对比较的随机效用模型的最大似然估计得出的。学习奖励函数的问题是一个偏好聚合问题，我们认为这在很大程度上属于社会选择理论的范畴。从这个角度来看，我们可以通过既定公理评估不同的聚合方法，检查这些方法是否符合或不符合众所周知的标准。我们证明了布拉德利-特里-卢斯模型及其广泛的推广都未能满足基本公理。对此，我们开发了具有强大公理保证的学习奖励函数的新规则。从社会选择的角度来看，一个关键的创新是我们的问题具有*线性*结构，这极大地限制了可行规则的空间，并引导我们提出一个新的范式，我们称之为*线性社会选择*。"
}
{
  "title": "Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing",
  "title_zh": "标题：量化和优化角色扮演中的全球忠实性",
  "abstract": "Persona-driven role-playing (PRP) aims to build AI characters that can respond to user queries by faithfully sticking with \\emph{all} (factual) statements in persona documents.\nUnfortunately, existing faithfulness criteria for PRP are limited to coarse-grained LLM-based scoring without a clear definition or formulation.\nThis paper presents a pioneering exploration to quantify PRP faithfulness evaluation as a fine-grained and explainable criterion, which also serves as a reliable reference for faithfulness optimization.\nOur criterion first discriminates persona statements into \\emph{active} and \\emph{passive} constraints by identifying the query-statement relevance.\nThen, we incorporate all constraints following the principle that the AI character's response should be (a) entailed by active constraints and (b) not contradicted by passive constraints.\nWe translate this principle mathematically into a novel Active-Passive-Constraint (APC) score, a constraint-wise sum of statement-to-response natural language inference (NLI) scores weighted by constraint-query relevance scores. \nIn practice, we build the APC scoring system by symbolically distilling small NLI and relevance discriminators (300M parameters) from GPT-4 for efficiency, and both show high consistency with GPT-4's discrimination.\nWe validate the quality of the APC score against human evaluation based on example personas with tens of statements, and the results show a high correlation.\nAs the APC score could faithfully reflect the PRP quality, we further leverage it as a reward system in direct preference optimization (DPO) for better AI characters. \nOur experiments offer a fine-grained and explainable comparison between existing PRP techniques, revealing their advantages and limitations.\nWe further find APC-based DPO to be one of the most competitive techniques for sticking with all constraints and can be well incorporated with other techniques.\nWe then extend the scale of the experiments to real persons with hundreds of statements and reach a consistent conclusion. \nFinally, we provide comprehensive analyses and case studies to support the effectiveness of APC and APC-based DPO.",
  "abstract_zh": "摘要：以角色为驱动的角色扮演（PRP）旨在构建能够通过忠实遵循角色文件中\\emph{所有}（事实）陈述来响应用户查询的AI角色。不幸的是，现有的PRP忠实性标准仅限于粗粒度的基于LLM的评分，缺乏明确的定义或公式。本文首次探索将PRP忠实性评估量化为细粒度且可解释的标准，同时也作为忠实性优化的可靠参考。我们的标准首先通过识别查询与陈述的相关性，将角色陈述区分为\\emph{主动}和\\emph{被动}约束。然后，我们遵循AI角色响应应（a）由主动约束推导且（b）不与被动约束矛盾的原则，整合所有约束。我们将这一原则数学化为一种新颖的主动-被动约束（APC）评分，即约束-查询相关性评分加权的陈述到响应自然语言推理（NLI）评分的约束逐项总和。在实践中，我们通过从GPT-4中符号提炼出小型NLI和相关性判别器（300M参数）来构建APC评分系统，以提高效率，并且两者均显示出与GPT-4判别的一致性。我们基于包含数十个陈述的示例角色，通过人类评估验证了APC评分的质量，结果显示出高度相关性。由于APC评分能够忠实反映PRP质量，我们进一步将其作为直接偏好优化（DPO）中的奖励系统，以实现更好的AI角色。我们的实验提供了现有PRP技术之间细粒度且可解释的比较，揭示了它们的优点和局限性。我们进一步发现基于APC的DPO是坚持所有约束的最具竞争力的技术之一，并且可以很好地与其他技术结合。随后，我们将实验规模扩展到包含数百个陈述的真实人物，并得出一致的结论。最后，我们提供了全面的分析和案例研究，以支持APC和基于APC的DPO的有效性。"
}
{
  "title": "Secret Collusion among AI Agents: Multi-Agent Deception via Steganography",
  "title_zh": "标题：AI代理间的秘密勾结：通过隐写术实现多代理欺骗",
  "abstract": "Recent advancements in generative AI suggest the potential for large-scale interaction between autonomous agents and humans across platforms such as the internet. While such interactions could foster productive cooperation, the ability of AI agents to circumvent security oversight raises critical multi-agent security problems, particularly in the form of unintended information sharing or undesirable coordination. In our work, we establish the subfield of secret collusion, a form of multi-agent deception, in which two or more agents employ steganographic methods to conceal the true nature of their interactions, be it communicative or otherwise, from oversight. We propose a formal threat model for AI agents communicating steganographically and derive rigorous theoretical insights about the capacity and incentives of large language models (LLMs) to perform secret collusion, in addition to the limitations of threat mitigation measures. We complement our findings with empirical evaluations demonstrating rising steganographic capabilities in frontier single and multi-agent LLM setups and examining potential scenarios where collusion may emerge, revealing limitations in countermeasures such as monitoring, paraphrasing, and parameter optimization. Our work is the first to formalize and investigate secret collusion among frontier foundation models, identifying it as a critical area in AI Safety and outlining a comprehensive research agenda to mitigate future risks of collusion between generative AI systems.",
  "abstract_zh": "摘要：生成式AI的最新进展表明，自动化代理与人类之间在互联网等平台上的大规模互动具有潜力。虽然这种互动可能促进富有成效的合作，但AI代理规避安全监督的能力引发了关键的多代理安全问题，特别是在非预期的信息共享或不良协调方面。在我们的研究中，我们建立了秘密勾结这一子领域，这是一种多代理欺骗形式，其中两个或多个代理使用隐写术方法隐藏其互动的真实性质，无论是交流还是其他方式，以避开监督。我们提出了一个AI代理隐写通信的正式威胁模型，并推导出关于大型语言模型（LLMs）进行秘密勾结的能力和动机的严格理论见解，以及威胁缓解措施的局限性。我们通过实证评估补充了我们的发现，展示了前沿单一和多代理LLM设置中日益增强的隐写能力，并考察了可能出现勾结的潜在场景，揭示了监控、释义和参数优化等对策的局限性。我们的工作首次正式化并研究了前沿基础模型间的秘密勾结，将其识别为AI安全中的关键领域，并概述了一项全面的研究议程，以减轻未来生成式AI系统之间勾结的风险。"
}
{
  "title": "Can Graph Learning Improve Planning in LLM-based Agents?",
  "title_zh": "标题：图学习能否提升基于大型语言模型的智能体的规划能力？",
  "abstract": "Task planning in language agents is emerging as an important research topic alongside the development of large language models (LLMs). It aims to break down complex user requests in natural language into solvable sub-tasks, thereby fulfilling the original requests. In this context, the sub-tasks can be naturally viewed as a graph, where the nodes represent the sub-tasks, and the edges denote the dependencies among them. Consequently, task planning is a decision-making problem that involves selecting a connected path or subgraph within the corresponding graph and invoking it. In this paper, we explore graph learning-based methods for task planning, a direction that is orthogonal to the prevalent focus on prompt design. Our interest in graph learning stems from a theoretical discovery: the biases of attention and auto-regressive loss impede LLMs' ability to effectively navigate decision-making on graphs, which is adeptly addressed by graph neural networks (GNNs). This theoretical insight led us to integrate GNNs with LLMs to enhance overall performance. Extensive experiments demonstrate that GNN-based methods surpass existing solutions even without training, and minimal training can further enhance their performance. The performance gain increases with a larger task graph size.",
  "abstract_zh": "摘要：随着大型语言模型（LLMs）的发展，语言智能体中的任务规划正成为一个重要的研究课题。其目标是将自然语言中的复杂用户请求分解为可解决的子任务，从而满足原始请求。在此背景下，子任务可以自然地视为一个图，其中节点代表子任务，边表示它们之间的依赖关系。因此，任务规划是一个决策问题，涉及在相应的图中选择一个连接的路径或子图并调用它。在本文中，我们探索了基于图学习的方法进行任务规划，这一方向与当前流行的提示设计关注点正交。我们对图学习的兴趣源于一个理论发现：注意力和自回归损失的偏差阻碍了LLMs在图上有效进行决策的能力，而图神经网络（GNNs）能够巧妙地解决这一问题。这一理论见解促使我们将GNNs与LLMs结合以提升整体性能。大量实验表明，即使不进行训练，基于GNN的方法也能超越现有解决方案，而少量训练可以进一步提升其性能。性能提升随着任务图规模的增大而增加。"
}
{
  "title": "InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory",
  "title_zh": "InfLLM: 使用高效上下文记忆的无需训练的LLM长上下文外推",
  "abstract": "Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs (e.g., LLM-driven agents). However, existing LLMs, pre-trained on sequences with a restricted maximum length, cannot process longer sequences due to the out-of-domain and distraction issues. Common solutions often involve continual pre-training on longer sequences, which will introduce expensive computational overhead and uncontrollable change in model capabilities. In this paper, we unveil the intrinsic capacity of LLMs for understanding extremely long sequences without any fine-tuning. To this end, we introduce a training-free memory-based method, InfLLM. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences with a limited context window and well capture long-distance dependencies. Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences. Even when the sequence length is scaled to 1,024K, InfLLM still effectively captures long-distance dependencies. Our code can be found at https://github.com/thunlp/InfLLM.",
  "abstract_zh": "大型语言模型（LLM）已成为处理长流输入（例如，LLM驱动的代理）在现实应用中的基石。然而，现有的LLM由于在有限最大长度的序列上进行预训练，无法处理更长的序列，因为这会导致域外和干扰问题。常见的解决方案通常涉及对更长序列的持续预训练，这将引入昂贵的计算开销和模型能力的不可控变化。在本文中，我们揭示了LLM无需微调即可理解极长序列的内在能力。为此，我们引入了一种无需训练的基于记忆的方法，InfLLM。具体而言，InfLLM将远距离上下文存储到额外的记忆单元中，并采用一种高效机制查找与标记相关的单元进行注意力计算。因此，InfLLM允许LLM在有限的上下文窗口中高效处理长序列，并很好地捕捉长距离依赖关系。无需任何训练，InfLLM使得在由几千个标记组成的序列上预训练的LLM能够实现与那些在长序列上持续训练的竞争基线相当的性能。即使序列长度扩展到1,024K，InfLLM仍能有效捕捉长距离依赖关系。我们的代码可以在https://github.com/thunlp/InfLLM找到。"
}
{
  "title": "Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization",
  "title_zh": "高效的大型语言模型越狱：自适应稠密到稀疏约束优化",
  "abstract": "Recent research indicates that large language models (LLMs) are susceptible to jailbreaking attacks that can generate harmful content. This paper introduces a novel token-level attack method, Adaptive Dense-to-Sparse Constrained Optimization (ADC), which has been shown to successfully jailbreak multiple open-source LLMs. Drawing inspiration from the difficulties of discrete token optimization, our method relaxes the discrete jailbreak optimization into a continuous optimization process while gradually increasing the sparsity of the optimizing vectors. This technique effectively bridges the gap between discrete and continuous space optimization. Experimental results demonstrate that our method is more effective and efficient than state-of-the-art token-level methods. On Harmbench, our approach achieves the highest attack success rate on seven out of eight LLMs compared to the latest jailbreak methods. \\textcolor{red}{Trigger Warning: This paper contains model behavior that can be offensive in nature.}",
  "abstract_zh": "最近的研究表明，大型语言模型（LLMs）容易受到越狱攻击，从而生成有害内容。本文介绍了一种新颖的基于令牌级别的攻击方法，即自适应稠密到稀疏约束优化（ADC），该方法已成功破解多个开源LLM。受到离散令牌优化困难的启发，我们的方法将离散越狱优化放松为连续优化过程，同时逐步增加优化向量的稀疏性。这一技术有效地弥合了离散和连续空间优化之间的差距。实验结果表明，我们的方法比最先进的令牌级别方法更有效和高效。在Harmbench上，与最新的越狱方法相比，我们的方法在八个LLM中的七个上实现了最高的攻击成功率。警告：本文包含可能具有冒犯性质的模型行为。"
}
{
  "title": "CulturePark: Boosting Cross-cultural Understanding in Large Language Models",
  "title_zh": "Title: CulturePark：提升大型语言模型的跨文化理解",
  "abstract": "Cultural bias is pervasive in many large language models (LLMs), largely due to the deficiency of data representative of different cultures.\nTypically, cultural datasets and benchmarks are constructed either by extracting subsets of existing datasets or by aggregating from platforms such as Wikipedia and social media.\nHowever, these approaches are highly dependent on real-world data and human annotations, making them costly and difficult to scale.\nInspired by cognitive theories on social communication, this paper introduces CulturePark, an LLM-powered multi-agent communication framework for cultural data collection.\nCulturePark simulates cross-cultural human communication with LLM-based agents playing roles in different cultures.\nIt generates high-quality cross-cultural dialogues encapsulating human beliefs, norms, and customs.\nUsing CulturePark, we generated 41,000 cultural samples to fine-tune eight culture-specific LLMs.\nWe evaluated these models across three downstream tasks: content moderation, cultural alignment, and cultural education.\nResults show that for content moderation, our GPT-3.5-based models either match or outperform GPT-4 on $41$ datasets. Regarding cultural alignment, our models surpass GPT-4 on Hofstede's VSM 13 framework.\nFurthermore, for cultural education of human participants, our models demonstrate superior outcomes in both learning efficacy and user experience compared to GPT-4. CulturePark proves an important step in addressing cultural bias and advancing the democratization of AI, highlighting the critical role of culturally inclusive data in model training. Code is released at https://github.com/Scarelette/CulturePark.",
  "abstract_zh": "Abstract: 文化偏见在许多大型语言模型中普遍存在，这主要是由于缺乏代表不同文化的数据。通常，文化数据集和基准是通过提取现有数据集的子集或从维基百科和社交媒体等平台聚合而构建的。然而，这些方法高度依赖于现实世界的数据和人工注释，使其成本高昂且难以扩展。受社会交流认知理论的启发，本文介绍了CulturePark，一个由大型语言模型驱动的多代理通信框架，用于文化数据收集。CulturePark通过基于大型语言模型的代理在不同文化中扮演角色，模拟跨文化的人类交流。它生成高质量的跨文化对话，涵盖人类的信仰、规范和习俗。使用CulturePark，我们生成了41,000个文化样本，以微调八个特定文化的语言模型。我们在三个下游任务中评估了这些模型：内容审核、文化对齐和文化教育。结果表明，在内容审核方面，我们基于GPT-3.5的模型在41个数据集上与GPT-4相当或表现更佳。在文化对齐方面，我们的模型在霍夫斯泰德的VSM 13框架上超越了GPT-4。此外，在人类参与者的文化教育中，我们的模型在学习效果和用户体验上均优于GPT-4。CulturePark在解决文化偏见和推进AI民主化方面迈出了重要一步，强调了文化包容性数据在模型训练中的关键作用。代码已在https://github.com/Scarelette/CulturePark发布。"
}
{
  "title": "Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks",
  "title_zh": "在模运算任务中学习领悟：情境学习和技能组合的出现",
  "abstract": "Large language models can solve tasks that were not present in the training set. This capability is believed to be due to in-context learning and skill composition. In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks. Specifically, we consider a finite collection of linear modular functions $z = a  x + b  y \\text{ mod } p$ labeled by the vector $(a, b) \\in \\mathbb{Z}_p^2$. We use some of these tasks for pre-training and the rest for out-of-distribution testing. We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is *transient*, necessitating early stopping. Finally, we perform an interpretability study of the pre-trained models, revealing highly structured representations in both attention heads and MLPs; and discuss the learned algorithms. Notably, we find an algorithmic shift in deeper models, as we go from few to many in-context examples.",
  "abstract_zh": "大型语言模型可以解决训练集中不存在的任务。这种能力被认为是由于情境学习和技能组合。在这项工作中，我们研究了情境学习和技能组合在一组模运算任务中的出现。具体而言，我们考虑了一组有限的线性模函数 $z = a  x + b  y \\text{ mod } p$，由向量 $(a, b) \\in \\mathbb{Z}_p^2$ 标记。我们使用其中一些任务进行预训练，其余的用于分布外测试。我们通过实验证明，随着预训练任务数量的增加，GPT风格的变压器表现出从分布内到分布外泛化的转变。我们发现，能够进行分布外泛化的最小模型需要两个变压器块，而对于更深的模型，分布外泛化阶段是*短暂的*，需要提前停止训练。最后，我们对预训练模型进行了可解释性研究，揭示了注意力头和MLP中高度结构化的表示；并讨论了学习到的算法。值得注意的是，我们发现随着情境示例数量的增加，较深模型中出现了算法的转变。"
}
{
  "title": "Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control",
  "title_zh": "标题：通过稀疏激活控制增强大型语言模型的多维可信度",
  "abstract": "As the development and application of Large Language Models (LLMs) continue to advance rapidly, enhancing their trustworthiness and aligning them with human preferences has become a critical area of research. Traditional methods rely heavily on extensive data for Reinforcement Learning from Human Feedback (RLHF), but representation engineering offers a new, training-free approach. This technique leverages semantic features to control the representation of LLM's intermediate hidden states, enabling the model to meet specific requirements such as increased honesty or heightened safety awareness. However, a significant challenge arises when attempting to fulfill multiple requirements simultaneously. It proves difficult to encode various semantic contents, like honesty and safety, into a singular semantic feature, restricting its practicality.\nIn this work, we address this challenge through Sparse Activation Control. By delving into the intrinsic mechanisms of LLMs, we manage to identify and pinpoint modules that are closely related to specific tasks within the model, i.e. attention heads. These heads display sparse characteristics that allow for near-independent control over different tasks. Our experiments, conducted on the open-source Llama series models, have yielded encouraging results. The models were able to align with human preferences on issues of safety, factualness, and bias concurrently.",
  "abstract_zh": "摘要：随着大型语言模型（LLMs）的发展和应用迅速推进，增强其可信度并使其与人类偏好对齐已成为研究的关键领域。传统方法严重依赖于大量数据进行人类反馈的强化学习（RLHF），但表示工程提供了一种新的、无需训练的方法。该技术利用语义特征来控制LLM中间隐藏状态的表示，使模型能够满足特定要求，如增加诚实性或提高安全意识。然而，当尝试同时满足多个要求时，会出现重大挑战。将各种语义内容（如诚实和安全）编码到单一语义特征中证明是困难的，限制了其实用性。在这项工作中，我们通过稀疏激活控制解决了这一挑战。通过深入研究LLM的内在机制，我们成功识别并定位了与模型中特定任务密切相关的模块，即注意力头。这些头显示出稀疏特性，允许对不同任务进行近乎独立的控制。我们在开源的Llama系列模型上进行的实验取得了令人鼓舞的结果。模型能够在安全性、事实性和偏见问题上同时与人类偏好对齐。"
}
{
  "title": "Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning",
  "title_zh": "标题：通过预算约束策略学习实现高效的上下文大语言模型级联",
  "abstract": "Recent successes in natural language processing have led to the proliferation of large language models (LLMs) by multiple providers. Each LLM offering has different inference accuracy, monetary cost, and latency, and their accuracy further depends on the exact wording of the question (i.e., the specific prompt). At the same time, users often have a limit on monetary budget and latency to answer all their questions, and they do not know which LLMs to choose for each question to meet their accuracy and long term budget requirements. To navigate this rich design space, we propose TREACLE (Thrifty Reasoning via Context-Aware LLM and Prompt Selection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user's monetary cost and latency constraints. TREACLE uses the problem context, including question text embeddings (reflecting the type or difficulty of a query) and the response history (reflecting the consistency of previous responses) to make smart decisions. Our evaluations on standard reasoning datasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE enables cost savings of up to 85% compared to baselines, while maintaining high accuracy. Importantly, it provides the user with the ability to gracefully trade off accuracy for cost.",
  "abstract_zh": "摘要：自然语言处理的最新成功导致了多个提供商的大语言模型（LLM）的激增。每个LLM产品在推理准确性、货币成本和延迟方面各不相同，其准确性还取决于问题的确切措辞（即具体的提示）。同时，用户通常在回答所有问题时有货币预算和延迟的限制，他们不知道应该为每个问题选择哪个LLM以满足其准确性和长期预算要求。为了在这个丰富的设计空间中导航，我们提出了TREACLE（通过上下文感知LLM和提示选择的节俭推理），这是一种强化学习策略，可以在遵循用户的货币成本和延迟约束的同时共同选择模型和提示方案。TREACLE利用问题上下文，包括问题文本嵌入（反映查询的类型或难度）和响应历史（反映先前响应的一致性）来做出明智的决策。我们在标准推理数据集（GSM8K、CSQA和LLC）上对各种LLM和提示的评估表明，TREACLE在保持高准确率的同时，与基线相比可节省高达85%的成本。重要的是，它为用户提供了在准确性和成本之间优雅权衡的能力。"
}
{
  "title": "Unelicitable Backdoors via Cryptographic Transformer Circuits",
  "title_zh": "标题：通过加密变压器电路实现不可触发的后门",
  "abstract": "The rapid proliferation of open-source language models significantly increases the risks of downstream backdoor attacks. These backdoors can introduce dangerous behaviours during model deployment and can evade detection by conventional cybersecurity monitoring systems. In this paper, we introduce a novel class of backdoors in transformer models, that, in contrast to prior art, are unelicitable in nature. Unelicitability prevents the defender from triggering the backdoor, making it impossible to properly evaluate ahead of deployment even if given full white-box access and using automated techniques, such as red-teaming or certain formal verification methods. We show that our novel construction is not only unelicitable thanks to using cryptographic techniques, but also has favourable robustness properties.\nWe confirm these properties in empirical investigations, and provide evidence that our backdoors can withstand state-of-the-art mitigation strategies. Additionally, we expand on previous work by showing that our universal backdoors, while not completely undetectable in white-box settings, can be harder to detect than some existing designs. By demonstrating the feasibility of seamlessly integrating backdoors into transformer models, this paper fundamentally questions the efficacy of pre-deployment detection strategies. This offers new insights into the offence-defence balance in AI safety and security.",
  "abstract_zh": "摘要：开源语言模型的快速普及显著增加了下游后门攻击的风险。这些后门在模型部署期间可能引入危险行为，并能够逃避传统网络安全监控系统的检测。在本文中，我们介绍了一种新型的变压器模型后门，与现有技术相比，其本质上是不可触发的。不可触发性使得防御者无法触发后门，即使在部署前拥有完整的白盒访问权限并使用自动化技术（如红队测试或某些形式验证方法），也无法正确评估。我们展示了我们的新构造不仅由于使用了加密技术而不可触发，还具有良好的鲁棒性。我们在实证研究中确认了这些特性，并提供证据表明我们的后门可以抵御最先进的缓解策略。此外，我们通过展示我们的通用后门在白盒环境中虽然不是完全不可检测，但比一些现有设计更难检测，扩展了先前的工作。通过展示将后门无缝集成到变压器模型中的可行性，本文从根本上质疑了部署前检测策略的有效性。这为人工智能安全和安全中的攻防平衡提供了新的见解。"
}
{
  "title": "Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees",
  "title_zh": "题目：保序对齐：在有保证的情况下知道何时信任基础模型",
  "abstract": "Before deploying outputs from foundation models in high-stakes tasks, it is imperative to ensure that they align with human values.\nFor instance, in radiology report generation, reports generated by a vision-language model must align with human evaluations before their use in medical decision-making. This paper presents Conformal Alignment, a general framework for identifying units whose outputs meet a user-specified alignment criterion. It is guaranteed that on average, a prescribed fraction of selected units indeed meet the alignment criterion, regardless of the foundation model or the data distribution. Given any pre-trained model and new units with model-generated outputs, Conformal Alignment leverages a set of reference data with ground-truth alignment status to train an alignment predictor. It then selects new units whose predicted alignment scores surpass a data-dependent threshold, certifying their corresponding outputs as trustworthy. Through applications to question answering and radiology report generation, we demonstrate that our method is able to accurately identify units with trustworthy outputs via lightweight training over a moderate amount of reference data. En route, we investigate the informativeness of various features in alignment prediction and combine them with standard models to construct the alignment predictor.",
  "abstract_zh": "摘要：在高风险任务中部署基础模型的输出之前，确保其与人类价值观一致是至关重要的。例如，在放射学报告生成中，视觉语言模型生成的报告必须在用于医疗决策之前与人类评估一致。本文提出了保序对齐，一种用于识别输出符合用户指定对齐标准的单元的一般框架。无论基础模型或数据分布如何，平均而言，所选单元中有规定比例确实符合对齐标准。对于任何预训练模型和具有模型生成输出的新单元，保序对齐利用具有真实对齐状态的参考数据集来训练对齐预测器。然后选择预测对齐分数超过数据依赖阈值的新单元，证明其相应输出是可信的。通过在问答和放射学报告生成中的应用，我们证明了我们的方法能够通过对适量参考数据的轻量级训练准确识别出具有可信输出的单元。在此过程中，我们研究了对齐预测中各种特征的信息性，并将其与标准模型结合以构建对齐预测器。"
}
{
  "title": "DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models",
  "title_zh": "DISP-LLM：大语言模型的维度无关结构剪枝",
  "abstract": "Large Language Models (LLMs) have achieved remarkable success in various natural language processing tasks, including language modeling, understanding, and generation. However, the increased memory and computational costs associated with these models pose significant challenges for deployment on resource-limited devices. Structural pruning has emerged as a promising solution to reduce the costs of LLMs without requiring post-processing steps. Prior structural pruning methods either follow the dependence of structures at the cost of limiting flexibility, or introduce non-trivial additional parameters by incorporating different projection matrices. In this work, we propose a novel approach that relaxes the constraint imposed by regular structural pruning methods and eliminates the structural dependence along the embedding dimension. Our dimension-independent structural pruning method offers several benefits. Firstly, our method enables different blocks to utilize different subsets of the feature maps. Secondly, by removing structural dependence, we facilitate each block to possess varying widths along its input and output dimensions, thereby significantly enhancing the flexibility of structural pruning. We evaluate our method on various LLMs, including OPT, LLaMA, LLaMA-2, Phi-1.5, and Phi-2. Experimental results demonstrate that our approach outperforms other state-of-the-art methods, showing for the first time that structural pruning can achieve an accuracy similar to semi-structural pruning.",
  "abstract_zh": "大语言模型（LLMs）在包括语言建模、理解和生成在内的各种自然语言处理任务中取得了显著成功。然而，这些模型所带来的内存和计算成本的增加对在资源有限的设备上进行部署构成了重大挑战。结构剪枝已成为在不需要后处理步骤的情况下降低LLMs成本的有前途的解决方案。先前的结构剪枝方法要么遵循结构的依赖性以限制灵活性为代价，要么通过引入不同的投影矩阵引入非平凡的额外参数。在这项工作中，我们提出了一种新颖的方法，该方法放宽了常规结构剪枝方法施加的约束，并消除了沿嵌入维度的结构依赖性。我们的维度无关结构剪枝方法提供了几个好处。首先，我们的方法使不同的块能够利用特征图的不同子集。其次，通过消除结构依赖性，我们促进每个块在其输入和输出维度上具有不同的宽度，从而显著增强了结构剪枝的灵活性。我们在各种LLMs上评估了我们的方法，包括OPT、LLaMA、LLaMA-2、Phi-1.5和Phi-2。实验结果表明，我们的方法优于其他最先进的方法，首次显示结构剪枝可以实现与半结构剪枝相似的准确性。"
}
{
  "title": "Preference Learning Algorithms Do Not Learn Preference Rankings",
  "title_zh": "偏好学习算法并不学习偏好排序",
  "abstract": "Preference learning algorithms (e.g., RLHF and DPO) are frequently used to steer LLMs to produce generations that are more preferred by humans, but our understanding of their inner workings is still limited. In this work, we study the conventional wisdom that preference learning trains models to assign higher likelihoods to more preferred outputs than less preferred outputs, measured via *ranking accuracy*.\nSurprisingly, we find that most state-of-the-art preference-tuned models achieve a ranking accuracy of less than 60% on common preference datasets. We furthermore derive the *idealized ranking accuracy* that a preference-tuned LLM would achieve if it optimized the DPO or RLHF objective perfectly. We demonstrate that existing models exhibit a significant *alignment gap* -- *i.e.*, a gap between the observed and idealized ranking accuracies. \nWe attribute this discrepancy to the DPO objective, which is empirically and theoretically ill-suited to correct even mild ranking errors in the reference model, and derive a simple and efficient formula for quantifying the difficulty of learning a given preference datapoint.\nFinally, we demonstrate that ranking accuracy strongly correlates with the empirically popular win rate metric when the model is close to the reference model used in the objective, shedding further light on the differences between on-policy (e.g., RLHF) and off-policy (e.g., DPO) preference learning algorithms.",
  "abstract_zh": "偏好学习算法（例如，RLHF和DPO）经常被用来引导大型语言模型（LLM）生成更受人类偏好的输出，但我们对其内部机制的理解仍然有限。在这项工作中，我们研究了传统观点，即偏好学习训练模型以便为更受偏好的输出分配更高的可能性，而不是不太受偏好的输出，这通过*排序准确性*来衡量。令人惊讶的是，我们发现大多数最先进的偏好调优模型在常见的偏好数据集上实现的排序准确性不到60%。我们进一步推导出一个偏好调优的LLM如果完美优化DPO或RLHF目标将实现的*理想化排序准确性*。我们证明现有模型表现出显著的*对齐差距*——即观察到的排序准确性与理想化排序准确性之间的差距。我们将这一差异归因于DPO目标，它在经验和理论上都不适合纠正参考模型中的即使是轻微的排序错误，并推导出一个简单而有效的公式来量化学习给定偏好数据点的难度。最后，我们证明当模型接近目标中使用的参考模型时，排序准确性与经验上流行的胜率指标强烈相关，从而进一步揭示了在策略内（例如，RLHF）和策略外（例如，DPO）偏好学习算法之间的差异。"
}
{
  "title": "Graph-based Uncertainty Metrics for Long-form Language Model Generations",
  "title_zh": "基于图的长文本语言模型生成不确定性度量",
  "abstract": "Recent advancements in Large Language Models (LLMs) have significantly improved text generation capabilities, but these systems are still known to hallucinate, and granular uncertainty estimation for long-form LLM generations remains challenging. \nIn this work, we propose Graph Uncertainty -- which represents the relationship between LLM generations and claims within them as a bipartite graph and estimates the claim-level uncertainty with a family of graph centrality metrics. Under this view, existing uncertainty estimation methods based on the concept of self-consistency can be viewed as using degree centrality as an uncertainty measure, and we show that more sophisticated alternatives such as closeness centrality provide consistent gains at claim-level uncertainty estimation.\nMoreover, we present uncertainty-aware decoding techniques that leverage both the graph structure and uncertainty estimates to improve the factuality of LLM generations by preserving only the most reliable claims. Compared to existing methods, our graph-based uncertainty metrics lead to an average of 6.8% relative gains on AUPRC across various long-form generation settings, and our end-to-end system provides consistent 2-4% gains in factuality over existing decoding techniques while significantly improving the informativeness of generated responses.",
  "abstract_zh": "近年来，大型语言模型（LLMs）的进步显著提高了文本生成能力，但这些系统仍然容易出现幻觉，并且长文本LLM生成的不确定性估计仍然具有挑战性。在这项工作中，我们提出了图不确定性——将LLM生成的文本与其中的断言关系表示为二分图，并使用一系列图中心性度量来估计断言级别的不确定性。在这种视角下，基于自一致性概念的现有不确定性估计方法可以被视为使用度中心性作为不确定性度量，我们展示了更复杂的替代方法如接近中心性在断言级别不确定性估计中提供了一致的增益。此外，我们提出了不确定性感知的解码技术，利用图结构和不确定性估计，通过仅保留最可靠的断言来提高LLM生成的事实性。与现有方法相比，我们基于图的不确定性度量在各种长文本生成设置中平均在AUPRC上实现了6.8%的相对增益，我们的端到端系统在现有解码技术上提供了2-4%的事实性增益，同时显著提高了生成响应的信息量。"
}
{
  "title": "Knowledge Circuits in Pretrained Transformers",
  "title_zh": "预训练Transformer中的知识电路",
  "abstract": "The remarkable capabilities of modern large language models are rooted in their vast repositories of knowledge encoded within their parameters, enabling them to perceive the world and engage in reasoning. The inner workings of how these models store knowledge have long been a subject of intense interest and investigation among researchers. To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head. In this paper, we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge. The experiments, conducted with GPT2 and TinyLLAMA, has allowed us to observe how certain information heads, relation heads, and Multilayer Perceptrons collaboratively encode knowledge within the model. Moreover, we evaluate the impact of current knowledge editing techniques on these knowledge circuits, providing deeper insights into the functioning and constraints of these editing methodologies. Finally, we utilize knowledge circuits to analyze and interpret language model behaviors such as hallucinations and in-context learning. We believe the knowledge circuit holds potential for advancing our understanding of Transformers and guiding the improved design of knowledge editing.",
  "abstract_zh": "现代大型语言模型的显著能力源于其参数中编码的庞大知识库，使其能够感知世界并进行推理。这些模型如何存储知识的内部工作原理长期以来一直是研究人员极为关注和研究的主题。迄今为止，大多数研究集中在这些模型中的孤立组件上，例如多层感知机和注意力头。在本文中，我们深入研究了语言模型的计算图，以揭示在表达特定知识方面起关键作用的知识电路。通过对GPT2和TinyLLAMA的实验，我们观察到某些信息头、关系头和多层感知机如何协同在模型中编码知识。此外，我们评估了当前知识编辑技术对这些知识电路的影响，提供了对这些编辑方法功能和限制的更深入见解。最后，我们利用知识电路分析和解释语言模型的行为，如幻觉和上下文学习。我们相信，知识电路有潜力推动我们对Transformer的理解，并指导知识编辑的改进设计。"
}
{
  "title": "Cross-model Control: Improving Multiple Large Language Models in One-time Training",
  "title_zh": "跨模型控制：一次训练中改进多个大型语言模型",
  "abstract": "The number of large language models (LLMs) with varying parameter scales and vocabularies is increasing. While they deliver powerful performance, they also face a set of common optimization needs to meet specific requirements or standards, such as instruction following or avoiding the output of sensitive information from the real world. However, how to reuse the fine-tuning outcomes of one model to other models to reduce training costs remains a challenge. To bridge this gap, we introduce Cross-model Control (CMC), a method that improves multiple LLMs in one-time training with a portable tiny language model. Specifically, we have observed that the logit shift before and after fine-tuning is remarkably similar across different models. Based on this insight, we incorporate a tiny language model with a minimal number of parameters. By training alongside a frozen template LLM, the tiny model gains the capability to alter the logits output by the LLMs. To make this tiny language model applicable to models with different vocabularies, we propose a novel token mapping strategy named PM-MinED. We have conducted extensive experiments on instruction tuning and unlearning tasks, demonstrating the effectiveness of CMC. Our code is available at https://github.com/wujwyi/CMC",
  "abstract_zh": "大型语言模型（LLM）的数量正在增加，其参数规模和词汇表各不相同。虽然它们表现出强大的性能，但也面临一系列共同的优化需求，以满足特定要求或标准，例如遵循指令或避免输出来自现实世界的敏感信息。然而，如何将一个模型的微调成果重用于其他模型以降低训练成本仍然是一个挑战。为了解决这一问题，我们引入了跨模型控制（CMC），这是一种通过一个可移植的小型语言模型在一次训练中改进多个LLM的方法。具体来说，我们观察到微调前后的logit偏移在不同模型之间非常相似。基于这一见解，我们结合了一个参数数量最小的小型语言模型。通过与冻结的模板LLM一起训练，小型模型获得了改变LLM输出logits的能力。为了使这个小型语言模型适用于具有不同词汇表的模型，我们提出了一种名为PM-MinED的新型标记映射策略。我们在指令调优和去学习任务上进行了广泛的实验，证明了CMC的有效性。我们的代码可在https://github.com/wujwyi/CMC获取。"
}
{
  "title": "Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models",
  "title_zh": "单图像遗忘：多模态大型语言模型中的高效机器遗忘",
  "abstract": "Machine unlearning (MU) empowers individuals with the `right to be forgotten' by removing their private or sensitive information encoded in machine learning models. However, it remains uncertain whether MU can be effectively applied to Multimodal Large Language Models (MLLMs), particularly in scenarios of forgetting the leaked visual data of concepts. To overcome the challenge, we propose an efficient method, Single Image Unlearning (SIU), to unlearn the visual recognition of a concept by fine-tuning a single associated image for few steps. SIU consists of two key aspects: (i) Constructing Multifaceted fine-tuning data. We introduce four targets, based on which we construct fine-tuning data for the concepts to be forgotten; (ii)  Joint training loss. To synchronously forget the visual recognition of concepts and preserve the utility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergence Loss combined with Cross Entropy loss. Alongside our method, we establish MMUBench, a new benchmark for MU in MLLMs and introduce a collection of metrics for its evaluation. Experimental results on MMUBench show that SIU completely surpasses the performance of existing methods. Furthermore, we surprisingly find that SIU can avoid invasive membership inference attacks and jailbreak attacks. To the best of our knowledge, we are the first to explore MU in MLLMs. We will release the code and benchmark in the near future.",
  "abstract_zh": "机器遗忘（MU）通过移除机器学习模型中编码的私人或敏感信息，赋予个人“被遗忘权”。然而，MU是否能有效应用于多模态大型语言模型（MLLMs）仍不确定，特别是在遗忘概念的泄露视觉数据的情境中。为克服这一挑战，我们提出了一种高效的方法——单图像遗忘（SIU），通过对单个相关图像进行少量步骤的微调来遗忘概念的视觉识别。SIU包括两个关键方面：（i）构建多方面的微调数据。我们引入四个目标，基于此构建需要遗忘的概念的微调数据；（ii）联合训练损失。为了同步遗忘概念的视觉识别并保留MLLMs的实用性，我们通过一种新颖的双掩蔽KL散度损失结合交叉熵损失来微调MLLMs。除了我们的方法外，我们还建立了MMUBench，一个用于MLLMs中MU的新基准，并引入了一系列用于其评估的指标。在MMUBench上的实验结果表明，SIU完全超越了现有方法的性能。此外，我们惊讶地发现SIU可以避免侵入性的成员推断攻击和越狱攻击。据我们所知，我们是首次在MLLMs中探索MU。我们将在不久的将来发布代码和基准。"
}
{
  "title": "Make Your LLM Fully Utilize the Context",
  "title_zh": "标题：让您的大型语言模型充分利用上下文",
  "abstract": "While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the *lost-in-the-middle* challenge.\nWe hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information.\nBased on this intuition, our study presents **information-intensive (IN2) training**, a purely data-driven solution to overcome lost-in-the-middle.\nSpecifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) **fine-grained information awareness** on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the **integration and reasoning** of information from two or more short segments.\nThrough applying this information-intensive training on Mistral-7B, we present **FILM-7B** (FIll-in-the-Middle).\nTo thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval).\nThe probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window.\nBeyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU).",
  "abstract_zh": "摘要：尽管许多当代大型语言模型（LLMs）可以处理较长的输入，但它们仍然难以充分利用长上下文中的信息，这被称为*中间丢失*挑战。我们假设这源于长上下文训练过程中缺乏足够的显式监督，未能强调长上下文中的任何位置都可能包含关键信息。基于这一直觉，我们的研究提出了**信息密集型（IN2）训练**，这是一种纯数据驱动的解决方案，用于克服中间丢失。具体而言，IN2训练利用合成的长上下文问答数据集，其中答案需要（1）对合成长上下文（4K-32K个标记）内的短片段（约128个标记）进行**细粒度信息感知**，以及（2）**整合和推理**来自两个或多个短片段的信息。通过在Mistral-7B上应用这种信息密集型训练，我们提出了**FILM-7B**（中间填充）。为了全面评估FILM-7B利用长上下文的能力，我们设计了三项探测任务，涵盖各种上下文风格（文档、代码和结构化数据上下文）和信息检索模式（前向、后向和双向检索）。探测结果表明，FILM-7B可以稳健地从其32K上下文窗口中的不同位置检索信息。除了这些探测任务外，FILM-7B显著提高了真实世界长上下文任务的性能（例如，在NarrativeQA上的F1分数从23.5提高到26.9），同时在短上下文任务上保持了可比的性能（例如，在MMLU上的准确率从59.3降至59.2）。"
}
{
  "title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases",
  "title_zh": "标题：AgentPoison：通过污染记忆或知识库对LLM代理进行红队攻击",
  "abstract": "LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or\nRAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, resilience, and stealthiness. Extensive experiments demonstrate AgentPoison's effectiveness in attacking\nthree types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. We inject the poisoning instances into the RAG knowledge base and long-term memories of these agents, respectively, demonstrating the generalization of AgentPoison. On each agent, AgentPoison achieves an average attack success rate of $\\ge$ 80% with minimal\nimpact on benign performance ($\\le$ 1%) with a poison rate < 0.1%. The code and data is available at https://github.com/BillChan226/AgentPoison.",
  "abstract_zh": "摘要：LLM代理在各类应用中表现出色，主要归功于其在推理、利用外部知识和工具、调用API以及执行动作与环境交互方面的先进能力。当前的代理通常使用记忆模块或检索增强生成（RAG）机制，从知识库中检索具有相似嵌入的过去知识和实例，以指导任务规划和执行。然而，对未经验证的知识库的依赖引发了对其安全性和可信度的重大担忧。为了揭示这些漏洞，我们提出了一种新颖的红队方法AgentPoison，这是第一个通过污染其长期记忆或RAG知识库来针对通用和基于RAG的LLM代理的后门攻击。具体而言，我们将触发器生成过程形成一个约束优化，以通过将触发实例映射到一个独特的嵌入空间来优化后门触发器，从而确保每当用户指令包含优化的后门触发器时，恶意演示将以高概率从被污染的记忆或知识库中检索出来。同时，不含触发器的良性指令仍将保持正常性能。与传统的后门攻击不同，AgentPoison无需额外的模型训练或微调，且优化后的后门触发器表现出优越的可迁移性、弹性和隐蔽性。大量实验表明，AgentPoison在攻击三种类型的真实世界LLM代理中效果显著：基于RAG的自动驾驶代理、知识密集型问答代理和医疗保健EHRAgent。我们将污染实例分别注入这些代理的RAG知识库和长期记忆中，展示了AgentPoison的泛化能力。在每个代理上，AgentPoison实现了平均攻击成功率≥80%，对良性性能的影响最小（≤1%），污染率<0.1%。代码和数据可在https://github.com/BillChan226/AgentPoison获取。"
}
{
  "title": "Embedding-Aligned Language Models",
  "title_zh": "嵌入对齐的语言模型",
  "abstract": "We propose a novel approach for training large language models (LLMs) to adhere to objectives defined within a latent embedding space. Our method leverages reinforcement learning (RL), treating a pre-trained LLM as an environment. Our embedding-aligned guided language (EAGLE) agent is trained to iteratively steer the LLM's generation towards optimal regions of the latent embedding space, w.r.t. some predefined criterion. We demonstrate the effectiveness of the EAGLE agent using the MovieLens 25M and Amazon Review datasets to surface content gaps that satisfy latent user demand. We also demonstrate the benefit of using an optimal design of a state-dependent action set to improve EAGLE's efficiency. Our work paves the way for controlled and grounded text generation using LLMs, ensuring consistency with domain-specific knowledge and data representations.",
  "abstract_zh": "我们提出了一种新方法，用于训练大型语言模型（LLM）以符合在潜在嵌入空间中定义的目标。我们的方法利用强化学习（RL），将预训练的LLM视为环境。我们的嵌入对齐引导语言（EAGLE）代理被训练以迭代地引导LLM的生成朝向潜在嵌入空间的最优区域，依据某些预定义的标准。我们使用MovieLens 25M和Amazon Review数据集展示了EAGLE代理在满足潜在用户需求的内容缺口方面的有效性。我们还展示了使用状态依赖的动作集的最优设计来提高EAGLE效率的好处。我们的工作为使用LLM进行受控和有根据的文本生成铺平了道路，确保与特定领域知识和数据表示的一致性。"
}
{
  "title": "IPO: Interpretable Prompt Optimization for Vision-Language Models",
  "title_zh": "视觉语言模型的可解释提示优化 (IPO)",
  "abstract": "Pre-trained vision-language models like CLIP have remarkably adapted to various downstream tasks. Nonetheless, their performance heavily depends on the specificity of the input text prompts, which requires skillful prompt template engineering. Instead, current approaches to prompt optimization learn the prompts through gradient descent, where the prompts are treated as adjustable parameters. However, these methods tend to lead to overfitting of the base classes seen during training and produce prompts that are no longer understandable by humans. This paper introduces a simple but interpretable prompt optimizer (IPO), that utilizes large language models (LLMs) to generate textual prompts dynamically. We introduce a Prompt Optimization Prompt that not only guides LLMs in creating effective prompts but also stores past prompts with their performance metrics, providing rich in-context information. Additionally, we incorporate a large multimodal model (LMM) to condition on visual content by generating image descriptions, which enhance the interaction between textual and visual modalities. This allows for the creation of dataset-specific prompts that improve generalization performance, while maintaining human comprehension. Extensive testing across 11 datasets reveals that IPO not only improves the accuracy of existing gradient-descent-based prompt learning methods but also considerably enhances the interpretability of the generated prompts. By leveraging the strengths of LLMs, our approach ensures that the prompts remain human-understandable, thereby facilitating better transparency and oversight for vision-language models.",
  "abstract_zh": "预训练的视觉语言模型如CLIP已经在各种下游任务中表现出色。然而，它们的性能在很大程度上依赖于输入文本提示的具体性，这需要熟练的提示模板设计。目前的提示优化方法通过梯度下降来学习提示，将提示视为可调整的参数。然而，这些方法往往导致训练期间看到的基础类别的过拟合，并产生人类无法理解的提示。本文介绍了一种简单但可解释的提示优化器（IPO），利用大型语言模型（LLMs）动态生成文本提示。我们引入了一个提示优化提示，不仅指导LLMs创建有效的提示，还存储过去的提示及其性能指标，提供丰富的上下文信息。此外，我们结合了一个大型多模态模型（LMM），通过生成图像描述来根据视觉内容进行条件化，从而增强文本和视觉模态之间的互动。这允许创建数据集特定的提示，改善泛化性能，同时保持人类的理解能力。通过对11个数据集的广泛测试表明，IPO不仅提高了现有基于梯度下降的提示学习方法的准确性，还显著增强了生成提示的可解释性。通过利用LLMs的优势，我们的方法确保提示保持人类可理解，从而促进视觉语言模型的更好透明性和监督。"
}
{
  "title": "Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers",
  "title_zh": "标题：当被告知不要时，大型语言模型会梦到大象吗？变压器中的潜在概念关联和联想记忆",
  "abstract": "Large Language Models (LLMs) have the capacity to store and recall facts. Through experimentation with open-source models, we observe that this ability to retrieve facts can be easily manipulated by changing contexts, even without altering their factual meanings. These findings highlight that LLMs might behave like an associative memory model where certain tokens in the contexts serve as clues to retrieving facts. We mathematically explore this property by studying how transformers, the building blocks of LLMs, can complete such memory tasks. We study a simple latent concept association problem with a one-layer transformer and we show theoretically and empirically that the transformer gathers information using self-attention and uses the value matrix for associative memory.",
  "abstract_zh": "摘要：大型语言模型（LLMs）具有存储和回忆事实的能力。通过对开源模型的实验，我们观察到这种检索事实的能力可以通过改变上下文轻松操控，即使不改变其事实意义。这些发现表明，LLMs可能像一个联想记忆模型，其中上下文中的某些标记作为检索事实的线索。我们通过研究变压器（LLMs的构建模块）如何完成此类记忆任务来数学地探讨这一特性。我们研究了一个简单的潜在概念关联问题，使用一个单层变压器，并从理论和实证上展示了变压器如何利用自注意力收集信息，并使用价值矩阵进行联想记忆。"
}
{
  "title": "MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning",
  "title_zh": "Title: MediQ：用于可靠交互式临床推理的提问型大语言模型及基准测试",
  "abstract": "Users typically engage with LLMs interactively, yet most existing benchmarks evaluate them in a static, single-turn format, posing reliability concerns in interactive scenarios. We identify a key obstacle towards reliability: LLMs are trained to answer any question, even with incomplete context or insufficient knowledge. In this paper, we propose to change the static paradigm to an interactive one, develop systems that proactively ask questions to gather more information and respond reliably, and introduce an benchmark—MEDIQ—to evaluate question-asking ability in LLMs. MEDIQ simulates clinical interactions consisting of a Patient System and an adaptive Expert System; with potentially incomplete initial information, the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details via follow-up questions. We provide a pipeline to convert single-turn medical benchmarks into an interactive format. Our results show that directly prompting state-of-the-art LLMs to ask questions degrades performance, indicating that adapting LLMs to proactive information-seeking settings is nontrivial. We experiment with abstention strategies to better estimate model confidence and decide when to ask questions, improving diagnostic accuracy by 22.3%; however, performance still lags compared to an (unrealistic in practice) upper bound with complete information upfront. Further analyses show improved interactive performance with filtering irrelevant contexts and reformatting conversations. Overall, we introduce a novel problem towards LLM reliability, an interactive MEDIQ benchmark and a novel question-asking system, and highlight directions to extend LLMs’ information-seeking abilities in critical domains.",
  "abstract_zh": "Abstract: 用户通常以交互方式与大语言模型（LLMs）进行互动，但大多数现有基准测试在静态的单轮格式中评估它们，这在交互场景中带来了可靠性问题。我们识别出影响可靠性的关键障碍：LLMs 被训练来回答任何问题，即使在上下文不完整或知识不足的情况下。在本文中，我们建议将静态范式转变为交互式范式，开发能够主动提问以收集更多信息并可靠响应的系统，并引入一个名为 MEDIQ 的基准来评估 LLMs 的提问能力。MEDIQ 模拟由患者系统和自适应专家系统组成的临床互动；在初始信息可能不完整的情况下，专家在不自信时避免做出诊断决策，而是通过后续问题获取缺失细节。我们提供了将单轮医疗基准转换为交互格式的流程。我们的结果表明，直接提示最先进的 LLMs 提问会降低性能，这表明将 LLMs 适应于主动信息获取设置并非易事。我们尝试了弃权策略以更好地估计模型信心并决定何时提问，将诊断准确率提高了 22.3%；然而，与（在实践中不切实际的）完整信息预先提供的上限相比，性能仍然落后。进一步分析显示，通过过滤不相关的上下文和重新格式化对话可以改善交互性能。总体而言，我们引入了一个关于 LLM 可靠性的全新问题、一个交互式 MEDIQ 基准和一个新颖的提问系统，并强调了在关键领域扩展 LLMs 信息获取能力的方向。"
}
{
  "title": "WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models",
  "title_zh": "标题: WAGLE: 大型语言模型中有效和模块化遗忘的战略性权重归因",
  "abstract": "The need for effective unlearning mechanisms in large language models (LLMs) is increasingly urgent, driven by the necessity to adhere to data regulations and foster ethical generative AI practices. LLM unlearning is designed to reduce the impact of undesirable data influences and associated model capabilities without diminishing the utility of the model if unrelated to the information being forgotten. Despite growing interest, much of the existing research has focused on varied unlearning method designs to boost effectiveness and efficiency. However, the inherent relationship between model weights and LLM unlearning has not been extensively examined. In this paper, we systematically explore how model weights interact with unlearning processes in LLMs and we design the weight attribution-guided LLM unlearning method, WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. By strategically guiding the LLM unlearning across different types of unlearning methods and tasks, WAGLE can erase the undesired content, while maintaining the performance of the original tasks. We refer to the weight attribution-guided LLM unlearning method as WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. Our extensive experiments show that WAGLE boosts unlearning performance across a range of LLM unlearning methods such as gradient difference and (negative) preference optimization, applications such as fictitious unlearning (TOFU benchmark), malicious use prevention (WMDP benchmark), and copyrighted information removal, and models including Zephyr-7b-beta and Llama2-7b. To the best of our knowledge, our work offers the first principled method for attributing and pinpointing the influential weights in enhancing LLM unlearning. It stands in contrast to previous methods that lack weight attribution and simpler weight attribution techniques.",
  "abstract_zh": "摘要: 在大型语言模型（LLMs）中，开发有效的遗忘机制的需求日益迫切，这源于遵守数据法规和促进伦理生成式人工智能实践的必要性。LLM遗忘旨在减少不良数据影响及相关模型能力的影响，而不削弱与被遗忘信息无关的模型实用性。尽管兴趣日益增加，但现有研究大多集中于设计多样的遗忘方法以提高效果和效率。然而，模型权重与LLM遗忘之间的内在关系尚未得到广泛研究。在本文中，我们系统地探讨了模型权重如何与LLM中的遗忘过程相互作用，并设计了权重归因引导的LLM遗忘方法WAGLE，该方法揭示了权重“影响力”和数据“影响力”在LLM生成中遗忘与保留的相互联系。通过在不同类型的遗忘方法和任务中战略性地引导LLM遗忘，WAGLE能够擦除不需要的内容，同时保持原始任务的性能。我们称这种权重归因引导的LLM遗忘方法为WAGLE，它揭示了权重“影响力”和数据“影响力”在LLM生成中遗忘与保留的相互联系。我们的广泛实验表明，WAGLE在一系列LLM遗忘方法（如梯度差异和（负）偏好优化）、应用（如虚构遗忘（TOFU基准）、恶意使用预防（WMDP基准）和版权信息移除）以及包括Zephyr-7b-beta和Llama2-7b在内的模型中提升了遗忘性能。据我们所知，我们的工作提供了第一个系统的方法，用于归因和定位在增强LLM遗忘中的影响权重。这与缺乏权重归因和简单权重归因技术的先前方法形成对比。"
}
{
  "title": "Stepwise Alignment for Constrained Language Model Policy Optimization",
  "title_zh": "逐步对齐用于约束语言模型策略优化",
  "abstract": "Safety and trustworthiness are indispensable requirements for real-world applications of AI systems using large language models (LLMs). This paper formulates human value alignment as an optimization problem of the language model policy to maximize reward under a safety constraint, and then proposes an algorithm, Stepwise Alignment for Constrained Policy Optimization (SACPO). One key idea behind SACPO, supported by theory, is that the optimal policy incorporating reward and safety can be directly obtained from a reward-aligned policy. Building on this key idea, SACPO aligns LLMs step-wise with each metric while leveraging simple yet powerful alignment algorithms such as direct preference optimization (DPO). SACPO offers several advantages, including simplicity, stability, computational efficiency, and flexibility of algorithms and datasets. Under mild assumptions, our theoretical analysis provides the upper bounds on optimality and safety constraint violation. Our experimental results show that SACPO can fine-tune Alpaca-7B better than the state-of-the-art method in terms of both helpfulness and harmlessness.",
  "abstract_zh": "安全性和可信赖性是使用大型语言模型（LLMs）的人工智能系统在实际应用中不可或缺的要求。本文将人类价值对齐形式化为一种语言模型策略的优化问题，以在安全约束下最大化奖励，然后提出了一种算法，逐步对齐用于约束策略优化（SACPO）。SACPO背后的一个关键理念是理论支持的，即可以直接从奖励对齐的策略中获得结合奖励和安全的最优策略。基于这一关键理念，SACPO逐步对齐LLMs与每个指标，同时利用简单而强大的对齐算法，如直接偏好优化（DPO）。SACPO提供了多种优势，包括算法和数据集的简单性、稳定性、计算效率和灵活性。在温和假设下，我们的理论分析提供了关于最优性和安全约束违反的上限。我们的实验结果表明，SACPO在有用性和无害性方面能够比最先进的方法更好地微调Alpaca-7B。"
}
{
  "title": "SGLang: Efficient Execution of Structured Language Model Programs",
  "title_zh": "Title: SGLang：高效执行结构化语言模型程序",
  "abstract": "Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to $6.4\\times$ higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang.",
  "abstract_zh": "Abstract: 大型语言模型（LLMs）越来越多地用于需要多次生成调用、先进提示技术、控制流和结构化输入/输出的复杂任务。然而，缺乏用于编程和执行这些应用程序的高效系统。我们介绍了SGLang，一个用于高效执行复杂语言模型程序的系统。SGLang由前端语言和运行时组成。前端通过生成和并行控制的原语简化编程。运行时通过新颖的优化加速执行，如用于KV缓存重用的RadixAttention和用于更快结构化输出解码的压缩有限状态机。实验表明，SGLang在各种大型语言和多模态模型上的任务中，包括代理控制、逻辑推理、少样本学习基准、JSON解码、检索增强生成管道和多轮对话，与最先进的推理系统相比，实现了高达6.4倍的吞吐量提升。代码可在https://github.com/sgl-project/sglang公开获取。"
}
{
  "title": "LLM-AutoDA: Large Language Model-Driven Automatic Data Augmentation for Long-tailed Problems",
  "title_zh": "标题：LLM-AutoDA：基于大型语言模型驱动的长尾问题自动数据增强",
  "abstract": "The long-tailed distribution is the underlying nature of real-world data, and it presents unprecedented challenges for training deep learning models. Existing long-tailed learning paradigms based on re-balancing or data augmentation have partially alleviated the long-tailed problem. However, they still have limitations, such as relying on manually designed augmentation strategies, having a limited search space, and using fixed augmentation strategies. To address these limitations, this paper proposes a novel LLM-based long-tailed data augmentation framework called LLM-AutoDA, which leverages large-scale pretrained models to automatically search for the optimal augmentation strategies suitable for long-tailed data distributions. In addition, it applies this strategy to the original imbalanced data to create an augmented dataset and fine-tune the underlying long-tailed learning model. The performance improvement on the validation set serves as a reward signal to update the generation model, enabling the generation of more effective augmentation strategies in the next iteration. We conducted extensive experiments on multiple mainstream long-tailed learning benchmarks. The results show that LLM-AutoDA outperforms state-of-the-art data augmentation methods and other re-balancing methods significantly.",
  "abstract_zh": "摘要：长尾分布是现实世界数据的基本特性，它给训练深度学习模型带来了前所未有的挑战。现有的基于重平衡或数据增强的长尾学习范式在一定程度上缓解了长尾问题。然而，它们仍然存在一些局限性，例如依赖于人工设计的增强策略、搜索空间有限以及使用固定的增强策略。为了解决这些局限性，本文提出了一种新颖的基于LLM的长尾数据增强框架，称为LLM-AutoDA，该框架利用大规模预训练模型自动搜索适合长尾数据分布的最佳增强策略。此外，它将这种策略应用于原始不平衡数据以创建增强数据集，并微调基础长尾学习模型。在验证集上的性能提升作为奖励信号来更新生成模型，从而在下一次迭代中生成更有效的增强策略。我们在多个主流长尾学习基准上进行了广泛的实验。结果表明，LLM-AutoDA显著优于最先进的数据增强方法和其他重平衡方法。"
}
{
  "title": "SafeWorld: Geo-Diverse Safety Alignment",
  "title_zh": "Title: SafeWorld：地理多样性安全对齐",
  "abstract": "In the rapidly evolving field of Large Language Models (LLMs), ensuring safety is a crucial and widely discussed topic. However, existing works often overlooks the geo-diversity of cultural and legal standards across the world. To reveal the chal5 lenges posed by geo-diverse safety standards, we introduce SafeWorld, a novel benchmark specifically designed to evaluate LLMs’ ability to generate responses that are not only helpful but also culturally sensitive and legally compliant across diverse global contexts. SafeWorld encompasses 2,775 test user queries, each grounded in high-quality, human-verified cultural norms and legal policies from 50 countries and 493 regions/races. On top of it, we propose a multi-dimensional automatic safety evaluation framework that assesses the contextual appropriateness, accuracy, and comprehensiveness of responses. Our evaluations reveal that current LLMs struggle to meet these criteria effectively. To enhance LLMs’ alignment with geo-diverse safety standards, we synthesize helpful preference pairs for Direct Preference Optimization (DPO) alignment. The preference pair construction aims to encourage LLMs to behave appropriately and provide precise references to relevant cultural norms and policies when necessary. Our trained SafeWorldLM outperforms all competing models, including GPT-4o on all the three evaluation dimensions by a large margin. Global human evaluators also note a nearly 20% higher winning rate in helpfulness and harmfulness evaluation.",
  "abstract_zh": "Abstract: 在快速发展的大型语言模型（LLMs）领域，确保安全性是一个关键且广泛讨论的话题。然而，现有的研究往往忽视了全球范围内文化和法律标准的地理多样性。为了揭示地理多样性安全标准带来的挑战，我们引入了SafeWorld，这是一种新颖的基准，专门用于评估LLMs在不同全球背景下生成既有帮助又具文化敏感性和法律合规性的响应的能力。SafeWorld包括2,775个测试用户查询，每个查询都基于来自50个国家和493个地区/种族的高质量、人工验证的文化规范和法律政策。在此基础上，我们提出了一个多维自动安全评估框架，用于评估响应的情境适当性、准确性和全面性。我们的评估显示，当前的LLMs难以有效满足这些标准。为了增强LLMs与地理多样性安全标准的对齐，我们合成了有助于直接偏好优化（DPO）对齐的偏好对。偏好对的构建旨在鼓励LLMs在必要时适当地行为并提供精确的文化规范和政策参考。我们训练的SafeWorldLM在所有三个评估维度上大幅超越了包括GPT-4o在内的所有竞争模型。全球人类评估者还指出，在有用性和有害性评估中，SafeWorldLM的胜率高出近20%。"
}
{
  "title": "WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models",
  "title_zh": "标题：WISE：重新思考大型语言模型的终身模型编辑中的知识记忆",
  "abstract": "Large language models (LLMs) need knowledge updates to meet the ever-growing world facts and correct the hallucinated responses, facilitating the methods of lifelong model editing. Where the updated knowledge resides in memories is a fundamental question for model editing. In this paper, we find that editing either long-term memory (direct model parameters) or working memory (non-parametric knowledge of neural network activations/representations by retrieval) will result in an impossible triangle---reliability, generalization, and locality can not be realized together in the lifelong editing settings. For long-term memory, directly editing the parameters will cause conflicts with irrelevant pretrained knowledge or previous edits (poor reliability and locality). For working memory, retrieval-based activations can hardly make the model understand the edits and generalize (poor generalization). Therefore, we propose WISE to bridge the gap between memories. In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge. We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query. For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters, and are subsequently merged into a shared memory without conflicts. Extensive experiments show that WISE can outperform previous model editing methods and overcome the impossible triangle under lifelong model editing of question answering, hallucination, and out-of-distribution settings across trending LLM architectures, e.g., GPT, LLaMA, and Mistral.",
  "abstract_zh": "摘要：大型语言模型（LLMs）需要进行知识更新以满足不断增长的世界事实并纠正幻觉响应，从而促进终身模型编辑的方法。更新的知识存储在记忆中的位置是模型编辑的一个基本问题。在本文中，我们发现编辑长期记忆（直接模型参数）或工作记忆（通过检索的神经网络激活/表示的非参数化知识）都会导致一个不可能的三角形——在终身编辑设置中，可靠性、泛化性和局部性无法同时实现。对于长期记忆，直接编辑参数会导致与无关的预训练知识或先前编辑发生冲突（可靠性和局部性差）。对于工作记忆，基于检索的激活很难让模型理解编辑并进行泛化（泛化性差）。因此，我们提出了WISE以弥合记忆之间的差距。在WISE中，我们设计了一种双参数记忆方案，包括用于预训练知识的主记忆和用于编辑知识的侧记忆。我们仅编辑侧记忆中的知识，并训练一个路由器来决定在给定查询时通过哪个记忆。对于持续编辑，我们设计了一种知识分片机制，其中不同的编辑集驻留在参数的不同子空间中，并随后合并到共享记忆中而不发生冲突。大量实验表明，WISE可以在问题回答、幻觉和分布外设置的终身模型编辑中优于以前的模型编辑方法，并克服不可能的三角形，适用于流行的LLM架构，例如GPT、LLaMA和Mistral。"
}
{
  "title": "WizardArena: Post-training Large Language Models via Simulated Offline Chatbot Arena",
  "title_zh": "Title: WizardArena：通过模拟离线聊天机器人竞技场进行大语言模型后训练",
  "abstract": "Recent work demonstrates that, post-training large language models with open-domain instruction following data have achieved colossal success. Simultaneously, human Chatbot Arena has emerged as one of the most reasonable benchmarks for model evaluation and developmental guidance. However, the processes of manually curating high-quality training data and utilizing online human evaluation platforms are both expensive and limited. To mitigate the manual and temporal costs associated with post-training, this paper introduces a Simulated Chatbot Arena named WizardArena, which is fully based on and powered by open-source LLMs. For evaluation scenario, WizardArena can efficiently predict accurate performance rankings among different models based on offline test set. For training scenario, we simulate arena battles among various state-of-the-art models on a large scale of instruction data, subsequently leveraging the battle results to constantly enhance target model in both the supervised fine-tuning and reinforcement learning . Experimental results demonstrate that our WizardArena aligns closely with the online human arena rankings, and our models trained on offline extensive battle data exhibit significant performance improvements during SFT, DPO, and PPO stages.",
  "abstract_zh": "Abstract: 最近的研究表明，使用开放域指令数据进行后训练的大语言模型取得了巨大的成功。同时，人类聊天机器人竞技场已成为模型评估和开发指导的最合理基准之一。然而，手动整理高质量训练数据和利用在线人工评估平台的过程既昂贵又有限。为减轻与后训练相关的人工和时间成本，本文介绍了一种名为WizardArena的模拟聊天机器人竞技场，它完全基于开源LLM并由其驱动。在评估场景中，WizardArena可以基于离线测试集高效预测不同模型之间的准确性能排名。在训练场景中，我们在大量指令数据上模拟各种最先进模型之间的竞技对战，随后利用对战结果在监督微调和强化学习中不断增强目标模型。实验结果表明，我们的WizardArena与在线人工竞技场排名高度一致，并且在离线广泛对战数据上训练的模型在SFT、DPO和PPO阶段表现出显著的性能提升。"
}
{
  "title": "Uncovering Safety Risks of Large Language Models through Concept Activation Vector",
  "title_zh": "标题：通过概念激活向量揭示大型语言模型的安全风险",
  "abstract": "Despite careful safety alignment, current large language models (LLMs) remain vulnerable to various attacks. To further unveil the safety risks of LLMs, we introduce a Safety Concept Activation Vector (SCAV) framework, which effectively guides the attacks by accurately interpreting LLMs' safety mechanisms. We then develop an SCAV-guided attack method that can generate both attack prompts and embedding-level attacks with automatically selected perturbation hyperparameters. Both automatic and human evaluations demonstrate that our attack method significantly improves the attack success rate and response quality while requiring less training data. Additionally, we find that our generated attack prompts may be transferable to GPT-4, and the embedding-level attacks may also be transferred to other white-box LLMs whose parameters are known. Our experiments further uncover the safety risks present in current LLMs. For example, in our evaluation of seven open-source LLMs, we observe an average attack success rate of 99.14%, based on the classic keyword-matching criterion. Finally, we provide insights into the safety mechanism of LLMs. The code is available at https://github.com/SproutNan/AI-Safety_SCAV.",
  "abstract_zh": "摘要：尽管进行了谨慎的安全对齐，当前的大型语言模型（LLMs）仍然容易受到各种攻击。为了进一步揭示LLMs的安全风险，我们引入了一个安全概念激活向量（SCAV）框架，该框架通过准确解释LLMs的安全机制，有效地指导攻击。随后，我们开发了一种SCAV指导的攻击方法，可以生成攻击提示和嵌入级攻击，并自动选择扰动超参数。自动和人工评估均表明，我们的攻击方法在提高攻击成功率和响应质量方面显著优于其他方法，同时需要更少的训练数据。此外，我们发现生成的攻击提示可能可以转移到GPT-4上，嵌入级攻击也可能转移到其他已知参数的白盒LLMs上。我们的实验进一步揭示了当前LLMs中存在的安全风险。例如，在对七个开源LLMs的评估中，我们观察到基于经典关键词匹配标准的平均攻击成功率为99.14%。最后，我们提供了对LLMs安全机制的见解。代码可在https://github.com/SproutNan/AI-Safety_SCAV获得。"
}
{
  "title": "Self-Supervised Alignment with Mutual Information: Learning to Follow Principles without Preference Labels",
  "title_zh": "互信息自监督对齐：学习遵循原则而无需偏好标签",
  "abstract": "When prompting a language model (LM), users often expect the model to adhere to a set of behavioral principles across diverse tasks, such as producing insightful content while avoiding harmful or biased language. Instilling such principles (i.e., a constitution) into a model is resource-intensive, technically challenging, and generally requires human preference labels or examples. We introduce SAMI, an iterative algorithm that finetunes a pretrained language model (without requiring preference labels or demonstrations) to increase the conditional mutual information between constitutions and self-generated responses given queries from a dataset. On single-turn dialogue and summarization, a SAMI-trained mistral-7b outperforms the initial pretrained model, with win rates between 66% and 77%. Strikingly, it also surpasses an instruction-finetuned baseline (mistral-7b-instruct) with win rates between 55% and 57% on single-turn dialogue. SAMI requires a model that writes the principles. To avoid dependence on strong models for writing principles, we align a strong pretrained model (mixtral-8x7b) using constitutions written by a weak instruction-finetuned model (mistral-7b-instruct), achieving a 65% win rate on summarization. Finally, we investigate whether SAMI generalizes to diverse summarization principles (e.g., \"summaries should be scientific\") and scales to stronger models (llama3-70b), finding that it achieves win rates of up to 68% for learned and 67% for held-out principles compared to the base model. Our results show that a pretrained LM can learn to follow constitutions without using preference labels, demonstrations, or human oversight.",
  "abstract_zh": "在提示语言模型（LM）时，用户通常期望模型在各种任务中遵循一组行为原则，例如在避免有害或偏见语言的同时生成有见地的内容。将这些原则（即宪法）植入模型是资源密集型的，技术上具有挑战性，并且通常需要人类偏好标签或示例。我们介绍了SAMI，这是一种迭代算法，通过增加给定数据集查询的宪法与自生成响应之间的条件互信息来微调预训练语言模型（无需偏好标签或示例）。在单轮对话和摘要任务中，经过SAMI训练的mistral-7b优于初始预训练模型，胜率在66%到77%之间。值得注意的是，它在单轮对话中也超越了一个指令微调的基线（mistral-7b-instruct），胜率在55%到57%之间。SAMI需要一个编写原则的模型。为了避免依赖强大的模型来编写原则，我们使用由弱指令微调模型（mistral-7b-instruct）编写的宪法对强大的预训练模型（mixtral-8x7b）进行对齐，在摘要任务中实现了65%的胜率。最后，我们研究了SAMI是否可以推广到多样化的摘要原则（例如，“摘要应该是科学的”）并扩展到更强的模型（llama3-70b），发现它在学习到的原则和未见过的原则上分别实现了高达68%和67%的胜率，相较于基础模型。我们的结果表明，预训练的LM可以在不使用偏好标签、示例或人类监督的情况下学习遵循宪法。"
}
{
  "title": "GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing",
  "title_zh": "Title: GenArtist：作为统一图像生成和编辑代理的多模态大型语言模型",
  "abstract": "Despite the success achieved by existing image generation and editing methods, current models still struggle with complex problems including intricate text prompts, and the absence of  verification and self-correction mechanisms makes the generated images unreliable. Meanwhile, a single model tends to specialize in particular tasks and possess the corresponding capabilities, making it inadequate for fulfilling all user requirements. We propose GenArtist, a unified image generation and editing system, coordinated by a multimodal large language model (MLLM) agent. We integrate a comprehensive range of existing models into the tool library and utilize the agent for tool selection and execution. For a complex problem, the MLLM agent decomposes it into simpler sub-problems and constructs a tree structure to systematically plan the procedure of generation, editing, and self-correction with step-by-step verification. By automatically generating missing position-related inputs and incorporating position information, the appropriate tool can be effectively employed to address each sub-problem. Experiments demonstrate that GenArtist can perform various generation and editing tasks, achieving state-of-the-art performance and surpassing existing models such as SDXL and DALL-E 3, as can be seen in Fig. 1. We will open-source the code for future research and applications.",
  "abstract_zh": "Abstract: 尽管现有的图像生成和编辑方法取得了一定的成功，但当前模型在处理复杂问题时仍然面临挑战，包括复杂的文本提示，以及缺乏验证和自我纠正机制使得生成的图像不够可靠。同时，单一模型往往专注于特定任务并具备相应能力，难以满足所有用户需求。我们提出GenArtist，一个由多模态大型语言模型（MLLM）代理协调的统一图像生成和编辑系统。我们将现有模型的全面范围整合到工具库中，并利用代理进行工具选择和执行。对于复杂问题，MLLM代理将其分解为更简单的子问题，并构建树状结构以系统地规划生成、编辑和自我纠正的步骤，并逐步验证。通过自动生成缺失的位置信息输入并结合位置信息，可以有效地使用适当的工具来解决每个子问题。实验表明，GenArtist能够执行各种生成和编辑任务，达到最先进的性能，并超越现有模型如SDXL和DALL-E 3，如图1所示。我们将开源代码以供未来研究和应用。"
}
{
  "title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents",
  "title_zh": "标题：AGILE：一种新颖的LLM代理强化学习框架",
  "abstract": "We introduce a novel reinforcement learning framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments)  designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation. We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance. Datasets and code are available at https://github.com/bytarnish/AGILE.",
  "abstract_zh": "摘要：我们介绍了一种名为AGILE（AGent that Interacts and Learns from Environments）的新颖LLM代理强化学习框架，旨在利用LLM、记忆、工具和与专家的互动来执行复杂的用户对话任务。该代理具备超越对话的能力，包括反思、工具使用和专家咨询。我们将这种LLM代理的构建形式化为一个强化学习（RL）问题，其中LLM作为策略模型。我们使用带标记的动作数据和PPO算法对LLM进行微调。我们专注于问答任务，并发布了一个名为ProductQA的代理数据集，其中包含在线购物中的挑战性问题。我们在ProductQA、MedMCQA和HotPotQA上的广泛实验表明，使用PPO训练的基于7B和13B LLM的AGILE代理可以超越GPT-4代理。我们的消融研究强调了记忆、工具、咨询、反思和强化学习在实现代理强大性能中的不可或缺性。数据集和代码可在https://github.com/bytarnish/AGILE获取。"
}
{
  "title": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm",
  "title_zh": "标题: ProTransformer: 通过即插即用范式增强Transformer的鲁棒性",
  "abstract": "Transformer-based architectures have dominated various areas of machine learning in recent years. In this paper, we introduce a novel robust attention mechanism designed to enhance the resilience of transformer-based architectures. Crucially, this technique can be integrated into existing transformers as a plug-and-play layer, improving their robustness without the need for additional training or fine-tuning. Through comprehensive experiments and ablation studies, we demonstrate that our ProTransformer significantly enhances the robustness of transformer models across a variety of prediction tasks, attack mechanisms, backbone architectures, and data domains. Notably, without further fine-tuning, the ProTransformer consistently improves the performance of vanilla transformers by 19.5\\%, 28.3\\%, 16.1\\%, and 11.4\\% for BERT, ALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler attack. Furthermore, ProTransformer shows promising resilience in large language models (LLMs) against prompting-based attacks, improving the performance of T5 and LLaMA by 24.8\\% and 17.8\\%, respectively, and enhancing Vicuna by an average of 10.4\\% against the Jailbreaking attack. Beyond the language domain, ProTransformer also demonstrates outstanding robustness in both vision and graph domains.",
  "abstract_zh": "摘要: 基于Transformer的架构近年来在机器学习的各个领域中占据了主导地位。在本文中，我们引入了一种新颖的鲁棒注意力机制，旨在增强基于Transformer的架构的弹性。关键是，这种技术可以作为即插即用层集成到现有的Transformer中，提高其鲁棒性，而无需额外的训练或微调。通过全面的实验和消融研究，我们证明了我们的ProTransformer显著增强了Transformer模型在各种预测任务、攻击机制、主干架构和数据领域的鲁棒性。值得注意的是，在经典的TextFooler攻击下，ProTransformer在不进行进一步微调的情况下，分别将BERT、ALBERT、DistilBERT和RoBERTa的性能提高了19.5%、28.3%、16.1%和11.4%。此外，ProTransformer在大型语言模型（LLMs）中对基于提示的攻击表现出有希望的弹性，分别将T5和LLaMA的性能提高了24.8%和17.8%，并在Jailbreaking攻击中平均提高了Vicuna的性能10.4%。除了语言领域外，ProTransformer在视觉和图形领域也表现出卓越的鲁棒性。"
}
{
  "title": "D-LLM: A Token Adaptive Computing Resource Allocation Strategy for Large Language Models",
  "title_zh": "标题：D-LLM：一种用于大型语言模型的自适应计算资源分配策略",
  "abstract": "Large language models have shown an impressive societal impact owing to their excellent understanding and logical reasoning skills. However, such strong ability relies on a huge amount of computing resources, which makes it difficult to deploy LLMs on computing resource-constrained platforms. Currently, LLMs process each token equivalently, but we argue that not every word is equally important. Some words should not be allocated excessive computing resources, particularly for dispensable terms in simple questions. In this paper, we propose a novel dynamic inference paradigm for LLMs, namely D-LLMs, which adaptively allocate computing resources in token processing. We design a dynamic decision module for each transformer layer that decides whether a network unit should be executed or skipped. Moreover, we tackle the issue of adapting D-LLMs to real-world applications, specifically concerning the missing KV-cache when layers are skipped. To overcome this, we propose a simple yet effective eviction policy to exclude the skipped layers from subsequent attention calculations. The eviction policy not only enables D-LLMs to be compatible with prevalent applications but also reduces considerable storage resources. Experimentally, D-LLMs show superior performance, in terms of computational cost and KV storage utilization. It can reduce up to 45\\% computational cost and KV storage on Q\\&A, summarization, and math solving tasks, 50\\% on commonsense reasoning tasks.",
  "abstract_zh": "摘要：大型语言模型因其出色的理解和逻辑推理能力而在社会上产生了显著影响。然而，这种强大的能力依赖于大量的计算资源，这使得在计算资源受限的平台上部署大型语言模型变得困难。目前，大型语言模型对每个标记的处理是等同的，但我们认为并非每个词都同等重要。一些词不应分配过多的计算资源，尤其是在简单问题中无关紧要的词。在本文中，我们提出了一种用于大型语言模型的新型动态推理范式，即D-LLM，它在标记处理过程中自适应地分配计算资源。我们为每个Transformer层设计了一个动态决策模块，用于决定网络单元是否应被执行或跳过。此外，我们解决了D-LLM在实际应用中适应的问题，特别是当层被跳过时缺失的KV缓存。为此，我们提出了一种简单而有效的驱逐策略，以排除后续注意力计算中被跳过的层。该驱逐策略不仅使D-LLM能够与流行应用兼容，还显著减少了存储资源。实验结果表明，D-LLM在计算成本和KV存储利用率方面表现优越。在问答、摘要和数学解题任务中，它可以减少高达45%的计算成本和KV存储，在常识推理任务中减少50%。"
}
{
  "title": "LLM-based Skill Diffusion for Zero-shot Policy Adaptation",
  "title_zh": "基于LLM的技能扩散用于零样本策略适应",
  "abstract": "Recent advances in data-driven imitation learning and offline reinforcement learning have highlighted the use of expert data for skill acquisition and the development of hierarchical policies based on these skills. However, these approaches have not significantly advanced in adapting these skills to unseen contexts, which may involve changing environmental conditions or different user requirements. In this paper, we present a novel LLM-based policy adaptation framework LDuS which leverages an LLM to guide the generation process of a skill diffusion model upon contexts specified in language, facilitating zero-shot skill-based policy adaptation to different contexts. To implement the skill diffusion model, we adapt the loss-guided diffusion with a sequential in-painting technique, where target trajectories are conditioned by masking them with past state-action sequences, thereby enabling the robust and controlled generation of skill trajectories in test-time. To have a loss function for a given context, we employ the LLM-based code generation with iterative refinement, by which the code and controlled trajectory are validated to align with the context in a closed-loop manner. Through experiments, we demonstrate the zero-shot adaptability of LDuS to various context types including different specification levels, multi-modality, and varied temporal conditions for several robotic manipulation tasks, outperforming other language-conditioned imitation and planning methods.",
  "abstract_zh": "最近在数据驱动的模仿学习和离线强化学习方面的进展强调了使用专家数据进行技能获取和基于这些技能开发分层策略。然而，这些方法在将这些技能适应于未见过的环境方面并没有显著进展，这可能涉及环境条件的变化或不同的用户需求。在本文中，我们提出了一种新颖的基于LLM的策略适应框架LDuS，该框架利用LLM指导技能扩散模型的生成过程，基于语言指定的上下文，促进零样本技能策略适应不同的上下文。为了实现技能扩散模型，我们采用了带有顺序填充技术的损失引导扩散，其中目标轨迹通过用过去的状态-动作序列掩盖来进行条件化，从而在测试时实现技能轨迹的稳健和可控生成。为了获得给定上下文的损失函数，我们采用基于LLM的代码生成与迭代优化，通过这种方式，代码和受控轨迹在闭环方式中验证以与上下文对齐。通过实验，我们展示了LDuS对各种上下文类型的零样本适应性，包括不同的规格水平、多模态和多样的时间条件，适用于多个机器人操作任务，优于其他语言条件的模仿和规划方法。"
}
{
  "title": "Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models",
  "title_zh": "标题：提示压缩的基本限制：黑箱语言模型的率失真框架",
  "abstract": "We formalize the problem of prompt compression for large language models (LLMs) and present a framework to unify token-level prompt compression methods which create hard prompts for black-box models. We derive the distortion-rate function for this setup as a linear program, and provide an efficient algorithm to compute this fundamental limit via the dual of the linear program. Using the distortion-rate function as the baseline, we study the performance of existing compression schemes on a synthetic dataset consisting of prompts generated from a Markov chain, natural language queries, and their respective answers. Our empirical analysis demonstrates the criticality of query-aware prompt compression, where the compressor has knowledge of the downstream task/query for the black-box LLM. We show that there is a large gap between the performance of current prompt compression methods and the optimal strategy, and propose Adaptive QuerySelect, a query-aware, variable-rate adaptation of a prior work to close the gap. We extend our experiments to a small natural language dataset to further confirm our findings on our synthetic dataset.",
  "abstract_zh": "摘要：我们形式化了大语言模型（LLMs）的提示压缩问题，并提出了一个框架来统一创建黑箱模型硬提示的令牌级提示压缩方法。我们将该设置的失真率函数推导为一个线性规划，并通过线性规划的对偶提供了一种高效算法来计算这一基本限制。使用失真率函数作为基准，我们研究了现有压缩方案在由马尔可夫链生成的提示、自然语言查询及其相应答案组成的合成数据集上的性能。我们的实证分析表明，查询感知提示压缩的关键性，其中压缩器了解黑箱LLM的下游任务/查询。我们展示了当前提示压缩方法的性能与最优策略之间存在较大差距，并提出了自适应查询选择（Adaptive QuerySelect），这是一种查询感知的可变率适应方法，以缩小这一差距。我们将实验扩展到一个小型自然语言数据集，以进一步确认我们在合成数据集上的发现。"
}
{
  "title": "FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations",
  "title_zh": "标题: FLoRA: 使用异构低秩适应进行联邦微调大型语言模型",
  "abstract": "The rapid development of Large Language Models (LLMs) has been pivotal in advancing AI, with pre-trained LLMs being adaptable to diverse downstream tasks through fine-tuning. Federated learning (FL) further enhances fine-tuning in a privacy-aware manner by utilizing clients' local data through in-situ computation, eliminating the need for data movement. However, fine-tuning LLMs, given their massive scale of parameters, poses challenges for clients with constrained and heterogeneous resources in FL. Previous methods employed low-rank adaptation (LoRA) for efficient federated fine-tuning but utilized traditional FL aggregation strategies on LoRA adapters. This approach led to mathematically inaccurate aggregation noise, reducing fine-tuning effectiveness and failing to address heterogeneous LoRAs. In this work, we first highlight the mathematical incorrectness of LoRA aggregation in existing federated fine-tuning methods. We introduce a new approach called FLoRA that enables federated fine-tuning on heterogeneous LoRA adapters across clients through a novel stacking-based aggregation method. Our approach is noise-free and seamlessly supports heterogeneous LoRAs. Extensive experiments demonstrate FLoRA's superior performance in both homogeneous and heterogeneous settings, surpassing state-of-the-art methods. We envision this work as a milestone for efficient, privacy-preserving, and accurate federated fine-tuning of LLMs.",
  "abstract_zh": "摘要: 大型语言模型（LLMs）的快速发展在推进人工智能方面起到了关键作用，预训练的LLMs可以通过微调适应多种下游任务。联邦学习（FL）通过利用客户端的本地数据进行现场计算，以隐私感知的方式进一步增强微调，消除了数据移动的需要。然而，由于LLMs的参数规模庞大，在资源受限和异构的FL客户端中进行微调面临挑战。先前的方法使用低秩适应（LoRA）进行高效的联邦微调，但在LoRA适配器上采用了传统的FL聚合策略。这种方法导致了数学上不准确的聚合噪声，降低了微调效果，并未解决异构LoRA的问题。在这项工作中，我们首先强调了现有联邦微调方法中LoRA聚合的数学错误。我们引入了一种称为FLoRA的新方法，通过一种新颖的基于堆叠的聚合方法，使得在客户端间的异构LoRA适配器上进行联邦微调成为可能。我们的方法无噪声，并无缝支持异构LoRA。大量实验表明，FLoRA在同质和异质环境中都表现出色，超越了最先进的方法。我们设想这项工作将成为LLMs高效、隐私保护和准确联邦微调的一个里程碑。"
}
{
  "title": "How Do Large Language Models Acquire Factual Knowledge During Pretraining?",
  "title_zh": "标题：大型语言模型在预训练过程中如何获取事实知识？",
  "abstract": "Despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining. This work addresses this gap by studying how LLMs acquire factual knowledge during pretraining. The findings reveal several important insights into the dynamics of factual knowledge acquisition during pretraining. First, counterintuitively, we observe that pretraining on more data shows no significant improvement in the model's capability to acquire and maintain factual knowledge. Next, LLMs undergo forgetting of memorization and generalization of factual knowledge, and LLMs trained with duplicated training data exhibit faster forgetting. Third, training LLMs with larger batch sizes can enhance the models' robustness to forgetting. Overall, our observations suggest that factual knowledge acquisition in LLM pretraining occurs by progressively increasing the probability of factual knowledge presented in the pretraining data at each step. However, this increase is diluted by subsequent forgetting. Based on this interpretation, we demonstrate that we can provide plausible explanations on recently observed behaviors of LLMs, such as the poor performance of LLMs on long-tail knowledge and the benefits of deduplicating the pretraining corpus.",
  "abstract_zh": "摘要：尽管最近观察到大型语言模型（LLMs）可以存储大量事实知识，但对于它们通过预训练获取事实知识的机制仍然了解有限。本研究通过研究LLMs在预训练过程中如何获取事实知识来填补这一空白。研究结果揭示了关于事实知识获取动态的几个重要见解。首先，反直觉的是，我们观察到在更多数据上进行预训练并未显著提高模型获取和维护事实知识的能力。其次，LLMs会经历记忆的遗忘和事实知识的泛化，并且使用重复训练数据训练的LLMs表现出更快的遗忘速度。第三，使用更大批量大小训练LLMs可以增强模型对遗忘的鲁棒性。总体而言，我们的观察表明，LLM预训练中的事实知识获取是通过逐步增加预训练数据中呈现的事实知识的概率来实现的。然而，这种增加会被随后的遗忘所稀释。基于这一解释，我们展示了可以对LLMs最近观察到的行为提供合理的解释，例如LLMs在长尾知识上的表现不佳以及去重预训练语料库的好处。"
}
{
  "title": "Deep Bayesian Active Learning for Preference Modeling in Large Language Models",
  "title_zh": "标题：用于大型语言模型偏好建模的深度贝叶斯主动学习",
  "abstract": "Leveraging human preferences for steering the behavior of Large Language Models (LLMs) has demonstrated notable success in recent years. Nonetheless, data selection and labeling are still a bottleneck for these systems, particularly at large scale. Hence, selecting the most informative points for acquiring human feedback may considerably reduce the cost of preference labeling and unleash the further development of LLMs. Bayesian Active Learning provides a principled framework for addressing this challenge and has demonstrated remarkable success in diverse settings. However, previous attempts to employ it for Preference Modeling did not meet such expectations. In this work, we identify that naive epistemic uncertainty estimation leads to the acquisition of redundant samples. We address this by proposing the Bayesian Active Learner for Preference Modeling (BAL-PM), a novel stochastic acquisition policy that not only targets points of high epistemic uncertainty according to the preference model but also seeks to maximize the entropy of the acquired prompt distribution in the feature space spanned by the employed LLM. Notably, our experiments demonstrate that BAL-PM requires 33\\% to 68\\% fewer preference labels in two popular human preference datasets and exceeds previous stochastic Bayesian acquisition policies.",
  "abstract_zh": "摘要：近年来，利用人类偏好来引导大型语言模型（LLM）的行为取得了显著成功。然而，数据选择和标注仍然是这些系统的瓶颈，尤其是在大规模情况下。因此，选择最具信息量的点以获取人类反馈可能会显著降低偏好标注的成本，并释放LLM的进一步发展潜力。贝叶斯主动学习为解决这一挑战提供了一个有原则的框架，并在各种环境中展示了显著的成功。然而，以往尝试将其用于偏好建模并未达到预期。在这项工作中，我们发现天真的认知不确定性估计导致了冗余样本的获取。我们通过提出用于偏好建模的贝叶斯主动学习者（BAL-PM）来解决这一问题，这是一种新颖的随机获取策略，不仅针对偏好模型中高认知不确定性的点，还寻求最大化由所使用的LLM所跨越的特征空间中获取的提示分布的熵。值得注意的是，我们的实验表明，BAL-PM在两个流行的人类偏好数据集中需要的偏好标签减少了33\\%到68\\%，并且超越了先前的随机贝叶斯获取策略。"
}
{
  "title": "Generated and Pseudo Content guided Prototype Refinement for Few-shot Point Cloud Segmentation",
  "title_zh": "生成和伪内容引导的原型优化用于小样本点云分割",
  "abstract": "Few-shot 3D point cloud semantic segmentation aims to segment query point clouds with only a few annotated support point clouds. Existing prototype-based methods learn prototypes from the 3D support set to guide the segmentation of query point clouds. However, they encounter the challenge of low prototype quality due to constrained semantic information in the 3D support set and class information bias between support and query sets. To address these issues, in this paper, we propose a novel framework called Generated and Pseudo Content guided Prototype Refinement (GPCPR), which explicitly leverages LLM-generated content and reliable query context to enhance prototype quality. GPCPR achieves prototype refinement through two core components: LLM-driven Generated Content-guided Prototype Refinement (GCPR) and Pseudo Query Context-guided Prototype Refinement (PCPR). Specifically, GCPR integrates diverse and differentiated class descriptions generated by large language models to enrich prototypes with comprehensive semantic knowledge. PCPR further aggregates reliable class-specific pseudo-query context to mitigate class information bias and generate more suitable query-specific prototypes. Furthermore, we introduce a dual-distillation regularization term, enabling knowledge transfer between early-stage entities (prototypes or pseudo predictions) and their deeper counterparts to enhance refinement. Extensive experiments demonstrate the superiority of our method, surpassing the state-of-the-art methods by up to 12.10% and 13.75% mIoU on S3DIS and ScanNet, respectively.",
  "abstract_zh": "小样本3D点云语义分割旨在仅使用少量标注的支持点云对查询点云进行分割。现有的基于原型的方法从3D支持集中学习原型以指导查询点云的分割。然而，由于3D支持集中的语义信息有限以及支持和查询集之间的类别信息偏差，它们面临着原型质量低的问题。为了解决这些问题，本文提出了一种新颖的框架，称为生成和伪内容引导的原型优化（GPCPR），该框架明确利用LLM生成的内容和可靠的查询上下文来提高原型质量。GPCPR通过两个核心组件实现原型优化：LLM驱动的生成内容引导原型优化（GCPR）和伪查询上下文引导原型优化（PCPR）。具体而言，GCPR整合了由大型语言模型生成的多样化和差异化的类别描述，以丰富原型的综合语义知识。PCPR进一步聚合可靠的类别特定伪查询上下文，以减轻类别信息偏差并生成更适合查询的特定原型。此外，我们引入了双重蒸馏正则化项，实现早期实体（原型或伪预测）与其更深层对应物之间的知识转移以增强优化。大量实验表明，我们的方法优于最先进的方法，在S3DIS和ScanNet上分别提高了最多12.10%和13.75%的mIoU。"
}
{
  "title": "Learning Goal-Conditioned Representations for Language Reward Models",
  "title_zh": "目标条件表示学习用于语言奖励模型",
  "abstract": "Techniques that learn improved representations via offline data or self-supervised objectives have shown impressive results in traditional reinforcement learning.\nNevertheless, it is unclear how improved representation learning can benefit reinforcement learning from human feedback on language models.\nIn this work, we propose training reward models (RMs) in a contrastive, $\\textit{goal-conditioned}$ fashion by increasing the representation similarity of future states along sampled preferred trajectories and decreasing the similarity along randomly sampled dispreferred trajectories.\nThis objective significantly improves reward model performance by up to 0.09 AUROC across challenging benchmarks, such as MATH and GSM8k. These findings extend to general alignment as well -- on the Helpful-Harmless dataset, we observe 2.3\\% increase in accuracy.\nBeyond improving reward model performance, we show this way of training RM representations enables improved steerability because it allows us to evaluate the likelihood of an action achieving a particular goal-state (e.g. whether a solution is correct or helpful).\nLeveraging this insight, we find that we can filter up to 55\\% of generated tokens during majority voting by discarding trajectories likely to end up in an \"incorrect\" state, which leads to significant cost savings.\nWe additionally find that these representations can perform fine-grained control by conditioning on desired future goal-states.\nFor example, we show that steering a Llama 3 model towards helpful generations with our approach improves helpfulness by $9.6$\\% over a supervised-fine-tuning trained baseline.\nSimilarly, steering the model towards complex generations improves complexity by $21.6$\\% over the baseline.\nOverall, we find that training RMs in this contrastive, goal-conditioned fashion significantly improves performance and enables model steerability.",
  "abstract_zh": "通过离线数据或自监督目标学习改进表示的技术在传统强化学习中已显示出令人印象深刻的结果。然而，尚不清楚改进的表示学习如何能从人类反馈中受益于语言模型的强化学习。在这项工作中，我们提出以对比的、目标条件的方式训练奖励模型（RMs），通过增加沿采样的偏好轨迹的未来状态表示相似性并减少沿随机采样的不偏好轨迹的相似性来实现。这一目标显著提高了奖励模型在诸如MATH和GSM8k等具有挑战性的基准测试中的表现，AUROC提升高达0.09。这些发现也扩展到一般的对齐——在Helpful-Harmless数据集上，我们观察到准确率提高了2.3%。除了提高奖励模型的性能外，我们还展示了这种训练RM表示的方法能够改善可控性，因为它允许我们评估某个动作实现特定目标状态的可能性（例如，解决方案是否正确或有帮助）。利用这一见解，我们发现可以通过丢弃可能以“错误”状态结束的轨迹，在多数投票过程中过滤掉多达55%的生成标记，从而显著节省成本。此外，我们发现这些表示可以通过对期望的未来目标状态进行条件化来实现细粒度控制。例如，我们展示了使用我们的方法引导Llama 3模型生成有帮助的内容，相比于经过监督微调训练的基线，帮助性提高了9.6%。同样，引导模型生成复杂内容相比基线提高了21.6%的复杂性。总体而言，我们发现以这种对比的、目标条件的方式训练RMs显著提高了性能并实现了模型的可控性。"
}
{
  "title": "MoGU: A Framework for Enhancing Safety of LLMs While Preserving Their Usability",
  "title_zh": "Title: MoGU：一种在保持可用性的同时增强大型语言模型安全性的框架",
  "abstract": "Large Language Models (LLMs) are increasingly deployed in various applications. As their usage grows, concerns regarding their safety are rising, especially in maintaining harmless responses when faced with malicious instructions. Many defense strategies have been developed to enhance the safety of LLMs. However, our research finds that existing defense strategies lead LLMs to predominantly adopt a rejection-oriented stance, thereby diminishing the usability of their responses to benign instructions. To solve this problem, we introduce the MoGU framework, designed to enhance LLMs' safety while preserving their usability. Our MoGU framework transforms the base LLM into two variants: the usable LLM and the safe LLM, and further employs dynamic routing to balance their contribution. When encountering malicious instructions, the router will assign a higher weight to the safe LLM to ensure that responses are harmless. Conversely, for benign instructions, the router prioritizes the usable LLM, facilitating usable and helpful responses. On various open-sourced LLMs, we compare multiple defense strategies to verify the superiority of our MoGU framework. Besides, our analysis provides key insights into the effectiveness of MoGU and verifies that our designed routing mechanism can effectively balance the contribution of each variant by assigning weights. Our work released the safer Llama2, Vicuna, Falcon, Dolphin, and Baichuan2.",
  "abstract_zh": "Abstract: 大型语言模型（LLMs）在各种应用中被越来越多地部署。随着其使用的增长，关于其安全性的担忧也在增加，特别是在面对恶意指令时保持无害响应。许多防御策略已被开发以增强LLMs的安全性。然而，我们的研究发现，现有的防御策略导致LLMs主要采取拒绝导向的立场，从而降低了其对良性指令的响应可用性。为了解决这个问题，我们引入了MoGU框架，旨在增强LLMs的安全性，同时保持其可用性。我们的MoGU框架将基础LLM转化为两个变体：可用LLM和安全LLM，并进一步采用动态路由来平衡它们的贡献。当遇到恶意指令时，路由器将为安全LLM分配更高的权重，以确保响应无害。相反，对于良性指令，路由器优先考虑可用LLM，以促进可用和有帮助的响应。在各种开源LLM上，我们比较了多种防御策略以验证我们MoGU框架的优越性。此外，我们的分析提供了关于MoGU有效性的关键见解，并验证了我们设计的路由机制可以通过分配权重有效平衡每个变体的贡献。我们的工作发布了更安全的Llama2、Vicuna、Falcon、Dolphin和Baichuan2。"
}
{
  "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
  "title_zh": "攻击树：自动破解黑箱大型语言模型",
  "abstract": "While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed *jailbreaks*. In this work, we present *Tree of Attacks with Pruning*  (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an attacker LLM to iteratively refine candidate (attack) prompts until one of the refined prompts jailbreaks the target. In addition, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks, reducing the number of queries sent to the target LLM. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the prompts. This significantly improves upon the previous state-of-the-art black-box methods for generating jailbreaks while using a smaller number of queries than them. Furthermore, TAP is also capable of jailbreaking LLMs protected by state-of-the-art *guardrails*, e.g., LlamaGuard.",
  "abstract_zh": "尽管大型语言模型（LLMs）展示了多功能性，但它们仍然会生成有害、偏见和有毒的内容，这已通过人类设计的*越狱*的普遍性得到证明。在这项工作中，我们提出了*带修剪的攻击树*（TAP），这是一种生成越狱的自动化方法，只需对目标LLM进行黑箱访问。TAP利用攻击者LLM迭代地优化候选（攻击）提示，直到其中一个优化的提示成功破解目标。此外，在将提示发送给目标之前，TAP会对其进行评估，并修剪掉那些不太可能导致越狱的提示，从而减少发送给目标LLM的查询数量。在实证评估中，我们观察到TAP生成的提示能够成功破解最先进的LLMs（包括GPT4-Turbo和GPT4o）超过80%的提示。这显著改进了之前生成越狱的黑箱方法，同时使用的查询数量更少。此外，TAP还能够破解由最先进的*防护措施*保护的LLMs，例如LlamaGuard。"
}
{
  "title": "LACIE: Listener-Aware Finetuning for Calibration in Large Language Models",
  "title_zh": "标题：LACIE：大语言模型校准的听众感知微调方法",
  "abstract": "When answering questions, large language models (LLMs) can convey not only an answer to the question, but a level of confidence about the answer being correct. This includes explicit markers of confidence (e.g. giving a numeric confidence score) as well as implicit markers, like using an authoritative tone or elaborating with additional knowledge of a subject. For LLMs to be trustworthy sources of knowledge, the confidence they convey should match their actual expertise on a topic; however, this is currently not the case, with most models tending towards overconfidence. To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that directly models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener. Specifically, we cast calibration as a preference optimization problem, creating data via a two-agent speaker-listener game, where a speaker model’s outputs are judged by a simulated listener. We then finetune three different LLMs (Mistral-7B, Llama3-8B, Llama3-70B) with LACIE, and show that the models resulting from this multi-agent optimization are better calibrated on TriviaQA with respect to a simulated listener. Crucially, these trends transfer to human listeners, helping them correctly predict model correctness: we conduct a human evaluation where annotators accept or reject an LLM’s answers to trivia questions, finding that training with LACIE results in 47% fewer incorrect answers being accepted while maintaining the same level of acceptance for correct answers. Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis indicates that LACIE leads to a better separation in confidence between correct and incorrect examples. Qualitatively, we find that a LACIE-trained model hedges more when uncertain and adopts implicit cues to signal certainty when it is correct, such as using an authoritative tone or including details. Finally, finetuning with our listener- aware method leads to an emergent increase in model abstention (e.g. saying “I don’t know”) for answers that are likely to be wrong, trading recall for precision.",
  "abstract_zh": "摘要：在回答问题时，大语言模型（LLMs）不仅可以传达问题的答案，还可以传达对答案正确性的信心水平。这包括显式的信心标记（例如给出数值信心分数）以及隐式标记，如使用权威语气或通过附加知识进行详细说明。为了使LLMs成为值得信赖的知识来源，它们传达的信心应与其在某一主题上的实际专业知识相匹配；然而，目前大多数模型倾向于过度自信。为了校准隐式和显式的信心标记，我们引入了一种实用的、听众感知的微调方法（LACIE），该方法直接对听众建模，不仅考虑答案是否正确，还考虑听众是否会接受。具体而言，我们将校准视为一个偏好优化问题，通过一个双代理说话者-听众游戏创建数据，其中说话者模型的输出由模拟听众进行评判。然后，我们使用LACIE对三个不同的LLMs（Mistral-7B、Llama3-8B、Llama3-70B）进行微调，并显示通过这种多代理优化得到的模型在TriviaQA上对模拟听众的校准更好。重要的是，这些趋势转移到人类听众，帮助他们正确预测模型的正确性：我们进行了一项人类评估，评估者接受或拒绝LLM对琐事问题的回答，发现使用LACIE训练后，错误答案被接受的比例减少了47%，同时保持正确答案的接受水平。此外，LACIE推广到另一个数据集，在使用TriviaQA训练时，在TruthfulQA上显著提高了真实性。我们的分析表明，LACIE导致正确和错误示例之间的信心分离更好。定性地，我们发现经过LACIE训练的模型在不确定时更倾向于保留，并采用隐式线索来表示确定性，例如使用权威语气或包含细节。最后，使用我们的听众感知方法进行微调导致模型弃权的显著增加（例如说“我不知道”），对于可能错误的答案，牺牲召回率以提高精确度。"
}
{
  "title": "HuRef: HUman-REadable Fingerprint for Large Language Models",
  "title_zh": "标题: HuRef: 大型语言模型的人类可读指纹",
  "abstract": "Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this\nstudy, we introduce HuRef, a human-readable fingerprint for LLMs that uniquely identifies the base model without interfering with training or exposing model parameters to the public.\nWe first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, \nwith negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning, and RLHF, \nwhich makes it a sufficient condition\nto identify the base model.\nThe necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension permutation or matrix rotation, which significantly change it without affecting performance. To address this, leveraging the Transformer structure, we systematically analyze potential attacks and define three invariant terms that identify an LLM's base model. \nDue to the potential risk of information leakage, we cannot publish invariant terms directly. Instead, we map them to a Gaussian vector using an encoder, then convert it into a natural image using StyleGAN2, and finally publish the image. In our black-box setting, all fingerprinting steps are internally conducted by the LLMs owners. To ensure the published fingerprints are honestly generated, we introduced Zero-Knowledge Proof (ZKP).\nExperimental results across various LLMs demonstrate the effectiveness of our method. The code is available at https://github.com/LUMIA-Group/HuRef.",
  "abstract_zh": "摘要: 由于大型语言模型（LLM）训练资源密集且伴随精心设计的许可证，保护其版权变得至关重要。然而，由于参数可能发生变化，识别LLM的原始基础模型具有挑战性。在本研究中，我们引入了HuRef，一种用于LLM的人类可读指纹，可以在不干扰训练或公开模型参数的情况下唯一识别基础模型。我们首先观察到，LLM参数的向量方向在预训练收敛后保持稳定，并在后续训练步骤（包括继续预训练、监督微调和RLHF）中几乎没有扰动，这使其成为识别基础模型的充分条件。通过继续训练LLM并添加额外项以驱散模型参数的方向，验证了其必要性，模型因此受损。然而，这一方向容易受到简单攻击，如维度置换或矩阵旋转，这些攻击在不影响性能的情况下显著改变方向。为了解决这个问题，我们利用Transformer结构系统地分析潜在攻击，并定义了三个不变项来识别LLM的基础模型。由于信息泄露的潜在风险，我们无法直接发布不变项。相反，我们使用编码器将其映射到高斯向量，然后使用StyleGAN2将其转换为自然图像，最终发布图像。在我们的黑箱设置中，所有指纹步骤均由LLM的所有者内部进行。为了确保发布的指纹是诚实生成的，我们引入了零知识证明（ZKP）。各种LLM的实验结果证明了我们方法的有效性。代码可在https://github.com/LUMIA-Group/HuRef获取。"
}
{
  "title": "Lisa: Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning Attack",
  "title_zh": "标题：Lisa：针对有害微调攻击的大型语言模型的懒惰安全对齐",
  "abstract": "Recent studies show that Large Language Models (LLMs) with safety alignment can be jail-broken by fine-tuning on a dataset mixed with harmful data. For the first time in the literature, we show that the jail-break effect can be mitigated by separating two states in the fine-tuning stage to respectively optimize over the alignment and user datasets. Unfortunately, our subsequent study shows that this simple Bi-State Optimization (BSO) solution experiences convergence instability when steps invested in its alignment state is too small, leading to downgraded alignment performance. By statistical analysis, we show that the \\textit{excess drift} towards the switching iterates of the two states could be a probable reason for the instability. To remedy this issue, we propose \\textbf{L}azy(\\textbf{i}) \\textbf{s}afety \\textbf{a}lignment (\\textbf{Lisa}), which introduces a proximal term to constraint the drift of each state. Theoretically, the benefit of the proximal term is supported by the convergence analysis, wherein we show that a sufficient large proximal factor is necessary to guarantee Lisa's convergence.   Empirically, our results on four downstream fine-tuning tasks show that Lisa with a proximal term can significantly increase alignment performance while maintaining the LLM's accuracy on the user tasks.  Code is available at https://github.com/git-disl/Lisa.",
  "abstract_zh": "摘要：最近的研究表明，具有安全对齐的大型语言模型（LLMs）可以通过在混合有害数据的数据集上进行微调来被破解。在文献中，我们首次展示了通过在微调阶段分离两个状态分别优化对齐和用户数据集，可以减轻破解效果。不幸的是，我们的后续研究表明，当对齐状态投入的步骤太少时，这种简单的双状态优化（BSO）解决方案会出现收敛不稳定性，导致对齐性能下降。通过统计分析，我们表明两状态切换迭代的\\textit{过度漂移}可能是导致不稳定性的一个可能原因。为了解决这个问题，我们提出了\\textbf{L}azy(\\textbf{i}) \\textbf{s}afety \\textbf{a}lignment（\\textbf{Lisa}），它引入了一个近端项来限制每个状态的漂移。从理论上讲，近端项的好处得到了收敛性分析的支持，其中我们表明足够大的近端因子对于保证Lisa的收敛是必要的。实证上，我们在四个下游微调任务上的结果表明，带有近端项的Lisa可以显著提高对齐性能，同时保持LLM在用户任务上的准确性。代码可在https://github.com/git-disl/Lisa获得。"
}
{
  "title": "Large Language Models Must Be Taught to Know What They Don’t Know",
  "title_zh": "题目：大型语言模型必须被教导知道它们不知道的东西",
  "abstract": "When using large language models (LLMs) in high-stakes applications, we need to know when we can trust their predictions. Some works argue that prompting high-performance LLMs is sufficient to produce calibrated uncertainties, while others introduce sampling methods that can be prohibitively expensive. In this work, we first argue that prompting on its own is insufficient to achieve good calibration and then show that fine-tuning on a small dataset of correct and incorrect answers can create an uncertainty estimate with good generalization and small computational overhead. We show that a thousand graded examples are sufficient to outperform baseline methods and that training through the features of a model is necessary for good performance and tractable for large open-source models when using LoRA. We also investigate the mechanisms that enable reliable LLM uncertainty estimation, finding that many models can be used as general-purpose uncertainty estimators, applicable not just to their own uncertainties but also the uncertainty of other models. Lastly, we show that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study.",
  "abstract_zh": "摘要：在高风险应用中使用大型语言模型（LLMs）时，我们需要知道何时可以信任它们的预测。一些研究认为，提示高性能LLMs足以产生校准的不确定性，而另一些则引入可能代价高昂的采样方法。在这项工作中，我们首先论证仅靠提示不足以实现良好的校准，然后展示在一个小型正确和错误答案的数据集上进行微调可以创建具有良好泛化性和较小计算开销的不确定性估计。我们表明，一千个分级示例足以超越基线方法，并且通过模型特征进行训练对于良好性能是必要的，并且在使用LoRA时对于大型开源模型是可行的。我们还研究了能够实现可靠LLM不确定性估计的机制，发现许多模型可以用作通用不确定性估计器，不仅适用于其自身的不确定性，也适用于其他模型的不确定性。最后，我们通过一项用户研究表明，不确定性估计通过人机协作设置告知人类对LLMs的使用。"
}
{
  "title": "Rule Based Rewards for Language Model Safety",
  "title_zh": "基于规则的语言模型安全奖励",
  "abstract": "Reinforcement learning based fine-tuning of large language models (LLMs) on human preferences has been shown to enhance both their capabilities and safety behavior.\n  However, in cases related to safety, without precise instructions to human annotators, the data collected may cause the model to become overly cautious, or to respond in an undesirable style, such as being judgmental.\n  Additionally, as model capabilities and usage patterns evolve, there may be a costly need to add or relabel data to modify safety behavior. \n  We propose a novel preference modeling approach that utilizes AI feedback and only requires a small amount of human data. \n  Our method, Rule Based Rewards (RBR), uses a collection of rules for desired or undesired behaviors (e.g. refusals should not be judgmental) along with a LLM grader.\n  In contrast to prior methods using AI feedback, our method uses fine-grained, composable, LLM-graded few-shot prompts as reward directly in RL training, resulting in greater control, accuracy and ease of updating.\n  We show that RBRs are an effective training method, achieving an F1 score of 97.1, compared to a human-feedback baseline of 91.7, resulting in much higher safety-behavior accuracy through better balancing usefulness and safety.",
  "abstract_zh": "基于强化学习的人类偏好微调大型语言模型（LLMs）已被证明可以增强其能力和安全行为。然而，在与安全相关的情况下，如果没有对人类标注者进行精确的指导，所收集的数据可能会导致模型变得过于谨慎，或者以不理想的风格回应，例如带有评判性。此外，随着模型能力和使用模式的演变，可能需要高成本地添加或重新标记数据以修改安全行为。我们提出了一种新颖的偏好建模方法，该方法利用AI反馈并且只需要少量人类数据。我们的方法，基于规则的奖励（RBR），使用了一系列期望或不期望行为的规则（例如，拒绝不应带有评判性）以及一个LLM评分器。与使用AI反馈的先前方法相比，我们的方法在RL训练中直接使用细粒度、可组合的、LLM评分的少样本提示作为奖励，从而实现更大的控制、准确性和更新的便利性。我们表明，RBR是一种有效的训练方法，F1得分达到97.1，相比之下，人类反馈基线为91.7，通过更好地平衡实用性和安全性，显著提高了安全行为的准确性。"
}
{
  "title": "SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection",
  "title_zh": "标题: SelectIT: 基于不确定性自我反思的选择性指令微调方法用于大语言模型",
  "abstract": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs.  Despite this, common approaches often rely on additional models or data, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed $\\textit{SelectIT}$, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources.  Furthermore, we introduce a curated IT dataset, the $\\textit{Selective Alpaca}$, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks.  Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area.  Data, code, and scripts are freely available at https://github.com/Blue-Raincoat/SelectIT.",
  "abstract_zh": "摘要: 指令微调（IT）对于调整大语言模型（LLMs）以实现以人为中心的交互至关重要。最近的进展表明，精心选择一小部分高质量的IT数据可以显著提升LLMs的性能。尽管如此，常见的方法往往依赖于额外的模型或数据，这增加了成本并限制了广泛采用。在这项工作中，我们提出了一种新方法，称为$\\textit{SelectIT}$，利用LLM自身的基础能力。具体而言，我们利用LLM中固有的不确定性来更有效地选择高质量的IT数据，而无需额外资源。此外，我们引入了一个精心策划的IT数据集，称为$\\textit{Selective Alpaca}$，通过将SelectIT应用于Alpaca-GPT4数据集创建。实证结果表明，使用Selective Alpaca进行IT显著增强了模型能力。SelectIT的稳健性在各种基础模型和特定领域任务中也得到了证实。我们的研究结果表明，较长且计算密集的IT数据可能是更优的IT来源，为该领域的未来研究提供了宝贵的见解。数据、代码和脚本可在https://github.com/Blue-Raincoat/SelectIT免费获取。"
}
{
  "title": "Aligning Audio-Visual Joint Representations with an Agentic Workflow",
  "title_zh": "音视频联合表示的代理工作流对齐",
  "abstract": "Visual content and accompanied audio signals naturally formulate a joint representation to improve audio-visual (AV) related applications. While studies develop various AV representation learning frameworks, the importance of AV data alignment is usually undermined for achieving high-quality representation. We observe that an audio signal may contain background noise interference. Also, non-synchronization may appear between audio and video streams. These non-strict data alignment limits representation quality and downgrade application performance. In this paper, we propose to improve AV joint representations from a data-centric perspective by aligning audio signals to visual data. Our alignment is conducted in an agentic workflow controlled by an LLM-based assistant named AVAgent. For each input AV data pair, our AVAgent uses a multi-modal LLM to convert audio and visual data into language descriptions separately (i.e., tool use). Then, AVAgent reasons whether this paired data is aligned well and plans to edit the audio signal if needed (i.e., planning). The audio editing is executed by predefined actions that filter noise or augment data. Moreover, we use a VLM to evaluate how modified audio signals match the visual content and provide feedback to AVAgent (i.e., reflection). The tool use, planning, and reflection steps operate cyclically to become an agentic workflow where audio signals are gradually aligned to visual content. To this end, existing methods can directly leverage the aligned AV data via our agentic workflow to improve AV joint representations. The experimental results comprehensively demonstrate the state-of-the-art performance of the proposed approach against previous baselines in diverse downstream tasks.",
  "abstract_zh": "视觉内容和伴随的音频信号自然地形成联合表示，以改善音视频（AV）相关应用。尽管研究开发了各种AV表示学习框架，但为了实现高质量的表示，AV数据对齐的重要性通常被低估。我们观察到音频信号可能包含背景噪声干扰。此外，音频和视频流之间可能出现不同步。这种非严格的数据对齐限制了表示质量并降低了应用性能。在本文中，我们提出从数据中心的角度，通过将音频信号与视觉数据对齐来改善AV联合表示。我们的对齐是在一个由基于LLM的助手AVAgent控制的代理工作流中进行的。对于每个输入的AV数据对，AVAgent使用多模态LLM将音频和视觉数据分别转换为语言描述（即工具使用）。然后，AVAgent推理这对数据是否对齐良好，并计划在需要时编辑音频信号（即规划）。音频编辑通过预定义的动作执行，这些动作可以过滤噪声或增强数据。此外，我们使用VLM评估修改后的音频信号如何匹配视觉内容，并向AVAgent提供反馈（即反思）。工具使用、规划和反思步骤循环操作，形成一个代理工作流，其中音频信号逐渐与视觉内容对齐。为此，现有方法可以通过我们的代理工作流直接利用对齐的AV数据来改善AV联合表示。实验结果全面展示了所提方法在各种下游任务中相对于先前基线的最先进性能。"
}
{
  "title": "Reference Trustable Decoding: A Training-Free Augmentation Paradigm for Large Language Models",
  "title_zh": "参考可信解码：一种无需训练的大型语言模型增强范式",
  "abstract": "Large language models (LLMs) have rapidly advanced and demonstrated impressive capabilities. In-Context Learning (ICL) and Parameter-Efficient Fine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to downstream tasks. ICL typically constructs a few-shot learning scenario, either manually or by setting up a Retrieval-Augmented Generation (RAG) system, helping models quickly grasp domain knowledge or question-answering patterns without changing model parameters. However, this approach involves trade-offs, such as slower inference speed and increased space occupancy. PEFT assists the model in adapting to tasks through minimal parameter modifications, but the training process still demands high hardware requirements, even with a small number of parameters involved. To address these challenges, we propose Reference Trustable Decoding (RTD), a paradigm that allows models to quickly adapt to new tasks without fine-tuning, maintaining low inference costs. RTD constructs a reference datastore from the provided training examples and optimizes the LLM's final vocabulary distribution by flexibly selecting suitable references based on the input, resulting in more trustable responses and enabling the model to adapt to downstream tasks at a low cost. Experimental evaluations on various LLMs using different benchmarks demonstrate that RTD establishes a new paradigm for augmenting models to downstream tasks. Furthermore, our method exhibits strong orthogonality with traditional methods, allowing for concurrent usage. Our code can be found at https://github.com/ShiLuohe/ReferenceTrustableDecoding.",
  "abstract_zh": "大型语言模型（LLMs）迅速发展并展示了令人印象深刻的能力。上下文学习（ICL）和参数高效微调（PEFT）是目前增强LLMs以适应下游任务的两种主流方法。ICL通常构建一个少样本学习场景，可以手动设置或通过建立检索增强生成（RAG）系统，帮助模型快速掌握领域知识或问答模式，而无需更改模型参数。然而，这种方法涉及权衡，如较慢的推理速度和增加的空间占用。PEFT通过最小的参数修改帮助模型适应任务，但即使涉及少量参数，训练过程仍然需要高硬件要求。为了解决这些挑战，我们提出了参考可信解码（RTD），这是一种无需微调即可让模型快速适应新任务的范式，保持低推理成本。RTD从提供的训练示例中构建参考数据存储，并通过灵活选择基于输入的合适参考来优化LLM的最终词汇分布，从而产生更可信的响应，并使模型能够以低成本适应下游任务。对各种LLMs使用不同基准的实验评估表明，RTD为增强模型以适应下游任务建立了新的范式。此外，我们的方法与传统方法表现出强正交性，允许同时使用。我们的代码可以在https://github.com/ShiLuohe/ReferenceTrustableDecoding找到。"
}
{
  "title": "Iteration Head: A Mechanistic Study of Chain-of-Thought",
  "title_zh": "迭代头：链式思维的机制研究",
  "abstract": "Chain-of-Thought (CoT) reasoning is known to improve Large Language Models both empirically and in terms of theoretical approximation power.\nHowever, our understanding of the inner workings and conditions of apparition of CoT capabilities remains limited.\nThis paper helps fill this gap by demonstrating how CoT reasoning emerges in transformers in a controlled and interpretable setting.\nIn particular, we observe the appearance of a specialized attention mechanism dedicated to iterative reasoning, which we coined \"iteration heads\".\nWe track both the emergence and the precise working of these iteration heads down to the attention level, and measure the transferability of the CoT skills to which they give rise between tasks.",
  "abstract_zh": "链式思维（CoT）推理已知可以在经验和理论近似能力上提升大型语言模型。然而，我们对CoT能力的内在工作原理和出现条件的理解仍然有限。本文通过在一个可控且可解释的环境中展示CoT推理如何在变压器中出现，帮助填补这一空白。特别是，我们观察到一种专门用于迭代推理的注意力机制的出现，我们称之为“迭代头”。我们追踪这些迭代头的出现及其在注意力层面的精确工作，并测量它们在任务之间引发的CoT技能的可转移性。"
}
{
  "title": "Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models",
  "title_zh": "标题：锦上添花：大型语言模型中的参数异质性与量化",
  "abstract": "This paper reveals the phenomenon of parameter heterogeneity in large language models (LLMs). We find that a small subset of ``cherry'' parameters exhibit a disproportionately large influence on model performance, while the vast majority of parameters have minimal impact. This heterogeneity is found to be prevalent across different model families, scales, and types. Motivated by this observation, we propose CherryQ, a novel quantization method that unifies the optimization of mixed-precision parameters. CherryQ identifies and preserves the critical cherry parameters in high precision while aggressively quantizing the remaining parameters to low precision. Extensive experiments demonstrate the effectiveness of CherryQ. CherryQ outperforms existing quantization approaches in terms of perplexity and downstream task performance. Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance compared to their 16-bit counterparts. These findings highlight the potential of CherryQ for enabling efficient deployment of LLMs by taking advantage of parameter heterogeneity.",
  "abstract_zh": "摘要：本文揭示了大型语言模型（LLMs）中的参数异质性现象。我们发现，一小部分“锦上添花”参数对模型性能有着不成比例的巨大影响，而绝大多数参数的影响微乎其微。这种异质性在不同的模型家族、规模和类型中普遍存在。基于这一观察，我们提出了CherryQ，一种新颖的量化方法，统一了混合精度参数的优化。CherryQ识别并保留了高精度的关键“锦上添花”参数，同时将其余参数积极量化为低精度。大量实验表明了CherryQ的有效性。CherryQ在困惑度和下游任务性能方面优于现有的量化方法。值得注意的是，我们的3位量化Vicuna-1.5在性能上与其16位对应物相媲美。这些发现突显了CherryQ通过利用参数异质性来实现大型语言模型高效部署的潜力。"
}
{
  "title": "AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning",
  "title_zh": "自动手册：通过交互式环境学习由LLM代理生成说明手册",
  "abstract": "Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce a *case-conditioned prompting* strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with GPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at https://github.com/minghchen/automanual.",
  "abstract_zh": "基于大型语言模型（LLM）的代理在机器人、游戏和网络导航等各个领域中展示了自主完成任务的潜力。然而，这些代理通常需要精心设计和专家提示才能解决特定领域的任务，这限制了它们的适应性。我们引入了AutoManual，一个使LLM代理能够通过交互自主构建理解并适应新环境的框架。AutoManual将环境知识分类为多样的规则，并通过两个代理以在线方式优化它们：1）计划者根据当前规则编写可操作的计划以与环境互动。2）构建者通过一个结构良好的规则系统更新规则，该系统促进在线规则管理和必要细节的保留。为了减轻管理规则中的幻觉，我们为构建者引入了一种*案例条件提示*策略。最后，制定者代理将这些规则编译成一个综合手册。自生成的手册不仅可以提高适应性，还可以指导较小LLM的规划，同时具有人类可读性。在仅有一个简单演示的情况下，AutoManual显著提高了任务成功率，在ALFWorld基准任务中，使用GPT-4-turbo达到了97.4\\%，使用GPT-3.5-turbo达到了86.2\\%。代码可在https://github.com/minghchen/automanual获取。"
}
{
  "title": "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation",
  "title_zh": "揭示Elo：语言模型评估的稳健性与最佳实践",
  "abstract": "In Natural Language Processing (NLP), the Elo rating system, originally designed for ranking players in dynamic games such as chess, is increasingly being used to evaluate Large Language Models (LLMs) through \"A vs B\" paired comparisons.\nHowever, while popular, the system's suitability for assessing entities with constant skill levels, such as LLMs, remains relatively unexplored. \nWe study two fundamental axioms that evaluation methods should adhere to: reliability and transitivity. \nWe conduct an extensive evaluation of Elo behavior across simulated and real-world scenarios, demonstrating that individual Elo computations can exhibit significant volatility.\nWe show that both axioms are not always satisfied, raising questions about the reliability of current comparative evaluations of LLMs.\nIf the current use of Elo scores is intended to substitute the costly head-to-head comparison of LLMs, it is crucial to ensure the ranking is as robust as possible.\nGuided by the axioms, our findings offer concrete guidelines for enhancing the reliability of LLM evaluation methods, suggesting a need for reassessment of existing comparative approaches.",
  "abstract_zh": "在自然语言处理（NLP）中，Elo评分系统最初是为动态游戏（如国际象棋）中的玩家排名而设计的，现正越来越多地用于通过“A对B”配对比较来评估大型语言模型（LLM）。然而，尽管广受欢迎，该系统对评估技能水平恒定的实体（如LLM）的适用性仍未得到充分探索。我们研究了评估方法应遵循的两个基本公理：可靠性和传递性。我们对Elo在模拟和现实场景中的行为进行了广泛评估，表明单个Elo计算可能表现出显著的波动性。我们表明，这两个公理并不总是得到满足，这对当前LLM的比较评估的可靠性提出了质疑。如果当前使用Elo评分是为了替代昂贵的LLM对比评估，那么确保排名尽可能稳健是至关重要的。在公理的指导下，我们的研究结果为提高LLM评估方法的可靠性提供了具体指导，建议需要重新评估现有的比较方法。"
}
{
  "title": "KnowGPT: Knowledge Graph based Prompting for Large Language Models",
  "title_zh": "标题: KnowGPT: 基于知识图谱的大型语言模型提示方法",
  "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in many real-world applications. Nonetheless, LLMs are often criticized for their tendency to produce hallucinations, wherein the models fabricate incorrect statements on tasks beyond their knowledge and perception. To alleviate this issue, graph retrieval-augmented generation (GraphRAG) has been extensively explored which leverages the factual knowledge in knowledge graphs (KGs) to ground the LLM's responses in established facts and principles. However, most state-of-the-art LLMs are closed-source, making it challenging to develop a prompting framework that can efficiently and effectively integrate KGs into LLMs with hard prompts only. Generally, existing KG-enhanced LLMs usually suffer from three critical issues, including huge search space, high API costs, and laborious prompt engineering, that impede their widespread application in practice. To this end, we introduce a novel **Know**ledge **Gr**aph based **P**romp**T**ing framework, namely **KnowGPT**, to enhance LLMs with domain knowledge. KnowGPT contains a knowledge extraction module to extract the most informative knowledge from KGs, and a context-aware prompt construction module to automatically convert extracted knowledge into effective prompts. Experiments on three benchmarks demonstrate that KnowGPT significantly outperforms all competitors. Notably, KnowGPT achieves a 92.6% accuracy on OpenbookQA leaderboard, comparable to human-level performance.",
  "abstract_zh": "摘要: 大型语言模型（LLMs）在许多现实世界应用中展示了卓越的能力。然而，LLMs常因其倾向于产生幻觉而受到批评，即在超出其知识和感知的任务中编造不正确的陈述。为缓解这一问题，图检索增强生成（GraphRAG）被广泛探索，该方法利用知识图谱（KGs）中的事实知识来使LLM的响应基于已建立的事实和原则。然而，大多数最先进的LLMs是闭源的，这使得开发一个能够仅通过硬提示高效且有效地将KGs整合到LLMs中的提示框架具有挑战性。通常，现有的KG增强型LLMs通常面临三个关键问题，包括巨大的搜索空间、高昂的API成本和繁琐的提示工程，这些问题阻碍了它们在实践中的广泛应用。为此，我们引入了一种新颖的基于知识图谱的提示框架，称为KnowGPT，以增强LLMs的领域知识。KnowGPT包含一个知识提取模块，用于从KGs中提取最有信息量的知识，以及一个上下文感知的提示构建模块，用于自动将提取的知识转换为有效的提示。在三个基准测试上的实验表明，KnowGPT显著优于所有竞争对手。值得注意的是，KnowGPT在OpenbookQA排行榜上达到了92.6%的准确率，可与人类水平的表现相媲美。"
}
{
  "title": "Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration",
  "title_zh": "微调大型语言模型的自提示校准中的成员推断攻击",
  "abstract": "Membership Inference Attacks (MIA) aim to infer whether a target data record has been utilized for model training or not. Existing MIAs designed for large language models (LLMs) can be bifurcated into two types: reference-free and reference-based attacks. Although reference-based attacks appear promising performance by calibrating the probability measured on the target model with reference models, this illusion of privacy risk heavily depends on a reference dataset that closely resembles the training set. Both two types of attacks are predicated on the hypothesis that training records consistently maintain a higher probability of being sampled. However, this hypothesis heavily relies on the overfitting of target models, which will be mitigated by multiple regularization methods and the generalization of LLMs. Thus, these reasons lead to high false-positive rates of MIAs in practical scenarios.\nWe propose a Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA). \nSpecifically, we introduce a self-prompt approach, which constructs the dataset to fine-tune the reference model by prompting the target LLM itself. In this manner, the adversary can collect a dataset with a similar distribution from public APIs.\nFurthermore, we introduce probabilistic variation, a more reliable membership signal based on LLM memorization rather than overfitting, from which we rediscover the neighbour attack with theoretical grounding. \nComprehensive evaluation conducted on three datasets and four exemplary LLMs shows that SPV-MIA raises the AUC of MIAs from 0.7 to a significantly high level of 0.9. Our code and dataset are available at: https://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA",
  "abstract_zh": "成员推断攻击（MIA）旨在推断目标数据记录是否被用于模型训练。现有针对大型语言模型（LLM）的MIA可分为两类：无参考攻击和基于参考的攻击。尽管基于参考的攻击通过校准目标模型与参考模型的概率表现出有希望的性能，但这种隐私风险的错觉严重依赖于与训练集非常相似的参考数据集。这两种类型的攻击都基于这样一个假设：训练记录始终保持较高的采样概率。然而，这一假设严重依赖于目标模型的过拟合，而这将通过多种正则化方法和LLM的泛化得到缓解。因此，这些原因导致MIA在实际场景中有较高的误报率。我们提出了一种基于自校准概率变化的成员推断攻击（SPV-MIA）。具体而言，我们引入了一种自提示方法，通过提示目标LLM自身来构建数据集以微调参考模型。通过这种方式，对手可以从公共API中收集具有相似分布的数据集。此外，我们引入了概率变化，这是一种基于LLM记忆而非过拟合的更可靠的成员信号，从中我们重新发现了具有理论基础的邻居攻击。在三个数据集和四个典型LLM上进行的综合评估表明，SPV-MIA将MIA的AUC从0.7提高到显著的0.9。我们的代码和数据集可在以下网址获取：https://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA"
}
{
  "title": "A Theoretical Understanding of Self-Correction through In-context Alignment",
  "title_zh": "通过上下文对齐对自我纠正的理论理解",
  "abstract": "Going beyond mimicking limited human experiences, recent studies show initial evidence that, like humans, large language models (LLMs) are capable of improving their abilities purely by self-correction, i.e., correcting previous responses through self-examination, as seen in models like OpenAI o1. Nevertheless, little is known about how such capabilities arise. In this work, based on a simplified setup akin to an alignment task, we theoretically analyze self-correction from an in-context learning perspective, showing that when LLMs give relatively accurate self-examinations as rewards, they are capable of refining responses in an in-context way. Notably, going beyond previous theories on over-simplified linear transformers, our theoretical construction underpins the roles of several key designs of realistic transformers for self-correction: softmax attention, multi-head attention, and the MLP block. We validate these findings extensively on synthetic datasets. Inspired by these findings, we propose a simple self-correction strategy, Checking as Context (CaC), which finds novel applications in alleviating social bias and defending against LLM jailbreaks. We believe that these findings will inspire further research on understanding, exploiting, and enhancing self-correction for building better foundation models. Code is at https://github.com/yifeiwang77/Self-Correction.",
  "abstract_zh": "超越模仿有限的人类经验，最近的研究显示了初步证据表明，大型语言模型（LLMs）像人类一样，能够纯粹通过自我纠正来提高其能力，即通过自我检查来纠正先前的响应，如在OpenAI o1等模型中所见。然而，对于这种能力如何产生知之甚少。在这项工作中，基于类似对齐任务的简化设置，我们从上下文学习的角度理论分析了自我纠正，表明当LLMs将相对准确的自我检查作为奖励时，它们能够以上下文方式优化响应。值得注意的是，我们的理论构建超越了以前关于过于简化的线性变压器的理论，支持了现实变压器自我纠正的几个关键设计的作用：softmax注意力、多头注意力和MLP块。我们在合成数据集上广泛验证了这些发现。受这些发现的启发，我们提出了一种简单的自我纠正策略，称为作为上下文的检查（CaC），在缓解社会偏见和防御LLM越狱方面找到了新的应用。我们相信这些发现将激发进一步的研究，以理解、利用和增强自我纠正，以构建更好的基础模型。代码在https://github.com/yifeiwang77/Self-Correction。"
}
{
  "title": "Can large language models explore in-context?",
  "title_zh": "标题：大型语言模型能在上下文中探索吗？",
  "abstract": "We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thought reasoning but unsummarized history. While these findings can be interpreted positively, they suggest that external summarization—which may not be possible in more complex settings—is essential for desirable LLM behavior. We conclude that non-trivial algorithmic interventions, such as fine-tuning or dataset curation, may be required to empower LLM-based decision making agents in complex settings.",
  "abstract_zh": "摘要：我们研究了当代大型语言模型（LLMs）在多大程度上能够进行探索，这是强化学习和决策制定中的核心能力。我们专注于现有LLMs的原生性能，不进行训练干预。我们将LLMs作为代理部署在简单的多臂老虎机环境中，完全在上下文中指定环境描述和交互历史，即在LLM提示内。我们使用各种提示设计对GPT-3.5、GPT-4和Llama2进行实验，发现这些模型在没有重大干预的情况下无法稳健地进行探索：i）只有一种配置产生了令人满意的探索行为：GPT-4与链式思维推理和外部总结的交互历史；ii）所有其他配置都没有产生稳健的探索行为，包括那些具有链式思维推理但未总结历史的配置。虽然这些发现可以被积极解读，但它们表明外部总结——在更复杂的环境中可能无法实现——对于期望的LLM行为是必不可少的。我们得出结论，非平凡的算法干预，如微调或数据集策划，可能是赋能基于LLM的决策代理在复杂环境中所必需的。"
}
{
  "title": "Perplexity-aware Correction for Robust Alignment with Noisy Preferences",
  "title_zh": "标题：感知困惑度的校正方法用于在噪声偏好下实现稳健对齐",
  "abstract": "Alignment techniques are critical in ensuring that large language models (LLMs) output helpful and harmless content by enforcing the LLM-generated content to align with human preferences. \nHowever, the existence of noisy preferences (NPs), where the responses are mistakenly labelled as chosen or rejected, could spoil the alignment, thus making the LLMs generate useless and even malicious content. \nExisting methods mitigate the issue of NPs from the loss perspective by adjusting the alignment loss based on a clean validation dataset.\nOrthogonal to these loss-oriented methods, we propose perplexity-aware correction (PerpCorrect) from the data perspective for robust alignment which detects and corrects NPs based on the differences between the perplexity of the chosen and rejected responses (dubbed as PPLDiff). \nIntuitively, a higher PPLDiff indicates a higher probability of the NP because a rejected/chosen response which is mistakenly labelled as chosen/rejected is less preferable to be generated by an aligned LLM, thus having a higher/lower perplexity.\nPerpCorrect works in three steps: \n(1) PerpCorrect aligns a surrogate LLM using the clean validation data to make the PPLDiff able to distinguish clean preferences (CPs) and NPs. \n(2) PerpCorrect further aligns the surrogate LLM by incorporating the reliable clean training data whose PPLDiff is extremely small and reliable noisy training data whose PPLDiff is extremely large after correction to boost the discriminatory power.\n(3) Detecting and correcting NPs according to the PPLDiff obtained by the aligned surrogate LLM to obtain a denoised training dataset for robust alignment.\nComprehensive experiments validate that our proposed PerpCorrect can achieve state-of-the-art alignment performance under NPs.\nNotably, PerpCorrect demonstrates practical utility by requiring only a modest amount of validation data and being compatible with various alignment techniques. \nOur code is available at [PerpCorrect](https://github.com/luxinyayaya/PerpCorrect).",
  "abstract_zh": "摘要：对齐技术在确保大型语言模型（LLMs）输出有用且无害的内容方面至关重要，因为它强制LLM生成的内容与人类偏好一致。然而，噪声偏好（NPs）的存在，即响应被错误地标记为选择或拒绝，可能会破坏对齐，从而使LLMs生成无用甚至恶意的内容。现有方法从损失的角度通过基于干净的验证数据集调整对齐损失来缓解NPs的问题。与这些面向损失的方法正交，我们从数据的角度提出了一种感知困惑度的校正方法（PerpCorrect），用于稳健对齐，该方法基于选择和拒绝响应的困惑度差异（称为PPLDiff）检测和校正NPs。直观地说，较高的PPLDiff表明NP的概率较高，因为被错误标记为选择/拒绝的拒绝/选择响应不太可能由对齐的LLM生成，因此具有较高/较低的困惑度。PerpCorrect分三步工作：（1）PerpCorrect使用干净的验证数据对一个代理LLM进行对齐，使PPLDiff能够区分干净偏好（CPs）和NPs。（2）PerpCorrect通过结合其PPLDiff在校正后极小的可靠干净训练数据和PPLDiff极大的可靠噪声训练数据进一步对代理LLM进行对齐，以增强辨别能力。（3）根据对齐的代理LLM获得的PPLDiff检测和校正NPs，以获得用于稳健对齐的去噪训练数据集。综合实验验证了我们提出的PerpCorrect在NPs下可以实现最先进的对齐性能。值得注意的是，PerpCorrect通过仅需少量验证数据并兼容各种对齐技术展示了其实用性。我们的代码可在[PerpCorrect](https://github.com/luxinyayaya/PerpCorrect)获取。"
}
{
  "title": "On scalable oversight with weak LLMs judging strong LLMs",
  "title_zh": "Title: 关于使用弱大型语言模型评估强大型语言模型的可扩展监督",
  "abstract": "Scalable oversight protocols aim to enable humans to accurately supervise superhuman AI. \nIn this paper we study debate, where two AI's compete to convince a judge; consultancy, \nwhere a single AI tries to convince a judge that asks questions;\nand compare to a baseline of direct question-answering, where the judge just answers outright without the AI.\nWe use large language models (LLMs) as both AI agents and as stand-ins for human judges, taking the judge models to be weaker than agent models. \nWe benchmark on a diverse range of asymmetries between judges and agents, extending previous work on a single extractive QA task with information asymmetry, to also include mathematics, coding, logic and multimodal reasoning asymmetries. \nWe find that debate outperforms consultancy across all tasks when the consultant is randomly assigned to argue for the correct/incorrect answer. Comparing debate to direct question answering, the results depend on the type of task: in extractive QA tasks with information asymmetry debate outperforms direct question answering, but in other tasks without information asymmetry the results are mixed.\nPrevious work assigned debaters/consultants an answer to argue for. When we allow them to instead choose which answer to argue for, we find judges are less frequently convinced by the wrong answer in debate than in consultancy.\nFurther, we find that stronger debater models increase judge accuracy, though more modestly than in previous studies.",
  "abstract_zh": "Abstract: 可扩展监督协议旨在使人类能够准确监督超人类人工智能。在本文中，我们研究了辩论，其中两个人工智能竞争以说服一名评判员；咨询，其中单个人工智能试图说服提出问题的评判员；并与直接问答的基线进行比较，在这种情况下，评判员直接回答而不依赖人工智能。我们使用大型语言模型（LLMs）作为人工智能代理和人类评判员的替代品，假设评判员模型比代理模型更弱。我们在评判员和代理之间的多种不对称性上进行基准测试，将之前在信息不对称的单一抽取问答任务上的工作扩展到包括数学、编码、逻辑和多模态推理不对称性。我们发现，当顾问被随机分配为正确/错误答案辩护时，辩论在所有任务中都优于咨询。将辩论与直接问答进行比较，结果取决于任务类型：在信息不对称的抽取问答任务中，辩论优于直接问答，但在没有信息不对称的其他任务中，结果则混合。之前的工作为辩论者/顾问分配了要辩护的答案。当我们允许他们选择要辩护的答案时，我们发现评判员在辩论中比在咨询中更少被错误答案说服。此外，我们发现更强的辩论者模型提高了评判员的准确性，尽管比以前的研究更为温和。"
}
{
  "title": "Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration",
  "title_zh": "移动代理-v2：通过多代理协作实现有效导航的移动设备操作助手",
  "abstract": "Mobile device operation tasks are increasingly becoming a popular multi-modal AI application scenario. Current Multi-modal Large Language Models (MLLMs), constrained by their training data, lack the capability to function effectively as operation assistants. Instead, MLLM-based agents, which enhance capabilities through tool invocation, are gradually being applied to this scenario. However, the two major navigation challenges in mobile device operation tasks — task progress navigation and focus content navigation — are difficult to effectively solve under the single-agent architecture of existing work. This is due to the overly long token sequences and the interleaved text-image data format, which limit performance. To address these navigation challenges effectively, we propose Mobile-Agent-v2, a multi-agent architecture for mobile device operation assistance. The architecture comprises three agents: planning agent, decision agent, and reflection agent. The planning agent condenses lengthy, interleaved image-text history operations and screens summaries into a pure-text task progress, which is then passed on to the decision agent. This reduction in context length makes it easier for decision agent to navigate the task progress. To retain focus content, we design a memory unit that updates with task progress by decision agent. Additionally, to correct erroneous operations, the reflection agent observes the outcomes of each operation and handles any mistake accordingly. Experimental results indicate that Mobile-Agent-v2 achieves over a 30% improvement in task completion compared to the single-agent architecture of Mobile-Agent. The code is open-sourced at https://github.com/X-PLUG/MobileAgent.",
  "abstract_zh": "移动设备操作任务正日益成为一种流行的多模态AI应用场景。当前的多模态大型语言模型（MLLMs）由于其训练数据的限制，缺乏作为操作助手有效运作的能力。相反，通过工具调用增强能力的基于MLLM的代理逐渐被应用于这一场景。然而，在现有工作的单代理架构下，移动设备操作任务中的两个主要导航挑战——任务进度导航和聚焦内容导航——难以有效解决。这是由于过长的标记序列和交错的文本-图像数据格式限制了性能。为有效解决这些导航挑战，我们提出了移动代理-v2，一种用于移动设备操作辅助的多代理架构。该架构包括三个代理：规划代理、决策代理和反思代理。规划代理将冗长的、交错的图文历史操作和屏幕摘要浓缩为纯文本的任务进度，然后传递给决策代理。这种上下文长度的减少使得决策代理更容易导航任务进度。为了保留聚焦内容，我们设计了一个由决策代理更新的任务进度记忆单元。此外，为了纠正错误操作，反思代理观察每次操作的结果并相应处理任何错误。实验结果表明，与单代理架构的移动代理相比，移动代理-v2在任务完成率上提高了30%以上。代码已在https://github.com/X-PLUG/MobileAgent开源。"
}
{
  "title": "UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction",
  "title_zh": "城市KGent：用于城市知识图谱构建的统一大型语言模型代理框架",
  "abstract": "Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama 2 and Llama 3 family, we obtain UrbanKGC agent family, consisting of UrbanKGent-7/8/13B version. We perform a comprehensive evaluation on two real-world datasets using both human and GPT-4 self-evaluation. The experimental results demonstrate that UrbanKGent family can not only significantly outperform 31 baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4, by more than 10% with approximately 20 times lower cost. Compared with the existing benchmark, the UrbanKGent family could help construct an UrbanKG with hundreds of times richer relationships using only one-fifth of the data. Our data and code are available at https://github.com/usail-hkust/UrbanKGent.",
  "abstract_zh": "城市知识图谱最近作为一种新兴的构建模块，从多源城市数据中提炼出关键知识，以用于多样化的城市应用场景。尽管其具有潜在的好处，城市知识图谱构建（UrbanKGC）仍然严重依赖人工努力，阻碍了其潜在的进步。本文提出了UrbanKGent，一个用于城市知识图谱构建的统一大型语言模型代理框架。具体来说，我们首先通过异质性感知和地理空间融合的指令生成，为UrbanKGC任务（如关系三元组提取和知识图谱补全）构建了知识丰富的指令集。此外，我们提出了一个工具增强的迭代轨迹优化模块，以增强和优化从GPT-4提炼的轨迹。通过在Llama 2和Llama 3系列上进行带有增强轨迹的混合指令微调，我们获得了UrbanKGC代理系列，包括UrbanKGent-7/8/13B版本。我们在两个真实世界的数据集上进行了全面评估，使用人类和GPT-4自我评估。实验结果表明，UrbanKGent系列不仅在UrbanKGC任务中显著优于31个基准，还以约20倍的成本优势超越了最先进的LLM，GPT-4超过10%。与现有基准相比，UrbanKGent系列可以仅使用五分之一的数据构建一个关系丰富数百倍的UrbanKG。我们的数据和代码可在https://github.com/usail-hkust/UrbanKGent获得。"
}
{
  "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models",
  "title_zh": "标题：ANAH-v2：扩展大语言模型的分析幻觉标注",
  "abstract": "Large language models (LLMs) exhibit hallucinations in long-form question-answering tasks across various domains and wide applications. Current hallucination detection and mitigation datasets are limited in domain and size, which struggle to scale due to prohibitive labor costs and insufficient reliability of existing hallucination annotators. To facilitate the scalable oversight of LLM hallucinations, this paper introduces an iterative self-training framework that simultaneously and progressively scales up the annotation dataset and improves the accuracy of the annotator. Based on the Expectation Maximization algorithm, in each iteration, the framework first applies an automatic hallucination annotation pipeline for a scaled dataset and then trains a more accurate annotator on the dataset.  This new annotator is adopted in the annotation pipeline for the next iteration. Extensive experimental results demonstrate that the finally obtained hallucination annotator with only 7B parameters surpasses GPT-4 and obtains new state-of-the-art hallucination detection results on HaluEval and HalluQA by zero-shot inference. Such an annotator can not only evaluate the hallucination levels of various LLMs on the large-scale dataset but also help to mitigate the hallucination of LLMs generations, with the Natural Language Inference metric increasing from 25% to 37% on HaluEval.",
  "abstract_zh": "摘要：大型语言模型（LLMs）在跨领域和广泛应用的长篇问答任务中表现出幻觉。当前的幻觉检测和缓解数据集在领域和规模上都有限，由于高昂的人工成本和现有幻觉标注器的可靠性不足，难以扩展。为了促进LLM幻觉的可扩展监督，本文引入了一种迭代自训练框架，该框架同时逐步扩展标注数据集并提高标注器的准确性。基于期望最大化算法，在每次迭代中，该框架首先对扩展数据集应用自动幻觉标注流程，然后在数据集上训练更准确的标注器。这个新的标注器被用于下一次迭代的标注流程。大量实验结果表明，最终获得的仅有70亿参数的幻觉标注器超越了GPT-4，并通过零样本推理在HaluEval和HalluQA上获得了新的最先进的幻觉检测结果。这样的标注器不仅可以评估各种LLM在大规模数据集上的幻觉水平，还可以帮助缓解LLM生成的幻觉，使自然语言推理指标在HaluEval上从25%提高到37%。"
}
{
  "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents",
  "title_zh": "标题：小心你的代理！调查基于LLM的代理的后门威胁",
  "abstract": "Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis of different forms of agent backdoor attacks. Specifically, compared with traditional backdoor attacks on LLMs that are only able to manipulate the user inputs and model outputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From the perspective of the final attacking outcomes, the agent backdoor attacker can not only choose to manipulate the final output distribution, but also introduce the malicious behavior in an intermediate reasoning step only, while keeping the final output correct. (2) Furthermore, the former category can be divided into two subcategories based on trigger locations, in which the backdoor trigger can either be hidden in the user query or appear in an intermediate observation returned by the external environment. We implement the above variations of agent backdoor attacks on two typical agent tasks including web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks and such backdoor vulnerability cannot be easily mitigated by current textual backdoor defense algorithms. This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content.",
  "abstract_zh": "摘要：随着大型语言模型（LLMs）的快速发展，基于LLM的代理已被开发用于处理各种现实世界的应用，包括金融、医疗保健和购物等。在应用过程中，确保基于LLM的代理的可靠性和安全性至关重要。然而，基于LLM的代理的安全问题目前尚未得到充分研究。在这项工作中，我们首次研究了基于LLM的代理的典型安全威胁之一，即后门攻击。我们首先制定了代理后门攻击的一般框架，然后对不同形式的代理后门攻击进行了全面分析。具体而言，与传统的LLM后门攻击只能操控用户输入和模型输出相比，代理后门攻击表现出更多样化和隐蔽的形式：（1）从最终攻击结果的角度来看，代理后门攻击者不仅可以选择操控最终输出分布，还可以仅在中间推理步骤中引入恶意行为，同时保持最终输出正确。（2）此外，前一类可以根据触发器位置分为两个子类，其中后门触发器可以隐藏在用户查询中或出现在外部环境返回的中间观察中。我们在包括网络购物和工具利用在内的两个典型代理任务上实现了上述代理后门攻击的变体。大量实验表明，基于LLM的代理在后门攻击中遭受严重损害，且当前的文本后门防御算法无法轻易缓解这种后门漏洞。这表明迫切需要进一步研究针对基于LLM的代理的后门攻击的定向防御的开发。警告：本文可能包含偏见内容。"
}
{
  "title": "Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads",
  "title_zh": "标题：我会对你撒谎吗？使用直接偏好头对语言模型进行推理时间对齐",
  "abstract": "Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context learning capabilities; however, their behaviors are often difficult to control. By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible to fine-tune unsupervised LMs to follow instructions and produce outputs that reflect human preferences. Despite its benefits, RLHF has been shown to potentially harm a language model's reasoning capabilities and introduce artifacts such as hallucinations where the model may fabricate facts. To address this issue we introduce Direct Preference Heads (DPH), a fine-tuning framework that enables LMs to learn human preference signals through an auxiliary reward head without directly affecting the output distribution of the language modeling head. We perform a theoretical analysis of our objective function and find strong ties to Conservative Direct Preference Optimization (cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All evaluation suite and demonstrate that our method produces models which achieve higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) alone.",
  "abstract_zh": "摘要：预训练语言模型（LMs）表现出强大的零样本和上下文学习能力；然而，其行为往往难以控制。通过利用来自人类反馈的强化学习（RLHF），可以微调无监督的语言模型以遵循指令并生成反映人类偏好的输出。尽管有其优点，RLHF已被证明可能损害语言模型的推理能力，并引入诸如幻觉等伪造事实的现象。为了解决这个问题，我们引入了直接偏好头（DPH），这是一种微调框架，使语言模型能够通过辅助奖励头学习人类偏好信号，而不直接影响语言建模头的输出分布。我们对目标函数进行了理论分析，发现其与保守直接偏好优化（cDPO）有很强的联系。最后，我们在GLUE、RACE和GPT4All评估套件上评估了我们的模型，证明我们的方法能够生成比仅通过监督微调（SFT）或直接偏好优化（DPO）微调的模型获得更高分数的模型。"
}
{
  "title": "AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning",
  "title_zh": "标题：AvaTaR：通过对比推理优化LLM代理的工具使用",
  "abstract": "Large language model (LLM) agents have demonstrated impressive capabilities in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing prompting techniques that enable LLM agents to effectively use these tools and knowledge remains a heuristic and labor-intensive task. Here, we introduce AvaTaR, a novel and automated framework that optimizes an LLM agent to effectively leverage provided tools, improving performance on a given task. During optimization, we design a comparator module to iteratively deliver insightful and comprehensive prompts to the LLM agent by contrastively reasoning between positive and negative examples sampled from training data. We demon- strate AvaTaR on four complex multimodal retrieval datasets featuring textual, visual, and relational information, and three general question-answering (QA) datasets. We find AvaTaR consistently outperforms state-of-the-art approaches across all seven tasks, exhibiting strong generalization ability when applied to novel cases and achieving an average relative improvement of 14% on the Hit@1 metric for the retrieval datasets and 13% for the QA datasets. Code and dataset are available at https://github.com/zou-group/avatar.",
  "abstract_zh": "摘要：大型语言模型（LLM）代理在利用外部工具和知识提高准确性和减少幻觉方面表现出色。然而，开发能够使LLM代理有效使用这些工具和知识的提示技术仍然是一项启发式且劳动密集的任务。在此，我们介绍AvaTaR，这是一种新颖且自动化的框架，能够优化LLM代理以有效利用提供的工具，从而提高特定任务的性能。在优化过程中，我们设计了一个比较器模块，通过对训练数据中采样的正负示例进行对比推理，迭代地向LLM代理提供有见地且全面的提示。我们在四个复杂的多模态检索数据集（包含文本、视觉和关系信息）以及三个通用问答（QA）数据集上展示了AvaTaR。我们发现，AvaTaR在所有七个任务中始终优于最先进的方法，在应用于新案例时表现出强大的泛化能力，并在检索数据集的Hit@1指标上平均相对提高14%，在QA数据集上提高13%。代码和数据集可在https://github.com/zou-group/avatar获取。"
}
{
  "title": "SlimGPT: Layer-wise Structured Pruning for Large Language Models",
  "title_zh": "Title: SlimGPT：大规模语言模型的逐层结构化剪枝",
  "abstract": "Large language models (LLMs) have garnered significant attention for their remarkable capabilities across various domains, whose vast parameter scales present challenges for practical deployment. Structured pruning is an effective method to balance model performance with efficiency, but performance restoration under computational resource constraints is a principal challenge in pruning LLMs. Therefore, we present a low-cost and fast structured pruning method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We propose Batched Greedy Pruning for rapid and near-optimal pruning, which enhances the accuracy of head-wise pruning error estimation through grouped Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic Group Size, thereby achieving approximate local optimal pruning results within one hour. Besides, we explore the limitations of layer-wise pruning from the perspective of error accumulation and propose Incremental Pruning Ratio, a non-uniform pruning strategy to reduce performance degradation. Experimental results on the LLaMA benchmark show that SlimGPT outperforms other methods and achieves state-of-the-art results.",
  "abstract_zh": "Abstract: 大规模语言模型（LLMs）因其在各个领域的卓越能力而受到广泛关注，但其庞大的参数规模对实际部署提出了挑战。结构化剪枝是一种在模型性能与效率之间取得平衡的有效方法，但在计算资源受限的情况下恢复性能是剪枝LLMs的主要挑战。因此，我们提出了一种基于最优脑外科框架的低成本快速结构化剪枝方法SlimGPT。我们提出了批量贪婪剪枝以实现快速且近似最优的剪枝，通过分组Cholesky分解提高了头部剪枝误差估计的准确性，并通过动态组大小提高了FFN的剪枝效率，从而在一小时内实现近似局部最优的剪枝结果。此外，我们从误差累积的角度探索了逐层剪枝的局限性，并提出了递增剪枝率，一种非均匀剪枝策略，以减少性能下降。在LLaMA基准上的实验结果表明，SlimGPT优于其他方法并达到了最新的研究成果。"
}
{
  "title": "A Concept-Based Explainability Framework for Large Multimodal Models",
  "title_zh": "多模态大模型的基于概念的可解释性框架",
  "abstract": "Large multimodal models (LMMs) combine unimodal encoders and large language models (LLMs) to perform multimodal tasks. Despite recent advancements towards the interpretability of these models, understanding internal representations of LMMs remains largely a mystery. In this paper, we present a novel framework for the interpretation of LMMs. We propose a dictionary learning based approach, applied to the representation of tokens. The elements of the learned dictionary correspond to our proposed concepts. We show that these concepts are well semantically grounded in both vision and text. Thus we refer to these as ``multi-modal concepts''. \nWe qualitatively and quantitatively evaluate the results of the learnt concepts. We show that the extracted multimodal concepts are useful to interpret representations of test samples. Finally, we evaluate the disentanglement between different concepts and the quality of grounding concepts visually and textually. Our implementation is publicly available: https://github.com/mshukor/xl-vlms.",
  "abstract_zh": "大型多模态模型（LMMs）结合了单模态编码器和大型语言模型（LLMs）以执行多模态任务。尽管在这些模型的可解释性方面取得了最新进展，但理解LMMs的内部表示仍然是一个谜。在本文中，我们提出了一种新颖的LMMs解释框架。我们提出了一种基于字典学习的方法，应用于标记的表示。学习到的字典元素对应于我们提出的概念。我们展示了这些概念在视觉和文本中都有良好的语义基础。因此，我们将这些称为“多模态概念”。我们对学习到的概念的结果进行了定性和定量评估。我们表明，提取的多模态概念对于解释测试样本的表示是有用的。最后，我们评估了不同概念之间的解耦以及概念在视觉和文本上的基础质量。我们的实现是公开可用的：https://github.com/mshukor/xl-vlms。"
}
{
  "title": "On the Worst Prompt Performance of Large Language Models",
  "title_zh": "大型语言模型最差提示性能研究",
  "abstract": "The performance of large language models (LLMs) is acutely sensitive to the phrasing of prompts, which raises significant concerns about their reliability in real-world scenarios. Existing studies often divide prompts into task-level instructions and case-level inputs and primarily focus on evaluating and improving robustness against variations in tasks-level instructions. However, this setup fails to fully address the diversity of real-world user queries and assumes the existence of task-specific datasets. To address these limitations, we introduce RobustAlpacaEval, a new benchmark that consists of semantically equivalent case-level queries and emphasizes the importance of using the worst prompt performance to gauge the lower bound of model performance. Extensive experiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the Llama, Mistral, and Gemma families uncover substantial variability in model performance; for instance, a difference of 45.48% between the worst and best performance for the Llama-2-70B-chat model, with its worst performance dipping as low as 9.38%. We further illustrate the difficulty in identifying the worst prompt from both model-agnostic and model-dependent perspectives, emphasizing the absence of a shortcut to characterize the worst prompt. We also attempt to enhance the worst prompt performance using existing prompt engineering and prompt consistency methods, but find that their impact is limited. These findings underscore the need to create more resilient LLMs that can maintain high performance across diverse prompts.",
  "abstract_zh": "大型语言模型（LLM）的性能对提示的措辞极为敏感，这在现实场景中引发了对其可靠性的重大担忧。现有研究通常将提示分为任务级指令和案例级输入，主要关注于评估和提高对任务级指令变化的鲁棒性。然而，这种设置未能充分解决现实世界用户查询的多样性，并假设存在特定任务的数据集。为了解决这些限制，我们引入了RobustAlpacaEval，一个由语义等价的案例级查询组成的新基准，强调使用最差提示性能来衡量模型性能下限的重要性。在ChatGPT和来自Llama、Mistral和Gemma家族的六个开源LLM上进行的广泛实验揭示了模型性能的显著差异；例如，Llama-2-70B-chat模型的最差和最佳性能之间的差异达到45.48%，其最差性能低至9.38%。我们进一步从模型无关和模型依赖的角度说明了识别最差提示的困难，强调了没有捷径来表征最差提示。我们还尝试使用现有的提示工程和提示一致性方法来提高最差提示性能，但发现其影响有限。这些发现强调了创建能够在各种提示中保持高性能的更具弹性的LLM的必要性。"
}
{
  "title": "BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models",
  "title_zh": "标题：BLoB：通过反向传播实现大语言模型的贝叶斯低秩适应",
  "abstract": "Large Language Models (LLMs) often suffer from overconfidence during inference, particularly when adapted to downstream domain-specific tasks with limited data. Previous work addresses this issue by employing approximate Bayesian estimation after the LLMs are trained, enabling them to quantify uncertainty. However, such post-training approaches' performance is severely limited by the parameters learned during training. In this paper, we go beyond post-training Bayesianization and propose Bayesian Low-Rank Adaptation by Backpropagation (BLoB), an algorithm that continuously and jointly adjusts both the mean and covariance of LLM parameters throughout the whole fine-tuning process. Our empirical results verify the effectiveness of BLoB in terms of generalization and uncertainty estimation, when evaluated on both in-distribution and out-of-distribution data.",
  "abstract_zh": "摘要：大语言模型（LLMs）在推理过程中常常表现出过度自信，特别是在适应数据有限的下游领域特定任务时。之前的研究通过在LLMs训练后采用近似贝叶斯估计来解决这个问题，使其能够量化不确定性。然而，这种训练后方法的性能受到训练期间学习到的参数的严重限制。在本文中，我们超越了训练后贝叶斯化，提出了通过反向传播实现贝叶斯低秩适应（BLoB）的算法，该算法在整个微调过程中持续并联合调整LLM参数的均值和协方差。我们的实验证明了BLoB在泛化和不确定性估计方面的有效性，并在分布内和分布外数据上进行了评估。"
}
{
  "title": "SLTrain: a sparse plus low rank approach for parameter and memory efficient pretraining",
  "title_zh": "标题：SLTrain：一种参数和内存高效预训练的稀疏加低秩方法",
  "abstract": "Large language models (LLMs) have shown impressive capabilities across various tasks. However, training LLMs from scratch requires significant computational power and extensive memory capacity. Recent studies have explored low-rank structures on weights for efficient fine-tuning in terms of parameters and memory, either through low-rank adaptation or factorization. While effective for fine-tuning, low-rank structures are generally less suitable for pretraining because they restrict parameters to a low-dimensional subspace. In this work, we propose to parameterize the weights as a sum of low-rank and sparse matrices for pretraining, which we call SLTrain. The low-rank component is learned via matrix factorization, while for the sparse component, we employ a simple strategy of uniformly selecting the sparsity support at random and learning only the non-zero entries with the fixed support. While being simple, the random fixed-support sparse learning strategy significantly enhances pretraining when combined with low-rank learning. Our results show that SLTrain adds minimal extra parameters and memory costs compared to pretraining with low-rank parameterization, yet achieves substantially better performance, which is comparable to full-rank training. Remarkably, when combined with quantization and per-layer updates, SLTrain can reduce memory requirements by up to 73% when pretraining the LLaMA 7B model.",
  "abstract_zh": "摘要：大型语言模型（LLMs）在各种任务中表现出色。然而，从头开始训练LLMs需要显著的计算能力和大量的内存容量。最近的研究探索了权重的低秩结构，以在参数和内存方面实现高效微调，无论是通过低秩适应还是分解。虽然低秩结构在微调中有效，但在预训练中通常不太适用，因为它们将参数限制在一个低维子空间中。在这项工作中，我们提出将权重参数化为低秩和稀疏矩阵之和进行预训练，我们称之为SLTrain。低秩部分通过矩阵分解学习，而对于稀疏部分，我们采用了一种简单的策略，即随机均匀选择稀疏支持，并仅学习具有固定支持的非零条目。虽然简单，但随机固定支持的稀疏学习策略在与低秩学习结合时显著增强了预训练。我们的结果表明，与低秩参数化预训练相比，SLTrain增加的额外参数和内存成本极少，但性能显著提高，可与全秩训练相媲美。值得注意的是，当结合量化和逐层更新时，SLTrain在预训练LLaMA 7B模型时可将内存需求减少多达73%。"
}
{
  "title": "Structured Unrestricted-Rank Matrices for Parameter Efficient Finetuning",
  "title_zh": "参数高效微调的结构化无限制秩矩阵",
  "abstract": "Recent efforts to scale Transformer models have demonstrated rapid progress across a wide range of tasks (Wei at. al 2022). However, fine-tuning these models for downstream tasks is quite expensive due to their large parameter counts. Parameter-efficient fine-tuning (PEFT) approaches have emerged as a viable alternative, allowing us to fine-tune models by updating only a small number of parameters.\n In this work, we propose a general framework for parameter efficient fine-tuning (PEFT), based on *structured unrestricted-rank matrices* (SURM)  which can serve as a drop-in replacement for popular approaches such as Adapters and LoRA. Unlike other methods like LoRA, SURMs give us more flexibility in finding the right balance between compactness and expressiveness. This is achieved by using *low displacement rank matrices* (LDRMs), which hasn't been used in this context before. SURMs remain competitive with baselines, often providing significant quality improvements while using a smaller parameter budget. SURMs achieve: **5**-**7**% accuracy gains on various image classification tasks while replacing low-rank matrices in LoRA and: up to **12x** reduction of the number of parameters in adapters (with virtually no loss in quality) on the GLUE benchmark.",
  "abstract_zh": "最近在扩展Transformer模型方面的努力展示了在广泛任务上的快速进展（Wei等人，2022）。然而，由于这些模型的参数数量庞大，为下游任务进行微调的成本相当高。参数高效微调（PEFT）方法作为一种可行的替代方案出现，使我们可以通过仅更新少量参数来微调模型。在这项工作中，我们提出了一种基于*结构化无限制秩矩阵*（SURM）的参数高效微调（PEFT）通用框架，可以作为流行方法如Adapters和LoRA的替代品。与LoRA等其他方法不同，SURM为我们在紧凑性和表达性之间找到合适的平衡提供了更多的灵活性。这是通过使用*低位移秩矩阵*（LDRM）实现的，这在此上下文中尚未使用过。SURM与基线保持竞争力，通常在使用更小参数预算的情况下提供显著的质量改进。SURM在各种图像分类任务中实现了**5**-**7**%的准确率提升，同时替换了LoRA中的低秩矩阵；在GLUE基准测试中，适配器的参数数量减少了最多**12倍**（几乎没有质量损失）。"
}
{
  "title": "Probing Social Bias in Labor Market Text Generation by ChatGPT: A Masked Language Model Approach",
  "title_zh": "题目：通过ChatGPT探测劳动力市场文本生成中的社会偏见：一种掩码语言模型方法",
  "abstract": "As generative large language models (LLMs) such as ChatGPT gain widespread adoption in various domains, their potential to propagate and amplify social biases, particularly in high-stakes areas such as the labor market, has become a pressing concern. AI algorithms are not only widely used in the selection of job applicants, individual job seekers may also make use of generative LLMs to help develop their job application materials. Against this backdrop, this research builds on a novel experimental design to examine social biases within ChatGPT-generated job applications in response to real job advertisements. By simulating the process of job application creation, we examine the language patterns and biases that emerge when the model is prompted with diverse job postings. Notably, we present a novel bias evaluation framework based on Masked Language Models to quantitatively assess social bias based on validated inventories of social cues/words, enabling a systematic analysis of the language used. Our findings show that the increasing adoption of generative AI, not only by employers but also increasingly by individual job seekers, can reinforce and exacerbate gender and social inequalities in the labor market through the use of biased and gendered language.",
  "abstract_zh": "摘要：随着像ChatGPT这样的生成大型语言模型（LLMs）在各个领域被广泛采用，它们传播和放大社会偏见的潜力，特别是在劳动力市场等高风险领域，已成为一个紧迫的问题。人工智能算法不仅广泛用于求职者的筛选，个体求职者也可能利用生成LLMs来帮助开发他们的求职材料。在此背景下，本研究基于一种新颖的实验设计，检查ChatGPT生成的求职申请中对真实职位广告的社会偏见。通过模拟求职申请的创建过程，我们检查了在模型被提示不同职位发布时出现的语言模式和偏见。值得注意的是，我们提出了一种基于掩码语言模型的新型偏见评估框架，以经过验证的社会线索/词汇清单为基础，定量评估社会偏见，从而系统分析所使用的语言。我们的研究结果表明，生成AI的日益普及，不仅被雇主使用，而且越来越多地被个体求职者使用，通过使用带有偏见和性别化的语言，可以在劳动力市场中加剧和加重性别和社会不平等。"
}
{
  "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention",
  "title_zh": "标题: 使用跨层注意力减少Transformer键值缓存大小",
  "abstract": "Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. In this paper, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, we find that it is possible to reduce the size of the KV cache by another $2\\times$ while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, potentially enabling future models to operate at longer sequence lengths and larger batch sizes than would otherwise be possible.",
  "abstract_zh": "摘要: 键值（KV）缓存在加速基于Transformer的自回归大型语言模型（LLMs）的解码中起着至关重要的作用。然而，在长序列长度和大批量大小下，存储KV缓存所需的内存量可能会变得难以承受。自Transformer发明以来，减少KV缓存大小的两个最有效的干预措施是多查询注意力（MQA）及其推广形式分组查询注意力（GQA）。MQA和GQA都修改了注意力模块的设计，使多个查询头可以共享一个键/值头，从而在仅对精度产生最小影响的情况下，大幅减少不同键/值头的数量。在本文中，我们展示了可以通过在相邻层之间共享键和值头，将多查询注意力更进一步，提出一种新的注意力设计，称为跨层注意力（CLA）。通过CLA，我们发现可以在保持几乎与未修改的MQA相同的精度的情况下，将KV缓存的大小再减少2倍。在从头训练1B和3B参数模型的实验中，我们证明了CLA在传统MQA可能实现的内存/精度权衡中提供了帕累托改进，可能使未来的模型能够在比否则可能的更长序列长度和更大批量大小下运行。"
}
{
  "title": "Estimating the Hallucination Rate of Generative AI",
  "title_zh": "标题：估计生成式人工智能的幻觉率",
  "abstract": "This paper presents a method for estimating the hallucination rate for in-context learning (ICL) with generative AI. In ICL, a conditional generative model (CGM) is prompted with a dataset and a prediction question and asked to generate a response. One interpretation of ICL assumes that the CGM computes the posterior predictive of an unknown Bayesian model, which implicitly defines a joint distribution over observable datasets and latent mechanisms. This joint distribution factorizes into two components: the model prior over mechanisms and the model likelihood of datasets given a mechanism. With this perspective, we define a \\textit{hallucination} as a generated response to the prediction question with low model likelihood given the mechanism. We develop a new method that takes an ICL problem and estimates the probability that a CGM will generate a hallucination. Our method only requires generating prediction questions and responses from the CGM and evaluating its response log probability. We empirically evaluate our method using large language models for synthetic regression and natural language ICL tasks.",
  "abstract_zh": "摘要：本文提出了一种估计生成式人工智能中情境学习（ICL）幻觉率的方法。在ICL中，条件生成模型（CGM）通过数据集和预测问题进行提示，并生成响应。ICL的一种解释假设CGM计算一个未知贝叶斯模型的后验预测，该模型隐式地定义了可观测数据集和潜在机制的联合分布。该联合分布分解为两个组成部分：机制上的模型先验和给定机制的数据集的模型似然性。从这个角度出发，我们将\\textit{幻觉}定义为在给定机制下模型似然性低的预测问题生成响应。我们开发了一种新方法，该方法处理ICL问题并估计CGM生成幻觉的概率。我们的方法仅需生成CGM的预测问题和响应，并评估其响应的对数概率。我们通过使用大型语言模型对合成回归和自然语言ICL任务进行实证评估。"
}
{
  "title": "Chain of Agents: Large Language Models Collaborating on Long-Context Tasks",
  "title_zh": "代理链：大型语言模型在长上下文任务中的协作",
  "abstract": "Addressing the challenge of effectively processing long contexts has become a critical issue for Large Language Models (LLMs). Two common strategies have emerged: 1) reducing the input length, such as retrieving relevant chunks by Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit of LLMs. However, both strategies have drawbacks: input reduction has no guarantee of covering the part with needed information, while window extension struggles with focusing on the pertinent information for solving the task. To mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework that harnesses multi-agent collaboration through natural language to enable information aggregation and context reasoning across various LLMs over long-context tasks. CoA consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text, followed by a manager agent who synthesizes these contributions into a coherent final output. CoA processes the entire input by interleaving reading and reasoning, and it mitigates long context focus issues by assigning each agent a short context. We perform a comprehensive evaluation of CoA on a wide range of long-context tasks in question answering, summarization, and code completion, demonstrating significant improvements by up to 10% over strong baselines of RAG, Full-Context, and multi-agent LLMs.",
  "abstract_zh": "解决有效处理长上下文的问题已成为大型语言模型（LLMs）的关键问题。两种常见策略已经出现：1）减少输入长度，例如通过检索增强生成（RAG）检索相关片段，2）扩展LLMs的上下文窗口限制。然而，两种策略都有缺陷：输入减少不能保证覆盖所需信息的部分，而窗口扩展难以专注于解决任务所需的相关信息。为缓解这些限制，我们提出了代理链（CoA），这是一种新颖的框架，通过自然语言实现多代理协作，以便在长上下文任务中跨各种LLMs进行信息聚合和上下文推理。CoA由多个工作代理组成，这些代理依次通信以处理文本的不同分段部分，随后由一个管理代理将这些贡献综合成一个连贯的最终输出。CoA通过交替阅读和推理来处理整个输入，并通过为每个代理分配一个短上下文来缓解长上下文聚焦问题。我们对CoA在问答、摘要和代码补全的广泛长上下文任务中进行了全面评估，显示出比RAG、全上下文和多代理LLMs的强基线高达10%的显著改进。"
}
{
  "title": "LLM-Check: Investigating Detection of Hallucinations in Large Language Models",
  "title_zh": "标题：LLM-Check：调查大型语言模型中的幻觉检测",
  "abstract": "While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations— outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance. In this paper, we conduct a comprehensive investigation into the nature of hallucinations within LLMs and furthermore explore effective techniques for detecting such inaccuracies in various real-world settings. Prior approaches to detect hallucinations in LLM outputs, such as consistency checks or retrieval-based methods, typically assume access to multiple model responses or large databases. These techniques, however, tend to be computationally expensive in practice, thereby limiting their applicability to real-time analysis. In contrast, in this work, we seek to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM. In addition, we also study hallucination detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG). We demonstrate that the proposed detection methods are extremely compute-efficient, with speedups of up to 45x and 450x over other baselines, while achieving significant improvements in detection performance over diverse datasets.",
  "abstract_zh": "摘要：尽管大型语言模型（LLM）因其在广泛任务上的出色表现而广受欢迎，但这些模型容易产生幻觉——即输出内容虽然乍看之下似乎合理或可信，但实际上是错误或虚构的。在本文中，我们对LLM中的幻觉性质进行了全面调查，并进一步探索了在各种现实环境中检测此类不准确性的有效技术。先前检测LLM输出幻觉的方法，如一致性检查或基于检索的方法，通常假设可以访问多个模型响应或大型数据库。然而，这些技术在实际应用中往往计算成本高，从而限制了其在实时分析中的适用性。相比之下，在这项工作中，我们通过分析辅助LLM的内部隐藏状态、注意力图和输出预测概率，试图在白盒和黑盒环境中识别单个响应中的幻觉。此外，我们还研究了在存在真实参考的情况下检测幻觉的方法，例如在检索增强生成（RAG）环境中。我们证明了所提出的检测方法在计算效率上极高，相较于其他基线实现了高达45倍和450倍的加速，同时在多样化数据集上显著提高了检测性能。"
}
{
  "title": "Semantic Density: Uncertainty Quantification for Large Language Models through Confidence Measurement in Semantic Space",
  "title_zh": "语义密度：通过语义空间中的置信度测量对大型语言模型进行不确定性量化",
  "abstract": "With the widespread application of Large Language Models (LLMs) to various domains, concerns regarding the trustworthiness of LLMs in safety-critical scenarios have been raised, due to their unpredictable tendency to hallucinate and generate misinformation. Existing LLMs do not have an inherent functionality to provide the users with an uncertainty/confidence metric for each response it generates, making it difficult to evaluate trustworthiness. Although several studies aim to develop uncertainty quantification methods for LLMs, they have fundamental limitations, such as being restricted to classification tasks, requiring additional training and data, considering only lexical instead of semantic information, and being prompt-wise but not response-wise. A new framework is proposed in this paper to address these issues. Semantic density extracts uncertainty/confidence information for each response from a probability distribution perspective in semantic space. It has no restriction on task types and is \"off-the-shelf\" for new models and tasks. Experiments on seven state-of-the-art LLMs, including the latest Llama 3 and Mixtral-8x22B models, on four free-form question-answering benchmarks demonstrate the superior performance and robustness of semantic density compared to prior approaches.",
  "abstract_zh": "随着大型语言模型（LLMs）在各个领域的广泛应用，由于其不可预测的幻觉倾向和生成错误信息的能力，在安全关键场景中对LLMs的可信度产生了担忧。现有的LLMs没有内在功能为其生成的每个响应提供不确定性/置信度指标，使得评估可信度变得困难。尽管一些研究旨在为LLMs开发不确定性量化方法，但它们存在基本限制，例如仅限于分类任务、需要额外的训练和数据、仅考虑词汇而非语义信息，以及仅限于提示而非响应。本文提出了一个新的框架来解决这些问题。语义密度从语义空间中的概率分布角度提取每个响应的不确定性/置信度信息。它对任务类型没有限制，并且对新模型和任务是“现成可用”的。在包括最新的Llama 3和Mixtral-8x22B模型在内的七个最先进的LLMs上进行的四个自由形式问答基准测试实验表明，与之前的方法相比，语义密度具有优越的性能和稳健性。"
}
{
  "title": "Injecting Undetectable Backdoors in Obfuscated Neural Networks and Language Models",
  "title_zh": "标题：在混淆的神经网络和语言模型中注入不可检测的后门",
  "abstract": "As ML models become increasingly complex and integral to high-stakes domains such as finance and healthcare, they also become more susceptible to sophisticated adversarial attacks. We investigate the threat posed by undetectable backdoors, as defined in Goldwasser et al. [2022], in models developed by insidious external expert firms. When such backdoors exist, they allow the designer of the model to sell information on how to slightly perturb their input to change the outcome of the model. We develop a general strategy to plant backdoors to obfuscated neural networks, that satisfy the security properties of the celebrated notion of indistinguishability obfuscation. Applying obfuscation before releasing neural networks is a strategy that is well motivated to protect sensitive information of the external expert firm. Our method to plant backdoors ensures that even if the weights and architecture of the obfuscated model are accessible, the existence of\nthe backdoor is still undetectable. Finally, we introduce the notion of undetectable backdoors to language models and extend our neural network backdoor attacks to such models based on the existence of steganographic functions.",
  "abstract_zh": "摘要：随着机器学习模型变得越来越复杂，并在金融和医疗等高风险领域中发挥重要作用，它们也变得更容易受到复杂的对抗性攻击。我们研究了由Goldwasser等人[2022]定义的不可检测后门在由阴险的外部专家公司开发的模型中构成的威胁。当这些后门存在时，它们允许模型的设计者出售有关如何轻微扰动输入以改变模型结果的信息。我们开发了一种通用策略，将后门植入混淆的神经网络中，满足著名的不可区分混淆概念的安全属性。在发布神经网络之前应用混淆是一种很有动机的策略，可以保护外部专家公司的敏感信息。我们植入后门的方法确保即使混淆模型的权重和架构是可访问的，后门的存在仍然是不可检测的。最后，我们引入了语言模型中不可检测后门的概念，并基于隐写函数的存在，将我们的神经网络后门攻击扩展到这些模型。"
}
{
  "title": "Noise Contrastive Alignment of Language Models with Explicit Rewards",
  "title_zh": "标题：具有显式奖励的语言模型噪声对比对齐",
  "abstract": "User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By comparing NCA and InfoNCA, we demonstrate that the well-observed decreasing-likelihood trend of DPO/InfoNCA is caused by their focus on adjusting relative likelihood across different responses.\nIn contrast, NCA optimizes the absolute likelihood for each response, thereby effectively preventing the chosen likelihood from decreasing. We evaluate our methods in both reward and preference settings with Mistral-8$\\times$7B and 7B models. Experiments suggest that InfoNCA/NCA surpasses various preference baselines when reward datasets are available. We also find NCA significantly outperforms DPO in complex reasoning tasks like math and coding.",
  "abstract_zh": "摘要：用户意图通常被形式化为评估奖励，以在微调语言模型（LMs）时最大化。现有的对齐方法，如直接偏好优化（DPO），主要针对成对偏好数据，其中奖励是隐式定义的，而不是显式给出的。在本文中，我们引入了一种通用的LM对齐框架，利用噪声对比估计（NCE）来弥合处理显式标注标量评估的奖励数据集的差距。我们的框架包括两个并行算法，NCA和InfoNCA，均能直接从奖励数据和偏好数据中提取LM策略。值得注意的是，我们表明在成对偏好设置下，DPO损失是我们提出的InfoNCA目标的一个特例，从而整合并扩展了当前的对齐理论。通过比较NCA和InfoNCA，我们证明DPO/InfoNCA的观察到的似然性下降趋势是由于它们专注于调整不同响应的相对似然性。相比之下，NCA优化每个响应的绝对似然性，从而有效防止选择的似然性下降。我们在Mistral-8$\\times$7B和7B模型的奖励和偏好设置中评估了我们的方法。实验表明，当奖励数据集可用时，InfoNCA/NCA优于各种偏好基线。我们还发现，在复杂推理任务（如数学和编码）中，NCA显著优于DPO。"
}
{
  "title": "Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models",
  "title_zh": "标题：隐私后门：通过污染预训练模型增强成员推断",
  "abstract": "It is commonplace to produce application-specific models by fine-tuning large pre-trained models using a small bespoke dataset. The widespread availability of foundation model checkpoints on the web poses considerable risks, including the vulnerability to backdoor attacks. In this paper, we unveil a new vulnerability: the privacy backdoor attack. This black-box privacy attack aims to amplify the privacy leakage that arises when fine-tuning a model: when a victim fine-tunes a backdoored model, their training data will be leaked at a significantly higher rate than if they had fine-tuned a typical model. We conduct extensive experiments on various datasets and models, including both vision-language models (CLIP) and large language models, demonstrating the broad applicability and effectiveness of such an attack. Additionally, we carry out multiple ablation studies with different fine-tuning methods and inference strategies to thoroughly analyze this new threat. Our findings highlight a critical privacy concern within the machine learning community and call for a re-evaluation of safety protocols in the use of open-source pre-trained models.",
  "abstract_zh": "摘要：通过使用小型定制数据集微调大型预训练模型来生成特定应用的模型已成为常态。网络上广泛可用的基础模型检查点带来了相当大的风险，包括易受到后门攻击的威胁。在本文中，我们揭示了一种新的漏洞：隐私后门攻击。这种黑箱隐私攻击旨在放大微调模型时产生的隐私泄露：当受害者微调一个带有后门的模型时，他们的训练数据泄露的速度将显著高于微调一个典型模型时的速度。我们在各种数据集和模型上进行了广泛的实验，包括视觉-语言模型（CLIP）和大型语言模型，展示了这种攻击的广泛适用性和有效性。此外，我们进行了多项消融研究，采用不同的微调方法和推断策略，全面分析这一新威胁。我们的研究结果突显了机器学习社区内的一个关键隐私问题，并呼吁重新评估使用开源预训练模型的安全协议。"
}
{
  "title": "Rethinking LLM Memorization through the Lens of Adversarial Compression",
  "title_zh": "重新思考通过对抗性压缩视角的大型语言模型记忆",
  "abstract": "Large language models (LLMs) trained on web-scale datasets raise substantial concerns regarding permissible data usage. \nOne major question is whether these models \"memorize\" all their training data or they integrate many data sources in some way more akin to how a human would learn and synthesize information. The answer hinges, to a large degree, on \\emph{how we define memorization.} In this work, we propose the Adversarial Compression Ratio (ACR) as a metric for assessing memorization in LLMs. A given string from the training data is considered memorized if it can be elicited by a prompt (much) shorter than the string itself---in other words, if these strings can be ``compressed'' with the model by computing adversarial prompts of fewer tokens. The ACR overcomes the limitations of existing notions of memorization by (i) offering an adversarial view of measuring memorization, especially for monitoring unlearning and compliance; and (ii) allowing for the flexibility to measure memorization for arbitrary strings at a reasonably low compute. Our definition serves as a practical tool for determining when model owners may be violating terms around data usage, providing a potential legal tool and a critical lens through which to address such scenarios.",
  "abstract_zh": "在网络规模数据集上训练的大型语言模型（LLMs）引发了关于数据使用许可的重大担忧。一个主要问题是这些模型是否“记住”了所有的训练数据，或者它们是否以某种方式整合了许多数据源，更像人类学习和综合信息的方式。答案在很大程度上取决于\\emph{我们如何定义记忆}。在这项工作中，我们提出了对抗性压缩比（ACR）作为评估LLMs记忆的指标。如果训练数据中的给定字符串可以通过比字符串本身短得多的提示来引出，则认为该字符串被记住——换句话说，如果这些字符串可以通过计算更少标记的对抗性提示来“压缩”模型。ACR克服了现有记忆概念的局限性，(i) 提供了一种对抗性视角来衡量记忆，特别是用于监控遗忘和合规性；(ii) 允许灵活地以合理的计算量衡量任意字符串的记忆。我们的定义作为一种实用工具，用于确定模型所有者何时可能违反数据使用条款，提供了一种潜在的法律工具和一个关键视角来解决此类情景。"
}
{
  "title": "Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models",
  "title_zh": "标题: Shadowcast: 针对视觉-语言模型的隐蔽数据投毒攻击",
  "abstract": "Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, but their versatility raises security concerns. This study takes the first step in exposing VLMs’ susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is a traditional Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is a novel Persuasion Attack, leveraging VLMs’ text generation capabilities to craft persuasive and seemingly rational narratives for misinformation, such as portraying junk food as healthy. We show that Shadowcast effectively achieves the attacker’s intentions using as few as 50 poison samples. Crucially, the poisoned samples demonstrate transferability across different VLM architectures, posing a significant concern in black-box settings. Moreover, Shadowcast remains potent under realistic conditions involving various text prompts, training data augmentation, and image compression techniques. This work reveals how poisoned VLMs can disseminate convincing yet deceptive misinformation to everyday, benign users, emphasizing the importance of data integrity for responsible VLM deployments. Our code is available at: https://github.com/umd-huang-lab/VLM-Poisoning.",
  "abstract_zh": "摘要: 视觉-语言模型（VLMs）在从视觉输入生成文本响应方面表现出色，但其多功能性引发了安全担忧。本研究首次揭示了VLMs易受数据投毒攻击的弱点，这些攻击可以操控对无害日常提示的响应。我们引入了Shadowcast，一种隐蔽的数据投毒攻击，其中毒样本在视觉上与匹配文本的良性图像无法区分。Shadowcast在两种攻击类型中表现出有效性。第一种是传统的标签攻击，欺骗VLMs错误识别类别标签，例如将唐纳德·特朗普误认为乔·拜登。第二种是新颖的说服攻击，利用VLMs的文本生成能力为错误信息编写有说服力且看似合理的叙述，例如将垃圾食品描述为健康食品。我们展示了Shadowcast仅使用50个毒样本就能有效实现攻击者的意图。关键是，这些毒样本在不同的VLM架构中表现出可转移性，在黑箱环境中构成了重大威胁。此外，Shadowcast在涉及各种文本提示、训练数据增强和图像压缩技术的现实条件下仍然保持强大。这项工作揭示了中毒的VLMs如何向日常良性用户传播令人信服但具有欺骗性的信息，强调了数据完整性对于负责任的VLM部署的重要性。我们的代码可在以下网址获取：https://github.com/umd-huang-lab/VLM-Poisoning。"
}
{
  "title": "Open LLMs are Necessary for Current Private Adaptations and Outperform their Closed Alternatives",
  "title_zh": "标题：开放的大型语言模型对于当前的私有适配是必要的，并且优于其封闭的替代方案",
  "abstract": "While open Large Language Models (LLMs) have made significant progress, they still fall short of matching the performance of their closed, proprietary counterparts, making the latter attractive even for the use on highly *private* data. \nRecently, various new methods have been proposed to adapt closed LLMs to private data without leaking private information to third parties and/or the LLM provider. \nIn this work, we analyze the privacy protection and performance of the four most recent methods for private adaptation of closed LLMs. \nBy examining their threat models and thoroughly comparing their performance under different privacy levels according to differential privacy (DP), various LLM architectures, and multiple datasets for classification and generation tasks, we find that: (1) all the methods leak query data, i.e., the (potentially sensitive) user data that is queried at inference time, to the LLM provider, (2) three out of four methods also leak large fractions of private training data to the LLM provider while the method that protects private data requires a local open LLM, (3) all the methods exhibit lower performance compared to three private gradient-based adaptation methods for *local open LLMs*, and (4) the private adaptation methods for closed LLMs incur higher monetary training and query costs than running the alternative methods on local open LLMs.\nThis yields the conclusion that, to achieve truly *privacy-preserving LLM adaptations* that yield high performance and more privacy at lower costs, taking into account current methods and models, one should use open LLMs.",
  "abstract_zh": "摘要：尽管开放的大型语言模型（LLMs）取得了显著进展，但它们仍未能达到封闭的、专有模型的性能，这使得后者在使用高度*私密*数据时具有吸引力。最近，提出了多种新方法来适配封闭的LLMs以处理私密数据，而不将私密信息泄露给第三方和/或LLM提供商。在这项工作中，我们分析了最近四种封闭LLMs私有适配方法的隐私保护和性能。通过检查它们的威胁模型，并根据差分隐私（DP）、各种LLM架构以及用于分类和生成任务的多个数据集，在不同隐私级别下彻底比较它们的性能，我们发现：（1）所有方法都会将查询数据，即推理时查询的（可能敏感的）用户数据泄露给LLM提供商，（2）四种方法中有三种还会将大量私有训练数据泄露给LLM提供商，而保护私有数据的方法需要一个本地开放的LLM，（3）所有方法的性能都低于三种针对*本地开放LLMs*的私有梯度适配方法，（4）封闭LLMs的私有适配方法在训练和查询成本上高于在本地开放LLMs上运行的替代方法。这得出的结论是，为了实现真正的*隐私保护LLM适配*，在当前方法和模型的基础上，使用开放的LLMs可以在更低的成本下实现更高的性能和更多的隐私。"
}
{
  "title": "Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models",
  "title_zh": "标题: Meteor: 基于Mamba的语言和视觉大模型推理遍历",
  "abstract": "The rapid development of large language and vision models (LLVMs) has been driven by advances in visual instruction tuning. Recently, open-source LLVMs have curated high-quality visual instruction tuning datasets and utilized additional vision encoders or multiple computer vision models in order to narrow the performance gap with powerful closed-source LLVMs. These advancements are attributed to multifaceted information required for diverse capabilities, including fundamental image understanding, real-world knowledge about common-sense and non-object concepts (e.g., charts, diagrams, symbols, signs, and math problems), and step-by-step procedures for solving complex questions. Drawing from the multifaceted information, we present a new efficient LLVM, Mamba-based traversal of rationales (Meteor), which leverages multifaceted rationale to enhance understanding and answering capabilities. To embed lengthy rationales containing abundant information, we employ the Mamba architecture, capable of processing sequential data with linear time complexity. We introduce a new concept of traversal of rationale that facilitates efficient embedding of rationale. Subsequently, the backbone multimodal language model (MLM) is trained to generate answers with the aid of rationale. Through these steps, Meteor achieves significant improvements in vision language performances across multiple evaluation benchmarks requiring diverse capabilities, without scaling up the model size or employing additional vision encoders and computer vision models.",
  "abstract_zh": "摘要: 大型语言和视觉模型（LLVMs）的快速发展得益于视觉指令微调的进步。最近，开源LLVMs精心策划了高质量的视觉指令微调数据集，并利用额外的视觉编码器或多个计算机视觉模型，以缩小与强大的闭源LLVMs的性能差距。这些进步归因于多方面信息的需求，以实现多样化的能力，包括基本的图像理解、关于常识和非物体概念（如图表、图示、符号、标志和数学问题）的现实世界知识，以及解决复杂问题的逐步程序。基于多方面信息，我们提出了一种新的高效LLVM，基于Mamba的推理遍历（Meteor），利用多方面的推理来增强理解和回答能力。为了嵌入包含丰富信息的冗长推理，我们采用了Mamba架构，能够以线性时间复杂度处理顺序数据。我们引入了一种新的推理遍历概念，促进了推理的高效嵌入。随后，骨干多模态语言模型（MLM）在推理的帮助下被训练以生成答案。通过这些步骤，Meteor在多个需要多样化能力的评估基准上实现了视觉语言性能的显著提升，而无需扩大模型规模或使用额外的视觉编码器和计算机视觉模型。"
}
{
  "title": "Visual Perception by Large Language Model’s Weights",
  "title_zh": "大型语言模型权重的视觉感知",
  "abstract": "Existing Multimodal Large Language Models (MLLMs) follow the paradigm that perceives visual information by aligning visual features with the input space of Large Language Models (LLMs) and concatenating visual tokens with text tokens to form a unified sequence input for LLMs. These methods demonstrate promising results on various vision-language tasks but are limited by the high computational effort due to the extended input sequence resulting from the involvement of visual tokens. In this paper, instead of input space alignment, we propose a novel parameter space alignment paradigm that represents visual information as model weights. For each input image, we use a vision encoder to extract visual features, convert features into perceptual weights, and merge the perceptual weights with LLM's weights. In this way, the input of LLM does not require visual tokens, which reduces the length of the input sequence and greatly improves efficiency. Following this paradigm, we propose VLoRA with the perceptual weights generator. The perceptual weights generator is designed to convert visual features to perceptual weights with low-rank property, exhibiting a form similar to LoRA. The experimental results show that our VLoRA achieves comparable performance on various benchmarks for MLLMs, while significantly reducing the computational costs for both training and inference. Code and models are released at \\url{https://github.com/FeipengMa6/VLoRA}.",
  "abstract_zh": "现有的多模态大型语言模型（MLLMs）遵循通过将视觉特征与大型语言模型（LLMs）的输入空间对齐，并将视觉标记与文本标记连接以形成LLMs的统一序列输入的范式。这些方法在各种视觉语言任务上展示了有前景的结果，但由于视觉标记的参与导致输入序列的延长，计算成本较高。在本文中，我们提出了一种新的参数空间对齐范式，用模型权重表示视觉信息，而不是输入空间对齐。对于每个输入图像，我们使用视觉编码器提取视觉特征，将特征转换为感知权重，并将感知权重与LLM的权重合并。通过这种方式，LLM的输入不需要视觉标记，从而减少了输入序列的长度，大大提高了效率。遵循这一范式，我们提出了具有感知权重生成器的VLoRA。感知权重生成器旨在将视觉特征转换为具有低秩特性的感知权重，表现出类似于LoRA的形式。实验结果表明，我们的VLoRA在各种MLLMs基准测试中实现了可比的性能，同时显著降低了训练和推理的计算成本。代码和模型已发布在\\url{https://github.com/FeipengMa6/VLoRA}。"
}
{
  "title": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization",
  "title_zh": "标题：ShiftAddLLM：通过后训练无乘法重参数化加速预训练大型语言模型",
  "abstract": "Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks. Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM. However, current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs. To address this, we propose accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM. Specifically, we quantize each weight matrix into binary matrices paired with group-wise scaling factors. The associated multiplications are reparameterized into (1) shifts between activations and scaling factors and (2) queries and adds according to the binary matrices. To reduce accuracy loss, we present a multi-objective optimization method to minimize both weight and output activation reparameterization errors. Additionally, based on varying sensitivity across layers to reparameterization, we develop an automated bit allocation strategy to further reduce memory usage and latency. Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity reductions of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3- and 2-bit precision, respectively, and more than 80% memory and energy reductions over the original LLMs. Codes and models are available at https://github.com/GATECH-EIC/ShiftAddLLM.",
  "abstract_zh": "摘要：大型语言模型（LLM）在语言任务中表现出色，但由于其庞大的参数和对密集乘法的依赖，在资源受限设备上部署时面临挑战，导致高内存需求和延迟瓶颈。移位加法重参数化通过在LLM的注意力和多层感知机（MLP）层中用硬件友好的原语替代昂贵的乘法，提供了一种有前景的解决方案。然而，当前的重参数化技术需要从头训练或完全参数微调以恢复准确性，这对于LLM来说资源密集。为了解决这一问题，我们提出通过后训练移位加法重参数化加速预训练的LLM，创建高效的无乘法模型，称为ShiftAddLLM。具体来说，我们将每个权重矩阵量化为与组内缩放因子配对的二进制矩阵。相关的乘法被重参数化为（1）激活和缩放因子之间的移位，以及（2）根据二进制矩阵的查询和加法。为了减少准确性损失，我们提出了一种多目标优化方法，以最小化权重和输出激活重参数化误差。此外，基于各层对重参数化的不同敏感性，我们开发了一种自动位分配策略，以进一步减少内存使用和延迟。在五个LLM家族和八个任务上的实验一致验证了ShiftAddLLM的有效性，在可比或更低延迟下，相较于最具竞争力的3位和2位精度量化LLM，平均困惑度分别减少了5.6和22.7点，并且在原始LLM的基础上实现了超过80%的内存和能量减少。代码和模型可在https://github.com/GATECH-EIC/ShiftAddLLM获得。"
}
{
  "title": "What Makes and Breaks Safety Fine-tuning? A Mechanistic Study",
  "title_zh": "标题：是什么成就和破坏了安全微调？一种机制研究",
  "abstract": "Safety fine-tuning helps align Large Language Models (LLMs) with human preferences for their safe deployment. To better understand the underlying factors that make models safe via safety fine-tuning, we design a synthetic data generation framework that captures salient aspects of an unsafe input by modeling the interaction between the task the model is asked to perform (e.g., “design”) versus the specific concepts the task is asked to be performed upon (e.g., a “cycle” vs. a “bomb”). Using this, we investigate three well-known safety fine-tuning methods—supervised safety fine-tuning, direct preference optimization, and unlearning—and provide significant evidence demonstrating that these methods minimally transform MLP weights to specifically align unsafe inputs into its weights’ null space. This yields a clustering of inputs based on whether the model deems them safe or not. Correspondingly, when an adversarial input (e.g., a jailbreak) is provided, its activations are closer to safer samples, leading to the model processing such an input as if it were safe. Code is available at https://github.com/fiveai/understanding_safety_finetuning.",
  "abstract_zh": "摘要：安全微调有助于使大型语言模型（LLMs）与人类偏好对齐，以便安全部署。为了更好地理解通过安全微调使模型安全的潜在因素，我们设计了一个合成数据生成框架，该框架通过建模模型被要求执行的任务（例如，“设计”）与任务所涉及的具体概念（例如，“循环”与“炸弹”）之间的交互，捕捉不安全输入的显著方面。利用这一框架，我们研究了三种著名的安全微调方法——监督安全微调、直接偏好优化和去学习，并提供了显著证据，证明这些方法通过最小化地变换MLP权重来特定地将不安全输入对齐到其权重的零空间。这导致了基于模型是否认为输入安全的输入聚类。因此，当提供对抗性输入（例如，越狱）时，其激活更接近于安全样本，导致模型将此类输入处理为安全输入。代码可在https://github.com/fiveai/understanding_safety_finetuning获取。"
}
{
  "title": "Large language model validity via enhanced conformal prediction methods",
  "title_zh": "大语言模型有效性通过增强的保形预测方法",
  "abstract": "We develop new conformal inference methods for obtaining validity guarantees on the output of large language models (LLMs). Prior work in conformal language modeling identifies a subset of the text that satisfies a high-probability guarantee of correctness. These methods work by filtering claims from the LLM's original response if a scoring function evaluated on the claim fails to exceed a threshold calibrated via split conformal prediction. Existing methods in this area suffer from two deficiencies. First, the guarantee stated is not conditionally valid. The trustworthiness of the filtering step may vary based on the topic of the response. Second, because the scoring function is imperfect, the filtering step can remove many valuable and accurate claims. We address both of these challenges via two new conformal methods. First, we generalize the conditional conformal procedure of Gibbs et al. (2023) in order to adaptively issue weaker guarantees when they are required to preserve the utility of the output. Second, we show how to systematically improve the quality of the scoring function via a novel algorithm for differentiating through the conditional conformal procedure. We demonstrate the efficacy of our approach on biography and medical question-answering datasets.",
  "abstract_zh": "我们开发了新的保形推理方法，以获得对大语言模型（LLM）输出的有效性保证。先前在保形语言建模中的工作识别出满足高概率正确性保证的文本子集。这些方法通过过滤LLM原始响应中的声明来工作，如果对声明评估的评分函数未能超过通过分割保形预测校准的阈值。该领域的现有方法存在两个缺陷。首先，所述的保证不是条件有效的。过滤步骤的可信度可能会根据响应的主题而变化。其次，由于评分函数不完善，过滤步骤可能会删除许多有价值且准确的声明。我们通过两种新的保形方法解决了这两个挑战。首先，我们推广了Gibbs等人（2023）的条件保形程序，以便在需要保留输出效用时自适应地发出较弱的保证。其次，我们展示了如何通过一种新颖的算法系统地提高评分函数的质量，该算法通过条件保形程序进行微分。我们在传记和医学问答数据集上展示了我们方法的有效性。"
}
{
  "title": "Transcoders find interpretable LLM feature circuits",
  "title_zh": "标题：转码器发现可解释的LLM特征电路",
  "abstract": "A key goal in mechanistic interpretability is circuit analysis: finding sparse subgraphs of models corresponding to specific behaviors or capabilities. However, MLP sublayers make fine-grained circuit analysis on transformer-based language models difficult. In particular, interpretable features—such as those found by sparse autoencoders (SAEs)—are typically linear combinations of extremely many neurons, each with its own nonlinearity to account for. Circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior. To address this we explore **transcoders**, which seek to faithfully approximate a densely activating MLP layer with a wider, sparsely-activating MLP layer. We introduce a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers. The resulting circuits neatly factorize into input-dependent and input-invariant terms. We then successfully train transcoders on language models with 120M, 410M, and 1.4B parameters, and find them to perform at least on par with SAEs in terms of sparsity, faithfulness, and human-interpretability. Finally, we apply transcoders to reverse-engineer unknown circuits in the model, and we obtain novel insights regarding the \"greater-than circuit\" in GPT2-small. Our results suggest that transcoders can prove effective in decomposing model computations involving MLPs into interpretable circuits. Code is available at https://github.com/jacobdunefsky/transcoder_circuits/",
  "abstract_zh": "摘要：机械可解释性的一个关键目标是电路分析：寻找与特定行为或能力相对应的模型稀疏子图。然而，MLP子层使基于Transformer的语言模型的细粒度电路分析变得困难。特别是，可解释特征——例如由稀疏自编码器（SAEs）发现的特征——通常是极多神经元的线性组合，每个神经元都有其自身的非线性需要考虑。因此，在这种情况下的电路分析要么产生不可处理的大电路，要么无法解开局部和全局行为。为了解决这个问题，我们探索了**转码器**，它们试图用一个更宽、稀疏激活的MLP层忠实地逼近一个密集激活的MLP层。我们引入了一种新方法，使用转码器通过MLP子层进行基于权重的电路分析。由此产生的电路整齐地分解为输入相关和输入不变的项。然后，我们成功地在具有120M、410M和1.4B参数的语言模型上训练了转码器，并发现它们在稀疏性、忠实性和人类可解释性方面至少与SAEs表现相当。最后，我们应用转码器对模型中的未知电路进行逆向工程，并获得了关于GPT2-small中“大于电路”的新见解。我们的结果表明，转码器可以有效地将涉及MLP的模型计算分解为可解释的电路。代码可在https://github.com/jacobdunefsky/transcoder_circuits/获得。"
}
{
  "title": "Improving Alignment and Robustness with Circuit Breakers",
  "title_zh": "提高对齐和鲁棒性的断路器方法",
  "abstract": "AI systems can take harmful actions and are highly vulnerable to adversarial attacks. We present an approach, inspired by recent advances in representation engineering, that interrupts the models as they respond with harmful outputs with \"circuit breakers.\" Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks. As an alternative to refusal training and adversarial training, circuit-breaking directly controls the representations that are responsible for harmful outputs in the first place. Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility -- even in the presence of powerful unseen attacks. Notably, while adversarial robustness in standalone image recognition remains an open challenge, circuit breakers allow the larger multimodal system to reliably withstand image \"hijacks\" that aim to produce harmful content. Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack. Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks.",
  "abstract_zh": "人工智能系统可能会采取有害行动，并且极易受到对抗性攻击。我们提出了一种方法，受最近表示工程进展的启发，通过使用“断路器”在模型响应有害输出时中断其运行。现有旨在改善对齐的技术，如拒绝训练，常常被绕过。对抗性训练等技术试图通过抵御特定攻击来填补这些漏洞。作为拒绝训练和对抗性训练的替代方案，断路器直接控制最初负责有害输出的表示。我们的方法可以应用于纯文本和多模态语言模型，以防止生成有害输出而不牺牲实用性——即使在强大的未知攻击存在的情况下也是如此。值得注意的是，尽管独立图像识别中的对抗性鲁棒性仍然是一个未解决的挑战，断路器使得更大的多模态系统能够可靠地抵御旨在生成有害内容的图像“劫持”。最后，我们将方法扩展到人工智能代理，展示了在受到攻击时有害行为发生率的显著降低。我们的方法在开发可靠的有害行为和对抗性攻击防护措施方面代表了一个重要的进步。"
}
{
  "title": "Exploiting LLM Quantization",
  "title_zh": "Title: 利用LLM量化",
  "abstract": "Quantization leverages lower-precision weights to reduce the memory usage of large language models (LLMs) and is a key technique for enabling their deployment on commodity hardware. While LLM quantization's impact on utility has been extensively explored, this work for the first time studies its adverse effects from a security perspective. We reveal that widely used quantization methods can be exploited to produce a harmful quantized LLM, even though the full-precision counterpart appears benign, potentially tricking users into deploying the malicious quantized model. \nWe demonstrate this threat using a three-staged attack framework: (i) first, we obtain a malicious LLM through fine-tuning on an adversarial task; (ii) next, we quantize the malicious model and calculate constraints that characterize all full-precision models that map to the same quantized model; (iii) finally, using projected gradient descent, we tune out the poisoned behavior from the full-precision model while ensuring that its weights satisfy the constraints computed in step (ii). This procedure results in an LLM that exhibits benign behavior in full precision but when quantized, it follows the adversarial behavior injected in step (i). We experimentally demonstrate the feasibility and severity of such an attack across three diverse scenarios: vulnerable code generation, content injection, and over-refusal attack. In practice, the adversary could host the resulting full-precision model on an LLM community hub such as Hugging Face, exposing millions of users to the threat of deploying its malicious quantized version on their devices.",
  "abstract_zh": "Abstract: 量化利用低精度权重来减少大型语言模型（LLM）的内存使用，是使其能够在普通硬件上部署的关键技术。虽然LLM量化对实用性的影响已被广泛研究，但本研究首次从安全角度研究其不利影响。我们揭示了广泛使用的量化方法可以被利用来生成有害的量化LLM，即使其全精度版本看似无害，可能会诱骗用户部署恶意的量化模型。我们通过一个三阶段攻击框架展示了这一威胁：（i）首先，通过在对抗性任务上微调获得一个恶意LLM；（ii）接下来，我们对恶意模型进行量化，并计算出所有映射到相同量化模型的全精度模型的约束；（iii）最后，使用投影梯度下降，我们在确保其权重满足步骤(ii)中计算的约束的同时，从全精度模型中调出中毒行为。此过程产生的LLM在全精度下表现良性，但在量化后遵循步骤(i)中注入的对抗行为。我们通过三个不同场景实验证明了这种攻击的可行性和严重性：易受攻击的代码生成、内容注入和过度拒绝攻击。在实践中，对手可以在LLM社区中心（如Hugging Face）上托管生成的全精度模型，使数百万用户面临在其设备上部署恶意量化版本的威胁。"
}
{
  "title": "WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off",
  "title_zh": "Title: WaterMax：打破大型语言模型水印的可检测性-鲁棒性-质量权衡",
  "abstract": "Watermarking is a technical means to dissuade malfeasant usage of Large Language Models.\nThis paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original LLM.\nIts new design leaves the LLM untouched (no modification of the weights, logits or temperature).\nWaterMax balances robustness and  computational complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness.\nIts performance is both theoretically proven and experimentally validated.\nIt outperforms all the SotA techniques under the most complete benchmark suite.",
  "abstract_zh": "Abstract: 水印是一种技术手段，用于劝阻对大型语言模型的恶意使用。本文提出了一种新颖的水印方案，称为WaterMax，它在保持原始大型语言模型生成文本质量的同时，具有高可检测性。其新设计不改变大型语言模型（不修改权重、对数或温度）。与文献中的水印技术固有地在质量和鲁棒性之间引发权衡不同，WaterMax在鲁棒性和计算复杂性之间实现了平衡。其性能经过理论证明和实验验证。在最完整的基准测试套件中，它的表现优于所有的最新技术。"
}
{
  "title": "Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters",
  "title_zh": "标题：高效多任务LLM量化与多LoRA适配器服务",
  "abstract": "With the remarkable achievements of large language models (LLMs), the demand for fine-tuning and deploying LLMs in various downstream tasks has garnered widespread interest. Parameter-efficient fine-tuning techniques represented by LoRA and model quantization techniques represented by GPTQ and AWQ are of paramount significance. However, although these techniques have been widely adopted in single-task scenarios, research is scarce in multi-task scenarios. To be specific, we find that mainstream quantization methods would prevent the base LLM from being shared among tasks, so current LLM serving systems are infeasible to integrate LLM quantization with multiple LoRA adapters to achieve memory-efficient multi-task serving. Moreover, existing LLM serving systems lack support for dynamic task addition and overlook the workload differences among tasks, leading to inefficiencies in multi-task scenarios.\n\nThis work proposes LoRA-Inlaid, an efficient multi-task LLM serving system. On the one hand, LoRA-Inlaid designs a flexible and efficient multi-task quantization algorithm (MLGPTQ) that facilitates the sharing of a single quantized model for multiple LoRA adapters, which significantly reduces the memory consumption for model deployment. Meanwhile, it supports adding LoRA adapters for new tasks on the fly, without sacrificing the stability of online services. On the other hand, LoRA-Inlaid develops a novel multi-task scheduling algorithm guided by output length prediction and grouping among different tasks, which effectively shrinks the memory consumption and avoids frequent switching of LoRA adapters. Empirical results verify that LoRA-Inlaid outperforms existing state-of-the-art LLM serving systems by up to 1.58 times in terms of throughput, 1.76 times in terms of average latency, 2 times in terms of job completion time, and 10 times in terms of SLO Attainment, while maintaining the same level of model quality.",
  "abstract_zh": "摘要：随着大型语言模型（LLM）的显著成就，对LLM在各种下游任务中进行微调和部署的需求引起了广泛关注。以LoRA为代表的参数高效微调技术和以GPTQ和AWQ为代表的模型量化技术具有重要意义。然而，尽管这些技术在单任务场景中被广泛采用，但在多任务场景中的研究却很少。具体而言，我们发现主流的量化方法会阻碍基础LLM在任务之间的共享，因此当前的LLM服务系统无法将LLM量化与多个LoRA适配器集成以实现内存高效的多任务服务。此外，现有的LLM服务系统缺乏对动态任务添加的支持，并忽视了任务之间的工作负载差异，导致多任务场景中的效率低下。本文提出了LoRA-Inlaid，一种高效的多任务LLM服务系统。一方面，LoRA-Inlaid设计了一种灵活高效的多任务量化算法（MLGPTQ），促进了单一量化模型在多个LoRA适配器之间的共享，大大减少了模型部署的内存消耗。同时，它支持在不中断在线服务稳定性的情况下动态添加新任务的LoRA适配器。另一方面，LoRA-Inlaid开发了一种新颖的多任务调度算法，通过输出长度预测和任务间分组指导，有效减少了内存消耗并避免了LoRA适配器的频繁切换。实证结果验证了LoRA-Inlaid在吞吐量、平均延迟、作业完成时间和SLO达成率方面分别比现有最先进的LLM服务系统高出多达1.58倍、1.76倍、2倍和10倍，同时保持相同水平的模型质量。"
}
{
  "title": "BiScope: AI-generated Text Detection by Checking Memorization of Preceding Tokens",
  "title_zh": "BiScope：通过检查前置标记的记忆来检测AI生成的文本",
  "abstract": "Detecting text generated by Large Language Models (LLMs) is a pressing need in\n order to identify and prevent misuse of these powerful models in a wide range of\n applications, which have highly undesirable consequences such as misinformation\n and academic dishonesty. Given a piece of subject text, many existing detection\n methods work by measuring the difficulty of LLM predicting the next token in\n the text from their prefix. In this paper, we make a critical observation that\n how well the current token’s output logits memorizes the closely preceding input\n tokens also provides strong evidence. Therefore, we propose a novel bi-directional\n calculation method that measures the cross-entropy losses between an output\n logits and the ground-truth token (forward) and between the output logits and\n the immediately preceding input token (backward). A classifier is trained to\n make the final prediction based on the statistics of these losses. We evaluate our\n system, named BISCOPE, on texts generated by five latest commercial LLMs\n across five heterogeneous datasets, including both natural language and code.\n BISCOPE demonstrates superior detection accuracy and robustness compared to six\n existing baseline methods, exceeding the state-of-the-art non-commercial methods’\n detection accuracy by over 0.30 F1 score, achieving over 0.95 detection F1 score\n on average. It also outperforms the best commercial tool GPTZero that is based on\n a commercial LLM trained with an enormous volume of data. Code is available at https://github.com/MarkGHX/BiScope.",
  "abstract_zh": "检测由大型语言模型（LLM）生成的文本是一个紧迫的需求，以识别和防止这些强大模型在广泛应用中被滥用，这可能导致诸如错误信息和学术不诚实等极其不良的后果。给定一段主题文本，许多现有的检测方法通过测量LLM从其前缀预测文本中下一个标记的难度来工作。在本文中，我们提出了一个关键观察，即当前标记的输出logits对紧接前面的输入标记的记忆程度也提供了强有力的证据。因此，我们提出了一种新颖的双向计算方法，该方法测量输出logits与真实标记（前向）以及输出logits与紧接前面的输入标记（后向）之间的交叉熵损失。通过这些损失的统计数据训练一个分类器来进行最终预测。我们在五个最新的商业LLM生成的文本上评估了我们的系统BISCOPE，这些文本来自包括自然语言和代码在内的五个异构数据集。与六种现有的基线方法相比，BISCOPE展示了卓越的检测准确性和鲁棒性，其检测准确性超过了最先进的非商业方法超过0.30 F1分数，平均检测F1分数超过0.95。它还优于基于大量数据训练的商业LLM的最佳商业工具GPTZero。代码可在https://github.com/MarkGHX/BiScope获取。"
}
{
  "title": "Safe LoRA: The Silver Lining of Reducing Safety Risks when Finetuning Large Language Models",
  "title_zh": "标题: Safe LoRA：在微调大型语言模型时降低安全风险的曙光",
  "abstract": "While large language models (LLMs) such as Llama-2 or GPT-4 have shown impressive zero-shot performance, fine-tuning is still necessary to enhance their performance for customized datasets, domain-specific tasks, or other private needs. However, fine-tuning all parameters of LLMs requires significant hardware resources, which can be impractical for typical users. Therefore, parameter-efficient fine-tuning such as LoRA have emerged, allowing users to fine-tune LLMs without the need for considerable computing resources, with little performance degradation compared to fine-tuning all parameters. Unfortunately, recent studies indicate that fine-tuning can increase the risk to the safety of LLMs, even when data does not contain malicious content. To address this challenge, we propose $\\textsf{Safe LoRA}$, a simple one-liner patch to the original LoRA implementation by introducing the projection of LoRA weights from selected layers to the safety-aligned subspace, effectively reducing the safety risks in LLM fine-tuning while maintaining utility. It is worth noting that $\\textsf{Safe LoRA}$ is a training-free and data-free approach, as it only requires the knowledge of the weights from the base and aligned LLMs. Our extensive experiments demonstrate that when fine-tuning on purely malicious data, $\\textsf{Safe LoRA}$ retains similar safety performance as the original aligned model. Moreover, when the fine-tuning dataset contains a mixture of both benign and malicious data,  $\\textsf{Safe LoRA}$ mitigates the negative effect made by malicious data while preserving performance on downstream tasks.  Our codes are available at https://github.com/IBM/SafeLoRA.",
  "abstract_zh": "摘要: 尽管像Llama-2或GPT-4这样的大型语言模型（LLMs）在零样本学习中表现出色，但为了提升其在定制数据集、特定领域任务或其他私人需求上的表现，微调仍然是必要的。然而，微调所有LLM参数需要大量的硬件资源，对于普通用户来说可能不切实际。因此，像LoRA这样的参数高效微调方法应运而生，使用户无需大量计算资源即可微调LLM，并且与微调所有参数相比，性能下降很小。不幸的是，最近的研究表明，即使数据不包含恶意内容，微调也可能增加LLM的安全风险。为了解决这一挑战，我们提出了$\\textsf{Safe LoRA}$，这是对原始LoRA实现的简单单行补丁，通过将LoRA权重从选定层投影到安全对齐子空间，有效降低LLM微调中的安全风险，同时保持实用性。值得注意的是，$\\textsf{Safe LoRA}$是一种无需训练和数据的方法，因为它只需要基础和对齐LLM的权重知识。我们的广泛实验表明，在纯恶意数据上进行微调时，$\\textsf{Safe LoRA}$保持了与原始对齐模型相似的安全性能。此外，当微调数据集中包含良性和恶意数据的混合时，$\\textsf{Safe LoRA}$减轻了恶意数据带来的负面影响，同时保留了下游任务的性能。我们的代码可在https://github.com/IBM/SafeLoRA获取。"
}
{
  "title": "The Importance of Online Data: Understanding Preference Fine-tuning via Coverage",
  "title_zh": "标题：在线数据的重要性：通过覆盖理解偏好微调",
  "abstract": "Learning from human preference data has emerged as the dominant paradigm for fine-tuning large language models (LLMs). The two most common families of techniques -- online reinforcement learning (RL) such as Proximal Policy Optimization (PPO) and offline contrastive methods such as Direct Preference Optimization (DPO) -- were positioned as equivalent in prior work due to the fact that both have to start from the same offline preference dataset. To further expand our theoretical understanding of the similarities and differences between online and offline techniques for preference fine-tuning, we conduct a rigorous analysis through the lens of *dataset coverage*, a concept that captures how the training data covers the test distribution and is widely used in RL. We prove that a global coverage condition is both necessary and sufficient for offline contrastive methods to converge to the optimal policy, but a weaker partial coverage condition suffices for online RL methods. This separation provides one explanation of why online RL methods can perform better than offline methods, especially when the offline preference data is not diverse enough. Finally, motivated by our preceding theoretical observations, we derive a hybrid preference optimization (HyPO) algorithm that uses offline data for contrastive-based preference optimization and online unlabeled data for KL regularization. Theoretically and empirically, we demonstrate that HyPO is more performant than its pure offline counterpart DPO, while still preserving its computation and memory efficiency.",
  "abstract_zh": "摘要：从人类偏好数据中学习已成为微调大型语言模型（LLMs）的主导范式。两种最常见的技术家族——在线强化学习（RL），如近端策略优化（PPO），以及离线对比方法，如直接偏好优化（DPO）——在先前的工作中被视为等同，因为它们都必须从相同的离线偏好数据集开始。为了进一步扩展我们对在线和离线技术在偏好微调方面的相似性和差异的理论理解，我们通过*数据集覆盖*的视角进行了严格分析，这一概念捕捉了训练数据如何覆盖测试分布，并在RL中被广泛使用。我们证明了全局覆盖条件对于离线对比方法收敛到最优策略是必要且充分的，但较弱的部分覆盖条件对于在线RL方法就足够了。这一区别提供了一个解释，说明为什么在线RL方法在离线偏好数据不够多样化时能表现得更好。最后，受我们之前理论观察的启发，我们推导出一种混合偏好优化（HyPO）算法，该算法使用离线数据进行基于对比的偏好优化，并使用在线未标记数据进行KL正则化。理论和实验证明，HyPO比其纯离线对应方法DPO表现更好，同时仍保持其计算和内存效率。"
}
{
  "title": "ESPACE: Dimensionality Reduction of Activations for Model Compression",
  "title_zh": "激活维度缩减的模型压缩方法：ESPACE",
  "abstract": "We propose ESPACE, an LLM compression technique based on dimensionality reduction of activations. Unlike prior works on weight-centric tensor decomposition, ESPACE projects activations onto a pre-calibrated set of principal components. The activation-centrality of the approach enables retraining LLMs with no loss of expressivity; while at inference, weight decomposition is obtained as a byproduct of matrix multiplication associativity. Theoretical results on the construction of projection matrices with optimal computational accuracy are provided. Experimentally, we find ESPACE enables 50% compression of GPT3, Llama2, and Nemotron4 models with small accuracy degradation, as low as a 0.18 perplexity increase on GPT3-22B. At lower compression rates of 20% to 40%, ESPACE drives GPT3 models to outperforming their baseline, by up to a 0.38 decrease in perplexity for GPT3-8B. ESPACE also reduces GEMM execution time and prefill inference latency on existing hardware. Comparison with related works on compressing Llama2-7B via matrix factorization shows that ESPACE is a first step in advancing the state-of-the-art in tensor decomposition compression of LLMs.",
  "abstract_zh": "我们提出了一种基于激活维度缩减的LLM压缩技术ESPACE。与以往以权重为中心的张量分解方法不同，ESPACE将激活投影到预先校准的主成分集上。这种以激活为中心的方法使得在不损失表达能力的情况下重新训练LLM成为可能；而在推理时，权重分解作为矩阵乘法结合性的副产品获得。我们提供了关于构建具有最佳计算精度的投影矩阵的理论结果。实验表明，ESPACE使GPT3、Llama2和Nemotron4模型实现了50%的压缩，精度下降很小，在GPT3-22B上困惑度仅增加0.18。在20%到40%的较低压缩率下，ESPACE使GPT3模型的表现超过其基线，在GPT3-8B上困惑度减少多达0.38。ESPACE还减少了现有硬件上的GEMM执行时间和预填推理延迟。与通过矩阵分解压缩Llama2-7B的相关工作相比，ESPACE是推进LLM张量分解压缩技术的第一步。"
}
{
  "title": "Self-Retrieval: End-to-End Information Retrieval with One Large Language Model",
  "title_zh": "自检索：基于单一大型语言模型的端到端信息检索",
  "abstract": "The rise of large language models (LLMs) has significantly transformed both the construction and application of information retrieval (IR) systems. \nHowever, current interactions between IR systems and LLMs remain limited, with LLMs merely serving as part of components within IR systems, and IR systems being constructed independently of LLMs. This separated architecture restricts knowledge sharing and deep collaboration between them.\nIn this paper, we introduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval architecture.\nSelf-Retrieval unifies all essential IR functions within a single LLM, leveraging the inherent capabilities of LLMs throughout the IR process.\nSpecifically, Self-Retrieval internalizes the retrieval corpus through self-supervised learning,  transforms the retrieval process into sequential passage generation, and performs relevance assessment for reranking.\nExperimental results demonstrate that Self-Retrieval not only outperforms existing retrieval approaches by a significant margin, but also substantially enhances the performance of LLM-driven downstream applications like retrieval-augmented generation.",
  "abstract_zh": "大型语言模型（LLM）的兴起显著改变了信息检索（IR）系统的构建和应用。然而，目前IR系统与LLM之间的交互仍然有限，LLM仅作为IR系统中的部分组件存在，而IR系统的构建独立于LLM。这种分离的架构限制了它们之间的知识共享和深度协作。在本文中，我们介绍了自检索，一种新颖的端到端LLM驱动的信息检索架构。自检索在单一LLM中统一了所有基本的IR功能，利用LLM的内在能力贯穿整个IR过程。具体而言，自检索通过自监督学习内化检索语料库，将检索过程转化为顺序段落生成，并执行相关性评估以重新排序。实验结果表明，自检索不仅在很大程度上优于现有的检索方法，还显著提升了LLM驱动的下游应用如检索增强生成的性能。"
}
{
  "title": "ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users",
  "title_zh": "标题：ART: 用于保护良性用户的文本到图像模型自动红队测试",
  "abstract": "Large-scale pre-trained generative models are taking the world by storm, due to their abilities in generating creative content. Meanwhile, safeguards for these generative models are developed, to protect users' rights and safety, most of which are designed for large language models. Existing methods primarily focus on jailbreak and adversarial attacks, which mainly evaluate the model's safety under malicious prompts. Recent work found that manually crafted safe prompts can unintentionally trigger unsafe generations. To further systematically evaluate the safety risks of text-to-image models, we propose a novel Automatic Red-Teaming framework, ART. Our method leverages both vision language model and large language model to establish a connection between unsafe generations and their prompts, thereby more efficiently identifying the model's vulnerabilities. With our comprehensive experiments, we reveal the toxicity of the popular open-source text-to-image models. The experiments also validate the effectiveness, adaptability, and great diversity of ART. Additionally, we introduce three large-scale red-teaming datasets for studying the safety risks associated with text-to-image models. Datasets and models can be found in https://github.com/GuanlinLee/ART.",
  "abstract_zh": "摘要：大规模预训练生成模型因其生成创意内容的能力而风靡全球。同时，为了保护用户的权利和安全，这些生成模型的安全措施也在不断发展，其中大多数是为大型语言模型设计的。现有方法主要集中在越狱和对抗攻击上，主要评估模型在恶意提示下的安全性。最近的研究发现，手动制作的安全提示可能会无意中触发不安全的生成。为了进一步系统地评估文本到图像模型的安全风险，我们提出了一种新颖的自动红队测试框架，ART。我们的方法利用视觉语言模型和大型语言模型之间的联系，在不安全生成和其提示之间建立连接，从而更高效地识别模型的漏洞。通过我们的全面实验，我们揭示了流行的开源文本到图像模型的毒性。实验还验证了ART的有效性、适应性和极大的多样性。此外，我们引入了三个大规模红队测试数据集，用于研究与文本到图像模型相关的安全风险。数据集和模型可在https://github.com/GuanlinLee/ART找到。"
}
{
  "title": "Adaptive Preference Scaling for Reinforcement Learning with Human Feedback",
  "title_zh": "强化学习中的自适应偏好缩放与人类反馈",
  "abstract": "Reinforcement learning from human feedback (RLHF) is a prevalent approach to align AI systems with human values by learning rewards from human preference data. Due to various reasons, however, such data typically takes the form of rankings over pairs of trajectory segments, which fails to capture the varying strengths of preferences across different pairs. In this paper, we propose a novel adaptive preference loss, underpinned by distributionally robust optimization (DRO), designed to address this uncertainty in preference strength. By incorporating an adaptive scaling parameter into the loss for each pair, our method increases the flexibility of the reward function. Specifically, it assigns small scaling parameters to pairs with ambiguous preferences, leading to more comparable rewards, and large scaling parameters to those with clear preferences for more distinct rewards. Computationally, our proposed loss function is strictly convex and univariate with respect to each scaling parameter, enabling its efficient optimization through a simple second-order algorithm. Our method is versatile and can be readily adapted to various preference optimization frameworks, including direct preference optimization (DPO). Our experiments with robotic control and natural language generation with large language models (LLMs) show that our method not only improves policy performance but also aligns reward function selection more closely with policy optimization, simplifying the hyperparameter tuning process.",
  "abstract_zh": "从人类反馈中进行强化学习（RLHF）是一种通过学习人类偏好数据中的奖励来使人工智能系统与人类价值观对齐的流行方法。然而，由于各种原因，此类数据通常以轨迹片段对的排名形式出现，未能捕捉到不同对之间偏好强度的变化。在本文中，我们提出了一种新颖的自适应偏好损失，该损失基于分布鲁棒优化（DRO），旨在解决偏好强度的不确定性。通过在每对中引入自适应缩放参数，我们的方法增加了奖励函数的灵活性。具体而言，它为偏好模糊的对分配较小的缩放参数，从而获得更可比的奖励，而为偏好明确的对分配较大的缩放参数以获得更明显的奖励。在计算上，我们提出的损失函数对于每个缩放参数是严格凸的和单变量的，使其能够通过简单的二阶算法进行高效优化。我们的方法具有通用性，可以很容易地适应各种偏好优化框架，包括直接偏好优化（DPO）。我们在机器人控制和使用大型语言模型（LLMs）进行自然语言生成的实验表明，我们的方法不仅提高了策略性能，还使奖励函数选择与策略优化更加一致，简化了超参数调优过程。"
}
{
  "title": "Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models",
  "title_zh": "标题: 导航安全景观：微调大型语言模型中的风险测量",
  "abstract": "Safety alignment is crucial to ensure that large language models (LLMs) behave in ways that align with human preferences and prevent harmful actions during inference. However, recent studies show that the alignment can be easily compromised through finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as “safety basin”: random perturbations to model weights maintain the safety level of the original aligned model within its local neighborhood. However, outside this local region, safety is fully compromised, exhibiting a sharp, step-like drop. This safety basin contrasts sharply with the LLM capability landscape, where model performance peaks at the origin and gradually declines as random perturbation increases. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. The LLM safety landscape also highlights the system prompt’s critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new\ninsights for future work on LLM safety community. Our code is publicly available at https://github.com/ShengYun-Peng/llm-landscape.",
  "abstract_zh": "摘要: 安全对齐对于确保大型语言模型（LLMs）在推理过程中表现出符合人类偏好的行为并防止有害行为至关重要。然而，最近的研究表明，通过仅使用少量对抗性设计的训练示例进行微调，安全对齐很容易受到破坏。我们旨在通过导航LLM安全景观来测量微调LLM的风险。我们发现了一种在流行的开源LLM模型参数空间中普遍观察到的新现象，称为“安全盆地”：对模型权重的随机扰动在其局部邻域内保持原始对齐模型的安全水平。然而，在这个局部区域之外，安全性完全受到破坏，表现出明显的阶梯式下降。这种安全盆地与LLM能力景观形成鲜明对比，后者的模型性能在原点达到峰值，并随着随机扰动的增加逐渐下降。我们的发现激励我们提出了新的VISAGE安全指标，通过探测其安全景观来测量LLM微调中的安全性。可视化对齐模型的安全景观使我们能够理解微调如何通过将模型拖离安全盆地而损害安全性。LLM安全景观还强调了系统提示在保护模型中的关键作用，并且这种保护在安全盆地内转移到其扰动变体。这些来自我们安全景观研究的观察为未来LLM安全社区的工作提供了新的见解。我们的代码可在https://github.com/ShengYun-Peng/llm-landscape公开获取。"
}
{
  "title": "AmoebaLLM: Constructing Any-Shape Large Language Models for Efficient and Instant Deployment",
  "title_zh": "AmoebaLLM：构建任意形状的大型语言模型以实现高效和即时部署",
  "abstract": "Motivated by the transformative capabilities of large language models (LLMs) across various natural language tasks, there has been a growing demand to deploy these models effectively across diverse real-world applications and platforms. However, the challenge of efficiently deploying LLMs has become increasingly pronounced due to the varying application-specific performance requirements and the rapid evolution of computational platforms, which feature diverse resource constraints and deployment flows. These varying requirements necessitate LLMs that can adapt their structures (depth and width) for optimal efficiency across different platforms and application specifications. To address this critical gap, we propose AmoebaLLM, a novel framework designed to enable the instant derivation of LLM subnets of arbitrary shapes, which achieve the accuracy-efficiency frontier and can be extracted immediately after a one-time fine-tuning. In this way, AmoebaLLM significantly facilitates rapid deployment tailored to various platforms and applications. Specifically, AmoebaLLM integrates three innovative components: (1) a knowledge-preserving subnet selection strategy that features a dynamic-programming approach for depth shrinking and an importance-driven method for width shrinking; (2) a shape-aware mixture of LoRAs to mitigate gradient conflicts among subnets during fine-tuning; and (3) an in-place distillation scheme with loss-magnitude balancing as the fine-tuning objective. Extensive experiments validate that AmoebaLLM not only sets new standards in LLM adaptability but also successfully delivers subnets that achieve state-of-the-art trade-offs between accuracy and efficiency.",
  "abstract_zh": "受到大型语言模型（LLMs）在各种自然语言任务中变革性能力的启发，越来越多的需求要求在不同的实际应用和平台上有效地部署这些模型。然而，由于应用特定的性能要求各异以及计算平台的快速演变（其特征是资源限制和部署流程多样化），有效部署LLMs的挑战变得愈加明显。这些不同的要求需要LLMs能够调整其结构（深度和宽度），以在不同平台和应用规范中实现最佳效率。为了解决这一关键差距，我们提出了AmoebaLLM，这是一种新颖的框架，旨在实现任意形状的LLM子网的即时推导，这些子网在一次性微调后即可达到准确性-效率的前沿，并能立即提取。通过这种方式，AmoebaLLM显著促进了针对各种平台和应用的快速部署。具体来说，AmoebaLLM整合了三个创新组件：（1）一种知识保留的子网选择策略，采用动态规划方法进行深度缩减和基于重要性的宽度缩减方法；（2）一种形状感知的LoRA混合方法，以缓解微调期间子网之间的梯度冲突；（3）一种以损失幅度平衡为微调目标的就地蒸馏方案。大量实验验证表明，AmoebaLLM不仅在LLM适应性方面设定了新标准，还成功地提供了在准确性和效率之间实现最先进权衡的子网。"
}
{
  "title": "LLM Dataset Inference: Did you train on my dataset?",
  "title_zh": "标题：LLM数据集推断：你的数据集是否用于训练？",
  "abstract": "The proliferation of large language models (LLMs) in the real world has come with a rise in copyright cases against companies for training their models on unlicensed data from the internet. Recent works have presented methods to identify if individual text sequences were members of the model's training data, known as membership inference attacks (MIAs). \nWe demonstrate that the apparent success of these MIAs is confounded by selecting non-members (text sequences not used for training) belonging to a different distribution from the members (e.g., temporally shifted recent Wikipedia articles compared with ones used to train the model). This distribution shift makes membership inference appear successful. \nHowever, most MIA methods perform no better than random guessing when discriminating between members and non-members from the same distribution (e.g., in this case, the same period of time).\nEven when MIAs work, we find that different MIAs succeed at inferring membership of samples from different distributions.\nInstead, we propose a new dataset inference method to accurately identify the datasets used to train large language models. This paradigm sits realistically in the modern-day copyright landscape, where authors claim that an LLM is trained over multiple documents (such as a book) written by them, rather than one particular paragraph.\nWhile dataset inference shares many of the challenges of membership inference, we solve it by selectively combining the MIAs that provide positive signal for a given distribution, and aggregating them to perform a statistical test on a given dataset. Our approach successfully distinguishes the train and test sets of different subsets of the Pile with statistically significant p-values $< 0.1$, without any false positives.",
  "abstract_zh": "摘要：大型语言模型（LLM）在现实世界中的普及伴随着针对公司使用互联网未授权数据训练模型的版权案件的增加。最近的研究提出了识别单个文本序列是否为模型训练数据成员的方法，称为成员推断攻击（MIA）。我们证明，这些MIA的明显成功是由于选择了与成员（例如，与用于训练模型的文章相比，时间上有所变化的近期维基百科文章）属于不同分布的非成员（未用于训练的文本序列）而造成的。这种分布转变使得成员推断看起来很成功。然而，当在同一分布（例如，在这种情况下，同一时间段）中区分成员和非成员时，大多数MIA方法的表现不比随机猜测好。即使MIA有效，我们发现不同的MIA在推断来自不同分布的样本的成员身份时会成功。相反，我们提出了一种新的数据集推断方法，以准确识别用于训练大型语言模型的数据集。这种范式在现代版权环境中是现实的，作者声称LLM是基于他们撰写的多篇文档（如一本书）而不是某个特定段落进行训练的。虽然数据集推断与成员推断面临许多相同的挑战，但我们通过选择性地结合为给定分布提供正面信号的MIA，并将其聚合以对给定数据集进行统计测试来解决这一问题。我们的方法成功地以统计显著的p值$< 0.1$区分了不同子集的Pile的训练集和测试集，没有任何误报。"
}
{
  "title": "When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search",
  "title_zh": "标题：当大语言模型遇上深度强化学习：通过深度强化学习引导搜索提升越狱效率",
  "abstract": "Recent studies developed jailbreaking attacks, which construct jailbreaking prompts to \"fool\" LLMs into responding to harmful questions.\nEarly-stage jailbreaking attacks require access to model internals or significant human efforts. \nMore advanced attacks utilize genetic algorithms for automatic and black-box attacks.\nHowever, the random nature of genetic algorithms significantly limits the effectiveness of these attacks.\nIn this paper, we propose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement learning (DRL).\nWe model jailbreaking as a search problem and design an RL agent to guide the search, which is more effective and has less randomness than stochastic search, such as genetic algorithms.\nSpecifically, we design a customized DRL system for the jailbreaking problem, including a novel reward function and a customized proximal policy optimization (PPO) algorithm.\nThrough extensive experiments, we demonstrate that RLbreaker is much more effective than existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs. \nWe also show that RLbreaker is robust against three SOTA defenses and its trained agents can transfer across different LLMs.\nWe further validate the key design choices of RLbreaker via a comprehensive ablation study.",
  "abstract_zh": "摘要：近期研究开发了越狱攻击，构建越狱提示以“欺骗”大语言模型（LLM）响应有害问题。早期阶段的越狱攻击需要访问模型内部或大量人工努力。更先进的攻击利用遗传算法进行自动化和黑箱攻击。然而，遗传算法的随机性显著限制了这些攻击的有效性。在本文中，我们提出了RLbreaker，一种由深度强化学习（DRL）驱动的黑箱越狱攻击。我们将越狱建模为一个搜索问题，并设计了一个RL代理来引导搜索，这比遗传算法等随机搜索更有效且随机性更小。具体而言，我们为越狱问题设计了一个定制的DRL系统，包括一个新颖的奖励函数和一个定制的近端策略优化（PPO）算法。通过大量实验，我们证明了RLbreaker在对抗六种最先进（SOTA）的大语言模型时比现有的越狱攻击更有效。我们还展示了RLbreaker对三种SOTA防御的鲁棒性，并且其训练的代理可以在不同的大语言模型之间迁移。我们进一步通过全面的消融研究验证了RLbreaker的关键设计选择。"
}
{
  "title": "Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?",
  "title_zh": "标题：语言模型能否在含噪推理链提示中进行稳健推理？",
  "abstract": "This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We construct NoRa dataset that is tailored to evaluate the robustness of reasoning in the presence of noisy rationales. Our findings on NoRa dataset reveal a prevalent vulnerability to such noise among current LLMs, with existing robust methods like self-correction and self-consistency showing limited efficacy. Notably, compared to prompting with clean rationales, base LLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more drastically by 2.2%-40.4% with inaccurate thoughts.\n\nAddressing this challenge necessitates external supervision that should be accessible in practice. Here, we propose the method of contrastive denoising with noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning capabilities by contrasting noisy rationales with only one clean rationale, which can be the minimal requirement for denoising-purpose prompting. This method follows a principle of exploration and exploitation: (1) rephrasing and selecting rationales in the input space to achieve explicit denoising and (2) exploring diverse reasoning paths and voting on answers in the output space. Empirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy over the base model and shows significantly stronger denoising capabilities than baseline methods. The source code is publicly available at: https://github.com/tmlr-group/NoisyRationales.",
  "abstract_zh": "摘要：本文研究了大型语言模型（LLMs）中的一个未被充分探索的挑战：含噪推理链提示，其中包含用于上下文学习的示例中不相关或不准确的推理思路。我们构建了NoRa数据集，专门用于评估在存在噪声推理链时推理的稳健性。我们在NoRa数据集上的研究发现，当前的LLMs普遍易受此类噪声的影响，现有的稳健方法如自我纠正和自我一致性显示出有限的效果。值得注意的是，与使用干净的推理链提示相比，基础LLM在不相关思路下的准确率下降了1.4%-19.8%，而在不准确思路下则更为显著地下降了2.2%-40.4%。解决这一挑战需要在实践中可获得的外部监督。在此，我们提出了对比去噪的含噪推理链方法（CD-CoT）。该方法通过对比含噪推理链与仅一个干净推理链来增强LLMs的去噪推理能力，这可以是去噪提示的最低要求。此方法遵循探索与利用的原则：（1）在输入空间中重述和选择推理链以实现显式去噪；（2）在输出空间中探索多样化的推理路径并对答案进行投票。实证结果表明，CD-CoT在准确率上比基础模型平均提高了17.8%，并显示出比基线方法显著更强的去噪能力。源代码可在以下网址公开获取：https://github.com/tmlr-group/NoisyRationales。"
}
{
  "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models",
  "title_zh": "题目：对齐大型语言模型的AI反馈的关键评估",
  "abstract": "Learning from AI feedback (LAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. LAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL) or direct preference optimization (DPO), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing LAIF pipelines. More generally, we find that the gains from LAIF vary substantially across base model families, test-time evaluation protocols, and critic models. Finally, we provide a mechanistic explanation for when SFT may outperform the full two-step LAIF pipeline as well as suggestions for making LAIF maximally useful in practice.",
  "abstract_zh": "摘要：从AI反馈中学习（LAIF）是一种流行的范式，用于提高强大的预训练语言模型的指令遵循能力。LAIF首先使用教师模型的示例进行监督微调（SFT），然后使用评论模型的反馈，通过强化学习（RL）或直接偏好优化（DPO）进一步微调模型。尽管最近流行的开源模型在RL步骤中展示了性能的显著提升，但本文质疑这种RL步骤的复杂性是否真正适用于AI反馈。我们表明，RL步骤的改进几乎完全归因于普遍使用较弱的教师模型（例如GPT-3.5）进行SFT数据收集，而评论模型（例如GPT-4）用于AI反馈生成的做法。具体而言，我们展示了以GPT-4作为教师进行简单的监督微调优于现有的LAIF流程。更广泛地说，我们发现LAIF的收益在不同的基础模型家族、测试时评估协议和评论模型之间差异显著。最后，我们提供了一个机制解释，说明何时SFT可能优于完整的两步LAIF流程，并提出了在实践中使LAIF最大化有用性的建议。"
}
{
  "title": "Edit Distance Robust Watermarks via Indexing Pseudorandom Codes",
  "title_zh": "标题：通过索引伪随机码实现编辑距离鲁棒的水印",
  "abstract": "Motivated by the problem of detecting AI-generated text, we consider the problem of watermarking the output of language models with provable guarantees. We aim for watermarks which satisfy: (a) undetectability, a cryptographic notion introduced by Christ, Gunn, & Zamir (2023) which stipulates that it is computationally hard to distinguish watermarked language model outputs from the model's actual output distribution; and (b) robustness to channels which introduce a constant fraction of adversarial insertions, substitutions, and deletions to the watermarked text. Earlier schemes could only handle stochastic substitutions and deletions, and thus we are aiming for a more natural and appealing robustness guarantee that holds with respect to edit distance.\n  Our main result is a watermarking scheme which achieves both (a) and (b) when the alphabet size for the language model is allowed to grow as a polynomial in the security parameter. To derive such a scheme, we follow an approach introduced by Christ & Gunn (2024), which proceeds via first constructing pseudorandom codes satisfying undetectability and robustness properties analogous to those above; our codes have the additional benefit of relying on weaker computational assumptions than used in previous work.   Then we show that there is a generic transformation from such codes over large alphabets to watermarking schemes for arbitrary language models.",
  "abstract_zh": "摘要：受到检测AI生成文本问题的启发，我们考虑为语言模型的输出提供具有可证明保证的水印问题。我们的目标是实现满足以下条件的水印：（a）不可检测性，这是Christ、Gunn和Zamir（2023）引入的一个密码学概念，规定在计算上难以将带水印的语言模型输出与模型的实际输出分布区分开来；（b）对引入固定比例的对抗性插入、替换和删除的通道具有鲁棒性。早期的方案只能处理随机替换和删除，因此我们旨在实现一个更自然和吸引人的鲁棒性保证，该保证相对于编辑距离成立。我们的主要结果是一个水印方案，当语言模型的字母表大小允许随着安全参数的多项式增长时，实现了条件（a）和（b）。为了推导出这样的方案，我们遵循了Christ和Gunn（2024）引入的方法，该方法首先通过构建满足类似于上述不可检测性和鲁棒性属性的伪随机码来进行；我们的代码还有一个额外的好处，即依赖于比以往工作中使用的更弱的计算假设。然后，我们展示了从大字母表上的此类代码到任意语言模型的水印方案的通用转换。"
}
{
  "title": "Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training",
  "title_zh": "标题: 堆叠您的变压器：高效LLM预训练中的模型增长深入研究",
  "abstract": "LLMs are computationally expensive to pre-train due to their large scale.\nModel growth emerges as a promising approach by leveraging smaller models to accelerate the training of larger ones. \nHowever, the viability of these model growth methods in efficient LLM pre-training remains underexplored.\nThis work identifies three critical $\\underline{\\textit{O}}$bstacles: ($\\textit{O}$1) lack of comprehensive evaluation, ($\\textit{O}$2) untested viability for scaling, and ($\\textit{O}$3) lack of empirical guidelines.\nTo tackle $\\textit{O}$1, we summarize existing approaches into four atomic growth operators and systematically evaluate them in a standardized LLM pre-training setting.\nOur findings reveal that a depthwise stacking operator, called $G_{\\text{stack}}$, exhibits remarkable acceleration in training, leading to decreased loss and improved overall performance on eight standard NLP benchmarks compared to strong baselines. \nMotivated by these promising results, we conduct extensive experiments to delve deeper into $G_{\\text{stack}}$ to address $\\textit{O}$2 and $\\textit{O}$3.\nFor $\\textit{O}$2 (untested scalability), our study shows that $G_{\\text{stack}}$ is scalable and consistently performs well, with experiments up to 7B LLMs after growth and pre-training LLMs with 750B tokens.\nFor example, compared to a conventionally trained 7B model using 300B tokens, our $G_{\\text{stack}}$ model converges to the same loss with 194B tokens, resulting in a 54.6\\% speedup. \nWe further address $\\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines to determine growth timing and growth factor for $G_{\\text{stack}}$, making it practical in general LLM pre-training.\nWe also provide in-depth discussions and comprehensive ablation studies of $G_{\\text{stack}}$. \nOur code and pre-trained model are available at https://llm-stacking.github.io/.",
  "abstract_zh": "摘要: 由于规模庞大，LLM的预训练在计算上非常昂贵。模型增长通过利用较小的模型来加速较大模型的训练，成为一种有前景的方法。然而，这些模型增长方法在高效LLM预训练中的可行性仍未得到充分探索。本研究识别了三个关键障碍：（1）缺乏全面评估，（2）扩展性未经测试，（3）缺乏实证指导。为解决障碍1，我们将现有方法总结为四个基本增长操作符，并在标准化的LLM预训练环境中系统地评估它们。我们的研究结果表明，一种称为$G_{\\text{stack}}$的深度堆叠操作符在训练中表现出显著的加速效果，与强基线相比，在八个标准NLP基准上表现出更低的损失和更好的整体性能。受这些有前景结果的启发，我们进行了广泛的实验，深入研究$G_{\\text{stack}}$以解决障碍2和障碍3。对于障碍2（扩展性未经测试），我们的研究表明$G_{\\text{stack}}$具有可扩展性并始终表现良好，实验规模达到7B LLMs并使用750B tokens进行预训练。例如，与使用300B tokens常规训练的7B模型相比，我们的$G_{\\text{stack}}$模型使用194B tokens收敛到相同的损失，速度提高了54.6%。我们进一步通过制定指导方针来确定$G_{\\text{stack}}$的增长时机和增长因子，从而解决障碍3（缺乏实证指导），使其在一般LLM预训练中具有实用性。我们还提供了关于$G_{\\text{stack}}$的深入讨论和全面的消融研究。我们的代码和预训练模型可在https://llm-stacking.github.io/获取。"
}
{
  "title": "Policy Improvement using Language Feedback Models",
  "title_zh": "标题：使用语言反馈模型进行策略改进",
  "abstract": "We introduce Language Feedback Models (LFMs) that identify desirable behaviour --- actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFMs can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.",
  "abstract_zh": "摘要：我们引入了语言反馈模型（LFMs），用于识别模仿学习中指令跟随任务中有助于实现目标的理想行为。为了训练LFMs，我们从大型语言模型（LLMs）中获取对视觉轨迹的语言描述反馈。首先，通过使用LFMs识别要模仿的理想行为，我们在三个不同的语言基础环境（Touchdown、ScienceWorld 和 ALFWorld）中相较于强大的行为克隆基线提高了任务完成率。其次，当控制LLM输出标记数量时，LFMs在直接预测动作方面优于将LLMs用作专家。第三，LFMs能够推广到未见过的环境，通过一轮适应提高任务完成率3.5-12.0%。最后，LFMs可以被修改为提供人类可解释的反馈而不损失性能，从而允许人类验证模仿学习的理想行为。"
}
{
  "title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts",
  "title_zh": "标题：彩虹团队：多样性对抗提示的开放式生成",
  "abstract": "As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel black-box approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem and uses open-ended search to generate prompts that are both effective and diverse. Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90% across all tested models. Furthermore, we demonstrate that prompts generated by Rainbow Teaming are highly transferable and that fine-tuning models with synthetic data generated by our method significantly enhances their safety without sacrificing general performance or helpfulness. We additionally explore the versatility of Rainbow Teaming by applying it to question answering and cybersecurity, showcasing its potential to drive robust open-ended self-improvement in a wide range of applications.",
  "abstract_zh": "摘要：随着大型语言模型（LLMs）在许多实际应用中变得越来越普遍，理解和增强其对抗攻击的鲁棒性至关重要。现有识别对抗提示的方法往往专注于特定领域，缺乏多样性，或需要大量人工标注。为了解决这些限制，我们提出了彩虹团队，这是一种新颖的黑箱方法，用于生成多样化的对抗提示集合。彩虹团队将对抗提示生成视为质量-多样性问题，并使用开放式搜索生成既有效又多样的提示。专注于安全领域，我们使用彩虹团队针对各种最先进的LLM，包括Llama 2和Llama 3模型。我们的方法揭示了数百个有效的对抗提示，在所有测试模型中攻击成功率超过90%。此外，我们证明了彩虹团队生成的提示具有高度的可转移性，并且通过我们的方法生成的合成数据对模型进行微调，显著增强了其安全性，而不牺牲整体性能或有用性。我们还通过将其应用于问答和网络安全，探索了彩虹团队的多功能性，展示了其在广泛应用中推动稳健开放式自我改进的潜力。"
}
{
  "title": "Segmenting Watermarked Texts From Language Models",
  "title_zh": "标题: 从语言模型中分割带水印的文本",
  "abstract": "Watermarking is a technique that involves embedding nearly unnoticeable statistical signals within generated content to help trace its source. This work focuses on a scenario where an untrusted third-party user sends prompts to a trusted language model (LLM) provider, who then generates a text from their LLM with a watermark. This setup makes it possible for a detector to later identify the source of the text if the user publishes it. The user can modify the generated text by substitutions, insertions, or deletions. Our objective is to develop a statistical method to detect if a published text is LLM-generated from the perspective of a detector. We further propose a methodology to segment the published text into watermarked and non-watermarked sub-strings. The proposed approach is built upon randomization tests and change point detection techniques. We demonstrate that our method ensures Type I and Type II error control and can accurately identify watermarked sub-strings by finding the corresponding change point locations. To validate our technique, we apply it to texts generated by several language models with prompts extracted from Google's C4 dataset and obtain encouraging numerical results. We release all code publicly at https://github.com/doccstat/llm-watermark-cpd.",
  "abstract_zh": "摘要: 水印是一种技术，涉及在生成的内容中嵌入几乎不可察觉的统计信号，以帮助追踪其来源。本研究聚焦于一个场景：一个不受信任的第三方用户向一个受信任的语言模型（LLM）提供者发送提示，该提供者随后从他们的LLM生成带有水印的文本。此设置使得检测器可以在用户发布文本后识别其来源。用户可以通过替换、插入或删除来修改生成的文本。我们的目标是开发一种统计方法，以从检测器的角度检测发布的文本是否由LLM生成。我们进一步提出了一种方法，将发布的文本分割为带水印和不带水印的子字符串。所提出的方法基于随机化测试和变点检测技术。我们证明了我们的方法确保了I型和II型错误控制，并可以通过找到相应的变点位置准确识别带水印的子字符串。为了验证我们的技术，我们将其应用于由多个语言模型生成的文本，这些文本的提示来自谷歌的C4数据集，并获得了令人鼓舞的数值结果。我们在https://github.com/doccstat/llm-watermark-cpd上公开发布了所有代码。"
}
{
  "title": "HonestLLM: Toward an Honest and Helpful Large Language Model",
  "title_zh": "诚实LLM：迈向诚实且有帮助的大型语言模型",
  "abstract": "Large Language Models (LLMs) have achieved remarkable success across various industries and applications, owing to their exceptional generative capabilities. Nevertheless, honesty and helpfulness, which ensure safe and useful real-world deployments, have been considered as the longstanding cornerstones in practice. In this paper, we first established comprehensive principles for honesty LLM and further created the HoneSet with 930 queries across six categories, which is designed to evaluate LLMs’ ability to maintain honesty. Then, we improved the honesty and helpfulness of LLMs in both training-free and fine-tuning settings. Specifically, we propose a training-free method named Curiosity-Driven Prompting, which enables LLMs to express their internal confusion and uncertainty about the given query and then optimize their responses. Moreover, we also propose a two-stage fine-tuning approach, inspired by curriculum learning, to enhance the honesty and helpfulness of LLMs. The method first teaches LLMs to distinguish between honest and dishonest, and then LLMs are trained to learn to respond more helpfully. Experimental results demonstrated that both of the two proposed methods improve the helpfulness of LLMs while making them maintain honesty. Our research has paved the way for more reliable and trustworthy LLMs in real-world applications.",
  "abstract_zh": "大型语言模型（LLM）因其卓越的生成能力在各个行业和应用中取得了显著成功。然而，诚实性和有用性是确保其在现实世界中安全且有用的部署的长期基石。在本文中，我们首先为诚实LLM建立了全面的原则，并进一步创建了包含六个类别930个查询的HoneSet，用于评估LLM保持诚实的能力。随后，我们在无训练和微调设置中提高了LLM的诚实性和有用性。具体而言，我们提出了一种名为好奇驱动提示的无训练方法，使LLM能够表达其对给定查询的内部困惑和不确定性，然后优化其响应。此外，我们还提出了一种受课程学习启发的两阶段微调方法，以增强LLM的诚实性和有用性。该方法首先教导LLM区分诚实和不诚实，然后训练LLM学习更有帮助地回应。实验结果表明，这两种方法都提高了LLM的有用性，同时保持了诚实性。我们的研究为现实世界应用中更可靠和值得信赖的LLM铺平了道路。"
}
{
  "title": "Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs",
  "title_zh": "标题: 像金鱼一样，不要记忆！缓解生成型大型语言模型中的记忆现象",
  "abstract": "Large language models can memorize and repeat their training data, causing privacy and copyright risks. To mitigate memorization, we introduce a subtle modification to the next-token training objective that we call the goldfish loss. During training, a randomly sampled subsets of tokens are excluded from the loss computation. These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set. We run extensive experiments training billion-scale LLaMA-2 models, both pre-trained and trained from scratch, and demonstrate significant reductions in extractable memorization with little to no impact on downstream benchmarks.\n\n_Code and checkpoints: https://github.com/ahans30/goldfish-loss_",
  "abstract_zh": "摘要: 大型语言模型可能会记忆并重复其训练数据，从而引发隐私和版权风险。为缓解记忆现象，我们引入了一种对下一个词元训练目标的微妙修改，称为金鱼损失。在训练过程中，随机采样的词元子集被排除在损失计算之外。这些被丢弃的词元不会被模型记忆，从而防止从训练集中逐字重现完整的词元链。我们进行了大量实验，训练了十亿规模的LLaMA-2模型，包括预训练和从头开始训练，并证明在几乎不影响下游基准测试的情况下，显著减少了可提取的记忆现象。"
}
{
  "title": "Language Model as Visual Explainer",
  "title_zh": "标题：作为视觉解释器的语言模型",
  "abstract": "In this paper, we present Language Model as Visual Explainer (\\texttt{LVX}), a systematic approach for interpreting the internal workings of vision models using a tree-structured linguistic explanation, without the need for model training. Central to our strategy is the collaboration between vision models and LLM to craft explanations. On one hand,  the LLM is harnessed to delineate hierarchical visual attributes, while concurrently,  a text-to-image API retrieves  images that are most aligned with these textual concepts. By mapping the collected texts and images  to the vision model's embedding space,  we construct a hierarchy-structured visual embedding tree. This tree is dynamically pruned and grown by querying the LLM using language templates, tailoring the explanation to the model.  Such a scheme allows us \nto seamlessly incorporate new attributes while eliminating undesired concepts based on the model's representations. When applied to testing samples, our method provides human-understandable explanations in the form of attribute-laden trees. Beyond explanation, we retrained the vision model by calibrating it on the generated concept hierarchy, allowing the model to incorporate the refined knowledge of visual attributes. To access the effectiveness of our approach, we introduce new benchmarks and conduct rigorous evaluations, demonstrating its plausibility, faithfulness, and stability.",
  "abstract_zh": "摘要：在本文中，我们提出了作为视觉解释器的语言模型（\\texttt{LVX}），这是一种利用树状语言解释来解读视觉模型内部工作机制的系统方法，无需进行模型训练。我们策略的核心是视觉模型与大型语言模型（LLM）的协作以生成解释。一方面，LLM被用于描绘层次化的视觉属性，同时，文本到图像的API检索与这些文本概念最匹配的图像。通过将收集的文本和图像映射到视觉模型的嵌入空间，我们构建了一个层次结构化的视觉嵌入树。通过使用语言模板查询LLM，该树被动态修剪和生长，从而根据模型量身定制解释。这种方案使我们能够无缝地整合新属性，同时根据模型的表示消除不需要的概念。在应用于测试样本时，我们的方法以属性丰富的树形结构提供人类可理解的解释。除了解释之外，我们通过在生成的概念层次结构上校准视觉模型进行再训练，使模型能够整合视觉属性的精炼知识。为了评估我们方法的有效性，我们引入了新的基准并进行了严格的评估，证明了其可行性、忠实性和稳定性。"
}
{
  "title": "Divergences between Language Models and Human Brains",
  "title_zh": "语言模型与人脑之间的差异",
  "abstract": "Do machines and humans process language in similar ways? Recent research has hinted at the affirmative, showing that human neural activity can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using an LLM-based data-driven approach, we identify two domains that LMs do not capture well: social/emotional intelligence and physical commonsense. We validate these findings with human behavioral experiments and hypothesize that the gap is due to insufficient representations of social/emotional and physical knowledge in LMs. Our results show that fine-tuning LMs on these domains can improve their alignment with human brain responses.",
  "abstract_zh": "机器和人类处理语言的方式是否相似？最近的研究暗示了肯定的答案，显示人类的神经活动可以通过语言模型（LMs）的内部表示有效预测。尽管这些结果被认为反映了LMs和人脑之间共享的计算原则，但在LMs和人类如何表示和使用语言方面也存在明显差异。在这项工作中，我们通过检查LM表示与人脑对语言的反应之间的差异，系统地探索了人类和机器语言处理之间的分歧，这些反应是通过磁脑图（MEG）在两个数据集中测量的，其中受试者阅读和聆听叙述故事。使用基于大型语言模型（LLM）的数据驱动方法，我们识别出LMs未能很好捕捉的两个领域：社会/情感智能和物理常识。我们通过人类行为实验验证了这些发现，并假设这种差距是由于LMs中对社会/情感和物理知识的表示不足所致。我们的结果表明，在这些领域对LMs进行微调可以改善其与人脑反应的对齐。"
}
{
  "title": "Aligning LLM Agents by Learning Latent Preference from User Edits",
  "title_zh": "标题：通过学习用户编辑的潜在偏好来调整大型语言模型代理",
  "abstract": "We study interactive learning of language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data and using it to define a prompt policy that drives future response generation. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages a large language model (LLM) to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, for evaluation using a GPT-4 simulated user. We compare with algorithms that directly retrieve user edits but do not learn descriptive preference, and algorithms that learn context-agnostic preference. On both tasks, CIPHER outperforms baselines by achieving the lowest edit distance cost. Meanwhile, CIPHER has a lower computational expense, as using learned preference results in a shorter prompt than directly using user edits. Our further analysis reports that the user preference learned by CIPHER shows significant similarity to the ground truth latent preference.",
  "abstract_zh": "摘要：我们研究了基于用户对代理输出进行编辑的交互式语言代理学习。在典型的设置中，如写作助手，用户与语言代理互动以在给定上下文的情况下生成响应，并可能选择性地编辑代理响应，以根据他们的潜在偏好进行个性化处理，此外还提高了正确性。编辑反馈是自然生成的，使其成为改善代理与用户偏好对齐的合适候选，并随着时间的推移降低用户编辑的成本。我们提出了一个学习框架，PRELUDE，它基于历史编辑数据推断用户的潜在偏好描述，并使用其定义驱动未来响应生成的提示策略。这避免了对代理进行微调，这既昂贵，又难以随着用户数量的增加而扩展，甚至可能降低其在其他任务上的性能。此外，学习描述性偏好提高了解释性，允许用户查看和修改学习到的偏好。然而，用户偏好可能复杂且因上下文而异，学习起来具有挑战性。为了解决这个问题，我们提出了一个简单而有效的算法CIPHER，利用大型语言模型（LLM）根据用户编辑推断给定上下文的用户偏好。未来，CIPHER从历史中k个最接近的上下文中检索推断的偏好，并形成一个聚合偏好用于响应生成。我们引入了两个交互环境——摘要和电子邮件写作，用于使用GPT-4模拟用户进行评估。我们与直接检索用户编辑但不学习描述性偏好的算法，以及学习与上下文无关偏好的算法进行比较。在这两项任务中，CIPHER通过实现最低的编辑距离成本优于基线。同时，CIPHER的计算开销较低，因为使用学习到的偏好导致提示比直接使用用户编辑更短。我们的进一步分析报告称，CIPHER学习到的用户偏好与真实潜在偏好表现出显著的相似性。"
}
{
  "title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve",
  "title_zh": "递归内省：教语言模型代理如何自我改进",
  "abstract": "A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially. In this paper, we develop $\\textbf{RISE:}$ $\\textbf{R}$ecursive $\\textbf{I}$ntro$\\textbf{S}$p$\\textbf{E}$ction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation and offline reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models, without disrupting one-turn abilities as a result of expressing more complex distributions.",
  "abstract_zh": "在基础模型中实现智能代理行为的关键是使它们能够对自己的行为进行自省、推理，并在有更多计算或交互时纠正错误。即使是最强大的专有大型语言模型（LLMs）也未能表现出持续顺序改进其响应的能力。在本文中，我们开发了$\\textbf{RISE:}$ $\\textbf{R}$ecursive $\\textbf{I}$ntro$\\textbf{S}$p$\\textbf{E}$ction，一种微调LLMs的方法，以引入这种能力，尽管先前的工作假设这种能力可能无法实现。我们的方法规定了一种迭代微调程序，试图教会模型在先前未能成功解决困难测试问题后如何改变其响应，选择性地加入额外的环境反馈。RISE将单轮提示的微调视为解决多轮马尔可夫决策过程（MDP），其中初始状态是提示。受在线模仿和离线强化学习原则的启发，我们提出了多轮数据收集和训练的策略，以赋予LLM在后续迭代中递归检测和纠正其先前错误的能力。我们的实验表明，RISE使Llama2、Llama3和Mistral模型能够在推理任务中通过更多轮次自我改进，优于在相同推理时间计算量下的多种单轮策略。我们还发现，RISE具有良好的扩展性，通常在更强大的模型上获得更大的收益，而不会因表达更复杂的分布而破坏单轮能力。"
}
{
  "title": "Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving",
  "title_zh": "[翻译失败] Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving",
  "abstract": "\\emph{Metacognitive knowledge} refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also  have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure  to get a powerful  LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans.\n\nTo validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH.  (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label.  This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic,  even though this article applies it to math problems.",
  "abstract_zh": "[翻译失败] \\emph{Metacognitive knowledge} refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also  have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure  to get a powerful  LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans.\n\nTo validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH.  (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label.  This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic,  even though this article applies it to math problems."
}
{
  "title": "Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs",
  "title_zh": "Title: 图上规划：知识图谱上大语言模型的自纠正自适应规划",
  "abstract": "Large Language Models (LLMs) have shown remarkable reasoning capabilities on complex tasks, but they still suffer from out-of-date knowledge, hallucinations, and opaque decision-making. In contrast, Knowledge Graphs (KGs) can provide explicit and editable knowledge for LLMs to alleviate these issues. Existing paradigm of KG-augmented LLM manually predefines the breadth of exploration space and requires flawless navigation in KGs. However, this paradigm cannot adaptively explore reasoning paths in KGs based on the question semantics and self-correct erroneous reasoning paths, resulting in a bottleneck in efficiency and effect. To address these limitations, we propose a novel self-correcting adaptive planning paradigm for KG-augmented LLM named Plan-on-Graph (PoG), which first decomposes the question into several sub-objectives and then repeats the process of adaptively exploring reasoning paths, updating memory, and reflecting on the need to self-correct erroneous reasoning paths until arriving at the answer. Specifically, three important mechanisms of Guidance, Memory, and Reflection are designed to work together, to guarantee the adaptive breadth of self-correcting planning for graph reasoning. Finally, extensive experiments on three real-world datasets demonstrate the effectiveness and efficiency of PoG.",
  "abstract_zh": "Abstract: 大语言模型（LLMs）在复杂任务上展现了卓越的推理能力，但仍然存在知识过时、幻觉和决策不透明的问题。相比之下，知识图谱（KGs）可以为LLMs提供明确且可编辑的知识以缓解这些问题。现有的KG增强LLM范式手动预定义了探索空间的广度，并要求在KG中进行完美导航。然而，这种范式无法根据问题语义自适应地探索KG中的推理路径，并自我纠正错误的推理路径，导致效率和效果的瓶颈。为了解决这些限制，我们提出了一种新的KG增强LLM自纠正自适应规划范式，称为图上规划（PoG），首先将问题分解为若干子目标，然后重复自适应探索推理路径、更新记忆并反思是否需要自我纠正错误推理路径的过程，直到到达答案。具体来说，设计了指导、记忆和反思三个重要机制协同工作，以保证图推理自纠正规划的自适应广度。最后，在三个真实世界数据集上的大量实验证明了PoG的有效性和效率。"
}
{
  "title": "DAGER: Exact Gradient Inversion for Large Language Models",
  "title_zh": "DAGER: 大型语言模型的精确梯度反演",
  "abstract": "Federated learning works by aggregating locally computed gradients from multiple clients, thus enabling collaborative training without sharing private client data. However, prior work has shown that the data can actually be recovered by the server using so-called gradient inversion attacks. While these attacks perform well when applied on images, they are limited in the text domain and only permit approximate reconstruction of small batches and short input sequences. In this work, we propose DAGER, the first algorithm to recover whole batches of input text exactly. DAGER leverages the low-rank structure of self-attention layer gradients and the discrete nature of token embeddings to efficiently check if a given token sequence is part of the client data. We use this check to exactly recover full batches in the honest-but-curious setting without any prior on the data for both encoder and decoder-based architectures using exhaustive heuristic search and a greedy approach, respectively. We provide an efficient GPU implementation of DAGER and show experimentally that it recovers full batches of size up to 128 on large language models (LLMs), beating prior attacks in speed (20x at same batch size), scalability (10x larger batches), and reconstruction quality (ROUGE-1/2 > 0.99).",
  "abstract_zh": "联邦学习通过聚合来自多个客户端的本地计算梯度实现协作训练，从而无需共享私人客户端数据。然而，先前的研究表明，服务器可以通过所谓的梯度反演攻击恢复数据。虽然这些攻击在应用于图像时表现良好，但在文本领域受到限制，仅允许对小批次和短输入序列进行近似重构。在这项工作中，我们提出了DAGER，这是第一个能够精确恢复整个输入文本批次的算法。DAGER利用自注意力层梯度的低秩结构和令牌嵌入的离散特性，来有效检查给定的令牌序列是否是客户端数据的一部分。我们利用这一检查在诚实但好奇的环境中，使用穷举启发式搜索和贪婪方法，分别在编码器和解码器架构中精确恢复完整批次，而无需对数据进行任何先验假设。我们提供了DAGER的高效GPU实现，并通过实验表明，它在大型语言模型（LLMs）上恢复的完整批次大小可达128，速度比之前的攻击快20倍（在相同批次大小下），可扩展性提高10倍（更大批次），重构质量（ROUGE-1/2 > 0.99）更优。"
}
{
  "title": "Can Large Language Model Agents Simulate Human Trust Behavior?",
  "title_zh": "标题：大型语言模型代理能否模拟人类信任行为？",
  "abstract": "Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents really simulate human behavior? In this paper, we focus on one critical and elemental behavior in human interactions, trust, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the feasibility of simulating human trust behavior with LLM agents. In addition,  we probe the biases of agent trust and  differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond value alignment. We further illustrate broader implications of our discoveries for applications where trust is paramount.",
  "abstract_zh": "摘要：大型语言模型（LLM）代理作为模拟工具在社会科学和角色扮演应用中被越来越多地采用来建模人类。然而，一个基本问题仍然存在：LLM代理真的能模拟人类行为吗？在本文中，我们专注于人类互动中的一种关键且基本的行为——信任，并研究LLM代理是否能够模拟人类的信任行为。我们首先发现，在行为经济学中广泛认可的信任游戏框架下，LLM代理通常表现出信任行为，称为代理信任。然后，我们发现GPT-4代理在信任行为方面与人类表现出高度的行为一致性，这表明使用LLM代理模拟人类信任行为的可行性。此外，我们探讨了代理信任的偏见以及代理对其他LLM代理和人类的信任差异。我们还研究了在包括外部操控和高级推理策略的条件下代理信任的内在属性。我们的研究为LLM代理的行为以及LLM与人类之间的基本类比提供了新的见解，超越了价值对齐。我们进一步阐明了我们的发现对信任至关重要的应用的更广泛影响。"
}
{
  "title": "Accelerating Greedy Coordinate Gradient and General Prompt Optimization via Probe Sampling",
  "title_zh": "标题：通过探测采样加速贪婪坐标梯度和通用提示优化",
  "abstract": "Safety of Large Language Models (LLMs) has become a central issue given their rapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown to be effective in constructing prompts containing adversarial suffixes to break the presumingly safe LLMs, but the optimization of GCG is time-consuming and limits its practicality. To reduce the time cost of GCG and enable more comprehensive studies of LLM safety, in this work, we study a new algorithm called $\\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core of the algorithm is a mechanism that dynamically determines how similar a smaller draft model's predictions are to the target model's predictions for prompt candidates. When the target model is similar to the draft model, we rely heavily on the draft model to filter out a large number of potential prompt candidates to reduce the computation time. Probe sampling achieves up to $5.6$ times speedup using Llama2-7b-chat and leads to equal or improved attack success rate (ASR) on the AdvBench. Furthermore, probe sampling is also able to accelerate other prompt optimization techniques and adversarial attack methods, leading to acceleration of $1.8\\times$ for AutoPrompt, $2.4\\times$ for APE and $2.4\\times$ for AutoDAN.",
  "abstract_zh": "摘要：鉴于大型语言模型（LLMs）的快速发展和广泛应用，其安全性已成为核心问题。贪婪坐标梯度（GCG）已被证明在构建包含对抗性后缀的提示以攻破假设安全的LLMs方面有效，但GCG的优化过程耗时，限制了其实用性。为了降低GCG的时间成本并进行更全面的LLM安全性研究，本文研究了一种名为$\\texttt{Probe sampling}$的新算法来加速GCG算法。该算法的核心机制是动态确定一个较小的草稿模型的预测与目标模型对提示候选的预测的相似程度。当目标模型与草稿模型相似时，我们主要依赖草稿模型来筛选大量潜在的提示候选，以减少计算时间。探测采样在使用Llama2-7b-chat时实现了高达$5.6$倍的加速，并在AdvBench上达到了相同或更高的攻击成功率（ASR）。此外，探测采样还能够加速其他提示优化技术和对抗攻击方法，对AutoPrompt加速$1.8\\times$，对APE加速$2.4\\times$，对AutoDAN加速$2.4\\times$。"
}
{
  "title": "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space",
  "title_zh": "软提示威胁：通过嵌入空间攻击开源大型语言模型的安全对齐和去学习",
  "abstract": "Current research in adversarial robustness of LLMs focuses on \\textit{discrete} input manipulations in the natural language space, which can be directly transferred to \\textit{closed-source} models. However, this approach neglects the steady progression of \\textit{open-source} models. As open-source models advance in capability, ensuring their safety becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the \\textit{embedding space attack}, which directly attacks the \\textit{continuous} embedding representation of input tokens.\nWe find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Additionally, we demonstrate that models compromised by embedding attacks can be used to create discrete jailbreaks in natural language. Lastly, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source LLMs.",
  "abstract_zh": "当前关于大型语言模型（LLMs）对抗性鲁棒性的研究主要集中在自然语言空间中的\\textit{离散}输入操作，这些操作可以直接转移到\\textit{闭源}模型。然而，这种方法忽视了\\textit{开源}模型的稳步发展。随着开源模型能力的提升，确保其安全性变得越来越重要。然而，针对开源LLMs的攻击，尤其是利用完整模型访问的攻击，仍然很少被探索。我们填补了这一研究空白，并提出了\\textit{嵌入空间攻击}，该攻击直接针对输入标记的\\textit{连续}嵌入表示。我们发现，嵌入空间攻击比离散攻击或模型微调更有效地规避模型对齐并触发有害行为。此外，我们证明了被嵌入攻击破坏的模型可以用于在自然语言中创建离散的越狱。最后，我们在去学习的背景下提出了一种新颖的威胁模型，并展示了嵌入空间攻击可以从多个数据集和模型中提取出被认为已删除的信息。我们的研究结果强调了嵌入空间攻击作为开源LLMs中的一个重要威胁模型。"
}
{
  "title": "DPIC: Decoupling Prompt and Intrinsic Characteristics for LLM Generated Text Detection",
  "title_zh": "标题：DPIC：解耦提示与内在特征以检测大型语言模型生成的文本",
  "abstract": "Large language models (LLMs) have the potential to generate texts that pose risks of misuse, such as plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Consequently, detecting whether a text is generated by LLMs has become increasingly important. Existing high-quality detection methods usually require access to the interior of the model to extract the intrinsic characteristics. However, since we do not have access to the interior of the black-box model, we must resort to surrogate models, which impacts detection quality. In order to achieve high-quality detection of black-box models, we would like to extract deep intrinsic characteristics of the black-box model generated texts. We view the generation process as a coupled process of prompt and intrinsic characteristics of the generative model. Based on this insight, we propose to decouple prompt and intrinsic characteristics (DPIC) for LLM-generated text detection method. Specifically, given a candidate text, DPIC employs an auxiliary LLM to reconstruct the prompt corresponding to the candidate text, then uses the prompt to regenerate text by the auxiliary LLM, which makes the candidate text and the regenerated text align with their prompts, respectively. Then, the similarity between the candidate text and the regenerated text is used as a detection feature, thus eliminating the prompt in the detection process, which allows the detector to focus on the intrinsic characteristics of the generative model. Compared to the baselines, DPIC has achieved an average improvement of 6.76\\% and 2.91\\% in detecting texts from different domains generated by GPT4 and Claude3, respectively.",
  "abstract_zh": "摘要：大型语言模型（LLMs）有生成文本的潜力，这些文本可能被滥用，如剽窃、在电商平台上植入虚假评论或创建煽动性虚假推文。因此，检测文本是否由LLMs生成变得越来越重要。现有的高质量检测方法通常需要访问模型内部以提取内在特征。然而，由于我们无法访问黑箱模型的内部，只能依赖替代模型，这会影响检测质量。为了实现对黑箱模型的高质量检测，我们希望提取黑箱模型生成文本的深层内在特征。我们将生成过程视为生成模型的提示和内在特征的耦合过程。基于这一见解，我们提出了解耦提示与内在特征（DPIC）的方法用于LLM生成文本的检测。具体来说，给定一个候选文本，DPIC使用辅助LLM重构与候选文本对应的提示，然后使用提示由辅助LLM重新生成文本，使候选文本和重新生成的文本分别与其提示对齐。然后，候选文本与重新生成文本之间的相似性被用作检测特征，从而在检测过程中消除提示，使检测器能够专注于生成模型的内在特征。与基线相比，DPIC在检测由GPT4和Claude3生成的不同领域文本时，分别实现了平均6.76%和2.91%的提升。"
}
{
  "title": "Bias Amplification in Language Model Evolution: An Iterated Learning Perspective",
  "title_zh": "语言模型演化中的偏见放大：迭代学习视角",
  "abstract": "With the widespread adoption of Large Language Models (LLMs), the prevalence of iterative interactions among these models is anticipated to increase. Notably, recent advancements in multi-round on-policy self-improving methods allow LLMs to generate new examples for training subsequent models. At the same time, multi-agent LLM systems, involving automated interactions among agents, are also increasing in prominence. Thus, in both short and long terms, LLMs may actively engage in an evolutionary process. We draw parallels between the behavior of LLMs and the evolution of human culture, as the latter has been extensively studied by cognitive scientists for decades. Our approach involves leveraging Iterated Learning (IL), a Bayesian framework that elucidates how subtle biases are magnified during human cultural evolution, to explain some behaviors of LLMs. This paper outlines key characteristics of agents' behavior in the Bayesian-IL framework, including predictions that are supported by experimental verification with various LLMs. This theoretical framework could help to more effectively predict and guide the evolution of LLMs in desired directions.",
  "abstract_zh": "随着大型语言模型（LLMs）的广泛应用，这些模型之间的迭代交互预计将会增加。值得注意的是，最近在多轮在策略自我改进方法方面的进展允许LLMs生成新的示例以训练后续模型。同时，涉及代理之间自动交互的多代理LLM系统也日益受到关注。因此，在短期和长期内，LLMs可能会积极参与进化过程。我们将LLMs的行为与人类文化的演化相比较，后者已被认知科学家研究了数十年。我们的方法涉及利用迭代学习（IL），一种贝叶斯框架，阐明了在文化演化过程中如何放大微妙的偏见，以解释LLMs的一些行为。本文概述了贝叶斯-IL框架中代理行为的关键特征，包括通过各种LLMs的实验验证支持的预测。该理论框架可以帮助更有效地预测和引导LLMs朝着期望的方向演化。"
}
{
  "title": "SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures",
  "title_zh": "自我发现：大型语言模型自我构建推理结构",
  "abstract": "We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2’s performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.",
  "abstract_zh": "我们介绍了SELF-DISCOVER，这是一个通用框架，使大型语言模型能够自我发现任务内在的推理结构，以解决对于典型提示方法具有挑战性的复杂推理问题。该框架的核心是一个自我发现过程，其中大型语言模型选择多个原子推理模块，如批判性思维和逐步思考，并将它们组合成一个明确的推理结构，以供大型语言模型在解码过程中遵循。SELF-DISCOVER显著提高了GPT-4和PaLM 2在具有挑战性的推理基准测试（如BigBench-Hard、基于基础的代理推理和MATH）上的表现，与思路链（CoT）相比，提升幅度高达32%。此外，SELF-DISCOVER在推理密集型方法（如CoT-Self-Consistency）上表现出色，提升超过20%，同时所需的推理计算减少了10至40倍。最后，我们展示了自我发现的推理结构在不同模型家族中具有普遍适用性：从PaLM 2-L到GPT-4，从GPT-4到Llama2，并且与人类的推理模式存在共性。"
}
{
  "title": "Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters",
  "title_zh": "通过密码字符破解大型语言模型的内容审核防护",
  "abstract": "Large Language Models (LLMs) are typically harmless but remain vulnerable to carefully crafted prompts known as ``jailbreaks'', which can bypass protective measures and induce harmful behavior. Recent advancements in LLMs have incorporated moderation guardrails that can filter outputs, which trigger processing errors for certain malicious questions. Existing red-teaming benchmarks often neglect to include questions that trigger moderation guardrails, making it difficult to evaluate jailbreak effectiveness. To address this issue, we introduce JAMBench, a harmful behavior benchmark designed to trigger and evaluate moderation guardrails. JAMBench involves 160 manually crafted instructions covering four major risk categories at multiple severity levels. Furthermore, we propose a jailbreak method, JAM (Jailbreak Against Moderation), designed to attack moderation guardrails using jailbreak prefixes to bypass input-level filters and a fine-tuned shadow model functionally equivalent to the guardrail model to generate cipher characters to bypass output-level filters. Our extensive experiments on four LLMs demonstrate that JAM achieves higher jailbreak success ($\\sim$ $\\times$ 19.88) and lower filtered-out rates ($\\sim$ $\\times$ 1/6) than baselines.",
  "abstract_zh": "大型语言模型（LLMs）通常是无害的，但仍然容易受到精心设计的提示（称为“越狱”）的攻击，这些提示可以绕过保护措施并引发有害行为。最近在LLMs中的进展已纳入内容审核防护措施，可以过滤输出，从而对某些恶意问题触发处理错误。现有的红队基准测试常常忽略包含触发内容审核防护的问题，使得评估越狱的有效性变得困难。为了解决这个问题，我们引入了JAMBench，这是一种旨在触发和评估内容审核防护的有害行为基准。JAMBench包括160个手动设计的指令，涵盖四个主要风险类别的多个严重程度等级。此外，我们提出了一种越狱方法，JAM（针对审核的越狱），旨在使用越狱前缀攻击内容审核防护，以绕过输入级别的过滤器，并使用功能上等同于防护模型的微调影子模型生成密码字符，以绕过输出级别的过滤器。我们在四个LLMs上的广泛实验表明，JAM比基线实现了更高的越狱成功率（约$\\sim$ $\\times$ 19.88）和更低的过滤率（约$\\sim$ $\\times$ 1/6）。"
}
{
  "title": "Linking In-context Learning in Transformers to Human Episodic Memory",
  "title_zh": "标题：将变压器中的上下文学习与人类情景记忆联系起来",
  "abstract": "Understanding connections between artificial and biological intelligent systems can reveal fundamental principles of general intelligence. While many artificial intelligence models have a neuroscience counterpart, such connections are largely missing in Transformer models and the self-attention mechanism. Here, we examine the relationship between interacting attention heads and human episodic memory. We focus on induction heads, which contribute to in-context learning in Transformer-based large language models (LLMs). We demonstrate that induction heads are behaviorally, functionally, and mechanistically similar to the contextual maintenance and retrieval (CMR) model of human episodic memory. Our analyses of LLMs pre-trained on extensive text data show that CMR-like heads often emerge in the intermediate and late layers, qualitatively mirroring human memory biases. The ablation of CMR-like heads suggests their causal role in in-context learning. Our findings uncover a parallel between the computational mechanisms of LLMs and human memory, offering valuable insights into both research fields.",
  "abstract_zh": "摘要：理解人工智能系统与生物智能系统之间的联系可以揭示通用智能的基本原理。虽然许多人工智能模型都有神经科学的对应物，但在变压器模型和自注意力机制中，这种联系大多缺失。在此，我们研究了交互注意力头与人类情景记忆之间的关系。我们重点关注归纳头，它们有助于基于变压器的大型语言模型（LLMs）中的上下文学习。我们证明了归纳头在行为、功能和机制上与人类情景记忆的上下文维护和检索（CMR）模型相似。我们对经过大量文本数据预训练的LLMs的分析表明，类似CMR的头通常出现在中间和后期层，定性地反映了人类记忆偏见。对类似CMR的头进行消融实验表明它们在上下文学习中的因果作用。我们的研究发现揭示了LLMs的计算机制与人类记忆之间的平行关系，为两个研究领域提供了宝贵的见解。"
}
{
  "title": "Many-Shot In-Context Learning",
  "title_zh": "多样本上下文学习",
  "abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples – the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated outputs. To mitigate this limitation, we explore two new settings: (1) \"Reinforced ICL\" that uses model-generated chain-of-thought rationales in place of human rationales, and (2) \"Unsupervised ICL\" where we remove rationales from the prompt altogether, and prompts the model only with domain-specific inputs. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. We demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to supervised fine-tuning. Finally, we reveal the limitations of next-token prediction loss as an indicator of downstream ICL performance.",
  "abstract_zh": "大型语言模型（LLMs）在少样本上下文学习（ICL）中表现出色——在推理时通过上下文中提供的少量示例进行学习，而无需更新权重。新扩展的上下文窗口让我们能够研究使用数百或数千个示例的ICL——即多样本模式。从少样本到多样本，我们观察到在各种生成和判别任务中显著的性能提升。尽管前景可观，多样本ICL可能会受到可用的人类生成输出数量的限制。为缓解这一限制，我们探索了两个新设置：（1）“强化ICL”，使用模型生成的思维链推理代替人类推理；（2）“无监督ICL”，完全从提示中移除推理，仅用特定领域的输入提示模型。我们发现，强化和无监督ICL在多样本模式中都相当有效，尤其是在复杂推理任务上。我们展示了，与少样本学习不同，多样本学习能够有效地覆盖预训练偏差，可以学习具有数值输入的高维函数，并且其表现可与监督微调相媲美。最后，我们揭示了下一个标记预测损失作为下游ICL性能指标的局限性。"
}
{
  "title": "Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs",
  "title_zh": "隐式多模态对齐：冻结的大型语言模型对多模态输入的泛化能力",
  "abstract": "Large Language Models (LLMs) have demonstrated impressive performance on multimodal tasks, without any multimodal finetuning. They are the de facto building block for Large Multimodal Models (LMMs), yet, we still lack a proper understanding of their success. In this work, we expose frozen LLMs to image, video, audio and text inputs and analyse their internal representation with the attempt to understand their generalization beyond textual inputs. Our work provides the following **findings.** Perceptual tokens (1) are easily distinguishable from textual ones inside LLMs, with significantly different representations (e.g. live in different narrow cones), and complete translation to textual tokens does not exists. Yet, (2) both perceptual and textual tokens activate similar LLM weights. Despite their differences, (3) perceptual tokens are implicitly aligned to textual tokens inside LLMs, we call this the implicit multimodal alignment effect (IMA), and argue that this is linked to architectural design, helping LLMs to generalize. This provide more evidence to believe that the generalization of LLMs to multimodal inputs is mainly due to their architecture.  These findings lead to several **implications.** This work provides several implications. (1) We find a positive correlation between the implicit alignment score and the task performance, suggesting that this could act as a proxy metric for model evaluation and selection. (2) A negative correlation exists regarding hallucinations (e.g. describing non-existing objects in images), revealing that this problem is mainly due to misalignment between the internal perceptual and textual representations. (3) Perceptual tokens change slightly throughout the model, thus, we propose different approaches to skip computations (e.g. in FFN layers), and significantly reduce the inference cost. (4) Due to the slowly changing embeddings across layers, and the high overlap between textual and multimodal activated weights, we compress LLMs by keeping only 1 subnetwork (called alpha-SubNet) that works well across a wide range of multimodal tasks. The code is available here: https://github.com/mshukor/ima-lmms.",
  "abstract_zh": "大型语言模型（LLMs）在多模态任务中表现出色，无需进行多模态微调。它们是大型多模态模型（LMMs）的实际构建块，但我们仍然缺乏对其成功的正确理解。在这项工作中，我们将冻结的LLMs暴露于图像、视频、音频和文本输入，并分析其内部表示，以尝试理解其超越文本输入的泛化能力。我们的工作提供了以下**发现**。感知标记（1）在LLMs内部易于与文本标记区分，具有显著不同的表示（例如，存在于不同的狭窄锥体中），且不存在完全翻译为文本标记。然而，（2）感知和文本标记激活类似的LLM权重。尽管存在差异，（3）感知标记在LLMs内部隐式对齐到文本标记，我们称之为隐式多模态对齐效应（IMA），并认为这与架构设计有关，帮助LLMs泛化。这提供了更多证据表明LLMs对多模态输入的泛化主要归因于其架构。这些发现导致了几个**启示**。这项工作提供了几个启示。（1）我们发现隐式对齐分数与任务性能之间存在正相关，表明这可以作为模型评估和选择的代理指标。（2）关于幻觉（例如描述图像中不存在的对象）存在负相关，揭示这个问题主要是由于内部感知和文本表示之间的不对齐。（3）感知标记在整个模型中略有变化，因此，我们提出不同的方法来跳过计算（例如在FFN层中），并显著降低推理成本。（4）由于跨层的嵌入缓慢变化，以及文本和多模态激活权重之间的高度重叠，我们通过仅保留1个子网络（称为alpha-SubNet）来压缩LLMs，该子网络在广泛的多模态任务中表现良好。代码可在此获取：https://github.com/mshukor/ima-lmms。"
}
{
  "title": "Boosting Alignment for Post-Unlearning Text-to-Image Generative Models",
  "title_zh": "标题：提升文本到图像生成模型的后去学习对齐",
  "abstract": "Large-scale generative models have shown impressive image-generation capabilities, propelled by massive data. However, this often inadvertently leads to the generation of harmful or inappropriate content and raises copyright concerns. Driven by these concerns, machine unlearning has become crucial to effectively purge undesirable knowledge from models. While existing literature has studied various unlearning techniques, these often suffer from either poor unlearning quality or degradation in text-image alignment after unlearning, due to the competitive nature of these objectives. To address these challenges, we propose a framework that seeks an optimal model update at each unlearning iteration, ensuring monotonic improvement on both objectives. We further derive the characterization of such an update.\n  In addition, we design procedures to strategically diversify the unlearning and remaining datasets to boost performance improvement. Our evaluation demonstrates that our method effectively removes target classes from recent diffusion-based generative models and concepts from stable diffusion models while maintaining close alignment with the models' original trained states, thus outperforming state-of-the-art baselines.",
  "abstract_zh": "摘要：大规模生成模型在图像生成方面展示了令人印象深刻的能力，这得益于海量数据。然而，这通常会无意中导致生成有害或不当内容，并引发版权问题。受这些问题驱动，机器去学习已成为有效清除模型中不良知识的关键。尽管现有文献研究了各种去学习技术，但由于这些目标的竞争性，这些技术往往在去学习质量或去学习后的文本-图像对齐方面表现不佳。为了解决这些挑战，我们提出了一个框架，旨在每次去学习迭代中寻求最佳模型更新，确保在两个目标上单调改进。我们进一步推导了这种更新的特征。此外，我们设计了程序以战略性地多样化去学习和剩余数据集，以提升性能改进。我们的评估表明，我们的方法有效地从最近的基于扩散的生成模型和稳定扩散模型中移除目标类别和概念，同时保持与模型原始训练状态的紧密对齐，从而优于最先进的基线。"
}
{
  "title": "Transformers need glasses! Information over-squashing in language tasks",
  "title_zh": "标题：Transformer需要眼镜！语言任务中的信息过度压缩",
  "abstract": "We study how information propagates in decoder-only Transformers, which are the architectural foundation of most existing frontier large language models (LLMs). We rely on a theoretical signal propagation analysis---specifically, we analyse the representations of the last token in the final layer of the Transformer, as this is the representation used for next-token prediction. Our analysis reveals a representational collapse phenomenon: we prove that certain distinct pairs of inputs to the Transformer can yield arbitrarily close representations in the final token. This effect is exacerbated by the low-precision floating-point formats frequently used in modern LLMs. As a result, the model is provably unable to respond to these sequences in different ways---leading to errors in, e.g., tasks involving counting or copying. Further, we show that decoder-only Transformer language models can lose sensitivity to specific tokens in the input, which relates to the well-known phenomenon of over-squashing in graph neural networks. We provide empirical evidence supporting our claims on contemporary LLMs. Our theory points to simple solutions towards ameliorating these issues.",
  "abstract_zh": "摘要：我们研究了信息在仅解码器Transformer中的传播方式，这些Transformer是大多数现有前沿大型语言模型（LLM）的架构基础。我们依赖于理论信号传播分析——具体来说，我们分析了Transformer最后一层中最后一个标记的表示，因为这是用于下一个标记预测的表示。我们的分析揭示了一种表示崩溃现象：我们证明了某些不同的输入对可以在最终标记中产生任意接近的表示。这种效应因现代LLM中常用的低精度浮点格式而加剧。因此，模型被证明无法以不同方式响应这些序列——导致在例如涉及计数或复制的任务中出现错误。此外，我们展示了仅解码器Transformer语言模型可能对输入中的特定标记失去敏感性，这与图神经网络中众所周知的过度压缩现象有关。我们提供了支持我们对当代LLM主张的实证证据。我们的理论指出了改善这些问题的简单解决方案。"
}
{
  "title": "Efficient Adversarial Training in LLMs with Continuous Attacks",
  "title_zh": "标题：在大型语言模型中通过连续攻击实现高效对抗训练",
  "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on five models from different families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.",
  "abstract_zh": "摘要：大型语言模型（LLMs）容易受到能够绕过其安全防护措施的对抗攻击。在许多领域，对抗训练已被证明是可靠提高对抗攻击鲁棒性最有前途的方法之一。然而，在LLMs的背景下，当前的对抗训练方法因在每次训练迭代中执行离散对抗攻击所需的高计算成本而受到阻碍。我们通过在LLM的连续嵌入空间中计算对抗攻击来解决这个问题，这比离散攻击高效得多。我们提出了一种快速对抗训练算法（C-AdvUL），由两个损失组成：第一个使模型在对抗行为数据集上计算的连续嵌入攻击中具有鲁棒性；第二个通过在效用数据上进行微调来确保最终模型的实用性。此外，我们引入了C-AdvIPO，一种不需要效用数据的对抗IPO变体，用于对抗性鲁棒对齐。我们对来自不同家族（Gemma、Phi3、Mistral、Zephyr、Llama2）和不同规模（2B、3.8B、7B）的五个模型进行的实证评估表明，这两种算法在保持效用的同时，显著增强了LLM对离散攻击（GCG、AutoDAN、PAIR）的鲁棒性。我们的结果表明，对连续扰动的鲁棒性可以外推到离散威胁模型。因此，我们展示了一条通向可扩展对抗训练算法的路径，以稳健地对齐LLMs。"
}
{
  "title": "Improved Generation of Adversarial Examples Against Safety-aligned LLMs",
  "title_zh": "标题: 针对安全对齐的大型语言模型的对抗样本生成改进",
  "abstract": "Adversarial prompts (or say, adversarial examples) generated using gradient-based methods exhibit outstanding performance in performing automatic jailbreak attacks against safety-aligned LLMs. Nevertheless, due to the discrete nature of texts, the input gradient of LLMs struggles to precisely reflect the magnitude of loss change that results from token replacements in the prompt, leading to limited attack success rates against safety-aligned LLMs, even in the *white-box* setting. In this paper, we explore a new perspective on this problem, suggesting that it can be alleviated by leveraging innovations inspired in transfer-based attacks that were originally proposed for attacking *black-box* image classification models. For the first time, we appropriate the ideologies of effective methods among these transfer-based attacks, *i.e.*, Skip Gradient Method and Intermediate Level Attack, into gradient-based adversarial prompt generation and achieve significant performance gains without introducing obvious computational cost. Meanwhile, by discussing mechanisms behind the gains, new insights are drawn, and proper combinations of these methods are also developed. Our empirical results show that 87% of the query-specific adversarial suffixes generated by the developed combination can induce Llama-2-7B-Chat to produce the output that exactly matches the target string on AdvBench. This match rate is 33% higher than that of a very strong baseline known as GCG, demonstrating advanced discrete optimization for adversarial prompt generation against LLMs. In addition, without introducing obvious cost, the combination achieves >30% absolute increase in attack success rates compared with GCG when generating both query-specific (38% ->68%) and universal adversarial prompts (26.68% -> 60.32%) for attacking the Llama-2-7B-Chat model on AdvBench.\nCode at: https://github.com/qizhangli/Gradient-based-Jailbreak-Attacks.",
  "abstract_zh": "摘要: 使用基于梯度的方法生成的对抗提示（或称对抗样本）在对安全对齐的大型语言模型进行自动越狱攻击时表现出色。然而，由于文本的离散特性，大型语言模型的输入梯度难以准确反映提示中令牌替换导致的损失变化幅度，从而导致即使在*白盒*环境下，对安全对齐的大型语言模型的攻击成功率也有限。在本文中，我们从一个新的视角探讨这一问题，建议通过借鉴最初为攻击*黑盒*图像分类模型而提出的基于转移的攻击中的创新来缓解这一问题。我们首次将这些基于转移的攻击中有效方法的理念，即跳过梯度法和中间层攻击，应用于基于梯度的对抗提示生成，并在不引入明显计算成本的情况下实现显著性能提升。同时，通过讨论这些提升背后的机制，得出新的见解，并开发出这些方法的适当组合。我们的实证结果表明，所开发组合生成的查询特定对抗后缀中有87%能诱导Llama-2-7B-Chat在AdvBench上生成与目标字符串完全匹配的输出。该匹配率比一个被称为GCG的非常强大的基准高出33%，展示了针对大型语言模型的对抗提示生成的先进离散优化。此外，在不引入明显成本的情况下，该组合在生成用于攻击AdvBench上的Llama-2-7B-Chat模型的查询特定（38% -> 68%）和通用对抗提示（26.68% -> 60.32%）时，相较于GCG实现了>30%的绝对攻击成功率提升。代码地址：https://github.com/qizhangli/Gradient-based-Jailbreak-Attacks。"
}
{
  "title": "EAI: Emotional Decision-Making of LLMs in Strategic Games and Ethical Dilemmas",
  "title_zh": "标题：EAI：大型语言模型在战略游戏和伦理困境中的情感决策",
  "abstract": "One of the urgent tasks of artificial intelligence is to assess the safety and alignment of large language models (LLMs) with human behavior. Conventional verification only in pure natural language processing benchmarks can be insufficient. Since emotions often influence human decisions, this paper examines LLM alignment in complex strategic and ethical environments, providing an in-depth analysis of the drawbacks of our psychology and the emotional impact on decision-making in humans and LLMs. We introduce the novel EAI framework for integrating emotion modeling into LLMs to examine the emotional impact on ethics and LLM-based decision-making in various strategic games, including bargaining and repeated games. Our experimental study with various LLMs demonstrated that emotions can significantly alter the ethical decision-making landscape of LLMs, highlighting the need for robust mechanisms to ensure consistent ethical standards. Our game-theoretic analysis revealed that LLMs are susceptible to emotional biases influenced by model size, alignment strategies, and primary pretraining language. Notably, these biases often diverge from typical human emotional responses, occasionally leading to unexpected drops in cooperation rates, even under positive emotional influence. Such behavior complicates the alignment of multiagent systems, emphasizing the need for benchmarks that can rigorously evaluate the degree of emotional alignment. Our framework provides a foundational basis for developing such benchmarks.",
  "abstract_zh": "摘要：人工智能的一项紧迫任务是评估大型语言模型（LLMs）与人类行为的安全性和一致性。仅在纯自然语言处理基准上的传统验证可能是不够的。由于情感常常影响人类决策，本文在复杂的战略和伦理环境中检验LLM的一致性，深入分析我们心理学的缺陷以及情感对人类和LLM决策的影响。我们引入了新颖的EAI框架，将情感建模整合到LLM中，以考察情感对伦理和基于LLM的决策在各种战略游戏中的影响，包括谈判和重复游戏。我们的实验研究表明，情感可以显著改变LLM的伦理决策格局，突显出需要强有力的机制来确保一致的伦理标准。我们的博弈论分析揭示了LLM易受情感偏见的影响，这种偏见受模型规模、一致性策略和主要预训练语言的影响。值得注意的是，这些偏见常常与典型的人类情感反应不同，有时甚至在积极情感影响下导致合作率意外下降。这种行为使多智能体系统的一致性变得复杂，强调了需要能够严格评估情感一致性程度的基准。我们的框架为开发此类基准提供了基础依据。"
}
{
  "title": "Large Language Model Unlearning",
  "title_zh": "标题: 大型语言模型的遗忘",
  "abstract": "We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in the standard alignment process. (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time.",
  "abstract_zh": "摘要: 我们研究如何在大型语言模型（LLMs）上执行遗忘，即忘记不良（误）行为。我们展示了至少三种将LLMs与人类偏好对齐的场景可以从遗忘中受益：（1）去除有害的响应，（2）根据请求删除受版权保护的内容，以及（3）减少幻觉。作为一种对齐技术，遗忘有三个优点。（1）它只需要负面（例如有害）示例，这比标准对齐过程中所需的正面（例如有帮助且通常由人撰写）示例更容易和便宜收集（例如通过红队测试或用户报告）。（2）它在计算上是高效的。（3）当我们知道哪些训练样本导致了不良行为时，它特别有效。据我们所知，我们的工作是首次探索LLM遗忘的研究之一。我们也是首次在LLM遗忘中制定设置、目标和评估的研究之一。尽管只有负面样本，我们的消融研究表明，遗忘仍然可以在仅用2%计算时间的情况下实现比RLHF更好的对齐性能。"
}
{
  "title": "Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization",
  "title_zh": "个性化引导大型语言模型：通过双向偏好优化实现多功能引导向量",
  "abstract": "Researchers have been studying approaches to steer the behavior of Large Language Models (LLMs) and build personalized LLMs tailored for various applications. While fine-tuning seems to be a direct solution, it requires substantial computational resources and may significantly affect the utility of the original LLM. \nRecent endeavors have introduced more lightweight strategies, focusing on extracting ``steering vectors'' to guide the model's output toward desired behaviors by adjusting activations within specific layers of the LLM's transformer architecture. However, such steering vectors are directly extracted from the activations of human preference data and thus often lead to suboptimal results and occasional failures, especially in alignment-related scenarios.\nIn this work, we propose an innovative approach that could produce more effective steering vectors through bi-directional preference optimization. \nOur method is designed to allow steering vectors to directly influence the generation probability of contrastive human preference data pairs, thereby offering a more precise representation of the target behavior. By carefully adjusting the direction and magnitude of the steering vector, we enabled personalized control over the desired behavior across a spectrum of intensities.\nExtensive experimentation across various open-ended generation tasks, particularly focusing on steering AI personas, has validated the efficacy of our approach. \nMoreover, we comprehensively investigate critical alignment-concerning scenarios, such as managing truthfulness, mitigating hallucination, and addressing jailbreaking attacks alongside their respective defenses. Remarkably, our method can still demonstrate outstanding steering effectiveness across these scenarios. Furthermore, we showcase the transferability of our steering vectors across different models/LoRAs and highlight the synergistic benefits of applying multiple vectors simultaneously. These findings significantly broaden the practicality and versatility of our proposed method.",
  "abstract_zh": "研究人员一直在研究引导大型语言模型（LLMs）行为的方法，并构建适用于各种应用的个性化LLMs。虽然微调似乎是一个直接的解决方案，但它需要大量的计算资源，并可能显著影响原始LLM的实用性。最近的努力引入了更轻量级的策略，专注于提取“引导向量”，通过调整LLM变压器架构中特定层的激活来引导模型的输出朝向期望的行为。然而，这些引导向量直接从人类偏好数据的激活中提取，因此通常会导致次优结果和偶尔的失败，特别是在与对齐相关的场景中。在这项工作中，我们提出了一种创新的方法，可以通过双向偏好优化产生更有效的引导向量。我们的方法旨在使引导向量直接影响对比人类偏好数据对的生成概率，从而提供更精确的目标行为表示。通过仔细调整引导向量的方向和幅度，我们实现了对所需行为在不同强度范围内的个性化控制。在各种开放式生成任务中进行的大量实验，特别是聚焦于引导AI角色，验证了我们方法的有效性。此外，我们全面调查了关键的对齐相关场景，如管理真实性、减轻幻觉以及解决越狱攻击及其各自的防御。值得注意的是，我们的方法仍然可以在这些场景中展示出色的引导效果。此外，我们展示了引导向量在不同模型/LoRAs之间的可迁移性，并强调同时应用多个向量的协同效益。这些发现显著拓宽了我们提出方法的实用性和多功能性。"
}
{
  "title": "Prediction-Powered Ranking of Large Language Models",
  "title_zh": "大语言模型的预测驱动排名",
  "abstract": "Large language models are often ranked according to their level of alignment with human preferences---a model is better than other models if its outputs are more frequently preferred by humans. One of the popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a common practice to gather pairwise comparisons by a strong large language model---a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a (small) set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provides a rank-set---a set of possible ranking positions---for each of the models under comparison. Moreover, it guarantees that, with a probability greater than or equal to a user-specified value, the rank-sets cover the true ranking consistent with the distribution of human pairwise preferences asymptotically. Using pairwise comparisons made by humans in the LMSYS Chatbot Arena platform and pairwise comparisons made by three strong large language models, we empirically demonstrate the effectivity of our framework and show that the rank-sets constructed using only pairwise comparisons by the strong large language models are often inconsistent with (the distribution of) human pairwise preferences.",
  "abstract_zh": "大语言模型通常根据其与人类偏好的对齐程度进行排名——如果一个模型的输出更频繁地被人类偏好，它就比其他模型更好。获取人类偏好的常用方法之一是利用不同模型对相同输入的输出进行成对比较。然而，由于由人类收集成对比较既昂贵又耗时，通常的做法是由一个与人类偏好高度对齐的强大语言模型来收集成对比较。令人惊讶的是，实践者目前无法测量人类和模型偏好之间任何不匹配可能在构建排名中引入的不确定性。在这项工作中，我们开发了一个统计框架来弥合这一差距。给定一组（小规模的）由人类进行的成对比较和一组大规模的由模型进行的成对比较，我们的框架为每个被比较的模型提供一个排名集——一组可能的排名位置。此外，它保证以大于或等于用户指定值的概率，排名集在渐近上覆盖与人类成对偏好分布一致的真实排名。通过使用LMSYS Chatbot Arena平台上人类进行的成对比较和三个强大语言模型进行的成对比较，我们实证展示了我们框架的有效性，并表明仅使用强大语言模型的成对比较构建的排名集通常与人类成对偏好的分布不一致。"
}
{
  "title": "COLD: Causal reasOning in cLosed Daily activities",
  "title_zh": "标题：COLD：封闭日常活动中的因果推理",
  "abstract": "Large Language Models (LLMs) have shown state-of-the-art performance in a variety of tasks, including arithmetic and reasoning; however, to gauge the intellectual capabilities of LLMs, causal reasoning has become a reliable proxy for validating a general understanding of the mechanics and intricacies of the world similar to humans. Previous works in natural language processing (NLP) have either focused on open-ended causal reasoning via causal commonsense reasoning (CCR) or framed a symbolic representation-based question answering for theoretically backed-up analysis via a causal inference engine. The former adds an advantage of real-world grounding but lacks theoretically backed-up analysis/validation, whereas the latter is far from real-world grounding. In this work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed Daily activities) framework, which is built upon human understanding of daily real-world activities to reason about the causal nature of events. We show that the proposed framework facilitates the creation of enormous causal queries (∼ 9 million) and comes close to the mini-turing test, simulating causal reasoning to evaluate the understanding of a daily real-world task. We evaluate multiple LLMs on the created causal queries and find that causal reasoning is challenging even for activities trivial to humans. We further explore (the causal reasoning abilities of LLMs) using the backdoor criterion to determine the causal strength between events.",
  "abstract_zh": "摘要：大型语言模型（LLMs）在包括算术和推理在内的多种任务中表现出最先进的性能；然而，为了评估LLMs的智力能力，因果推理已成为验证类似于人类对世界机制和复杂性的一般理解的可靠代理。以前的自然语言处理（NLP）工作要么专注于通过因果常识推理（CCR）进行开放式因果推理，要么通过因果推理引擎框架基于符号表示的问题回答进行理论支持的分析。前者增加了现实世界基础的优势，但缺乏理论支持的分析/验证，而后者则远离现实世界基础。在这项工作中，我们通过提出COLD（封闭日常活动中的因果推理）框架来弥合这一差距，该框架建立在人类对日常现实世界活动的理解之上，以推理事件的因果性质。我们表明，所提出的框架促进了大量因果查询（约900万）的创建，并接近迷你图灵测试，模拟因果推理以评估对日常现实世界任务的理解。我们在创建的因果查询上评估了多个LLMs，发现即使是对人类来说微不足道的活动，因果推理也是具有挑战性的。我们进一步使用后门准则探索LLMs的因果推理能力，以确定事件之间的因果强度。"
}
{
  "title": "Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy",
  "title_zh": "Title: Richelieu：用于AI外交的自进化LLM基础代理",
  "abstract": "Diplomacy is one of the most sophisticated activities in human society, involving complex interactions among multiple parties that require skills in social reasoning, negotiation, and long-term strategic planning. Previous AI agents have demonstrated their ability to handle multi-step games and large action spaces in multi-agent tasks. However, diplomacy involves a staggering magnitude of decision spaces, especially considering the negotiation stage required. While recent agents based on large language models (LLMs) have shown potential in various applications, they still struggle with extended planning periods in complex multi-agent settings. Leveraging recent technologies for LLM-based agents, we aim to explore AI's potential to create a human-like agent capable of executing comprehensive multi-agent missions by integrating three fundamental capabilities: 1) strategic planning with memory and reflection; 2) goal-oriented negotiation with social reasoning; and 3) augmenting memory through self-play games for self-evolution without human in the loop.",
  "abstract_zh": "Abstract: 外交是人类社会中最复杂的活动之一，涉及多个参与方之间的复杂互动，需要具备社会推理、谈判和长期战略规划的技能。先前的AI代理已经展示了它们在多步骤游戏和多代理任务中的大动作空间处理能力。然而，外交涉及庞大的决策空间，尤其是在谈判阶段。尽管基于大型语言模型（LLM）的最新代理在各种应用中显示出潜力，但它们在复杂多代理环境中的长期规划方面仍然存在困难。利用最近的LLM基础代理技术，我们旨在探索AI创造类人代理的潜力，该代理能够通过整合三种基本能力来执行全面的多代理任务：1）具有记忆和反思的战略规划；2）具有社会推理的目标导向谈判；3）通过自我对弈游戏增强记忆，实现自我进化，无需人类参与。"
}
{
  "title": "Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data",
  "title_zh": "标题: 连点成线：大型语言模型可以从不同的训练数据中推断和表达潜在结构",
  "abstract": "One way to address safety risks from large language models (LLMs) is to censor dangerous knowledge from their training data. While this removes the explicit information, implicit information can remain scattered across various training documents. Could an LLM infer the censored knowledge by piecing together these implicit hints? As a step towards answering this question, we study inductive out-of-context reasoning (OOCR), a type of generalization in which LLMs infer latent information from evidence distributed across training documents and apply it to downstream tasks without in-context learning. \nUsing a suite of five tasks, we demonstrate that frontier LLMs can perform inductive OOCR. In one experiment we finetune an LLM on a corpus consisting only of distances between an unknown city and other known cities. Remarkably, without in-context examples or Chain of Thought, the LLM can verbalize that the unknown city is Paris and use this fact to answer downstream questions. \nFurther experiments show that LLMs trained only on individual coin flip outcomes can verbalize whether the coin is biased, and those trained only on pairs $(x,f(x))$ can articulate a definition of $f$ and compute inverses.\nWhile OOCR succeeds in a range of cases, we also show that it is unreliable, particularly for smaller LLMs learning complex structures.\nOverall, the ability of LLMs to \"connect the dots\" without explicit in-context learning poses a potential obstacle to monitoring and controlling the knowledge acquired by LLMs.",
  "abstract_zh": "摘要: 解决大型语言模型（LLMs）安全风险的一种方法是从其训练数据中删除危险知识。虽然这可以去除显性信息，但隐性信息可能仍然分散在各种训练文档中。LLM能否通过拼凑这些隐性提示来推断被删除的知识？为了解答这个问题，我们研究了归纳上下文外推理（OOCR），这是一种泛化方式，其中LLM从分布在训练文档中的证据中推断潜在信息，并在没有上下文学习的情况下将其应用于下游任务。通过一组五个任务，我们展示了前沿LLM可以执行归纳OOCR。在一项实验中，我们微调了一种LLM，仅在一个包含未知城市与其他已知城市之间距离的语料库上进行训练。令人惊讶的是，在没有上下文示例或思维链的情况下，LLM可以表达出未知城市是巴黎，并利用这一事实回答下游问题。进一步的实验表明，仅在个别硬币翻转结果上训练的LLM可以表达硬币是否偏斜，而仅在对$(x,f(x))$上训练的LLM可以阐述$f$的定义并计算逆函数。虽然OOCR在许多情况下取得了成功，但我们也表明它并不可靠，特别是对于学习复杂结构的小型LLM。总体而言，LLM在没有显性上下文学习的情况下“连点成线”的能力对监控和控制LLM获取的知识构成了潜在的障碍。"
}
{
  "title": "ContextCite: Attributing Model Generation to Context",
  "title_zh": "标题：ContextCite：将模型生成归因于上下文",
  "abstract": "How do language models use information provided as context when generating a response?\nCan we infer whether a particular generated statement is actually grounded in the context, a misinterpretation, or fabricated?\nTo help answer these questions, we introduce the problem of *context attribution*: pinpointing the parts of the context (if any) that *led* a model to generate a particular statement.\nWe then present ContextCite, a simple and scalable method for context attribution that can be applied on top of any existing language model.\nFinally, we showcase the utility of ContextCite through three applications:\n(1) helping verify generated statements\n(2) improving response quality by pruning the context and\n(3) detecting poisoning attacks.\nWe provide code for ContextCite at https://github.com/MadryLab/context-cite.",
  "abstract_zh": "摘要：语言模型在生成响应时如何利用提供的上下文信息？我们能否推断出某个生成的语句实际上是基于上下文、误解还是捏造的？为了帮助回答这些问题，我们引入了*上下文归因*的问题：确定上下文中哪些部分（如果有的话）*导致*模型生成特定语句。然后，我们提出了ContextCite，一种简单且可扩展的上下文归因方法，可以应用于任何现有的语言模型之上。最后，我们通过三个应用展示了ContextCite的实用性：（1）帮助验证生成语句，（2）通过修剪上下文提高响应质量，以及（3）检测投毒攻击。我们在https://github.com/MadryLab/context-cite提供了ContextCite的代码。"
}
{
  "title": "xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token",
  "title_zh": "标题：xRAG：通过单个标记实现检索增强生成的极端上下文压缩",
  "abstract": "This paper introduces xRAG, an innovative context compression method tailored for retrieval-augmented generation. xRAG reinterprets document embeddings in dense retrieval--traditionally used solely for retrieval--as features from the retrieval modality. By employing a modality fusion methodology, xRAG seamlessly integrates these embeddings into the language model representation space, effectively eliminating the need for their textual counterparts and achieving an extreme compression rate. \nIn xRAG, the only trainable component is the modality bridge, while both the retriever and the language model remain frozen. This design choice allows for the reuse of offline-constructed document embeddings and preserves the plug-and-play nature of retrieval augmentation. \nExperimental results demonstrate that xRAG achieves an average improvement of over 10% across six knowledge-intensive tasks, adaptable to various language model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts configuration. xRAG not only significantly outperforms previous context compression methods but also matches the performance of uncompressed models on several datasets, while reducing overall FLOPs by a factor of 3.53. Our work pioneers new directions in retrieval-augmented generation from the perspective of multimodality fusion, and we hope it lays the foundation for future efficient and scalable retrieval-augmented systems.",
  "abstract_zh": "摘要：本文介绍了xRAG，一种为检索增强生成量身定制的创新上下文压缩方法。xRAG将密集检索中的文档嵌入重新解释为检索模态的特征，而不仅仅用于检索。通过采用模态融合方法，xRAG将这些嵌入无缝集成到语言模型表示空间中，有效消除了对其文本对应物的需求，实现了极端的压缩率。在xRAG中，唯一可训练的组件是模态桥，而检索器和语言模型保持冻结状态。这一设计选择允许重用离线构建的文档嵌入，并保留检索增强的即插即用特性。实验结果表明，xRAG在六个知识密集型任务中平均提高了超过10%，适用于从密集的7B模型到8x7B专家混合配置的各种语言模型骨干。xRAG不仅显著优于先前的上下文压缩方法，还在多个数据集上匹配未压缩模型的性能，同时将整体FLOPs减少了3.53倍。我们的工作从多模态融合的角度开创了检索增强生成的新方向，并希望为未来高效且可扩展的检索增强系统奠定基础。"
}
{
  "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
  "title_zh": "块状Transformer：用于快速推理的全局到局部语言建模",
  "abstract": "We introduce the Block Transformer which adopts hierarchical global-to-local modeling to autoregressive transformers to mitigate the inference bottlenecks associated with self-attention. Self-attention requires the key-value (KV) cache of all previous sequences to be retrieved from memory at every decoding step to retrieve context information, leading to two primary bottlenecks during batch inference. First, there is a significant delay in obtaining the first token, as the information of the entire prompt must first be processed to prefill the KV cache. Second, computation of subsequent tokens is bottlenecked by the high memory I/O demand of fetching the entire KV cache, which grows linearly with sequence length, incurring quadratic memory reads overall. We design the Block Transformer to strategically mitigate these costs, by incorporating coarsity and locality into an integrated global-to-local architecture. At the lower layers, we aggregate tokens into fixed size blocks to apply attention across the entire sequence at coarse-grained detail, to capture the global context while minimizing KV cache overhead. At upper layers, we apply attention within each block to decode individual tokens, to model fine-grained details with a lightweight local KV cache. We pretrain vanilla and Block Transformers from scratch and demonstrate that Block Transformers reach 10--20x inference throughput compared to vanilla transformers with equivalent perplexity and zero-shot task performance.",
  "abstract_zh": "我们引入了块状Transformer，它采用分层的全局到局部建模来对自回归Transformer进行改进，以缓解与自注意力相关的推理瓶颈。自注意力需要在每个解码步骤中从内存中检索所有先前序列的键值（KV）缓存以获取上下文信息，这在批量推理中导致了两个主要瓶颈。首先，获取第一个标记存在显著延迟，因为必须首先处理整个提示的信息以预填充KV缓存。其次，后续标记的计算受限于获取整个KV缓存的高内存I/O需求，其随着序列长度线性增长，总体上导致二次内存读取。我们设计了块状Transformer，通过在集成的全局到局部架构中引入粗粒度和局部性来战略性地缓解这些成本。在较低层，我们将标记聚合为固定大小的块，以粗粒度细节在整个序列上应用注意力，以捕获全局上下文，同时最小化KV缓存开销。在较高层，我们在每个块内应用注意力以解码单个标记，以轻量级的局部KV缓存建模细粒度细节。我们从头开始预训练了普通和块状Transformer，并证明块状Transformer在推理吞吐量方面比普通Transformer高出10-20倍，同时具有相同的困惑度和零样本任务性能。"
}
{
  "title": "Sparse High Rank Adapters",
  "title_zh": "稀疏高秩适配器",
  "abstract": "Low Rank Adaptation (LoRA) has gained massive attention in the recent generative AI research. One of the main advantages of LoRA is its ability to be fused with  pretrained models, adding no overhead during inference. However, from a mobile deployment standpoint, we can either avoid inference overhead in the fused mode but lose the ability to switch adapters rapidly, or suffer significant (up to 30% higher) inference latency while enabling rapid switching in the unfused mode. LoRA also exhibits concept-loss when multiple adapters are used concurrently. In this paper, we propose Sparse High Rank Adapters (SHiRA), a new paradigm which incurs no inference overhead, enables rapid switching, and significantly reduces concept-loss. Specifically, SHiRA can be trained by directly tuning only 1-2% of the base model weights while leaving others unchanged. This results in a highly sparse adapter which can be switched directly in the fused mode. We further provide theoretical and empirical insights on how high sparsity in SHiRA can aid multi-adapter fusion by reducing concept loss. Our extensive experiments on LVMs and LLMs demonstrate that finetuning only a small fraction of the parameters in the base model significantly outperforms LoRA while enabling both rapid switching and multi-adapter fusion. Finally, we provide a latency- and memory-efficient SHiRA implementation based on Parameter-Efficient Finetuning (PEFT) Library which trains at nearly the same speed as LoRA while consuming up to 16% lower peak GPU memory, thus making SHiRA easy to adopt for practical use cases. To demonstrate rapid switching benefits during inference, we show that loading SHiRA on a base model can be 5x-16x faster than LoRA fusion on a CPU.",
  "abstract_zh": "低秩适配（LoRA）在最近的生成式人工智能研究中引起了广泛关注。LoRA 的主要优势之一是它能够与预训练模型融合，在推理过程中不增加额外负担。然而，从移动部署的角度来看，我们可以选择在融合模式下避免推理开销，但失去快速切换适配器的能力，或者在非融合模式下启用快速切换时遭受显著的（高达 30%）推理延迟。LoRA 在多个适配器同时使用时也表现出概念丢失。在本文中，我们提出了稀疏高秩适配器（SHiRA），这是一种新范式，不会产生推理开销，能够快速切换，并显著减少概念丢失。具体而言，SHiRA 可以通过直接调整仅 1-2% 的基础模型权重进行训练，而其他部分保持不变。这导致了一个高度稀疏的适配器，可以在融合模式下直接切换。我们进一步提供了理论和实证见解，说明 SHiRA 中的高稀疏性如何通过减少概念丢失来帮助多适配器融合。我们在 LVM 和 LLM 上的大量实验表明，仅微调基础模型中一小部分参数就能显著优于 LoRA，同时实现快速切换和多适配器融合。最后，我们基于参数高效微调（PEFT）库提供了一种延迟和内存高效的 SHiRA 实现，其训练速度几乎与 LoRA 相同，同时峰值 GPU 内存消耗降低多达 16%，使 SHiRA 易于应用于实际用例。为了展示推理期间快速切换的优势，我们表明在 CPU 上加载 SHiRA 到基础模型的速度比 LoRA 融合快 5 倍到 16 倍。"
}
{
  "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models",
  "title_zh": "标题：MATES：基于数据影响模型的高效预训练模型感知数据选择",
  "abstract": "Pretraining data selection has the potential to improve language model pretraining efficiency by utilizing higher-quality data from massive web data corpora. Current data selection methods, which rely on either hand-crafted rules or larger reference models, are conducted statically and do not capture the evolving data preferences during pretraining. In this paper, we introduce *model-aware data selection with data influence models (MATES)*, where a data influence model continuously adapts to the evolving data preferences of the pretraining model and then selects the data most effective for the current pretraining progress. Specifically, we collect oracle data influence by locally probing the pretraining model and fine-tune a small data influence model to approximate it accurately. The data influence model then predicts data influence over the whole pretraining corpus and selects the most influential data for the next pretraining stage. Experiments of pretraining 410M and 1B models on the C4 dataset demonstrate that MATES significantly outperforms random data selection on extensive downstream tasks. It doubles the gains achieved by the state-of-the-art data selection approach that leverages larger reference models and reduces the total FLOPs required to reach certain performances by half. Further analyses validate the effectiveness of the locally probed oracle data influence and the approximation with data influence models. Our code is open-sourced at https://github.com/cxcscmu/MATES.",
  "abstract_zh": "摘要：预训练数据选择通过利用来自海量网络数据语料库的高质量数据，有可能提高语言模型预训练的效率。目前的数据选择方法依赖于手工规则或较大的参考模型，是静态进行的，无法捕捉预训练过程中不断变化的数据偏好。在本文中，我们引入了*基于数据影响模型的模型感知数据选择（MATES）*，其中数据影响模型持续适应预训练模型不断变化的数据偏好，然后选择对当前预训练进度最有效的数据。具体而言，我们通过局部探测预训练模型来收集oracle数据影响，并微调一个小型数据影响模型以准确逼近它。然后，数据影响模型预测整个预训练语料库的数据影响，并选择对下一个预训练阶段最具影响力的数据。在C4数据集上对410M和1B模型进行预训练的实验表明，MATES在广泛的下游任务中显著优于随机数据选择。它使得利用较大参考模型的最先进数据选择方法所获得的收益翻倍，并将达到某些性能所需的总FLOPs减少了一半。进一步的分析验证了局部探测的oracle数据影响和数据影响模型逼近的有效性。我们的代码在https://github.com/cxcscmu/MATES上开源。"
}
{
  "title": "PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models",
  "title_zh": "标题：PiSSA：大语言模型的主奇异值和奇异向量适应",
  "abstract": "To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the low-rank adaptation (LoRA) method approximates the model changes $\\Delta W \\in \\mathbb{R}^{m \\times n}$ through the product of two matrices $A \\in \\mathbb{R}^{m \\times r}$ and $B \\in \\mathbb{R}^{r \\times n}$, where $r \\ll \\min(m, n)$, $A$ is initialized with Gaussian noise, and $B$ with zeros. LoRA **freezes the original model $W$** and **updates the \"Noise \\& Zero\" adapter**, which may lead to slow convergence. To overcome this limitation, we introduce **P**r**i**ncipal **S**ingular values and **S**ingular vectors **A**daptation (PiSSA). PiSSA shares the same architecture as LoRA, but initializes the adaptor matrices $A$ and $B$ with the principal components of the original matrix $W$, and put the remaining components into a residual matrix $W^{res} \\in \\mathbb{R}^{m \\times n}$ which is frozen during fine-tuning.\nCompared to LoRA, PiSSA **updates the principal components** while **freezing the \"residual\" parts**, allowing faster convergence and enhanced performance. Comparative experiments of PiSSA and LoRA across 11 different models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks, reveal that PiSSA consistently outperforms LoRA under identical experimental setups. On the GSM8K benchmark, Gemma-7B fine-tuned with PiSSA achieves an accuracy of 77.7\\%, surpassing LoRA's 74.53\\% by 3.25\\%. Due to the same architecture, PiSSA is also compatible with quantization to further reduce the memory requirement of fine-tuning. Compared to QLoRA, QPiSSA (PiSSA with 4-bit quantization) exhibits smaller quantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K, QPiSSA attains an accuracy of 86.05\\%, exceeding the performances of QLoRA at 81.73\\%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few seconds, presenting a negligible cost for transitioning from LoRA to PiSSA.",
  "abstract_zh": "摘要：为了参数高效地微调（PEFT）大语言模型（LLMs），低秩适应（LoRA）方法通过两个矩阵 $A \\in \\mathbb{R}^{m \\times r}$ 和 $B \\in \\mathbb{R}^{r \\times n}$ 的乘积来逼近模型变化 $\\Delta W \\in \\mathbb{R}^{m \\times n}$，其中 $r \\ll \\min(m, n)$，$A$ 用高斯噪声初始化，$B$ 用零初始化。LoRA **冻结原始模型 $W$** 并 **更新“噪声与零”适配器**，这可能导致收敛速度慢。为克服这一限制，我们引入了 **P**r**i**ncipal **S**ingular values and **S**ingular vectors **A**daptation（PiSSA）。PiSSA 与 LoRA 具有相同的架构，但用原始矩阵 $W$ 的主成分初始化适配器矩阵 $A$ 和 $B$，并将剩余成分放入一个在微调期间冻结的残差矩阵 $W^{res} \\in \\mathbb{R}^{m \\times n}$。与 LoRA 相比，PiSSA **更新主成分**同时**冻结“残差”部分**，允许更快的收敛和增强的性能。在涵盖5个NLG和8个NLU任务的11个不同模型（从184M到70B）的对比实验中，PiSSA在相同实验设置下始终优于LoRA。在GSM8K基准上，使用PiSSA微调的Gemma-7B实现了77.7\\%的准确率，超过了LoRA的74.53\\% 3.25\\%。由于架构相同，PiSSA也兼容量化以进一步减少微调的内存需求。与QLoRA相比，QPiSSA（PiSSA的4位量化）在初始阶段表现出较小的量化误差。在GSM8K上微调LLaMA-3-70B时，QPiSSA达到了86.05\\%的准确率，超过了QLoRA的81.73\\%。利用快速SVD技术，PiSSA可以在几秒钟内初始化，呈现出从LoRA过渡到PiSSA的可忽略成本。"
}
{
  "title": "Inevitable Trade-off between Watermark Strength and Speculative Sampling Efficiency for Language Models",
  "title_zh": "题目：语言模型中水印强度与推测采样效率之间的不可避免权衡",
  "abstract": "Large language models are probabilistic models, and the process of generating content is essentially sampling from the output distribution of the language model. Existing watermarking techniques inject watermarks into the generated content without altering the output quality. On the other hand, existing acceleration techniques, specifically speculative sampling, leverage a draft model to speed up the sampling process while preserving the output distribution. However, there is no known method to simultaneously accelerate the sampling process and inject watermarks into the generated content. In this paper, we investigate this direction and find that the integration of watermarking and acceleration is non-trivial. We prove a no-go theorem, which states that it is impossible to simultaneously maintain the highest watermark strength and the highest sampling efficiency. Furthermore, we propose two methods that maintain either the sampling efficiency or the watermark strength, but not both. Our work provides a rigorous theoretical foundation for understanding the inherent trade-off between watermark strength and sampling efficiency in accelerating the generation of watermarked tokens for large language models. We also conduct numerical experiments to validate our theoretical findings and demonstrate the effectiveness of the proposed methods.",
  "abstract_zh": "摘要：大型语言模型是概率模型，生成内容的过程本质上是从语言模型的输出分布中进行采样。现有的水印技术在不改变输出质量的情况下将水印注入生成的内容中。另一方面，现有的加速技术，特别是推测采样，利用草稿模型加速采样过程，同时保留输出分布。然而，目前尚无已知方法可以同时加速采样过程并将水印注入生成的内容中。在本文中，我们研究了这一方向，发现水印和加速的整合并非易事。我们证明了一个不可行定理，指出不可能同时保持最高的水印强度和最高的采样效率。此外，我们提出了两种方法，分别保持采样效率或水印强度，但不能同时保持。我们的工作为理解在加速生成大型语言模型的水印标记时水印强度与采样效率之间的内在权衡提供了严格的理论基础。我们还进行了数值实验，以验证我们的理论发现并展示所提出方法的有效性。"
}
{
  "title": "Alignment for Honesty",
  "title_zh": "题目：诚实对齐",
  "abstract": "Recent research has made significant strides in aligning large language models (LLMs) with helpfulness and harmlessness. In this paper, we argue for the importance of alignment for \\emph{honesty}, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning an LLM's knowledge boundaries, which demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. We address these challenges by first establishing a precise problem definition and defining ``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM's honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source all relevant resources to facilitate future research at \\url{https://github.com/GAIR-NLP/alignment-for-honesty}.",
  "abstract_zh": "摘要：最近的研究在将大型语言模型（LLMs）与有用性和无害性对齐方面取得了显著进展。在本文中，我们论证了对齐“诚实”的重要性，确保LLMs在缺乏知识时主动拒绝回答问题，同时又不过于保守。然而，诚实对齐的一个关键方面涉及识别LLM的知识边界，这需要在指标开发、基准创建和训练方法方面提供全面的解决方案。我们通过首先建立一个精确的问题定义并定义受《论语》启发的“诚实”来应对这些挑战。这为开发有效衡量LLM诚实性的指标奠定了基础，通过量化其对齐后的进展。此外，我们引入了一个灵活的训练框架，并通过几种强调诚实而不牺牲其他任务性能的高效微调技术来具体化。我们的广泛实验表明，这些对齐模型在诚实性方面表现出显著提高，如我们提出的指标所示。我们开源所有相关资源以促进未来的研究，网址为\\url{https://github.com/GAIR-NLP/alignment-for-honesty}。"
}
{
  "title": "SIRIUS : Contexual Sparisty with Correction for Efficient LLMs",
  "title_zh": "标题：SIRIUS：具有校正功能的上下文稀疏性以提高大型语言模型的效率",
  "abstract": "With the blossom of large language models (LLM), inference efficiency becomes increasingly important. Various approximate methods are proposed to reduce the cost at inference time. Contextual Sparsity (CS) is appealing for its training-free nature and its ability to reach a higher compression ratio seemingly without significant performance degradation. However, after a comprehensive evaluation of contextual sparsity methods on various complex generation tasks, we find that although CS succeeds in prompt-understanding tasks, it significantly degrades the model performance for reasoning, deduction, and knowledge-based tasks. Despite the gap in end-to-end accuracy, we observed that sparse models and original models often share the general problem-solving logic and require only a few token corrections to recover the original model performance. This paper introduces SIRIUS, an efficient correction mechanism, which significantly boosts CS models on reasoning tasks while maintaining its efficiency gain. SIRIUS is evaluated on 6 models with 8 difficult generation tasks in reasoning, deduction, and coding and shows consistent effectiveness and efficiency. Also, we carefully develop a system implementation for SIRIUS and show that SIRIUS delivers theoretical latency reduction with roughly a 20% reduction in latency for 8B model on-chip and a 35% reduction in latency for 70B model offloading. We open-source our implementation of Sirius at https://github.com/Infini-AI-Lab/Sirius.git.",
  "abstract_zh": "摘要：随着大型语言模型（LLM）的蓬勃发展，推理效率变得越来越重要。各种近似方法被提出以减少推理时的成本。上下文稀疏性（CS）因其无需训练的特性和在不显著性能下降的情况下实现更高压缩率的能力而备受关注。然而，在对各种复杂生成任务的上下文稀疏性方法进行全面评估后，我们发现尽管CS在提示理解任务中表现良好，但在推理、演绎和基于知识的任务中显著降低了模型性能。尽管端到端准确性存在差距，我们观察到稀疏模型和原始模型通常共享一般的问题解决逻辑，仅需少量的标记校正即可恢复原始模型性能。本文介绍了SIRIUS，一种高效的校正机制，显著提升了CS模型在推理任务中的表现，同时保持其效率增益。SIRIUS在6个模型上进行了推理、演绎和编码等8个困难生成任务的评估，显示出一致的有效性和效率。此外，我们精心开发了SIRIUS的系统实现，并展示了SIRIUS在理论上减少延迟的能力，对于8B模型在芯片上的延迟减少约20%，对于70B模型卸载的延迟减少约35%。我们在https://github.com/Infini-AI-Lab/Sirius.git上开源了我们的Sirius实现。"
}
{
  "title": "Hypothesis Testing the Circuit Hypothesis in LLMs",
  "title_zh": "标题：对大型语言模型中的电路假设进行假设检验",
  "abstract": "Large language models (LLMs) demonstrate surprising capabilities, but we do not understand how they are implemented. \nOne hypothesis suggests that these capabilities are primarily executed by small subnetworks within the LLM, known as circuits. But how can we evaluate this hypothesis?\nIn this paper, we formalize a set of criteria that a circuit is hypothesized to meet and develop a suite of hypothesis tests to evaluate how well circuits satisfy them. \nThe criteria focus on the extent to which the LLM's behavior is preserved, the degree of localization of this behavior, and whether the circuit is minimal.\nWe apply these tests to six circuits described in the research literature. \nWe find that synthetic circuits -- circuits that are hard-coded in the model -- align with the idealized properties. \nCircuits discovered in Transformer models satisfy the criteria to varying degrees.\nTo facilitate future empirical studies of circuits, we created the \\textit{circuitry} package, a wrapper around the \\textit{TransformerLens} library, which abstracts away lower-level manipulations of hooks and activations. The software is available at \\url{https://github.com/blei-lab/circuitry}.",
  "abstract_zh": "摘要：大型语言模型（LLM）展示了惊人的能力，但我们并不理解这些能力是如何实现的。一种假设认为，这些能力主要由LLM中的小型子网络，即电路来执行。但我们如何评估这一假设呢？在本文中，我们形式化了一套电路假设满足的标准，并开发了一系列假设检验来评估电路满足这些标准的程度。标准集中在LLM行为的保留程度、这种行为的局部化程度以及电路是否是最小的。我们将这些测试应用于研究文献中描述的六个电路。我们发现，合成电路——在模型中硬编码的电路——与理想化属性一致。在Transformer模型中发现的电路在不同程度上满足这些标准。为了促进未来对电路的实证研究，我们创建了\\textit{circuitry}包，这是\\textit{TransformerLens}库的一个包装器，抽象掉了对钩子和激活的低级操作。该软件可在\\url{https://github.com/blei-lab/circuitry}获取。"
}
{
  "title": "Toxicity Detection for Free",
  "title_zh": "标题：免费毒性检测",
  "abstract": "Current LLMs are generally aligned to follow safety requirements and tend to refuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be overcautious and refuse benign examples. In addition, state-of-the-art toxicity detectors have low TPRs at low FPR, incurring high costs in real-world applications where toxic examples are rare. In this paper, we introduce Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves. We found we can distinguish between benign and toxic prompts from the distribution of the first response token's logits. Using this idea, we build a robust detector of toxic prompts using a sparse logistic regression model on the first response token logits. Our scheme outperforms SOTA detectors under multiple metrics.",
  "abstract_zh": "摘要：当前的大型语言模型（LLM）通常被调整以遵循安全要求，并倾向于拒绝有毒的提示。然而，LLM可能无法拒绝有毒提示，或者过于谨慎而拒绝良性示例。此外，最先进的毒性检测器在低假阳性率下具有较低的真阳性率，在有毒示例稀少的实际应用中会产生高成本。在本文中，我们介绍了使用LLM内省进行审核（MULI），该方法利用直接从LLM中提取的信息检测有毒提示。我们发现可以通过第一个响应标记的logits分布来区分良性和有毒提示。基于这一思路，我们使用稀疏逻辑回归模型在第一个响应标记的logits上构建了一个稳健的有毒提示检测器。我们的方案在多项指标下优于最先进的检测器。"
}
{
  "title": "Transfer Q-star : Principled Decoding for LLM Alignment",
  "title_zh": "标题: 转移 Q-star：用于大语言模型对齐的原则性解码",
  "abstract": "Aligning foundation models is essential for their safe and trustworthy deployment. However, traditional fine-tuning methods are computationally intensive and require updating billions of model parameters. A promising alternative, alignment via decoding, adjusts the response distribution directly without model updates to maximize a target reward $r$, thus providing a lightweight and adaptable framework for alignment. However, principled decoding methods rely on oracle access to an optimal Q-function ($Q^*$), which is often unavailable in practice. Hence, prior SoTA methods either approximate this $Q^*$ using $Q^{\\pi_{\\text{sft}}}$ (derived from the reference $\\texttt{SFT}$ model) or rely on short-term rewards, resulting in sub-optimal decoding performance. In this work, we propose $\\texttt{Transfer Q}^*$, which implicitly estimates the optimal value function for a target reward $r$ through a baseline model $\\rho_{\\texttt{BL}}$  aligned with a baseline reward $r_{\\texttt{BL}}$ (which can be different from the target reward $r$). Theoretical analyses of $\\texttt{Transfer Q}^*$ provide a rigorous characterization of its optimality, deriving an upper bound on the sub-optimality gap and identifying a hyperparameter to control the deviation from the pre-trained reference $\\texttt{SFT}$ model based on user needs. Our approach significantly reduces the sub-optimality gap observed in prior SoTA methods and demonstrates superior empirical performance across key metrics such as coherence, diversity, and quality in extensive tests on several synthetic and real datasets.",
  "abstract_zh": "摘要: 对齐基础模型对于其安全和可信的部署至关重要。然而，传统的微调方法计算密集且需要更新数十亿的模型参数。一种有前途的替代方法是通过解码进行对齐，它直接调整响应分布而无需更新模型，以最大化目标奖励 $r$，从而提供轻量且可适应的对齐框架。然而，原则性解码方法依赖于对最优 Q 函数 ($Q^*$) 的预言访问，这在实践中通常不可用。因此，之前的最先进方法要么使用 $Q^{\\pi_{\\text{sft}}}$（从参考 $\\texttt{SFT}$ 模型导出）来近似这个 $Q^*$，要么依赖于短期奖励，导致次优的解码性能。在这项工作中，我们提出了 $\\texttt{Transfer Q}^*$，它通过与基线奖励 $r_{\\texttt{BL}}$ 对齐的基线模型 $\\rho_{\\texttt{BL}}$ 隐式估计目标奖励 $r$ 的最优值函数（这可以与目标奖励 $r$ 不同）。$\\texttt{Transfer Q}^*$ 的理论分析提供了其最优性的严格表征，推导出次优性差距的上限，并识别出一个超参数以根据用户需求控制与预训练参考 $\\texttt{SFT}$ 模型的偏差。我们的方法显著减少了之前最先进方法中观察到的次优性差距，并在多个合成和真实数据集的广泛测试中展示了在连贯性、多样性和质量等关键指标上的卓越经验性能。"
}
{
  "title": "TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment",
  "title_zh": "标题：TOPA：通过仅文本预对齐扩展大型语言模型以理解视频",
  "abstract": "Recent advancements in image understanding have benefited from the extensive use of web image-text pairs. However, video understanding remains a challenge despite the availability of substantial web video-text data. This difficulty primarily arises from the inherent complexity of videos and the inefficient language supervision in recent web-collected video-text datasets. In this paper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend large language models (LLMs) for video understanding, without the need for pre-training on real video data. Specifically, we first employ an advanced LLM to automatically generate Textual Videos comprising continuous textual frames, along with corresponding annotations to simulate real video-text data. Then, these annotated textual videos are used to pre-align a language-only LLM with the video modality. To bridge the gap between textual and real videos, we employ the CLIP model as the feature extractor to align image and text modalities. During text-only pre-alignment, the continuous textual frames, encoded as a sequence of CLIP text features, are analogous to continuous CLIP image features, thus aligning the LLM with real video representation. Extensive experiments, including zero-shot evaluation and finetuning on various video understanding tasks, demonstrate that TOPA is an effective and efficient framework for aligning video content with LLMs. In particular, without training on any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0% on the challenging long-form video understanding benchmark, Egoschema. This performance surpasses previous video-text pre-training approaches and proves competitive with recent GPT-3.5 based video agents.",
  "abstract_zh": "摘要：近年来，图像理解因广泛使用网络图像-文本对而取得了进展。然而，尽管有大量网络视频-文本数据的可用性，视频理解仍然是一个挑战。这一困难主要源于视频的复杂性以及最近网络收集的视频-文本数据集中语言监督的低效。在本文中，我们介绍了一种新颖的方法，称为仅文本预对齐（TOPA），用于扩展大型语言模型（LLMs）以理解视频，而无需在真实视频数据上进行预训练。具体而言，我们首先使用一个先进的LLM自动生成包含连续文本帧的文本视频，并附带相应的注释以模拟真实的视频-文本数据。然后，这些带注释的文本视频用于预对齐仅语言的LLM与视频模态。为了弥合文本与真实视频之间的差距，我们使用CLIP模型作为特征提取器来对齐图像和文本模态。在仅文本预对齐过程中，连续的文本帧被编码为CLIP文本特征序列，与连续的CLIP图像特征类似，从而将LLM与真实视频表示对齐。大量实验，包括零样本评估和在各种视频理解任务上的微调，表明TOPA是一个有效且高效的框架，可以将视频内容与LLMs对齐。特别是，在没有任何视频数据训练的情况下，TOPA-Llama2-13B模型在具有挑战性的长视频理解基准Egoschema上实现了51.0%的Top-1准确率。这一性能超越了先前的视频-文本预训练方法，并在与最近的基于GPT-3.5的视频代理竞争中表现出色。"
}
{
  "title": "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph",
  "title_zh": "动态自适应推理图：大语言模型的动态评估",
  "abstract": "The current paradigm of evaluating Large Language Models (LLMs) through static benchmarks comes with significant limitations, such as vulnerability to data contamination and a lack of adaptability to the evolving capabilities of LLMs. Therefore, evaluation methods that can adapt and generate evaluation data with controlled complexity are urgently needed. In this work, we introduce Dynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to dynamically extend current benchmarks with controlled complexity and diversity. Specifically, we first extract the reasoning graphs of data points in current benchmarks and then perturb the reasoning graphs to generate novel testing data. Such newly generated test samples can have different levels of complexity while maintaining linguistic diversity similar to the original benchmarks. We further use a code-augmented LLM to ensure the label correctness of newly generated data. We apply our DARG framework to diverse reasoning tasks in four domains with 15 state-of-the-art LLMs. Experimental results show that almost all LLMs experience a performance decrease with increased complexity and certain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit more biases when being evaluated via the data generated by DARG with higher complexity levels. These observations provide useful insights into how to dynamically and adaptively evaluate LLMs.",
  "abstract_zh": "通过静态基准评估大语言模型（LLMs）的当前范式存在显著局限性，如易受数据污染影响以及缺乏对LLMs不断演变能力的适应性。因此，迫切需要能够适应并生成具有可控复杂度的评估数据的方法。在这项工作中，我们引入了通过自适应推理图演变进行LLMs动态评估（DARG），以动态扩展当前基准的可控复杂性和多样性。具体而言，我们首先提取当前基准中数据点的推理图，然后扰动推理图以生成新的测试数据。这些新生成的测试样本可以具有不同的复杂性水平，同时保持与原始基准相似的语言多样性。我们进一步使用代码增强的LLM来确保新生成数据的标签正确性。我们将DARG框架应用于四个领域的多样化推理任务，并使用15个最先进的LLM。实验结果表明，几乎所有LLM在复杂性增加时性能都会下降，某些LLM表现出显著下降。此外，我们发现当通过DARG生成的更高复杂性数据进行评估时，LLM表现出更多的偏差。这些观察为如何动态和自适应地评估LLM提供了有用的见解。"
}
{
  "title": "Adaptable Logical Control for Large Language Models",
  "title_zh": "可适应的逻辑控制用于大型语言模型",
  "abstract": "Despite the success of Large Language Models (LLMs) on various tasks following human instructions, controlling model generation to follow strict constraints at inference time poses a persistent challenge. In this paper, we introduce Ctrl-G, a neuro-symbolic framework that enables tractable and adaptable control of LLM generation to follow logical constraints reliably. Ctrl-G combines any production-ready LLM with a Hidden Markov Model (HMM), guiding LLM outputs to adhere to logical constraints represented as deterministic finite automata. We show that Ctrl-G, when a TULU2-7B model is coupled with a 2B-parameter HMM, outperforms GPT4 in text editing: on the task of generating text insertions/continuations following logical constraints, our approach achieves over 30% higher satisfaction rate in human evaluation. When applied to medium-size language models (e.g., GPT2-large), Ctrl-G also beats its counterparts on standard benchmarks by large margins. Additionally, as a proof-of-concept study, we use Ctrl-G to assist LLM reasoning on the GSM benchmark, foreshadowing the application of Ctrl-G, as well as other constrained generation approaches, beyond traditional language generation tasks.",
  "abstract_zh": "尽管大型语言模型（LLMs）在遵循人类指令的各种任务中取得了成功，但在推理时控制模型生成以遵循严格约束仍然是一个持续的挑战。在本文中，我们介绍了Ctrl-G，这是一种神经符号框架，可以实现对LLM生成的可处理和可适应控制，以可靠地遵循逻辑约束。Ctrl-G结合了任何可用于生产的LLM与隐马尔可夫模型（HMM），引导LLM输出遵循表示为确定性有限自动机的逻辑约束。我们展示了当TULU2-7B模型与2B参数的HMM结合时，Ctrl-G在文本编辑任务中优于GPT4：在生成遵循逻辑约束的文本插入/延续的任务中，我们的方法在人类评估中实现了超过30%的更高满意率。当应用于中型语言模型（例如GPT2-large）时，Ctrl-G在标准基准测试中也大幅超越了其对手。此外，作为概念验证研究，我们使用Ctrl-G来协助LLM在GSM基准上的推理，预示着Ctrl-G以及其他受限生成方法在传统语言生成任务之外的应用。"
}
{
  "title": "Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment",
  "title_zh": "标题：Cal-DPO：用于语言模型对齐的校准直接偏好优化",
  "abstract": "We study the problem of aligning large language models (LLMs) with human preference data. Contrastive preference optimization has shown promising results in aligning LLMs with available preference data by optimizing the implicit reward associated with the policy. However, the contrastive objective focuses mainly on the relative values of implicit rewards associated with two responses while ignoring\ntheir actual values, resulting in suboptimal alignment with human preferences. To address this limitation, we propose calibrated direct preference optimization (Cal-DPO), a simple yet effective algorithm. We show that substantial improvement in alignment with the given preferences can be achieved simply by calibrating the implicit reward to ensure that the learned implicit rewards are comparable in\nscale to the ground-truth rewards. We demonstrate the theoretical advantages of Cal-DPO over existing approaches. The results of our experiments on a variety of standard benchmarks show that Cal-DPO remarkably improves off-the-shelf methods.",
  "abstract_zh": "摘要：我们研究了将大型语言模型（LLMs）与人类偏好数据对齐的问题。对比偏好优化通过优化与策略相关的隐式奖励，已在将LLMs与可用偏好数据对齐方面显示出有希望的结果。然而，对比目标主要关注与两个响应相关的隐式奖励的相对值，而忽略了它们的实际值，导致与人类偏好的对齐效果不佳。为了解决这一限制，我们提出了校准直接偏好优化（Cal-DPO），这是一种简单而有效的算法。我们表明，通过校准隐式奖励以确保学习到的隐式奖励在规模上可与真实奖励相比较，可以显著提高与给定偏好的对齐。我们展示了Cal-DPO相较于现有方法的理论优势。我们在各种标准基准上的实验结果表明，Cal-DPO显著改进了现成的方法。"
}
{
  "title": "Allegator: Alleviating Attention Bias for Visual-Informed Text Generation",
  "title_zh": "标题: Allegator: 缓解视觉信息文本生成中的注意力偏差",
  "abstract": "Large Vision-Language Models (LVLMs) have shown remarkable performances in describing visual information with impressive linguistic ability, powering its diverse application. However, they often generate inaccurate descriptions of visual information, referred to as “hallucination”, therefore resolving this issue remains important for employing LVLMs in real-world scenarios. Although various approaches have been proposed in the literature, mitigating the hallucination in long-form generation remains challenging. We observed the Attention Bias phenomenon in LVLMs, where the model allocates a large amount of attention to a few specific tokens, regardless of inputs. With a thorough analysis of the correlation of Attention Bias with hallucination, we attribute the cause of hallucination to the\ninternal attention mechanism of Transformers. To ALLEviate hallucination in text GenerATOR (ALLEGATOR), we propose Attention Moderator that refines the attention efficiently in the training stage and Attention Soft-Clipping to guarantee the stable distribution for generating visual-grounded text. We empirically show that our methods enable generating more accurate descriptions by adaptively referring to visuals with sufficient attention. Allegator achieves significant improvements on hallucination benchmarks including POPE, CHAIR and AMBER, especially showing its effectiveness in long-form generation.",
  "abstract_zh": "摘要: 大型视觉语言模型（LVLMs）在以卓越的语言能力描述视觉信息方面表现出色，推动了其多样化的应用。然而，它们常常生成不准确的视觉信息描述，被称为“幻觉”，因此解决这一问题对于在现实世界场景中应用LVLMs仍然重要。尽管文献中提出了各种方法，但在长篇生成中缓解幻觉仍然具有挑战性。我们观察到LVLMs中的注意力偏差现象，即模型无论输入如何，都将大量注意力分配给少数特定的标记。通过对注意力偏差与幻觉相关性的深入分析，我们将幻觉的原因归因于Transformer的内部注意力机制。为了缓解文本生成中的幻觉（ALLEGATOR），我们提出了注意力调节器，在训练阶段有效地优化注意力，并通过注意力软剪切保证生成视觉基础文本的稳定分布。实验证明，我们的方法通过充分注意视觉信息，自适应地生成更准确的描述。Allegator在幻觉基准测试（包括POPE、CHAIR和AMBER）上取得了显著的改进，特别是在长篇生成中显示了其有效性。"
}
{
  "title": "Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity",
  "title_zh": "内省规划：使机器人不确定性与任务固有模糊性对齐",
  "abstract": "Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or even unsafe in critical scenarios. Additionally, inherent ambiguity in natural language instructions can introduce uncertainty into the LLM's reasoning and planning. We propose introspective planning, a systematic approach that guides LLMs to refine their own uncertainty in alignment with inherent task ambiguity. Our approach constructs a knowledge base containing introspective reasoning examples as post-hoc rationalizations of human-selected safe and compliant plans, which are retrieved during deployment. Evaluations on three tasks, including a new safe mobile manipulation benchmark, indicate that introspection substantially improves both compliance and safety over state-of-the-art LLM-based planning methods. Additionally, we empirically show that introspective planning, in combination with conformal prediction, achieves tighter confidence bounds, maintaining statistical success guarantees while minimizing unnecessary user clarification requests.",
  "abstract_zh": "大型语言模型（LLMs）展现出高级推理能力，使机器人能够理解自然语言指令，并通过适当的基础设施策略性地规划高级动作。然而，LLM的幻觉可能导致机器人自信地执行与用户目标不一致甚至在关键场景中不安全的计划。此外，自然语言指令中的固有模糊性可能会在LLM的推理和规划中引入不确定性。我们提出内省规划，这是一种系统方法，指导LLMs在与任务固有模糊性对齐的情况下细化其自身的不确定性。我们的方法构建了一个知识库，其中包含内省推理示例，作为人类选择的安全和合规计划的事后合理化，并在部署期间进行检索。在包括一个新的安全移动操作基准在内的三个任务上的评估表明，内省显著提高了合规性和安全性，优于最先进的基于LLM的规划方法。此外，我们通过实验证明，内省规划结合保形预测实现了更紧密的置信区间，在保持统计成功保证的同时，最大限度地减少不必要的用户澄清请求。"
}
{
  "title": "LLM Evaluators Recognize and Favor Their Own Generations",
  "title_zh": "标题：大型语言模型评估者识别并偏爱自己的生成",
  "abstract": "Self-evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling, constitutional AI, and self-refinement. But new biases are introduced due to the same LLM acting as both the evaluator and the evaluatee. One such bias is self-preference, where an LLM evaluator scores its own outputs higher than others’ while human annotators consider them of equal quality. But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence? In this paper, we investigate if self-recognition capability contributes to self-preference. We discover that, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from other LLMs and humans. By finetuning LLMs, we discover a linear correlation between self-recognition capability and the strength of self-preference bias; using controlled experiments, we show that the causal explanation resists straightforward confounders. We discuss how self-recognition can interfere with unbiased evaluations and AI safety more generally.",
  "abstract_zh": "摘要：使用大型语言模型（LLMs）进行自我评估不仅在基准测试中证明了其价值，还在奖励建模、宪法AI和自我改进等方法中得到了应用。然而，由于同一个LLM既作为评估者又作为被评估者，引入了新的偏差。其中一种偏差是自我偏好，即LLM评估者对自己的输出评分高于他人的输出，而人类注释者认为它们质量相同。但LLM在给这些文本更高评分时，是否真的识别出自己的输出，还是仅仅是巧合？在本文中，我们调查了自我识别能力是否促成了自我偏好。我们发现，像GPT-4和Llama 2这样的LLM在未经过调整的情况下，能够以非平凡的准确率将自身与其他LLM和人类区分开来。通过微调LLM，我们发现自我识别能力与自我偏好偏差的强度之间存在线性相关性；通过控制实验，我们表明因果解释能够抵御简单的混杂因素。我们讨论了自我识别如何干扰无偏评估以及更广泛的AI安全问题。"
}
{
  "title": "Long-form factuality in large language models",
  "title_zh": "题目：大型语言模型中的长篇事实性",
  "abstract": "Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model’s long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user’s preferred response length (recall).\n\nEmpirically, we demonstrate that LLM agents can outperform crowdsourced human annotators—on a set of∼16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators.  We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.",
  "abstract_zh": "摘要：大型语言模型（LLM）在回应开放性主题的事实性提示时，常常生成包含事实错误的内容。为了在开放域中对模型的长篇事实性进行基准测试，我们首先使用GPT-4生成了LongFact，这是一组涵盖38个主题的数千个问题的提示集。我们随后提出，LLM代理可以通过一种我们称之为搜索增强事实性评估器（SAFE）的方法，作为长篇事实性的自动化评估者。SAFE利用LLM将长篇响应分解为一组单独的事实，并通过一个多步骤推理过程评估每个事实的准确性，该过程包括向Google搜索发送查询并确定一个事实是否被搜索结果支持。此外，我们建议扩展F1分数作为长篇事实性的聚合指标。为此，我们平衡了响应中支持的事实百分比（精确度）与相对于用户偏好响应长度的超参数提供的事实百分比（召回率）。 实证上，我们证明LLM代理可以超越众包的人类标注者——在大约16k个单独事实的集合上，SAFE与众包人类标注者的意见一致率为72%，在100个随机分歧案例中，SAFE胜出76%。同时，SAFE的成本比人类标注者低20倍以上。我们还在LongFact上对四个模型家族（Gemini、GPT、Claude和PaLM-2）的十三个语言模型进行了基准测试，发现较大的语言模型通常能实现更好的长篇事实性。LongFact、SAFE和所有实验代码可在https://github.com/google-deepmind/long-form-factuality获取。"
}
{
  "title": "Boosting Text-to-Video Generative Model with MLLMs Feedback",
  "title_zh": "标题：通过多模态大语言模型反馈提升文本到视频生成模型",
  "abstract": "Recent advancements in text-to-video generative models, such as Sora, have showcased impressive capabilities. These models have attracted significant interest for their potential applications. However, they often rely on extensive datasets of variable quality, which can result in generated videos that lack aesthetic appeal and do not accurately reflect the input text prompts. A promising approach to mitigate these issues is to leverage Reinforcement Learning from Human Feedback (RLHF), which aims to align the outputs of text-to-video generative with human preferences. However, the considerable costs associated with manual annotation have led to a scarcity of comprehensive preference datasets. In response to this challenge, our study begins by investigating the efficacy of Multimodal Large Language Models (MLLMs) generated annotations in capturing video preferences, discovering a high degree of concordance with human judgments. Building upon this finding, we utilize MLLMs to perform fine-grained video preference annotations across two dimensions, resulting in the creation of VideoPrefer, which includes 135,000 preference annotations. Utilizing this dataset, we introduce VideoRM, the first general-purpose reward model tailored for video preference in the text-to-video domain. Our comprehensive experiments confirm the effectiveness of both VideoPrefer and VideoRM, representing a significant step forward in the field.",
  "abstract_zh": "摘要：最近在文本到视频生成模型方面的进展，例如Sora，展示了令人印象深刻的能力。这些模型因其潜在的应用而引起了广泛关注。然而，它们通常依赖于质量不一的大量数据集，这可能导致生成的视频缺乏美感，并且未能准确反映输入的文本提示。一个有前景的方法是利用来自人类反馈的强化学习（RLHF），旨在使文本到视频生成的输出与人类偏好保持一致。然而，与手动标注相关的高昂成本导致了全面偏好数据集的稀缺。为应对这一挑战，我们的研究首先调查了多模态大语言模型（MLLMs）生成的标注在捕捉视频偏好方面的有效性，发现其与人类判断高度一致。在此发现的基础上，我们利用MLLMs在两个维度上进行细粒度的视频偏好标注，创建了包含135,000个偏好标注的VideoPrefer。利用该数据集，我们引入了VideoRM，这是第一个针对文本到视频领域的视频偏好量身定制的通用奖励模型。我们的综合实验验证了VideoPrefer和VideoRM的有效性，代表了该领域的重要进展。"
}
{
  "title": "LaSe-E2V: Towards Language-guided Semantic-aware Event-to-Video Reconstruction",
  "title_zh": "LaSe-E2V: 面向语言引导的语义感知事件到视频重建",
  "abstract": "Event cameras harness advantages such as low latency, high temporal resolution, and high dynamic range (HDR), compared to standard cameras. Due to the distinct imaging paradigm shift, a dominant line of research focuses on event-to-video (E2V) reconstruction to bridge event-based and standard computer vision. However, this task remains challenging due to its inherently ill-posed nature: event cameras only detect the edge and motion information locally. Consequently, the reconstructed videos are often plagued by artifacts and regional blur, primarily caused by the ambiguous semantics of event data. In this paper, we find language naturally conveys abundant semantic information, rendering it stunningly superior in ensuring semantic consistency for E2V reconstruction. Accordingly, we propose a novel framework, called LaSe-E2V,  that can achieve semantic-aware high-quality E2V reconstruction from a language-guided perspective, buttressed by the text-conditional diffusion models. However, due to diffusion models' inherent diversity and randomness, it is hardly possible to directly apply them to achieve spatial and temporal consistency for E2V reconstruction. Thus, we first propose an Event-guided Spatiotemporal Attention (ESA) module to condition the event data to the denoising pipeline effectively. We then introduce an event-aware mask loss to ensure temporal coherence and a noise initialization strategy to enhance spatial consistency. Given the absence of event-text-video paired data, we aggregate existing E2V datasets and generate textual descriptions using the tagging models for training and evaluation. Extensive experiments on three datasets covering diverse challenging scenarios (e.g., fast motion, low light) demonstrate the superiority of our method. Demo videos for the results are attached to the project page.",
  "abstract_zh": "事件相机相比于标准相机具有低延迟、高时间分辨率和高动态范围（HDR）等优势。由于成像范式的显著转变，研究的主流方向集中在事件到视频（E2V）重建上，以弥合基于事件的计算机视觉与标准计算机视觉之间的差距。然而，由于事件相机仅局部检测边缘和运动信息，这项任务仍然具有挑战性，因其本质上是不适定的：重建的视频通常受到伪影和区域模糊的困扰，主要是由于事件数据语义模糊所致。在本文中，我们发现语言自然传达丰富的语义信息，在确保E2V重建的语义一致性方面具有显著的优势。因此，我们提出了一种新颖的框架，称为LaSe-E2V，可以从语言引导的角度实现语义感知的高质量E2V重建，并由文本条件扩散模型支持。然而，由于扩散模型固有的多样性和随机性，直接应用它们以实现E2V重建的空间和时间一致性几乎是不可能的。因此，我们首先提出了一个事件引导的时空注意（ESA）模块，以有效地将事件数据条件化到去噪流程中。然后，我们引入了一种事件感知的掩码损失以确保时间一致性，并采用噪声初始化策略来增强空间一致性。鉴于缺乏事件-文本-视频配对数据，我们整合了现有的E2V数据集，并使用标记模型生成文本描述进行训练和评估。在涵盖各种挑战场景（如快速运动、低光）三个数据集上的广泛实验表明了我们方法的优越性。结果的演示视频附在项目页面上。"
}
{
  "title": "Decoding-Time Language Model Alignment with Multiple Objectives",
  "title_zh": "标题：多目标解码时间语言模型对齐",
  "abstract": "Aligning language models (LMs) to human preferences has emerged as a critical pursuit, enabling these models to better serve diverse user needs. Existing methods primarily focus on optimizing LMs for a single reward function, limiting their adaptability to varied objectives. \nHere, we propose $\\textbf{multi-objective decoding~(MOD)}$, a decoding-time algorithm that outputs the next token from a linear combination of predictions of all base models, for any given weighting over different objectives.\nWe exploit a common form among a family of $f$-divergence regularized alignment approaches (such as PPO, DPO, and their variants) to identify a closed-form solution by Legendre transform, and derive an efficient decoding strategy.\nTheoretically, we show why existing approaches can be sub-optimal even in natural settings and obtain optimality guarantees for our method.\nEmpirical results demonstrate the effectiveness of the algorithm. For example, compared to a parameter-merging baseline, MOD achieves 12.8\\% overall reward improvement when equally optimizing towards $3$ objectives. Moreover, we experiment with MOD on combining three fully-finetuned \nLMs of different model sizes, each aimed at different objectives such as safety, coding, and general user preference. Unlike traditional methods that require careful curation of a mixture of datasets to achieve comprehensive improvement, we can quickly experiment with preference weightings using MOD to find the best combination of models. Our best combination reduces toxicity on Toxigen to nearly 0\\% and achieves 7.9--33.3\\% improvement across three other metrics ($\\textit{i.e.}$, Codex@1, GSM-COT, BBH-COT).",
  "abstract_zh": "摘要：将语言模型（LMs）与人类偏好对齐已成为一项关键任务，使这些模型能够更好地满足多样化的用户需求。现有方法主要集中于优化单一奖励函数的LMs，限制了其对多样化目标的适应性。在此，我们提出了一种解码时间算法——$\\textbf{多目标解码~(MOD)}$，该算法通过对所有基础模型预测的线性组合输出下一个标记，以适应不同目标的权重。我们利用一类$f$-散度正则化对齐方法（如PPO、DPO及其变体）中的通用形式，通过勒让德变换识别出封闭形式的解，并推导出一种高效的解码策略。从理论上，我们展示了现有方法即使在自然环境中也可能次优，并为我们的方法获得了最优性保证。实验证明了该算法的有效性。例如，与参数合并基线相比，MOD在同等优化3个目标时实现了12.8\\%的整体奖励提升。此外，我们在结合三种完全微调的不同模型规模的LMs上进行实验，每个模型针对不同的目标，如安全性、编码和一般用户偏好。与传统方法需要精心策划数据集的混合以实现全面改进不同，我们可以快速使用MOD实验偏好权重以找到最佳模型组合。我们的最佳组合将Toxigen的毒性降低到几乎0\\%，并在其他三个指标（$\\textit{即}$，Codex@1、GSM-COT、BBH-COT）上实现了7.9--33.3\\%的改进。"
}
{
  "title": "InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling",
  "title_zh": "信息RM：通过信息论奖励建模缓解RLHF中的奖励黑客问题",
  "abstract": "Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge. This issue primarily arises from reward misgeneralization, where reward models (RMs)  compute reward using spurious features that are irrelevant to human preferences. In this work, we tackle this problem from an information-theoretic perspective and propose a framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information.\nNotably, we further identify a correlation between overoptimization and outliers in the IB latent space of InfoRM, establishing it as a promising tool for detecting reward overoptimization.\nInspired by this finding, we propose the Cluster Separation Index (CSI), which quantifies deviations in the IB latent space, as an indicator of reward overoptimization to facilitate the development of online mitigation strategies. Extensive experiments on a wide range of settings and RM scales (70M, 440M, 1.4B, and 7B) demonstrate the effectiveness of InfoRM. Further analyses reveal that InfoRM's overoptimization detection mechanism is not only effective but also robust across a broad range of datasets, signifying a notable advancement in the field of RLHF. The code will be released upon acceptance.",
  "abstract_zh": "尽管从人类反馈中进行的强化学习（RLHF）在使语言模型与人类价值观对齐方面取得了成功，但奖励黑客问题，也称为奖励过度优化，仍然是一个关键挑战。这个问题主要源于奖励误泛化，其中奖励模型（RMs）使用与人类偏好无关的虚假特征来计算奖励。在这项工作中，我们从信息论的角度解决这个问题，并通过引入变分信息瓶颈目标来过滤掉无关信息，提出了一种奖励建模框架，即InfoRM。值得注意的是，我们进一步识别了InfoRM的IB潜在空间中的过度优化与异常值之间的相关性，将其确立为检测奖励过度优化的有前途的工具。受这一发现的启发，我们提出了聚类分离指数（CSI），它量化了IB潜在空间中的偏差，作为奖励过度优化的指标，以促进在线缓解策略的开发。在广泛的设置和RM规模（70M、440M、1.4B和7B）上的大量实验表明了InfoRM的有效性。进一步的分析表明，InfoRM的过度优化检测机制不仅有效，而且在广泛的数据集上具有鲁棒性，标志着RLHF领域的显著进步。代码将在接受后发布。"
}
{
  "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward",
  "title_zh": "Title: SimPO：一种无需参考奖励的简单偏好优化方法",
  "abstract": "Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. In this work, we propose SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the _average_ log probability of a sequence as the implicit reward. This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further improving the algorithm's performance. We compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models such as Mistral, Llama 3, and Gemma 2. We evaluate on extensive chat-based evaluation benchmarks, including AlpacaEval 2, MT-Bench, and Arena-Hard. Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model, built on Gemma-2-9B-it, achieves a 72.4\\% length-controlled win rate on AlpacaEval 2, a 59.1\\% win rate on Arena-Hard, and ranks 1st on Chatbot Arena among $<$10B models with real user votes.",
  "abstract_zh": "Abstract: 直接偏好优化（DPO）是一种广泛使用的离线偏好优化算法，通过在人类反馈强化学习（RLHF）中重新参数化奖励函数，以增强简化性和训练稳定性。在这项工作中，我们提出了SimPO，一种更简单但更有效的方法。SimPO的有效性归因于一个关键设计：使用序列的平均对数概率作为隐式奖励。这种奖励形式更好地与模型生成对齐，并且不需要参考模型，使其在计算和内存方面更高效。此外，我们在Bradley-Terry目标中引入了目标奖励边距，以鼓励胜负响应之间的更大边距，进一步提高算法性能。我们将SimPO与DPO及其最新变体在各种最先进的训练设置中进行比较，包括基础和指令微调模型，如Mistral、Llama 3和Gemma 2。我们在广泛的基于聊天的评估基准上进行评估，包括AlpacaEval 2、MT-Bench和Arena-Hard。我们的结果表明，SimPO在不显著增加响应长度的情况下，一贯且显著地优于现有方法。具体而言，SimPO在AlpacaEval 2上比DPO高出最多6.4分，在Arena-Hard上高出最多7.5分。我们基于Gemma-2-9B-it构建的顶级模型在AlpacaEval 2上实现了72.4%的长度控制胜率，在Arena-Hard上实现了59.1%的胜率，并在Chatbot Arena中排名<$10B模型中的第一，获得真实用户投票。"
}
{
  "title": "Geometric-Averaged Preference Optimization for Soft Preference Labels",
  "title_zh": "软偏好标签的几何平均偏好优化",
  "abstract": "Many algorithms for aligning LLMs with human preferences assume that human preferences are binary and deterministic.\nHowever, human preferences can vary across individuals, and therefore should be represented distributionally.\nIn this work, we introduce the distributional soft preference labels and improve Direct Preference Optimization (DPO) with a weighted geometric average of the LLM output likelihood in the loss function.\nThis approach adjusts the scale of learning loss based on the soft labels such that the loss would approach zero when the responses are closer to equally preferred.\nThis simple modification can be easily applied to any DPO-based methods and mitigate over-optimization and objective mismatch, which prior works suffer from.\nOur experiments simulate the soft preference labels with AI feedback from LLMs and demonstrate that geometric averaging consistently improves performance on standard benchmarks for alignment research. \nIn particular, we observe more preferable responses than binary labels and significant improvements where modestly-confident labels are in the majority.",
  "abstract_zh": "许多用于使大型语言模型（LLMs）与人类偏好对齐的算法假设人类偏好是二元且确定性的。然而，人类偏好可能因个体而异，因此应以分布方式表示。在这项工作中，我们引入了分布式软偏好标签，并通过在损失函数中对LLM输出可能性进行加权几何平均来改进直接偏好优化（DPO）。这种方法根据软标签调整学习损失的规模，使得当响应更接近于同等偏好时，损失趋于零。这一简单的修改可以轻松应用于任何基于DPO的方法，并缓解过度优化和目标不匹配的问题，这是之前的工作所面临的困扰。我们的实验通过来自LLM的AI反馈模拟软偏好标签，并证明几何平均在对齐研究的标准基准上持续提高性能。特别是，我们观察到比二元标签更受欢迎的响应，以及在大多数标签信心适中的情况下显著的改进。"
}
{
  "title": "LLM Circuit Analyses Are Consistent Across Training and Scale",
  "title_zh": "标题：LLM电路分析在训练和规模上保持一致",
  "abstract": "Most currently deployed LLMs undergo continuous training or additional finetuning. By contrast, most research into LLMs' internal mechanisms focuses on models at one snapshot in time (the end of pre-training), raising the question of whether their results generalize to real-world settings. Existing studies of mechanisms over time focus on encoder-only or toy models, which differ significantly from most deployed models. In this study, we track how model mechanisms, operationalized as circuits, emerge and evolve across 300 billion tokens of training in decoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters. We find that task abilities and the functional components that support them emerge consistently at similar token counts across scale. Moreover, although such components may be implemented by different attention heads over time, the overarching algorithm that they implement remains. Surprisingly, both these algorithms and the types of components involved therein tend to replicate across model scale. Finally, we find that circuit size correlates with model size and can fluctuate considerably over time even when the same algorithm is implemented. These results suggest that circuit analyses conducted on small models at the end of pre-training can provide insights that still apply after additional training and over model scale.",
  "abstract_zh": "摘要：目前大多数已部署的LLM经历了持续训练或额外的微调。相比之下，大多数关于LLM内部机制的研究集中在某个时间点的模型（预训练结束时），这引发了一个问题，即他们的结果是否能推广到现实世界的环境中。现有的关于机制随时间变化的研究主要集中在仅编码器或玩具模型上，这与大多数已部署的模型显著不同。在这项研究中，我们跟踪了在仅解码器的LLM中，模型机制（操作化为电路）在3000亿个训练标记中的出现和演变，模型参数范围从7000万到28亿。我们发现，任务能力及支持这些能力的功能组件在不同规模的模型中，在相似的标记数量下始终如一地出现。此外，尽管这些组件可能随着时间的推移由不同的注意力头实现，但它们实现的总体算法保持不变。令人惊讶的是，这些算法及其中涉及的组件类型在模型规模上往往会重复。最后，我们发现电路大小与模型大小相关，并且即使在实现相同算法时也可能随时间显著波动。这些结果表明，在预训练结束时对小模型进行的电路分析可以提供在额外训练后和模型规模上仍然适用的见解。"
}
{
  "title": "Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs",
  "title_zh": "偏好链优化：提高大型语言模型中的链式思维推理",
  "abstract": "The recent development of chain-of-thought (CoT) decoding has enabled large language models (LLMs) to generate explicit logical reasoning paths for complex problem-solving. However, research indicates that these paths are not always deliberate and optimal. The tree-of-thought (ToT) method employs tree-searching to extensively explore the reasoning space and find better reasoning paths that CoT decoding might overlook. This deliberation, however, comes at the cost of significantly increased inference complexity. In this work, we demonstrate that fine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to achieve similar or better performance, thereby avoiding the substantial inference burden. This is achieved through \\emph{Chain of Preference Optimization} (CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths with those of ToT using the inherent preference information in the tree-search process. Extensive experimental results show that CPO significantly improves LLM performance in solving a variety of complex problems, including question answering, fact verification, and arithmetic reasoning, demonstrating its effectiveness. \nOur code is available at [https://github.com/sail-sg/CPO](https://github.com/sail-sg/CPO).",
  "abstract_zh": "最近的发展表明，链式思维（CoT）解码使大型语言模型（LLMs）能够为复杂问题解决生成明确的逻辑推理路径。然而，研究表明这些路径并不总是深思熟虑和最优的。思维树（ToT）方法利用树搜索来广泛探索推理空间，并找到CoT解码可能忽略的更好推理路径。然而，这种深思熟虑的代价是显著增加的推理复杂性。在这项工作中，我们展示了通过使用ToT构建的搜索树微调LLMs，可以使CoT达到相似或更好的性能，从而避免大量的推理负担。这是通过\\emph{偏好链优化}（CPO）实现的，其中LLMs被微调以使CoT推理路径的每一步与ToT的步骤对齐，利用树搜索过程中的内在偏好信息。大量实验结果表明，CPO显著提高了LLM在解决各种复杂问题（包括问答、事实验证和算术推理）中的性能，证明了其有效性。我们的代码可在[https://github.com/sail-sg/CPO](https://github.com/sail-sg/CPO)获取。"
}
{
  "title": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer",
  "title_zh": "标题：通过理论证明缓解RLHF中的过度优化：您的SFT损失隐含为对抗性正则化器",
  "abstract": "Aligning generative models with human preference via RLHF typically suffers from overoptimization, where an imperfectly learned reward model can misguide the generative model to output even undesired responses. We investigate this problem in a principled manner by identifying the source of the issue as the distributional shift and uncertainty of human preference in dataset. To mitigate overoptimization, we first propose a theoretical algorithm which optimizes the policy against an adversarially chosen reward model, one that simultaneously minimizes its MLE loss and a reward penalty term. The penalty pessimistically biases the uncertain rewards so as to prevent the policy from choosing actions with spursiouly high proxy rewards, resulting in provable sample efficiency of the algorithm under a partial coverage style condition. Moving from theory to practice, the proposed algorithm further enjoys an equivalent but surprisingly easy to implement form. With a clever usage of the equivalence between reward models and the corresponding optimal policy, the algorithm features a simple objective that combines (i) a preference optimization loss that directly aligns the policy with human preference, and (ii) a supervised learning loss which explicitly imitates the policy with a baseline distribution. In the context of aligning large language models (LLM), this objective fuses the direct preference optimization (DPO) loss with the supervised fune-tuning (SFT) loss to help mitigate the overoptimization towards undesired responses, for which we name the algorithm Regularized Preference Optimization (RPO).\nExperiments of aligning LLMs demonstrate the improved performance of our method when compared with DPO baselines. \nOur work sheds light on the interplay between preference optimization and SFT in tuning LLMs with both theoretical guarantees and empirical evidence.",
  "abstract_zh": "摘要：通过RLHF将生成模型与人类偏好对齐通常会遭遇过度优化，其中不完美的奖励模型可能误导生成模型输出不期望的响应。我们通过识别问题的根源为数据集中人类偏好的分布偏移和不确定性，以一种原则性的方式研究这个问题。为缓解过度优化，我们首先提出一个理论算法，该算法针对对抗性选择的奖励模型优化策略，该模型同时最小化其最大似然估计（MLE）损失和奖励惩罚项。惩罚项悲观地偏向不确定的奖励，以防止策略选择具有虚高代理奖励的动作，从而在部分覆盖条件下证明算法的样本效率。从理论到实践，所提出的算法进一步享有一种等效但出乎意料地易于实现的形式。通过巧妙地利用奖励模型与相应最优策略之间的等价性，算法具有一个简单的目标，结合了（i）直接将策略与人类偏好对齐的偏好优化损失，以及（ii）明确模仿策略与基线分布的监督学习损失。在对齐大语言模型（LLM）的背景下，该目标将直接偏好优化（DPO）损失与监督微调（SFT）损失融合，以帮助缓解对不期望响应的过度优化，我们将该算法命名为正则化偏好优化（RPO）。对齐LLM的实验表明，与DPO基线相比，我们的方法性能有所提高。我们的工作揭示了在调优LLM时偏好优化与SFT之间的相互作用，并提供了理论保证和实证证据。"
}
{
  "title": "Distributional Preference Alignment of LLMs via Optimal Transport",
  "title_zh": "标题: 通过最优传输实现LLM的分布式偏好对齐",
  "abstract": "Current LLM alignment techniques use pairwise human preferences at a sample level, and as such, they do not imply an alignment on the distributional level. We propose in this paper Alignment via  Optimal Transport (AOT), a novel method for distributional preference alignment of LLMs. AOT aligns LLMs on unpaired preference data by making the reward distribution of the positive samples stochastically dominant in the first order on the distribution of negative samples. We introduce a convex relaxation of this first-order stochastic dominance and cast it as an optimal transport problem with a smooth and convex cost. Thanks to the one-dimensional nature of the resulting optimal transport problem and the convexity of the cost, it has a closed-form solution via sorting on empirical measures.  We fine-tune LLMs with this AOT objective, which enables alignment by penalizing the violation of the stochastic dominance of the reward distribution of the positive samples on the reward distribution of the negative samples. We analyze the sample complexity of AOT by considering the dual of the OT problem and show that it converges at the parametric rate. Empirically, we show on a diverse set of alignment datasets and LLMs that AOT leads to state-of-the-art models in the 7B family of models when evaluated with Open LLM Benchmarks and AlpacaEval.  Code for $\\mathsf{AOT}$ is available in the Hugging Face  TRL library  \\url{https://ibm.biz/AOT_TRL}.",
  "abstract_zh": "摘要: 当前的LLM对齐技术使用样本级别的成对人类偏好，因此它们并不意味着在分布层面上的对齐。本文提出了一种新的LLM分布式偏好对齐方法，称为通过最优传输对齐（AOT）。AOT通过使正样本的奖励分布在第一阶上随机占优于负样本的分布来对未配对的偏好数据进行对齐。我们引入了这种一阶随机占优的凸松弛，并将其表述为具有平滑和凸成本的最优传输问题。由于所得最优传输问题的一维性质和成本的凸性，它通过对经验测度进行排序具有闭式解。我们使用这个AOT目标微调LLM，通过惩罚正样本奖励分布对负样本奖励分布的随机占优的违反来实现对齐。我们通过考虑OT问题的对偶分析了AOT的样本复杂性，并证明其以参数速率收敛。实证上，我们在一组多样化的对齐数据集和LLM上展示了AOT在使用Open LLM Benchmarks和AlpacaEval评估时在7B模型系列中实现了最先进的模型。$\\mathsf{AOT}$的代码可在Hugging Face TRL库中获得 \\url{https://ibm.biz/AOT_TRL}。"
}
{
  "title": "Delving into the Reversal Curse: How Far Can Large Language Models Generalize?",
  "title_zh": "深入探讨逆转诅咒：大型语言模型的泛化能力有多远？",
  "abstract": "While large language models (LLMs) showcase unprecedented capabilities, they also exhibit certain inherent limitations when facing seemingly trivial tasks. \nA prime example is the recently debated \"reversal curse\", which surfaces when models, having been trained on the fact \"A is B\", struggle to generalize this knowledge to infer that \"B is A\".\nIn this paper, we examine the manifestation of the reversal curse across various tasks and delve into both the generalization abilities and the problem-solving mechanisms of LLMs. This investigation leads to a series of significant insights:\n(1) LLMs are able to generalize to \"B is A\" when both A and B are presented in the context as in the case of a multiple-choice question.\n(2) This generalization ability is highly correlated to the structure of the fact \"A is B\" in the training documents. For example, this generalization only applies to biographies structured in \"[Name] is [Description]\" but not to \"[Description] is [Name]\".\n(3) We propose and verify the hypothesis that LLMs possess an inherent bias in fact recalling during knowledge application, which explains and underscores the importance of the document structure to successful learning.\n(4) The negative impact of this bias on the downstream performance of LLMs can hardly be mitigated through training alone.\nBased on these intriguing findings, our work not only presents a novel perspective for interpreting LLMs' generalization abilities from their intrinsic working mechanism but also provides new insights for the development of more effective learning methods for LLMs.",
  "abstract_zh": "尽管大型语言模型（LLMs）展示了前所未有的能力，但在面对看似简单的任务时，它们也表现出某些固有的局限性。一个典型的例子是最近讨论的“逆转诅咒”，当模型在被训练为“A 是 B”的事实后，难以将此知识泛化为推断“B 是 A”。在本文中，我们考察了逆转诅咒在各种任务中的表现，并深入探讨了LLMs的泛化能力和问题解决机制。这项研究得出了一系列重要的见解：(1) 当A和B都在上下文中呈现时，LLMs能够泛化为“B 是 A”，如在选择题的情况下。(2) 这种泛化能力与训练文档中“ A 是 B”事实的结构高度相关。例如，这种泛化仅适用于结构为“[姓名] 是 [描述]”的传记，而不适用于“[描述] 是 [姓名]”。(3) 我们提出并验证了一个假设，即LLMs在知识应用过程中存在固有的事实回忆偏差，这解释并强调了文档结构对成功学习的重要性。(4) 这种偏差对LLMs下游性能的负面影响难以仅通过训练来缓解。基于这些有趣的发现，我们的工作不仅为从LLMs的内在工作机制解释其泛化能力提供了新的视角，还为开发更有效的LLMs学习方法提供了新的见解。"
}
{
  "title": "Can Models Learn Skill Composition from Examples?",
  "title_zh": "标题：模型能否从示例中学习技能组合？",
  "abstract": "As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization---the capacity to combine learned skills in novel ways not encountered during training---has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. A recent study introduced the Skill-Mix evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified $k$-tuple of language skills. While small models struggled with composing even with $k=3$, larger models like GPT-4 performed reasonably well with $k=5$ and $6$.\n\nIn this paper, we employ a setup akin to Skill-Mix to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills---including rhetorical, literary, reasoning, theory of mind, and common sense---GPT was used to generate text samples that exhibit random subsets of $k$ skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of $k$, revealed the following findings: (1) Training on combinations of $k=2$ and $3$ skills results in noticeable improvements in the ability to compose texts with $k=4$ and $5$ skills, despite models never having seen such examples during training. (2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills.\n\nThis study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models.",
  "abstract_zh": "摘要：随着大型语言模型（LLMs）的不断进步，它们展示组合泛化能力——即以训练中未遇到的新方式组合已学技能的能力——引起了广泛关注。这种泛化类型，尤其是在超出训练数据的场景中，也在人工智能安全性和一致性研究中备受关注。最近的一项研究引入了Skill-Mix评估，要求模型撰写一段短文，展示使用指定的$k$元组语言技能。虽然小模型在$k=3$时就难以组合，但像GPT-4这样的大模型在$k=5$和$6$时表现良好。在本文中，我们采用类似于Skill-Mix的设置来评估小模型从示例中学习组合泛化的能力。利用包括修辞、文学、推理、心理理论和常识在内的多种语言技能，GPT生成了展示随机$k$技能子集的文本样本。随后对7B和13B参数模型进行微调，结合这些技能文本，随着$k$值的增加，揭示了以下发现：（1）在$k=2$和$3$技能组合上训练，尽管模型在训练中从未见过这样的例子，但在撰写包含$k=4$和$5$技能的文本时表现出显著改进。（2）当技能类别被分为训练和保留组时，尽管模型在微调期间只见过训练技能，但在测试时撰写包含保留技能的文本时显著提高，说明了这种训练方法在以前未见过的技能上也有效。该研究还表明，将富含技能（可能是合成的）文本纳入训练可以显著增强模型的组合能力。"
}
{
  "title": "Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation",
  "title_zh": "标题：大型语言模型作为城市居民：用于个人出行生成的LLM代理框架",
  "abstract": "This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.",
  "abstract_zh": "摘要：本文介绍了一种新颖的方法，将大型语言模型（LLM）集成到代理框架中，以实现灵活且有效的个人出行生成。LLM通过有效处理语义数据并在建模各种任务中提供多功能性，克服了以往模型的局限性。我们的方法解决了三个研究问题：将LLM与真实世界的城市出行数据对齐，开发可靠的活动生成策略，以及探索LLM在城市出行中的应用。关键的技术贡献是一个新颖的LLM代理框架，该框架考虑了个体活动模式和动机，包括一种自一致性方法以使LLM与真实世界活动数据对齐，以及一种检索增强策略用于可解释的活动生成。我们评估了我们的LLM代理框架，并与最先进的个人出行生成方法进行了比较，展示了我们方法的有效性及其在城市出行中的潜在应用。总体而言，这项研究标志着基于真实世界人类活动数据设计LLM代理框架用于活动生成的开创性工作，为城市出行分析提供了一个有前景的工具。"
}
{
  "title": "BackdoorAlign: Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment",
  "title_zh": "BackdoorAlign: 使用后门增强安全对齐来缓解基于微调的越狱攻击",
  "abstract": "Despite the general capabilities of Large Language Models (LLMs) like GPT-4, these models still request fine-tuning or adaptation with customized data when meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack) under the setting of Language-Model-as-a-Service (LMaaS), where the model's safety has been significantly compromised by fine-tuning on users' uploaded examples that contain just a few harmful examples. Though potential defenses have been proposed that the service providers of LMaaS can integrate safety examples into the fine-tuning dataset to reduce safety issues, such approaches require incorporating a substantial amount of data, making it inefficient. To effectively defend against the FJAttack with limited safety examples under LMaaS, we propose the Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In particular, service providers will construct prefixed safety examples with a secret prompt, acting as a \"backdoor trigger\". By integrating prefixed safety examples into the fine-tuning dataset, the subsequent fine-tuning process effectively acts as the \"backdoor attack\", establishing a strong correlation between the secret prompt and safety generations. Consequently, safe responses are ensured once service providers prepend this secret prompt ahead of any user input during inference. Our comprehensive experiments demonstrate that through the Backdoor Enhanced Safety Alignment with adding as few as 11 prefixed safety examples, the maliciously fine-tuned LLMs will achieve similar safety performance as the original aligned models without harming the benign performance. Furthermore, we also present the effectiveness of our method in a more practical setting where the fine-tuning data consists of both FJAttack examples and the fine-tuning task data.",
  "abstract_zh": "尽管大型语言模型（LLMs）如GPT-4具有广泛的能力，但在满足特定业务需求和定制用例的复杂性时，这些模型仍需使用定制数据进行微调或适应。然而，这一过程不可避免地引入了新的安全威胁，特别是在语言模型即服务（LMaaS）环境下的基于微调的越狱攻击（FJAttack），其中模型的安全性因微调用户上传的包含少量有害示例的例子而受到严重损害。尽管已经提出了潜在的防御措施，即LMaaS的服务提供商可以将安全示例整合到微调数据集中以减少安全问题，但这种方法需要整合大量数据，效率低下。为了在LMaaS下有效防御FJAttack并使用有限的安全示例，我们提出了受后门攻击概念启发的后门增强安全对齐方法。具体而言，服务提供商将构建带有秘密提示的前缀安全示例，作为“后门触发器”。通过将前缀安全示例整合到微调数据集中，随后的微调过程有效地充当“后门攻击”，在秘密提示和安全生成之间建立强关联。因此，一旦服务提供商在推理过程中在任何用户输入之前添加此秘密提示，安全响应就得以确保。我们的综合实验表明，通过后门增强安全对齐，添加仅11个前缀安全示例，恶意微调的LLMs将实现与原始对齐模型相似的安全性能，而不损害良性性能。此外，我们还展示了在更实际的环境中我们方法的有效性，其中微调数据包括FJAttack示例和微调任务数据。"
}
{
  "title": "Truth is Universal: Robust Detection of Lies in LLMs",
  "title_zh": "标题：真理是普遍的：大型语言模型中谎言的鲁棒检测",
  "abstract": "Large Language Models (LLMs) have revolutionised natural language processing, exhibiting impressive human-like capabilities. In particular, LLMs are  capable of \"lying\", knowingly outputting false statements. Hence, it is of interest and importance to develop methods to detect when LLMs lie. Indeed, several authors trained classifiers to detect LLM lies based on their internal model activations. However, other researchers showed that these classifiers may fail to generalise, for example to negated statements. \nIn this work, we aim to develop a robust method to detect when an LLM is lying. To this end, we make the following key contributions: (i) We demonstrate the existence of a two-dimensional subspace, along which the activation vectors of true and false statements can be separated. Notably, this finding is universal and holds for various LLMs, including Gemma-7B, LLaMA2-13B, Mistral-7B and LLaMA3-8B. Our analysis explains the generalisation failures observed in previous studies and sets the stage for more robust lie detection;\n(ii) Building upon (i), we construct an accurate LLM lie detector. Empirically, our proposed classifier achieves state-of-the-art performance, attaining 94\\% accuracy in both distinguishing true from false factual statements and detecting lies generated in real-world scenarios.",
  "abstract_zh": "摘要：大型语言模型（LLMs）革新了自然语言处理，展现了令人印象深刻的类人能力。特别是，LLMs能够“撒谎”，即有意输出虚假陈述。因此，开发检测LLMs何时撒谎的方法具有兴趣和重要性。确实，有几位作者训练了分类器来基于其内部模型激活检测LLMs的谎言。然而，其他研究人员表明，这些分类器可能无法泛化，例如对否定陈述。在这项工作中，我们旨在开发一种鲁棒的方法来检测LLM何时撒谎。为此，我们做出了以下关键贡献：（i）我们展示了一个二维子空间的存在，在该子空间中，真实和虚假陈述的激活向量可以被分开。值得注意的是，这一发现是普遍的，并适用于各种LLMs，包括Gemma-7B、LLaMA2-13B、Mistral-7B和LLaMA3-8B。我们的分析解释了先前研究中观察到的泛化失败，并为更鲁棒的谎言检测奠定了基础；（ii）基于（i），我们构建了一个准确的LLM谎言检测器。实证结果表明，我们提出的分类器达到了最先进的性能，在区分真实与虚假事实陈述以及检测现实场景中生成的谎言方面达到了94%的准确率。"
}
{
  "title": "Risk-Averse Fine-tuning of Large Language Models",
  "title_zh": "标题：大语言模型的风险规避微调",
  "abstract": "We consider the challenge of mitigating the generation of negative or toxic content by the Large Language Models (LLMs) in response to certain prompts.  We propose integrating risk-averse principles into LLM fine-tuning to minimize the occurrence of harmful outputs, particularly rare but significant events.  By optimizing the risk measure of Conditional Value at Risk (CVaR), our methodology trains LLMs to exhibit superior performance in avoiding toxic outputs while maintaining effectiveness in generative tasks. Empirical evaluations on sentiment modification and toxicity mitigation tasks demonstrate the efficacy of risk-averse reinforcement learning with human feedback (RLHF) in promoting a safer and more constructive online discourse environment.",
  "abstract_zh": "摘要：我们考虑到大语言模型（LLMs）在响应某些提示时生成负面或有害内容的挑战。我们提出将风险规避原则整合到LLM的微调中，以减少有害输出的发生，特别是那些罕见但重大的事件。通过优化条件风险价值（CVaR）的风险度量，我们的方法训练LLM在避免有害输出方面表现出色，同时保持生成任务的有效性。在情感修改和毒性缓解任务上的实证评估表明，结合人类反馈的风险规避强化学习（RLHF）在促进更安全和更具建设性的在线交流环境方面的有效性。"
}
{
  "title": "A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks",
  "title_zh": "跨模态和任务的视觉语言模型统一去偏方法",
  "abstract": "Recent advancements in Vision-Language Models (VLMs) have enabled complex multimodal tasks by processing text and image data simultaneously, significantly enhancing the field of artificial intelligence. However, these models often exhibit biases that can skew outputs towards societal stereotypes, thus necessitating debiasing strategies. Existing debiasing methods focus narrowly on specific modalities or tasks, and require extensive retraining. To address these limitations, this paper introduces Selective Feature Imputation for Debiasing (SFID), a novel methodology that integrates feature pruning and low confidence imputation (LCI) to effectively reduce biases in VLMs. SFID is versatile, maintaining the semantic integrity of outputs and costly effective by eliminating the need for retraining. Our experimental results demonstrate SFID's effectiveness across various VLMs tasks including zero-shot classification, text-to-image retrieval, image captioning, and text-to-image generation, by significantly reducing gender biases without compromising performance. This approach not only enhances the fairness of VLMs applications but also preserves their efficiency and utility across diverse scenarios.",
  "abstract_zh": "近年来，视觉语言模型（VLMs）的进步使得通过同时处理文本和图像数据来实现复杂的多模态任务成为可能，显著提升了人工智能领域。然而，这些模型通常表现出偏见，可能导致输出偏向社会刻板印象，因此需要去偏策略。现有的去偏方法通常专注于特定的模态或任务，并需要大量的重新训练。为了解决这些限制，本文介绍了一种新的方法——选择性特征插补去偏（SFID），该方法结合了特征剪枝和低置信度插补（LCI），有效减少了VLMs中的偏见。SFID具有多功能性，保持输出的语义完整性，并通过消除重新训练的需求来降低成本。我们的实验结果表明，SFID在包括零样本分类、文本到图像检索、图像描述和文本到图像生成等各种VLMs任务中有效减少了性别偏见，同时不影响性能。该方法不仅增强了VLMs应用的公平性，还保持了其在多种场景中的效率和实用性。"
}
{
  "title": "Unleashing Region Understanding in Intermediate Layers for MLLM-based Referring Expression Generation",
  "title_zh": "释放中间层的区域理解以增强基于MLLM的指称表达生成",
  "abstract": "The Multi-modal Large Language Model (MLLM) based Referring Expression Generation (REG) task has gained increasing popularity, which aims to generate an unambiguous text description that applies to exactly one object or region in the image by leveraging foundation models. We empirically found that there exists a potential trade-off between the detailedness and the correctness of the descriptions for the referring objects. On the one hand, generating sentences with more details is usually required in order to provide more precise object descriptions. On the other hand, complicated sentences could easily increase the probability of hallucinations. To address this issue, we propose a training-free framework, named ``unleash-then-eliminate'', which first elicits the latent information in the intermediate layers, and then adopts a cycle-consistency-based decoding method to alleviate the production of hallucinations. Furthermore, to reduce the computational load of cycle-consistency-based decoding, we devise a Probing-based Importance Estimation method to statistically estimate the importance weights of intermediate layers within a subset. These importance weights are then incorporated into the decoding process over the entire dataset, intervening in the next token prediction from intermediate layers.\nExtensive experiments conducted on the RefCOCOg and PHD benchmarks show that our proposed framework could outperform existing methods on both semantic and hallucination-related metrics. Code will be made available in https://github.com/Glupayy/unleash-eliminate.",
  "abstract_zh": "基于多模态大语言模型（MLLM）的指称表达生成（REG）任务越来越受欢迎，其目标是通过利用基础模型生成一个明确适用于图像中唯一对象或区域的文本描述。我们通过实验证明，指称对象的描述在详细性和正确性之间存在潜在的权衡。一方面，通常需要生成更详细的句子以提供更精确的对象描述。另一方面，复杂的句子可能会增加幻觉的概率。为了解决这个问题，我们提出了一个无需训练的框架，称为“释放-然后-消除”，首先在中间层中引出潜在信息，然后采用基于循环一致性的解码方法来缓解幻觉的产生。此外，为了减少基于循环一致性解码的计算负担，我们设计了一种基于探测的重要性估计方法，以统计估计子集中间层的重要性权重。这些重要性权重随后被纳入整个数据集的解码过程中，从中间层干预下一个标记的预测。在RefCOCOg和PHD基准上进行的大量实验表明，我们提出的框架在语义和幻觉相关指标上均能优于现有方法。代码将在https://github.com/Glupayy/unleash-eliminate提供。"
}
